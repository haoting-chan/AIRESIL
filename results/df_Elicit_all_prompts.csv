title,authors,year,abstract,doi,type_of_reference
Specializing Large Language Models to Simulate Survey Response Distrib utions for Global Populations,"Cao, Yong; Liu, Haijiang; Arora, Arnav; Augenstein, Isabelle; Röttger, Paul; Hershcovich, Daniel",2025,"Large-scale surveys are essential tools for informing social science r esearch and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distribu tions. As a testbed, we use country-level results from two global cult ural surveys. We devise a fine-tuning method based on first-token prob abilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method su bstantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen ques tions, our results demonstrate the benefits of specialization for simu lation, which may accelerate progress towards sufficiently accurate si mulation in the future.",10.48550/ARXIV.2502.07068,JOUR
Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information,"Sun, Seungjong; Lee, Eungu; Nan, Dongyan; Zhao, Xiangying; Lee, Wonbyung; Jansen, Bernard J.; Kim, Jang Hyun",2024,"Large language models exhibit societal biases associated with demograp hic information, including race, gender, and others. Endowing such lan guage models with personalities based on demographic data can enable g enerating opinions that align with those of humans. Building on this i dea, we propose""random silicon sampling,""a method to emulate the opini ons of the human population sub-group. Our study analyzed 1) a languag e model that generates the survey responses that correspond with a hum an group based solely on its demographic distribution and 2) the appli cability of our methodology across various demographic subgroups and t hematic questions. Through random silicon sampling and using only grou p-level demographic information, we discovered that language models ca n generate response distributions that are remarkably similar to the a ctual U.S. public opinion polls. Moreover, we found that the replicabi lity of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mir roring a group's opinion using only demographic distribution and eluci date the effect of social biases in language models on such simulation s.",10.48550/ARXIV.2402.18144,JOUR
Donald Trumps in the Virtual Polls: Simulating and Predicting Public O pinions in Surveys Using Large Language Models,"Jiang, Shapeng; Wei, Lijia; Zhang, Chen",2024,"In recent years, large language models (LLMs) have attracted attention due to their ability to generate human-like text. As surveys and opin ion polls remain key tools for gauging public attitudes, there is incr easing interest in assessing whether LLMs can accurately replicate hum an responses. This study examines the potential of LLMs, specifically ChatGPT-4o, to replicate human responses in large-scale surveys and to predict election outcomes based on demographic data. Employing data f rom the World Values Survey (WVS) and the American National Election S tudies (ANES), we assess the LLM's performance in two key tasks: simul ating human responses and forecasting U.S. election results. In simula tions, the LLM was tasked with generating synthetic responses for vari ous socio-cultural and trust-related questions, demonstrating notable alignment with human response patterns across U.S.-China samples, thou gh with some limitations on value-sensitive topics. In prediction task s, the LLM was used to simulate voting behavior in past U.S. elections and predict the 2024 election outcome. Our findings show that the LLM replicates cultural differences effectively, exhibits in-sample predi ctive validity, and provides plausible out-of-sample forecasts, sugges ting potential as a cost-effective supplement for survey-based researc h.",,JOUR
Uncertainty Quantification for LLM-Based Survey Simulations,"Huang, Chengpiao; Wu, Yuhang; Wang, Kaizheng",2025,"We investigate the use of large language models (LLMs) to simulate hum an responses to survey questions, and perform uncertainty quantificati on to gain reliable insights. Our approach converts imperfect LLM-simu lated responses into confidence sets for population parameters of huma n responses, addressing the distribution shift between the simulated a nd real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confiden ce sets with poor coverage, while too few yield excessively loose esti mates. To resolve this, our method adaptively selects the simulation s ample size, ensuring valid average-case coverage guarantees. It is bro adly applicable to any LLM, irrespective of its fidelity, and any proc edure for constructing confidence sets. Additionally, the selected sam ple size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets an d LLMs.",10.48550/ARXIV.2502.17773,JOUR
Large Language Models as Subpopulation Representative Models: A Review ,"Simmons, Gabriel; Hare, Christopher",2023,"Of the many commercial and scientific opportunities provided by large language models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and Anthropic's Claude), one of the more intriguing applications has been the simulation of human behavior and opinion. LLMs have been used to g enerate human simulcra to serve as experimental participants, survey r espondents, or other independent agents, with outcomes that often clos ely parallel the observed behavior of their genuine human counterparts . Here, we specifically consider the feasibility of using LLMs to esti mate subpopulation representative models (SRMs). SRMs could provide an alternate or complementary way to measure public opinion among demogr aphic, geographic, or political segments of the population. However, t he introduction of new technology to the socio-technical infrastructur e does not come without risk. We provide an overview of behavior elici tation techniques for LLMs, and a survey of existing SRM implementatio ns. We offer frameworks for the analysis, development, and practical i mplementation of LLMs as SRMs, consider potential risks, and suggest d irections for future work.",10.48550/ARXIV.2310.17888,JOUR
Language Model Fine-Tuning on Scaled Survey Data for Predicting Distri butions of Public Opinions,"Suh, Joseph; Jahanparast, Erfan; Moon, Suhong; Kang, Minwoo; Chang, Serina",2025,"Large language models (LLMs) present novel opportunities in public opi nion research by predicting survey responses in advance during the ear ly stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering a pproaches have struggled to faithfully predict the distribution of sur vey responses from human subjects. In this work, we propose directly f ine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 7 0K subpopulation-response pairs from well-established public opinion s urveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulati ons, reducing the LLM-human gap by up to 46% compared to baselines, an d achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to i mprove opinion prediction for diverse, real-world subpopulations and t herefore enable more efficient survey designs. Our code is available a t https://github.com/JosephJeesungSuh/subpop.",10.48550/ARXIV.2502.16761,JOUR
Questioning the Survey Responses of Large Language Models,"Dominguez-Olmedo, Ricardo; Hardt, Moritz; Mendler-Dünner, Celestine",2023,"As large language models increase in capability, researchers have star ted to conduct surveys of all kinds on these models in order to invest igate the population represented by their responses. In this work, we critically examine language models' survey responses on the basis of t he well-established American Community Survey by the U.S. Census Burea u and investigate whether they elicit a faithful representations of an y human population. Using a de-facto standard multiple-choice promptin g technique and evaluating 39 different language models using systemat ic experiments, we establish two dominant patterns: First, models' res ponses are governed by ordering and labeling biases, leading to variat ions across models that do not persist after adjusting for systematic biases. Second, models' responses do not contain the entropy variation s and statistical signals typically found in human populations. As a r esult, a binary classifier can almost perfectly differentiate model-ge nerated data from the responses of the U.S. census. At the same time, models' relative alignment with different demographic subgroups can be predicted from the subgroups' entropy, irrespective of the model's tr aining data or training strategy. Taken together, our findings suggest caution in treating models' survey responses as equivalent to those o f human populations.",10.48550/ARXIV.2306.07951,JOUR
Employing large language models in survey research,"Jansen, Bernard J.; Jung, Soon-gyo; Salminen, Joni",2023,"This article discusses the promising potential of employing large lang uage models (LLMs) for survey research, including generating responses to survey items. LLMs can address some of the challenges associated w ith survey research regarding question-wording and response bias. They can address issues relating to a lack of clarity and understanding bu t cannot yet correct for sampling or nonresponse bias challenges. Whil e LLMs can assist with some of the challenges with survey research, at present, LLMs need to be used in conjunction with other methods and a pproaches. With thoughtful and nuanced approaches to development, LLMs can be used responsibly and beneficially while minimizing the associa ted risks.",10.1016/j.nlp.2023.100020,JOUR
AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction,"Kim, Junsol; Lee, Byungkyu",2023,"How can we use large language models (LLMs) to augment surveys? This p aper investigates three distinct applications of LLMs ﬁne-tuned by nat ionally representative surveys for opinion prediction – missing data i mputation, retrodiction, and zero-shot prediction. We present a new me thodological framework that incorporates neural embeddings of survey q uestions, individual beliefs, and temporal contexts to personalize LLM s in opinion prediction. Among 3,110 binarized opinions from 68,846 Am ericans in the General Social Survey from 1972 to 2021, our best model s based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and ρ = 0.99 for public opinion predictio n) and retrodiction (AUC = 0.86, ρ = 0.98). These remarkable predictio n capabilities allow us to ﬁll in missing trends with high conﬁdence a nd pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zero-shot prediction task (AUC = 0.73, ρ = 0.67), highlighting chal lenges presented by LLMs without human responses. Further, we ﬁnd that the best models’ accuracy is lower for individuals with low socioecon omic status, racial minorities, and non-partisan afﬁliations but highe r for ideologically sorted opinions in contemporary periods. We discus s practical constraints, socio-demographic representation, and ethical concerns regarding individual autonomy and privacy when using LLMs fo r opinion prediction. This paper showcases a new approach for leveragi ng LLMs to enhance nationally representative surveys by predicting mis sing responses and trends.",10.48550/ARXIV.2305.09620,JOUR
"Out of One, Many: Using Language Models to Simulate Human Samples","Argyle, Lisa P.; Busby, Ethan C.; Fulda, Nancy; Gubler, Joshua R.; Rytting, Christopher; Wingate, David",2023,"Abstract We propose and explore the possibility that language models c an be studied as effective proxies for specific human subpopulations i n social science research. Practical and research applications of arti ficial intelligence tools have sometimes been limited by problematic b iases (such as racism or sexism), which are often treated as uniform p roperties of the models. We show that the “algorithmic bias” within on e such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will caus e it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and exp lore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States . We then compare the silicon and human samples to demonstrate that th e information contained in GPT-3 goes far beyond surface similarity. I t is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human a ttitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understa nding of humans and society across a variety of disciplines.",10.1017/pan.2023.2,JOUR
Using LLMs to Model the Beliefs and Preferences of Targeted Population s,"Namikoshi, Keiichi; Filipowicz, Alex; Shamma, David A.; Iliev, Rumen; Hogan, Candice L.; Arechiga, Nikos",2024,"We consider the problem of aligning a large language model (LLM) to mo del the preferences of a human population. Modeling the beliefs, prefe rences, and behaviors of a specific population can be useful for a var iety of different applications, such as conducting simulated focus gro ups for new products, conducting virtual surveys, and testing behavior al interventions, especially for interventions that are expensive, imp ractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmar k and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of rea l human respondents on a survey of preferences for battery electric ve hicles (BEVs). We evaluate our models against their ability to match p opulation-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluat e a novel loss term to improve model performance on responses that req uire a numeric response.",10.48550/ARXIV.2403.20252,JOUR
Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives,"Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yuan; Ding, Jingtao; Zhou, Zhilun; Xu, Fengli; Li, Yong",2023,"Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors a nd interactions among diverse agents. Integrating large language model s into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landsca pe of utilizing large language models in agent-based modeling and simu lation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first intro duce the background of agent-based modeling and simulation and large l anguage model-empowered agents. We then discuss the motivation for app lying large language models to agent-based simulation and systematical ly analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comp rehensive overview of the recent works of large language model-empower ed agent-based modeling and simulation in multiple scenarios, which ca n be divided into four domains: cyber, physical, social, and hybrid, c overing simulation of both real-world and virtual environments. Finall y, since this area is new and quickly evolving, we discuss the open pr oblems and promising future directions.",10.48550/ARXIV.2312.11970,JOUR
Synthetic Replacements for Human Survey Data? The Perils of Large Lang uage Models,"Bisbee, James; Clinton, Joshua D.; Dorff, Cassy; Kenkel, Brenton; Larson, Jennifer M.",2024," Large language models (LLMs) offer new research possibilities for soc ial scientists, but their potential as “synthetic data” is still large ly unknown. In this paper, we investigate how accurately the popular L LM ChatGPT can recover public opinion, prompting the LLM to adopt diff erent “personas” and then provide feeling thermometer scores for 11 so ciopolitical groups. The average scores generated by ChatGPT correspon d closely to the averages in our baseline survey, the 2016–2020 Americ an National Election Study (ANES). Nevertheless, sampling by ChatGPT i s not reliable for statistical inference: there is less variation in r esponses than in the real surveys, and regression coefficients often d iffer significantly from equivalent estimates obtained using ANES data . We also document how the distribution of synthetic responses varies with minor changes in prompt wording, and we show how the same prompt yields significantly different results over a 3-month period. Altogeth er, our findings raise serious concerns about the quality, reliability , and reproducibility of synthetic survey data generated by LLMs.",10.1017/pan.2024.5,JOUR
Using GPT for Market Research,"Brand, James; Israeli, Ayelet; Ngwe, Donald",2023,"Large language models (LLMs) have quickly become popular as labor-augm enting tools for programming, writing, and many other processes that b enefit from quick text generation. In this paper we explore the uses a nd benefits of LLMs for researchers and practitioners who aim to under stand consumer preferences. We focus on the distributional nature of L LM responses, and query the Generative Pre-trained Transformer 3.5 (GP T-3.5) model to generate hundreds of survey responses to each prompt. We offer two sets of results to illustrate our approach and assess it. First, we show that GPT-3.5, a widely-used LLM, responds to sets of s urvey questions in ways that are consistent with economic theory and w ell-documented patterns of consumer behavior, including downward-slopi ng demand curves and state dependence. Second, we show that estimates of willingness-to-pay for products and features generated by GPT-3.5 a re of realistic magnitudes and match estimates from a recent study tha t elicited preferences from human consumers. We also offer preliminary guidelines for how best to query information from GPT-3.5 for marketi ng purposes and discuss potential limitations. https://papers.ssrn.com /sol3/papers.cfm?abstract_id=4395751",10.2139/ssrn.4395751,JOUR
Aligning Large Language Models with Human: A Survey,"Wang, Yufei; Zhong, Wanjun; Li, Liangyou; Mi, Fei; Zeng, Xingshan; Huang, Wenyong; Shang, Lifeng; Jiang, Xin; Liu, Qun",2023,"Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Pr ocessing (NLP) tasks. Despite their notable performance, these models are prone to certain limitations such as mis-understanding human instr uctions, generating potentially biased content, or factually incorrect (hallucinated) information. Hence, aligning LLMs with human expectati ons has become an active area of interest within the research communit y. This survey presents a comprehensive overview of these alignment te chnologies, including the following aspects. (1) Data collection : the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs. (2) Training methodologies : a detailed revie w of the prevailing training methods employed for LLM alignment. Our e xploration encompasses Supervised Fine-tuning, both Online and Ofﬂine human preference training, along with parameter-efﬁcient training mech anisms. (3) Model Evaluation : the methods for evaluating the effectiv eness of these human-aligned LLMs, presenting a multifaceted approach towards their assessment. In conclusion, we collate and distill our ﬁn dings, shedding light on several promising future research avenues in the ﬁeld. This survey, therefore, serves as a valuable resource for an yone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHu b link collecting the latest papers is available at",10.48550/ARXIV.2307.12966,JOUR
Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?,"Filippas, Apostolos; Horton, John J.; Manning, Benjamin S.",,"Large language models---because of how they are trained and designed-- -are implicit computational models of humans---a homo silicus. Social scientists can use LLMs like economists use homo economicus: LLMs can be given endowments, information, preferences, and so on, and then the ir behavior can be explored in scenarios via simulation. We replicate four experiments using this approach and find qualitatively similar re sults to the original. Benefits of this approach include trying new va riations for fresh insights, piloting studies via simulation, and sear ching for novel social science insights to test in the real world. The full version of the paper can be accessed at https://apostolos-filipp as.com/papers/hs.pdf.",10.1145/3670865.3673513,CONF
Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Des ign,"Tjuatja, Lindia; Chen, Valerie; Wu, Tongshuang; Talwalkwar, Ameet; Neubig, Graham",2024,"Abstract One widely cited barrier to the adoption of LLMs as proxies f or humans in subjective tasks is their sensitivity to prompt wording—b ut interestingly, humans also display sensitivities to instruction cha nges in the form of response biases. We investigate the extent to whic h LLMs reflect human response biases, if at all. We look to survey des ign, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literatu re. Drawing from these works, we design a dataset and framework to eva luate whether LLMs exhibit human-like response biases in survey questi onnaires. Our comprehensive evaluation of nine models shows that popul ar open and commercial LLMs generally fail to reflect human-like behav ior, particularly in models that have undergone RLHF. Furthermore, eve n if a model shows a significant change in the same direction as human s, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls o f using LLMs as human proxies, and underscore the need for finer-grain ed characterizations of model behavior.1",10.1162/tacl_a_00685,JOUR
Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study,"Hämäläinen, Perttu; Tavast, Mikke; Kunnari, Anton",,"Collecting data is one of the bottlenecks of Human-Computer Interactio n (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire respons es about experiencing video games as art, a topic not tractable with t raditional computational user models. We test whether synthetic respon ses can be distinguished from real responses, analyze errors of synthe tic data, and investigate content similarities between synthetic and r eal data. We conclude that GPT-3 can, in this context, yield believabl e accounts of HCI experiences. Given the low cost and high speed of LL M data generation, synthetic data should be useful in ideating and pil oting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsour cing of self-report data fundamentally unreliable.",10.1145/3544548.3580688,CONF
Large Language Models Meet NL2Code: A Survey,"Zan, Daoguang; Chen, Bei; Zhang, Fengji; Lu, Dianjie; Wu, Bingchao; Guan, Bei; Yongji, Wang; Lou, Jian-Guang",,"The task of generating code from a natural language description, or NL 2Code, is considered a pressing and significant challenge in code inte lligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking th e advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmark s and metrics. We provide an intuitive comparison of all existing mode ls on the HumanEval benchmark. Through in-depth observation and analys is, we provide some insights and conclude that the key factors contrib uting to the success of large language models for NL2Code are “Large S ize, Premium Data, Expert Tuning”. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progre ss through crowd-sourcing. To the best of our knowledge, this is the f irst survey of large language models for NL2Code, and we believe it wi ll contribute to the ongoing development of the field.",10.18653/v1/2023.acl-long.411,CONF
Frontiers: Can Large Language Models Capture Human Preferences?,"Goli, Ali; Singh, Amandeep",2024,This paper examines the potential of large language models to mimic hu man survey respondents and to derive their preferences.,10.1287/mksc.2023.0306,JOUR
Security and Privacy Challenges of Large Language Models: A Survey,"Das, Badhan Chandra; Amini, M. Hadi; Wu, Yanzhao",2025,"Large language models (LLMs) have demonstrated extraordinary capabilit ies and contributed to multiple fields, such as generating and summari zing text, language translation, and question-answering. Today, LLMs h ave become quite popular tools in natural language processing tasks, w ith the capability to analyze complicated linguistic patterns and prov ide relevant responses depending on the context. While offering signif icant advantages, these models are also vulnerable to security and pri vacy attacks, such as jailbreaking attacks, data poisoning attacks, an d personally identifiable information leakage attacks. This survey pro vides a thorough review of the security and privacy challenges of LLMs , along with the application-based risks in various domains, such as t ransportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks aga inst LLMs, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.",10.1145/3712001,JOUR
"Large Language Models(LLMs) on Tabular Data: Prediction, Generation, a nd Understanding -- A Survey","Fang, Xi; Xu, Weijie; Tan, Fiona Anting; Zhang, Jiani; Hu, Ziqing; Qi, Yanjun; Nickleach, Scott; Socolinsky, Diego; Sengamedu, Srinivasan; Faloutsos, Christos",2024,"Recent breakthroughs in large language modeling have facilitated rigor ous exploration of their application in diverse tasks related to tabul ar data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challen ges and opportunities. However, there is currently a lack of comprehen sive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datase ts, metrics, and methodologies utilized. It identifies strengths, limi tations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this v ital and rapidly evolving field. It also provides relevant code and da tasets references. Through this comprehensive review, we hope to provi de interested readers with pertinent references and insightful perspec tives, empowering them with the necessary tools and knowledge to effec tively navigate and address the prevailing challenges in the field.",10.48550/ARXIV.2402.17944,JOUR
"A Survey of Large Language Models for Healthcare: from Data, Technolog y, and Applications to Accountability and Ethics","He, Kai; Mao, Rui; Lin, Qika; Ruan, Yucheng; Lan, Xiang; Feng, Mengling; Cambria, Erik",2023,"The utilization of large language models (LLMs) in the Healthcare doma in has generated both excitement and concern due to their ability to e ffectively respond to freetext queries with certain professional knowl edge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with th e aim of providing an overview of the development roadmap from traditi onal Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effective ness of various Healthcare applications highlighting both the strength s and limitations. Secondly, we conduct a comparison between the previ ous PLMs and the latest LLMs, as well as comparing various LLMs with e ach other. Then we summarize related Healthcare training data, trainin g methods, optimization strategies, and usage. Finally, the unique con cerns associated with deploying LLMs in Healthcare settings are invest igated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from pers pectives of both computer science and Healthcare specialty. Besides th e discussion about Healthcare concerns, we supports the computer scien ce community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations , and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to L LMs. This shift encompasses a move from discriminative AI approaches t o generative AI approaches, as well as a shift from model-centered met hodologies to datacentered methodologies.",10.48550/ARXIV.2310.05694,JOUR
Large Language Models for Information Retrieval: A Survey,"Zhu, Yutao; Yuan, Huaying; Wang, Shuting; Liu, Jiongnan; Liu, Wenhan; Deng, Chenlong; Chen, Haonan; Liu, Zheng; Dou, Zhicheng; Wen, Ji-Rong",2023,"As a primary means of information acquisition, information retrieval ( IR) systems, such as search engines, have integrated themselves into o ur daily lives. These systems also serve as components of dialogue, qu estion-answering, and recommender systems. The trajectory of IR has ev olved dynamically from its origins in term-based methods to its integr ation with advanced neural models. While the neural models excel at ca pturing complex contextual signals and semantic nuances, thereby resha ping the IR landscape, they still face challenges such as data scarcit y, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combinatio n of both traditional methods (such as term-based sparse retrieval met hods with rapid response) and modern neural architectures (such as lan guage models with powerful language understanding capacity). Meanwhile , the emergence of large language models (LLMs), typified by ChatGPT a nd GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and rea soning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this researc h trajectory, it is necessary to consolidate existing methodologies an d provide nuanced insights through a comprehensive overview. In this s urvey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and re aders. Additionally, we explore promising directions, such as search a gents, within this expanding field.",10.48550/ARXIV.2308.07107,JOUR
Application and Evaluation of Large Language Models for the Generation of Survey Questions,"Maiorino, Antonio; Padgett, Zoe; Wang, Chun; Yakubovskiy, Misha; Jiang, Peng",,"Generative Language Models have shown promising results in various dom ains, and some of the most successful applications are related to ""con cept expansion"", which is the task of generating extensive text based on concise instructions provided through a ""seed"" prompt. In this pres entation we will discuss the recent work conducted by the Data Science team at SurveyMonkey, where we have recently introduced a new feature that harnesses Generative AI models to streamline the survey design p rocess. With this feature users can effortlessly initiate this process by specifying their desired objectives through a prompt, allowing the m to automate the creation of surveys that include the critical aspect s they wish to investigate. We will share our findings regarding some of the challenges encountered during the development of this feature. These include techniques for conditioning the model outputs, integrati ng generated text with industry-standard questions, fine-tuning Langua ge Models using semi-synthetic Data Generation techniques, and more. M oreover, we will showcase the Evaluation Methodology that we have deve loped to measure the quality of the generated surveys across several d imensions. This evaluation process is crucial in ensuring that the gen erated surveys align well with user expectations and serve their inten ded purpose effectively. Our goal is to demonstrate the promising pote ntial of Generative Language Models in the context of Survey Research, and we believe that sharing our learnings on these challenges and how we addressed them will be useful for practitioners working with Langu age Models on similar problems.",10.1145/3583780.3615506,CONF
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies,"Aher, Gati; Arriaga, Rosa I.; Kalai, Adam Tauman",2022,"We propose a method for using a large language model, such as GPT-3, t o simulate responses of different humans in a given context. We test o ur method by attempting to reproduce well-established economic, psycho linguistic, and social experiments. The method requires prompt templat es for each experiment. Simulations are run by varying the (hypothetic al) subject details, such as name, and analyzing the text generated by the language model. To validate our methodology, we use GPT-3 to simu late the Ultimatum Game , garden path sentences , risk aversion , and the Milgram Shock experiments. In order to address concerns of exposur e to these studies in training data, we also evaluate simulations on n ovel variants of these studies. We show that it is possible to simulat e responses of different people and that their responses are consisten t with prior human studies from the literature. Across all studies, th e distributions generated by larger language models better align with prior experimental results, suggesting a trend that future language mo dels may be used for even more faithful simulations of human responses . Our use of a language model for simulation is contrasted with anthro pomorphic views of a language model as having its own behavior.",10.48550/ARXIV.2208.10264,JOUR
Using large language models in psychology,"Demszky, Dorottya; Yang, Diyi; Yeager, David S.; Bryan, Christopher J.; Clapper, Margarett; Chandhok, Susannah; Eichstaedt, Johannes C.; Hecht, Cameron; Jamieson, Jeremy; Johnson, Meghann; Jones, Michaela; Krettek-Cobb, Danielle; Lai, Leslie; JonesMitchell, Nirel; Ong, Desmond C.; Dweck, Carol S.; Gross, James J.; Pennebaker, James W.",2023,"Large language models (LLMs), such as OpenAI's GPT-4, Google's Bard or Meta's LLaMa, have created unprecedented opportunities for analysing and generating language data on a massive scale. Because language data have a central role in all areas of psychology, this new technology h as the potential to transform the field. In this Perspective, we revie w the foundations of LLMs. We then explain how the way that LLMs are c onstructed enables them to effectively generate human-like linguistic output without the ability to think or feel like a human. We argue tha t although LLMs have the potential to advance psychological measuremen t, experimentation and practice, they are not yet ready for many of th e most transformative psychological applications — but further researc h and development may enable such use. Next, we examine four major con cerns about the application of LLMs to psychology, and how each might be overcome. Finally, we conclude with recommendations for investments that could help to address these concerns: field-initiated 'keystone' datasets; increased standardization of performance benchmarks; and sh ared computing and analysis infrastructure to ensure that the future o f LLM-powered research is equitable. Large language models (LLMs), whi ch can generate and score text in human-like ways, have the potential to advance psychological measurement, experimentation and practice. In this Perspective, Demszky and colleagues describe how LLMs work, conc erns about using them for psychological purposes, and how these concer ns might be addressed.",10.1038/s44159-023-00241-5,JOUR
Virtual Personas for Language Models via an Anthology of Backstories,"Moon, Suhong; Abdulhai, Marwa; Kang, Minwoo; Suh, Joseph; Soedarmadji, Widyadewi; Behar, Eran Kohen; Chan, David M.",2024,"Large language models (LLMs) are trained from vast repositories of tex t authored by millions of distinct authors, reflecting an enormous div ersity of human traits. While these models bear the potential to be us ed as approximations of human subjects in behavioral studies, prior ef forts have been limited in steering model responses to match individua l human users. In this work, we introduce Anthology, a method for cond itioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as backstories. We show that our me thodology enhances the consistency and reliability of experimental out comes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as par t of Pew Research Center’s American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the respons e distributions of human respondents and 27% improvement in consistenc y metrics.",10.48550/ARXIV.2407.06576,JOUR
Examining the Feasibility of Large Language Models as Survey Responden ts,"Kitadai, Ayato; Ogawa, Kazuhito; Nishino, Nariaki",,"This study examines the potential of large language models (LLMs) to s ubstitute for human respondents in survey research. Surveys serve as e ssential tools in fields like social science, marketing, and policy-ma king; however, traditional methods often require considerable time and costs. LLMs present a promising alternative to mitigate these burdens , though their reliability—particularly outside of U.S. contexts—remai ns uncertain. This study focuses on surveys conducted in Japan, compar ing the responses generated by LLMs to those of actual Japanese partic ipants. Our analysis reveals notable discrepancies due to inherent bia ses in LLMs, though adjusting the models to better align with specific personas can partially enhance the accuracy of simulated responses. W e emphasize the need for further research to fully understand the capa bilities and limitations of LLMs, aiming to refine their application i n diverse areas such as social sciences, marketing, and policy decisio n-making.",10.1109/bigdata62323.2024.10825497,CONF
Energy Social Surveys Replicated with Large Language Model Agents,"Fell, Michael",2024,"Large Language Models (LLMs) are artificial intelligence systems train ed to understand and predict human language. In this study I programma tically create numerous LLM agents with population-representative char acteristics, and prompt them provide survey responses with the aim of replicating existing energy social survey findings. Three studies are replicated, yielding moderate to high degrees of fidelity to the origi nal results. Potentially significant contributions of the approach inc lude improving the efficiency of research by identifying most promisin g interventions before conducting human studies, and simulating input from harder-to-access populations. However, there are also important p ractical and ethical challenges requiring of careful consideration.",10.2139/ssrn.4686345,JOUR
Simulating Human Opinions with Large Language Models,"Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vladimir; Rau, Lea; Schallner, Rene",,"Public and private organizations rely on opinion surveys to inform bus iness and policy decisions. Yet, empirical surveys are costly and time -consuming. Recent advances in large language models (LLMs) have spark ed interest in generating synthetic survey data, i.e., simulated answe rs based on target demographics, as an alternative to real human data. But how well can LLMs replicate human opinions? In this ongoing proje ct, we develop and critically evaluate methods for synthetic survey sa mpling. As an empirical benchmark, we collected responses from a repre sentative U.S. sample (n = 461) on preferences for a common consumer g ood (soft drinks). Then, we developed ASPIRE (Automated Synthetic Pers ona Interview and Response Engine), a tool that pairs each human parti cipant with a “digital twin” based on their demographic profile and ge nerates synthetic responses via LLM technology. Synthetic data achieve d better-than-chance accuracy in matching human responses and approxim ated aggregate subjective rankings for both binary and Likert-scale it ems. However, LLM-simulated data overestimated humans’ tendencies to p rovide positive ratings and exhibited substantially reduced variance c ompared to real data. The match of synthetic and real data was not sys tematically related to participants’ age, gender, or ethnicity, indica ting no demographic bias. Overall, while synthetic sampling shows prom ise for modeling aggregate opinion trends, it currently falls short in replicating the variability and complexity of real human opinions. We discuss insights of our ongoing project for accurate and responsible user opinion modeling via LLMs.",10.1145/3708319.3733685,CONF
"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge","Zhou, Hongjian; Liu, Fenglin; Gu, Boyang; Zou, Xinyu; Huang, Jinfa; Wu, Jinge; Li, Yiru; Chen, Sam S.; Zhou, Peilin; Liu, Junling; Hua, Yining; Mao, Chengfeng; You, Chenyu; Wu, Xian; Zheng, Yefeng; Clifton, Lei; Li, Zheng; Luo, Jiebo; Clifton, David A.",2023,"Large language models (LLMs), such as ChatGPT, have received substanti al attention due to their capabilities for understanding and generatin g human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tas ks (e.g., enhancing clinical diagnostics and providing medical educati on), a review of these efforts, particularly their development, practi cal applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development an d deployment of LLMs in medicine, including the challenges and opportu nities they face. In terms of development, we provide a detailed intro duction to the principles of existing medical LLMs, including their ba sic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioner s in developing medical LLMs tailored to their specific needs. In term s of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with stat e-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task perform ance of LLMs in a medical setting? 3) How have medical LLMs been emplo yed in real-world practice? 4) What challenges arise from the use of m edical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insig hts into the opportunities for LLMs in medicine and serve as a practic al resource. We also maintain a regularly updated list of practical gu ides on medical LLMs at: https://github.com/AI-in-Health/MedLLMsPracti calGuide.",10.48550/ARXIV.2311.05112,JOUR
LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Lik e Responses in Survey Simulations,"Cho, Suhyun; Kim, Jaeyun; Kim, Jang Hyun",2024,"This study explores whether large language models (LLMs) can learn a p erson’s opinions from their speech and act based on that knowledge. It also proposes the potential for utilizing such trained models in surv ey research. Traditional survey research collects information through standardized questions. However, surveys require repeated administrati on with new participants each time, which involves significant costs a nd time. With the recent advancements in LLMs, artificial intelligence (AI) has shown remarkable capabilities, often surpassing humans in ta sks that require natural language understanding (NLU) and natural lang uage generation (NLG). Despite this, research on whether AI can replic ate human thought processes in tasks such as text interpretation or qu estion-answering remains insufficient. This study proposes a Surveyed LLM, specialized for survey tasks, and a Doppelganger LLM that mimics human thought processes. It tests to what extent the Doppelganger mode l can replicate human judgment. Furthermore, it suggests the possibili ty of mimicking not only group distributions but also individual opini ons.",10.1109/access.2024.3502219,JOUR
A Survey of Confidence Estimation and Calibration in Large Language Mo dels,"Geng, Jiahui; Cai, Fengyu; Wang, Yuxia; Koeppl, Heinz; Nakov, Preslav; Gurevych, Iryna",,"Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impres sive performance, they can be unreliable due to factual errors in thei r generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce bet ter generations. There has been a lot of recent research aiming to add ress this, but there has been no comprehensive overview to organize it and to outline the main lessons learned. The present survey aims to b ridge this gap. In particular, we outline the challenges and we summar ize recent technical advancements for LLM confidence estimation and ca libration. We further discuss their applications and suggest promising directions for future work.",10.18653/v1/2024.naacl-long.366,CONF
Can large language models estimate public opinion about global warming ? An empirical assessment of algorithmic fidelity and bias,"Lee, Sanguk; Peng, Tai-Quan; Goldberg, Matthew H.; Rosenthal, Seth A.; Kotcher, John E.; Maibach, Edward W.; Leiserowitz, Anthony",2024,"Large language models (LLMs) can be used to estimate human attitudes a nd behavior, including measures of public opinion, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fideli ty and bias of LLMs in estimating public opinion about global warming. LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. Findings indicate that LLMs can effecti vely reproduce presidential voting behaviors but not global warming op inions unless the issue relevant covariates are included. When conditi oned on both demographic and covariates, GPT-4 demonstrates improved a ccuracy, ranging from 53% to 91%, in predicting beliefs and attitudes about global warming. Additionally, we find an algorithmic bias that u nderestimates the global warming opinions of Black Americans. While hi ghlighting the potential of LLMs to aid social science research, these results underscore the importance of conditioning, model selection, s urvey question format, and bias assessment when employing LLMs for sur vey simulation.",10.1371/journal.pclm.0000429,JOUR
Polling Latent Opinions: A Method for Computational Sociolinguistics U sing Transformer Language Models,"Feldman, Philip; Dant, Aaron; Foulds, James R.; Pan, Shemei",2022,"Text analysis of social media for sentiment, topic analysis, and other analysis depends initially on the selection of keywords and phrases t hat will be used to create the research corpora. However, keywords tha t researchers choose may occur infrequently, leading to errors that ar ise from using small samples. In this paper, we use the capacity for m emorization, interpolation, and extrapolation of Transformer Language Models such as the GPT series to learn the linguistic behaviors of a s ubgroup within larger corpora of Yelp reviews. We then use prompt-base d queries to generate synthetic text that can be analyzed to produce i nsights into specific opinions held by the populations that the models were trained on. Once learned, more specific sentiment queries can be made of the model with high levels of accuracy when compared to tradi tional keyword searches. We show that even in cases where a specific k eyphrase is limited or not present at all in the training corpora, the GPT is able to accurately generate large volumes of text that have th e correct sentiment.",10.48550/ARXIV.2204.07483,JOUR
Social Simulacra: Creating Populated Prototypes for Social Computing S ystems,"Park, Joon Sung; Popowski, Lindsay; Cai, Carrie; Morris, Meredith Ringel; Liang, Percy; Bernstein, Michael S.",,"Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many cha llenges do not arise until a system is populated at a larger scale. Ca n a designer understand how a social system might behave when populate d, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping techniqu e that generates a breadth of realistic social interactions that may e merge when a social computing system is populated. Social simulacra ta ke as input the designer’s description of a community’s design—goal, r ules, and member personas—and produce as output an instance of that de sign with simulated behavior, including posts, replies, and anti-socia l behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and th at they enable exploration of “what if?” scenarios where community mem bers or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large lan guage models’ training data already includes a wide variety of positiv e and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacr a from actual community behavior and that social computing designers s uccessfully refine their social computing designs when using social si mulacra.",10.1145/3526113.3545616,CONF
Large Language Models for Time Series: A Survey,"Zhang, Xiyuan; Chowdhury, Ranak Roy; Gupta, Rajesh K.; Shang, Jingbo",2024,"Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text , image and graphics, LLMs present a significant potential for analysi s of time series data, benefiting domains such as climate, IoT, health care, traffic, audio and finance. This survey paper provides an in-dep th exploration and a detailed taxonomy of the various methodologies em ployed to harness the power of LLMs for time series analysis. We addre ss the inherent challenge of bridging the gap between LLMs' original t ext data training and the numerical nature of time series data, and ex plore strategies for transferring and distilling knowledge from LLMs t o numerical time series analysis. We detail various methodologies, inc luding (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a brid ging mechanism, and (5) the combination of LLMs with tools. Additional ly, this survey offers a comprehensive overview of the existing multim odal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discusse d in the survey.",10.48550/ARXIV.2402.01801,JOUR
Empowering Time Series Analysis with Large Language Models: A Survey,"Jiang, Yushan; Pan, Zijie; Zhang, Xikun; Garg, Sahil; Schneider, Anderson; Nevmyvaka, Yuriy; Song, Dongjin",2024,"Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-p urpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well a s the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre -trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we p rovide a systematic overview of existing methods that leverage LLMs fo r time series analysis. Specifically, we first state the challenges an d motivations of applying language models in the context of time serie s as well as brief preliminaries of LLMs. Next, we summarize the gener al pipeline for LLM-based time series analysis, categorize existing me thods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific do mains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.",10.48550/ARXIV.2402.03182,JOUR
Large Language Models for Human-Like Autonomous Driving: A Survey,"Li, Yun; Katsumata, Kai; Javanmardi, Ehsan; Tsukada, Manabu",,"Large Language Models (LLMs), AI models trained on massive text corpor a with remarkable language understanding and generation capabilities, are transforming the field of Autonomous Driving (AD). As AD systems e volve from rule-based and optimization-based methods to learning-based techniques like deep reinforcement learning, they are now poised to e mbrace a third and more advanced category: knowledge-based AD empowere d by LLMs. This shift promises to bring AD closer to human-like AD. Ho wever, integrating LLMs into AD systems poses challenges in real-time inference, safety assurance, and deployment costs. This survey provide s a comprehensive and critical review of recent progress in leveraging LLMs for AD, focusing on their applications in modular AD pipelines a nd end-to-end AD systems. We highlight key advancements, identify pres sing challenges, and propose promising research directions to bridge t he gap between LLMs and AD, thereby facilitating the development of mo re human-like AD systems. The survey first introduces LLMs' key featur es and common training schemes, then delves into their applications in modular AD pipelines and end-to-end AD, respectively, followed by dis cussions on open challenges and future directions. Through this in-dep th analysis, we aim to provide insights and inspiration for researcher s and practitioners working at the intersection of AI and autonomous v ehicles, ultimately contributing to safer, smarter, and more human-cen tric AD technologies.",10.1109/itsc58415.2024.10919629,CONF
Aligning Language Models to User Opinions,"Hwang, EunJeong; Majumder, Bodhisattwa Prasad; Tandon, Niket",2023,"An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LL M into behaving as a certain persona, especially a user group or ideol ogical persona the model captured during its pertaining stage. But, ho w to best align an LLM with a specific user and not a demographic or i deological group remains an open question. Mining public opinion surve ys (by Pew Research), we find that the opinions of a user and their de mographics and ideologies are not mutual predictors. We use this insig ht to align LLMs by modeling both user opinions as well as user demogr aphics and ideology, achieving up to 7 points accuracy gains in predic ting public opinions from survey questions across a broad set of topic s. In addition to the typical approach of prompting LLMs with demograp hics and ideology, we discover that utilizing the most relevant past o pinions from individual users enables the model to predict user opinio ns more accurately.",10.48550/ARXIV.2305.14929,JOUR
From Individual to Society: A Survey on Social Simulation Driven by La rge Language Model-based Agents,"Mou, Xinyi; Ding, Xuanwen; He, Qi; Wang, Liang; Liang, Jingcong; Zhang, Xinnong; Sun, Libo; Lin, Jiayu; Zhou, Jie; Huang, Xuanjing; Wei, Zhongyu",2024,"Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns. Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the re plication of individual responses and facilitating studies on many int erdisciplinary studies. In this paper, we conduct a comprehensive surv ey of this field, illustrating the recent progress in simulation drive n by LLM-empowered agents. We categorize the simulations into three ty pes: (1) Individual Simulation, which mimics specific individuals or d emographic groups; (2) Scenario Simulation, where multiple agents coll aborate to achieve goals within specific contexts; and (3) Society Sim ulation, which models interactions within agent societies to reflect t he complexity and variety of real-world dynamics. These simulations fo llow a progression, ranging from detailed individual modeling to large -scale societal phenomena. We provide a detailed discussion of each si mulation type, including the architecture or key components of the sim ulation, the classification of objectives or scenarios and the evaluat ion method. Afterward, we summarize commonly used datasets and benchma rks. Finally, we discuss the trends across these three types of simula tion. A repository for the related sources is at {\url{https://github. com/FudanDISC/SocialAgent}}.",10.48550/ARXIV.2412.03563,JOUR
Demonstrations of the Potential of AI-based Political Issue Polling,"Sanders, Nathan E.; Ulinich, Alex; Schneier, Bruce",2023,"Political polling is a multi-billion dollar industry with outsized inf luence on the societal trajectory of the United States and nations aro und the world. However, it has been challenged by factors that stress its cost, availability, and accuracy. At the same time, artificial int elligence (AI) chatbots have become compelling stand-ins for human beh avior, powered by increasingly sophisticated large language models (LL Ms). Could AI chatbots be an effective tool for anticipating public op inion on controversial issues to the extent that they could be used by campaigns, interest groups, and polling firms? We have developed a pr ompt engineering methodology for eliciting human-like survey responses from ChatGPT, which simulate the response to a policy question of a p erson described by a set of demographic factors, and produce both an o rdinal numeric response score and a textual justification. We execute large scale experiments, querying for thousands of simulated responses at a cost far lower than human surveys. We compare simulated data to human issue polling data from the Cooperative Election Study (CES). We find that ChatGPT is effective at anticipating both the mean level an d distribution of public opinion on a variety of policy issues such as abortion bans and approval of the US Supreme Court, particularly in t heir ideological breakdown (correlation typically>85%). However, it is less successful at anticipating demographic-level differences. Moreov er, ChatGPT tends to overgeneralize to new policy issues that arose af ter its training data was collected, such as US support for involvemen t in the war in Ukraine. Our work has implications for our understandi ng of the strengths and limitations of the current generation of AI ch atbots as virtual publics or online listening platforms, future direct ions for LLM development, and applications of AI tools to the politica l domain. (Abridged)",10.48550/ARXIV.2307.04781,JOUR
Large Language Models for Cyber Security: A Systematic Literature Revi ew,"Xu, Hanxiang; Wang, Shenao; Li, Ningke; Wang, Kailong; Zhao, Yanjie; Chen, Kai; Yu, Ting; Liu, Yang; Wang, Haoyu",2024,"The rapid advancement of Large Language Models (LLMs) has opened up ne w opportunities for leveraging artificial intelligence in various doma ins, including cybersecurity. As the volume and sophistication of cybe r threats continue to grow, there is an increasing need for intelligen t systems that can automatically detect vulnerabilities, analyze malwa re, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity (LLM4Security). By comprehensively collecting over 30K relevant papers and systematically analyzing 127 papers from top security and softwar e engineering venues, we aim to provide a holistic view of how LLMs ar e being used to solve diverse problems across the cybersecurity domain . Through our analysis, we identify several key findings. First, we ob serve that LLMs are being applied to a wide range of cybersecurity tas ks, including vulnerability detection, malware analysis, network intru sion detection, and phishing detection. Second, we find that the datas ets used for training and evaluating LLMs in these tasks are often lim ited in size and diversity, highlighting the need for more comprehensi ve and representative datasets. Third, we identify several promising t echniques for adapting LLMs to specific cybersecurity domains, such as fine-tuning, transfer learning, and domain-specific pre-training. Fin ally, we discuss the main challenges and opportunities for future rese arch in LLM4Security, including the need for more interpretable and ex plainable models, the importance of addressing data privacy and securi ty concerns, and the potential for leveraging LLMs for proactive defen se and threat hunting. Overall, our survey provides a comprehensive ov erview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research.",10.48550/ARXIV.2405.04760,JOUR
"Using large language models to generate silicon samples in consumer an d marketing research: Challenges, opportunities, and guidelines","Sarstedt, Marko; Adler, Susanne J.; Rau, Lea; Schmitt, Bernd",2024,"Should consumer researchers employ silicon samples and artificially ge nerated data based on large language models, such as GPT, to mimic hum an respondents' behavior? In this paper, we review recent research tha t has compared result patterns from silicon and human samples, finding that results vary considerably across different domains. Based on the se results, we present specific recommendations for silicon sample use in consumer and marketing research. We argue that silicon samples hol d particular promise in upstream parts of the research process such as qualitative pretesting and pilot studies, where researchers collect e xternal information to safeguard follow‐up design choices. We also pro vide a critical assessment and recommendations for using silicon sampl es in main studies. Finally, we discuss ethical issues of silicon samp le use and present future research avenues.",10.1002/mar.21982,JOUR
Large Language Models respond to Influence like Humans,"Griffin, Lewis; Kleinberg, Bennett; Mozes, Maximilian; Mai, Kimberly; Vau, Maria Do Mar; Caldwell, Matthew; Mavor-Parker, Augustine",,"Two studies tested the hypothesis that a Large Language Model (LLM) ca n be used to model psychological change following exposure to influent ial input. The first study tested a generic mode of influence - the Il lusory Truth Effect (ITE) - where earlier exposure to a statement boos ts a later truthfulness test rating. Analysis of newly collected data from human and LLM-simulated subjects (1000 of each) showed the same p attern of effects in both populations; although with greater per state ment variability for the LLM. The second study concerns a specific mod e of influence – populist framing of news to increase its persuasion a nd political mobilization. Newly collected data from simulated subject s was compared to previously published data from a 15 country experime nt on 7286 human participants. Several effects from the human study we re replicated by the simulated study, including ones that surprised th e authors of the human study by contradicting their theoretical expect ations; but some significant relationships found in human data were no t present in the LLM data. Together the two studies support the view t hat LLMs have potential to act as models of the effect of influence.",10.18653/v1/2023.sicon-1.3,CONF
Assessing the Risk of Bias in Randomized Clinical Trials With Large La nguage Models,"Lai, Honghao; Ge, Long; Sun, Mingyao; Pan, Bei; Huang, Jiajie; Hou, Liangying; Yang, Qiuyu; Liu, Jiayi; Liu, Jianing; Ye, Ziying; Xia, Danni; Zhao, Weilong; Wang, Xiaoman; Liu, Ming; Talukdar, Jhalok Ronjan; Tian, Jinhui; Yang, Kehu; Estill, Janne",2024,This survey study evaluates the feasibility and reliability of using l arge language models to assess risk of bias in randomized clinical tri als.,10.1001/jamanetworkopen.2024.12687,JOUR
LLMs for Explainable AI: A Comprehensive Survey,"Bilal, Ahsan; Ebert, David; Lin, Beiyu",2025,"Large Language Models (LLMs) offer a promising approach to enhancing E xplainable AI (XAI) by transforming complex machine learning outputs i nto easy-to-understand narratives, making model predictions more acces sible to users, and helping bridge the gap between sophisticated model behavior and human interpretability. AI models, such as state-of-the- art neural networks and deep learning models, are often seen as""black boxes""due to a lack of transparency. As users cannot fully understand how the models reach conclusions, users have difficulty trusting decis ions from AI models, which leads to less effective decision-making pro cesses, reduced accountabilities, and unclear potential biases. A chal lenge arises in developing explainable AI (XAI) models to gain users' trust and provide insights into how models generate their outputs. Wit h the development of Large Language Models, we want to explore the pos sibilities of using human language-based models, LLMs, for model expla inabilities. This survey provides a comprehensive overview of existing approaches regarding LLMs for XAI, and evaluation techniques for LLM- generated explanation, discusses the corresponding challenges and limi tations, and examines real-world applications. Finally, we discuss fut ure directions by emphasizing the need for more interpretable, automat ed, user-centric, and multidisciplinary approaches for XAI via LLMs.",10.48550/ARXIV.2504.00125,JOUR
"Bank Run, Interrupted: Modeling Deposit Withdrawals with Generative AI ","Kazinnik, Sophia",2023,"I use a large language model to generate synthetic survey responses to different bank run scenarios. By assigning the model with specific de mographic characteristics, I simulate diverse populations of agents, a nd assess their responses to varied conditions and communication-based interventions related to a bank run. The simulated responses, reflect ing trends in bank deposit withdrawals across demographic categories, align with existing empirical studies. This novel application offers a cost-effective method to analyze hypothetical scenarios using simulat ed data, and highlights the potential of language models in providing insights into human behavior in economic context.",10.2139/ssrn.4656722,JOUR
Large language model for table processing: a survey,"Lu, Weizheng; Zhang, Jing; Fan, Ju; Fu, Zihao; Chen, Yueguo; Du, Xiaoyong",2025,"Tables, typically two-dimensional and structured to store large amount s of data, are essential in daily activities like database queries, sp readsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Lar ge Language Models (LLMs) or Visual Language Models (VLMs) offers sign ificant public benefits, garnering interest from academia and industry . This survey provides a comprehensive overview of table-related tasks , examining both user scenarios and technical aspects. It covers tradi tional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processi ng. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we hi ghlight several challenges, including processing implicit user intenti ons and extracting information from various table sources.",10.1007/s11704-024-40763-6,JOUR
Large Language Models for Market Research: A Data-augmentation Approac h,"Wang, Mengxin; Zhang, Dennis J.; Zhang, Heng",2024,"Large Language Models (LLMs) have transformed artificial intelligence by excelling in complex natural language processing tasks. Their abili ty to generate human-like text has opened new possibilities for market research, particularly in conjoint analysis, where understanding cons umer preferences is essential but often resource-intensive. Traditiona l survey-based methods face limitations in scalability and cost, makin g LLM-generated data a promising alternative. However, while LLMs have the potential to simulate real consumer behavior, recent studies high light a significant gap between LLM-generated and human data, with bia ses introduced when substituting between the two. In this paper, we ad dress this gap by proposing a novel statistical data augmentation appr oach that efficiently integrates LLM-generated data with real data in conjoint analysis. Our method leverages transfer learning principles t o debias the LLM-generated data using a small amount of human data. Th is results in statistically robust estimators with consistent and asym ptotically normal properties, in contrast to naive approaches that sim ply substitute human data with LLM-generated data, which can exacerbat e bias. We validate our framework through an empirical study on COVID- 19 vaccine preferences, demonstrating its superior ability to reduce e stimation error and save data and costs by 24.9\% to 79.8\%. In contra st, naive approaches fail to save data due to the inherent biases in L LM-generated data compared to human data. Another empirical study on s ports car choices validates the robustness of our results. Our finding s suggest that while LLM-generated data is not a direct substitute for human responses, it can serve as a valuable complement when used with in a robust statistical framework.",10.48550/ARXIV.2412.19363,JOUR
"A survey on fairness of large language models in e-commerce: progress, application, and challenge","Ren, Qingyang; Jiang, Zilin; Cao, Jinghan; Li, Sijia; Li, Chiqu; Liu, Yiyang; Huo, Shuning; He, Tiange; Chen, Yuan",2024,"This survey explores the fairness of large language models (LLMs) in e -commerce, examining their progress, applications, and the challenges they face. LLMs have become pivotal in the e-commerce domain, offering innovative solutions and enhancing customer experiences. This work pr esents a comprehensive survey on the applications and challenges of LL Ms in e-commerce. The paper begins by introducing the key principles u nderlying the use of LLMs in e-commerce, detailing the processes of pr etraining, fine-tuning, and prompting that tailor these models to spec ific needs. It then explores the varied applications of LLMs in e-comm erce, including product reviews, where they synthesize and analyze cus tomer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhan cing global accessibility; and product question and answer sections, w here they automate customer support. The paper critically addresses th e fairness challenges in e-commerce, highlighting how biases in traini ng data and algorithms can lead to unfair outcomes, such as reinforcin g stereotypes or discriminating against certain groups. These issues n ot only undermine consumer trust, but also raise ethical and legal con cerns. Finally, the work outlines future research directions, emphasiz ing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fair ness of these systems, ensuring they serve diverse global markets effe ctively and ethically. Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commer ce, offering insights into their potential and limitations, and guidin g future endeavors in creating fairer and more inclusive e-commerce en vironments.",10.48550/ARXIV.2405.13025,JOUR
Frontiers: Determining the Validity of Large Language Models for Autom ated Perceptual Analysis,"Li, Peiyao; Castelo, Noah; Katona, Zsolt; Sarvary, Miklos",2024,The paper explores the potential of Large Language Models to substitut e for or to augment human participants in market research.,10.1287/mksc.2023.0454,JOUR
S$^3$: Social-network Simulation System with Large Language Model-Empo wered Agents,"Gao, Chen; Lan, Xiaochong; Lu, Zhihong; Mao, Jinzhu; Piao, Jinghua; Wang, Huandong; Jin, Depeng; Li, Yong",2023,"Social network simulation plays a crucial role in addressing various c hallenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support , among others. In this work, we harness the formidable human-like cap abilities exhibited by large language models (LLMs) in sensing, reason ing, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\textbf{S}$ocial network $\textbf{S}$imulation $\te xtbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques t o ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivot al aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of po pulation-level phenomena, including the propagation of information, at titudes, and emotions. We conduct an evaluation encompassing two level s of simulation, employing real-world social network data. Encouraging ly, the results demonstrate promising accuracy. This work represents a n initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a sou rce of inspiration for the development of simulation systems within, b ut not limited to, social science.",10.48550/ARXIV.2307.14984,JOUR
Evaluating the Moral Beliefs Encoded in LLMs,"Scherrer, Nino; Shi, Claudia; Feder, Amir; Blei, David M.",2023,"This paper presents a case study on the design, administration, post-p rocessing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting be liefs encoded in LLMs. We introduce statistical measures and evaluatio n metrics that quantify the probability of an LLM""making a choice"", th e associated uncertainty, and the consistency of that choice. (2) We a pply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvi ous. We design a large-scale survey comprising 680 high-ambiguity mora l scenarios (e.g.,""Should I tell a white lie?"") and 687 low-ambiguity moral scenarios (e.g.,""Should I stop for a pedestrian on the road?""). Each scenario includes a description, two possible actions, and auxili ary labels indicating violated rules (e.g.,""do not kill""). We administ er the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models""choose""actions that align with comm onsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because th eir responses are sensitive to the question-wording. (c) Some models r eflect clear preferences in ambiguous scenarios. Specifically, closed- source models tend to agree with each other.",10.48550/ARXIV.2307.14324,JOUR
Generative AI Meets Open-Ended Survey Responses: Research Participant Use of AI and Homogenization,"Zhang, Simone; Xu, Janet; Alvero, AJ",2025,"The growing popularity of generative artificial intelligence (AI) tool s presents new challenges for data quality in online surveys and exper iments. This study examines participants’ use of large language models to answer open-ended survey questions and describes empirical tendenc ies in human versus large language model (LLM)-generated text response s. In an original survey of research participants recruited from a pop ular online platform for sourcing social science research subjects, 34 percent reported using LLMs to help them answer open-ended survey que stions. Simulations comparing human-written responses from three pre-C hatGPT studies with LLM-generated text reveal that LLM responses are m ore homogeneous and positive, particularly when they describe social g roups in sensitive questions. These homogenization patterns may mask i mportant underlying social variation in attitudes and beliefs among hu man subjects, raising concerns about data validity. Our findings shed light on the scope and potential consequences of participants’ LLM use in online research.",10.1177/00491241251327130,JOUR
A Survey of Large Language Model Agents for Question Answering,"Yue, Murong",2025,"This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significa nt limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these c hallenges by leveraging LLMs as their core reasoning engine. These age nts achieve superior QA results compared to traditional QA pipelines a nd naive LLM QA systems by enabling interaction with external environm ents. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, q uestion understanding, information retrieval, and answer generation. A dditionally, this paper identifies ongoing challenges and explores fut ure research directions to enhance the performance of LLM agent QA sys tems.",10.48550/ARXIV.2503.19213,JOUR
"Generative Agent Simulations of 1,000 People","Park, Joon Sung; Zou, Carolyn Q.; Shaw, Aaron; Hill, Benjamin Mako; Cai, Carrie; Morris, Meredith Ringel; Willer, Robb; Liang, Percy; Bernstein, Michael S.",2024,"The promise of human behavioral simulation--general-purpose computatio nal agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a no vel agent architecture that simulates the attitudes and behaviors of 1 ,052 real individuals--applying large language models to qualitative i nterviews about their lives, then measuring how well these agents repl icate the attitudes and behaviors of the individuals that they represe nt. The generative agents replicate participants' responses on the Gen eral Social Survey 85% as accurately as participants replicate their o wn answers two weeks later, and perform comparably in predicting perso nality traits and outcomes in experimental replications. Our architect ure reduces accuracy biases across racial and ideological groups compa red to agents given demographic descriptions. This work provides a fou ndation for new tools that can help investigate individual and collect ive behavior.",10.48550/ARXIV.2411.10109,JOUR
"Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data","Sinacola, Enzo; Pachot, Arnault; Petit, Thierry",2025,"Large Language Models (LLMs) offer a promising alternative to traditio nal survey methods, potentially enhancing efficiency and reducing cost s. In this study, we use LLMs to create virtual populations that answe r survey questions, enabling us to predict outcomes comparable to huma n responses. We evaluate several LLMs-including GPT-4o, GPT-3.5, Claud e 3.5-Sonnet, and versions of the Llama and Mistral models-comparing t heir performance to that of a traditional Random Forests algorithm usi ng demographic data from the World Values Survey (WVS). LLMs demonstra te competitive performance overall, with the significant advantage of requiring no additional training data. However, they exhibit biases wh en predicting responses for certain religious and population groups, u nderperforming in these areas. On the other hand, Random Forests demon strate stronger performance than LLMs when trained with sufficient dat a. We observe that removing censorship mechanisms from LLMs significan tly improves predictive accuracy, particularly for underrepresented de mographic segments where censored models struggle. These findings high light the importance of addressing biases and reconsidering censorship approaches in LLMs to enhance their reliability and fairness in publi c opinion research.",10.48550/ARXIV.2503.16498,JOUR
Predicting Responses to Psychological Questionnaires from Participants ’ Social Media Posts and Question Text Embeddings,"Vu, Huy; Abdurahman, Suhaib; Bhatia, Sudeep; Ungar, Lyle",,"Psychologists routinely assess people’s emotions and traits, such as t heir personality, by collecting their responses to survey questionnair es. Such assessments can be costly in terms of both time and money, an d often lack generalizability, as existing data cannot be used to pred ict responses for new survey questions or participants. In this study, we propose a method for predicting a participant’s questionnaire resp onse using their social media texts and the text of the survey questio n they are asked. Specifically, we use Natural Language Processing (NL P) tools such as BERT embeddings to represent both participants (via t he text they write) and survey questions as embeddings vectors, allowi ng us to predict responses for out-of-sample participants and question s. Our novel approach can be used by researchers to integrate new part icipants or new questions into psychological studies without the const raint of costly data collection, facilitating novel practical applicat ions and furthering the development of psychological theory. Finally, as a side contribution, the success of our model also suggests a new a pproach to study survey questions using NLP tools such as text embeddi ngs rather than response data used in traditional methods.",10.18653/v1/2020.findings-emnlp.137,CONF
Evaluating Large Language Models as Generative User Simulators for Con versational Recommendation,"Yoon, Se-eun; He, Zhankui; Echterhoff, Jessica Maria; McAuley, Julian",2024,"Synthetic users are cost-effective proxies for real users in the evalu ation of conversational recommender systems. Large language models sho w promise in simulating human-like behavior, raising the question of t heir ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accu rately emulate human behavior in conversational recommendation. This p rotocol is comprised of five tasks, each designed to evaluate a key pr operty that a synthetic user should exhibit: choosing which items to t alk about, expressing binary preferences, expressing open-ended prefer ences, requesting recommendations, and giving feedback. Through evalua tion of baseline simulators, we demonstrate these tasks effectively re veal deviations of language models from human behavior, and offer insi ghts on how to reduce the deviations with model selection and promptin g strategies.",10.48550/ARXIV.2403.09738,JOUR
Language Models Can Generate Human-Like Self-Reports of Emotion,"Tavast, Mikke; Kunnari, Anton; Hämäläinen, Perttu",,"Computational interaction and user modeling is presently limited in th e domain of emotions. We investigate a potential new approach to compu tational modeling of emotional response behavior, by using modern neur al language models to generate synthetic self-report data, and evaluat ing the human-likeness of the results. More specifically, we generate responses to the PANAS questionnaire with four different variants of t he recent GPT-3 model. Based on both data visualizations and multiple quantitative metrics, the human-likeness of the responses increases wi th model size, with the largest Davinci model variant generating the m ost human-like data.",10.1145/3490100.3516464,CONF
Large language models display human-like social desirability biases in Big Five personality surveys,"Salecha, Aadesh; Ireland, Molly E; Subrahmanya, Shashanka; Sedoc, João; Ungar, Lyle H; Eichstaedt, Johannes C",2024,"Abstract Large language models (LLMs) are becoming more widely used to simulate human participants and so understanding their biases is impo rtant. We developed an experimental framework using Big Five personali ty surveys and uncovered a previously undetected social desirability b ias in a wide range of LLMs. By systematically varying the number of q uestions LLMs were exposed to, we demonstrate their ability to infer w hen they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e. increased extraversion, decreased neuroticism, etc.). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4’s survey responses changing by 1.20 (human) SD and Llama 3’s by 0.98 SD, which are very large effects. This bias remains after questi on order randomization and paraphrasing. Reverse coding the questions decreases bias levels but does not eliminate them, suggesting that thi s effect cannot be attributed to acquiescence bias. Our findings revea l an emergent social desirability bias and suggest constraints on prof iling LLMs with psychometric tests and on this use of LLMs as proxies for human participants.",10.1093/pnasnexus/pgae533,JOUR
"Large language models for automated Q&amp;A involving legal documents: a survey on algorithms, frameworks and applications","Yang, Xiaoxian; Wang, Zhifeng; Wang, Qi; Wei, Ke; Zhang, Kaiqi; Shi, Jiangang",2024,"Purpose This study aims to adopt a systematic review approach to examine the e xisting literature on law and LLMs.It involves analyzing and synthesiz ing relevant research papers, reports and scholarly articles that disc uss the use of LLMs in the legal domain. The review encompasses variou s aspects, including an analysis of LLMs, legal natural language proce ssing (NLP), model tuning techniques, data processing strategies and f rameworks for addressing the challenges associated with legal question -and-answer (Q&A) systems. Additionally, the study explores potential applications and services that can benefit from the integration of LLM s in the field of intelligent justice.  Design/methodology/approach This paper surveys the state-of-the-art research on law LLMs and their application in the field of intelligent justice. The study aims to id entify the challenges associated with developing Q&A systems based on LLMs and explores potential directions for future research and develop ment. The ultimate goal is to contribute to the advancement of intelli gent justice by effectively leveraging LLMs.  Findings To effectively apply a law LLM, systematic research on LLM, legal NLP and model adjustment technology is required.  Originality/value This study contributes to the field of intelligent justice by providin g a comprehensive review of the current state of research on law LLMs.  ",10.1108/ijwis-12-2023-0256,JOUR
Multilingual Large Language Models: A Systematic Survey,"Zhu, Shaolin; Supryadi; Xu, Shaoyang; Sun, Haoran; Pan, Leiyu; Cui, Menglong; Du, Jiangcun; Jin, Renren; Branco, António; Xiong, Deyi",2024,"This paper provides a comprehensive survey of the latest research on m ultilingual large language models (MLLMs). MLLMs not only are able to understand and generate language across linguistic boundaries, but als o represent an important advancement in artificial intelligence. We fi rst discuss the architecture and pre-training objectives of MLLMs, hig hlighting the key components and methodologies that contribute to thei r multilingual capabilities. We then discuss the construction of multi lingual pre-training and alignment datasets, underscoring the importan ce of data quality and diversity in enhancing MLLM performance. An imp ortant focus of this survey is on the evaluation of MLLMs. We present a detailed taxonomy and roadmap covering the assessment of MLLMs' cros s-lingual knowledge, reasoning, alignment with human values, safety, i nterpretability and specialized applications. Specifically, we extensi vely discuss multilingual evaluation benchmarks and datasets, and expl ore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs from black to white boxes, we also address the interpretability of multilingual capabilities, cross-lingual transfer and language bias within these models. Finally, we provide a comprehensive review of re al-world applications of MLLMs across diverse domains, including biolo gy, medicine, computer science, mathematics and law. We showcase how t hese models have driven innovation and improvements in these specializ ed fields while also highlighting the challenges and opportunities in deploying MLLMs within diverse language communities and application sc enarios. We listed the paper related in this survey and publicly avail able at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers .",10.48550/ARXIV.2411.11072,JOUR
Use of Artificial Intelligence Chatbots for Cancer Treatment Informati on,"Chen, Shan; Kann, Benjamin H.; Foote, Michael B.; Aerts, Hugo J. W. L.; Savova, Guergana K.; Mak, Raymond H.; Bitterman, Danielle S.",2023,This survey study examines the performance of a large language model c hatbot in providing cancer treatment recommendations that are concorda nt with National Comprehensive Cancer Network guidelines.,10.1001/jamaoncol.2023.2954,JOUR
Language Models Trained on Media Diets Can Predict Public Opinion,"Chu, Eric; Andreas, Jacob; Ansolabehere, Stephen; Roy, Deb",2023,"Public opinion reflects and shapes societal behavior, but the traditio nal survey-based tools to measure it are limited. We introduce a novel approach to probe media diet models -- language models adapted to onl ine news, TV broadcast, or radio show content -- that can emulate the opinions of subpopulations that have consumed a set of media. To valid ate this method, we use as ground truth the opinions expressed in U.S. nationally representative surveys on COVID-19 and consumer confidence . Our studies indicate that this approach is (1) predictive of human j udgements found in survey response distributions and robust to phrasin g and channels of media exposure, (2) more accurate at modeling people who follow media more closely, and (3) aligned with literature on whi ch types of opinions are affected by media consumption. Probing langua ge models provides a powerful new method for investigating media effec ts, has practical applications in supplementing polls and forecasting public opinion, and suggests a need for further study of the surprisin g fidelity with which neural language models can predict human respons es.",10.48550/ARXIV.2303.16779,JOUR
Predicting Survey Responses: How and Why Semantics Shape Survey Statis tics on Organizational Behaviour,"Arnulf, Jan Ketil; Larsen, Kai Rune; Martinsen, Øyvind Lund; Bong, Chih How",2014,"Some disciplines in the social sciences rely heavily on collecting sur vey responses to detect empirical relationships among variables. We ex plored whether these relationships were a priori predictable from the semantic properties of the survey items, using language processing alg orithms which are now available as new research methods. Language proc essing algorithms were used to calculate the semantic similarity among all items in state-of-the-art surveys from Organisational Behaviour r esearch. These surveys covered areas such as transformational leadersh ip, work motivation and work outcomes. This information was used to ex plain and predict the response patterns from real subjects. Semantic a lgorithms explained 60–86% of the variance in the response patterns an d allowed remarkably precise prediction of survey responses from human s, except in a personality test. Even the relationships between indepe ndent and their purported dependent variables were accurately predicte d. This raises concern about the empirical nature of data collected th rough some surveys if results are already given a priori through the w ay subjects are being asked. Survey response patterns seem heavily det ermined by semantics. Language algorithms may suggest these prior to a dministering a survey. This study suggests that semantic algorithms ar e becoming new tools for the social sciences, opening perspectives on survey responses that prevalent psychometric theory cannot explain.",10.1371/journal.pone.0106361,JOUR
Harnessing Large Language Models to Simulate Realistic Human Responses to Social Engineering Attacks: A Case Study,"Asfour, Mohammad; Murillo, Juan Carlos",2023,"The research publication, “Generative Agents: Interactive Simulacra of Human Behavior,” by Stanford and Google in 2023 established that larg e language models (LLMs) such as GPT-4 can generate interactive agents with credible and emergent human-like behaviors. However, their appli cation in simulating human responses in cybersecurity scenarios, parti cularly in social engineering attacks, remains unexplored. In addressi ng that gap, this study explores the potential of LLMs, specifically t he Open AI GPT-4 model, to simulate a broad spectrum of human response s to social engineering attacks that exploit human social behaviors, f raming our primary research question: How does the simulated behavior of human targets, based on the Big Five personality traits, responds t o social engineering attacks? . This study aims to provide valuable in sights for organizations and researchers striving to systematically an alyze human behavior and identify prevalent human qualities, as define d by the Big Five personality traits, that are susceptible to social e ngineering attacks, specifically phishing emails. Also, it intends to offer recommendations for the cybersecurity industry and policymakers on mitigating these risks. The findings indicate that LLMs can provide realistic simulations of human responses to social engineering attack s, highlighting certain personality traits as more susceptible.",10.52306/2578-3289.1172,JOUR
Large Language Models as Psychological Simulators: A Methodological Gu ide,"Lin, Zhicheng",2025,"Large language models (LLMs) offer emerging opportunities for psycholo gical and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological sim ulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to i nvestigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demogra phic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototypi ng research instruments. For cognitive modeling, we synthesize emergin g approaches for probing internal representations, methodological adva nces in causal interventions, and strategies for relating model behavi or to human cognition. We address overarching challenges including pro mpt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects r eview. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emer ging empirical evidence about LLM performance--including systematic bi ases, cultural limitations, and prompt brittleness--to help researcher s wrangle these challenges and leverage the unique capabilities of LLM s in psychological research.",10.48550/ARXIV.2506.16702,JOUR
"Large Language Models for Wearable Sensor-Based Human Activity Recogni tion, Health Monitoring, and Behavioral Modeling: A Survey of Early Tr ends, Datasets, and Challenges","Ferrara, Emilio",2024,"The proliferation of wearable technology enables the generation of vas t amounts of sensor data, offering significant opportunities for advan cements in health monitoring, activity recognition, and personalized m edicine. However, the complexity and volume of these data present subs tantial challenges in data modeling and analysis, which have been addr essed with approaches spanning time series modeling to deep learning t echniques. The latest frontier in this domain is the adoption of large language models (LLMs), such as GPT-4 and Llama, for data analysis, m odeling, understanding, and human behavior monitoring through the lens of wearable sensor data. This survey explores the current trends and challenges in applying LLMs for sensor-based human activity recognitio n and behavior modeling. We discuss the nature of wearable sensor data , the capabilities and limitations of LLMs in modeling them, and their integration with traditional machine learning techniques. We also ide ntify key challenges, including data quality, computational requiremen ts, interpretability, and privacy concerns. By examining case studies and successful applications, we highlight the potential of LLMs in enh ancing the analysis and interpretation of wearable sensor data. Finall y, we propose future directions for research, emphasizing the need for improved preprocessing techniques, more efficient and scalable models , and interdisciplinary collaboration. This survey aims to provide a c omprehensive overview of the intersection between wearable sensor data and LLMs, offering insights into the current state and future prospec ts of this emerging field.",10.3390/s24155045,JOUR
Does GPT-3 know what the Most Important Issue is? Using Large Language Models to Code Open-Text Social Survey Responses At Scale,"Mellon, Jonathan; Bailey, Jack; Scott, Ralph; Breckwoldt, James; Miori, Marta",2022,"We examine the use of large language models (LLMs) like OpenAI's GPT-3 for coding open-ended survey responses. We compare GPT-3's performanc e on a classification task using data from the British Election Study Internet Panel (BESIP) with that of a human coder and an SVM (a tradit ional supervised learning algorithm) fitted to 576,325 manually labell ed observations. We find that while GPT-3's zero-shot performance is s lightly lower than a second human coder (97% agreement), GPT-3 is able to match the original human coder's collapsed category 95% of the tim e, and outperforms the SVM in terms of accuracy and bias. The results suggest that LLMs perform acceptably when coding this type of open-end ed survey response and may allow for greater use of open-ended questio ns in future.",10.2139/ssrn.4310154,JOUR
A Comprehensive Survey of Bias in LLMs: Current Landscape and Future D irections,"Ranjan, Rajesh; Gupta, Shailja; Singh, Surya Narayan",2024,"Large Language Models(LLMs) have revolutionized various applications i n natural language processing (NLP) by providing unprecedented text ge neration, translation, and comprehension capabilities. However, their widespread deployment has brought to light significant concerns regard ing biases embedded within these models. This paper presents a compreh ensive survey of biases in LLMs, aiming to provide an extensive review of the types, sources, impacts, and mitigation strategies related to these biases. We systematically categorize biases into several dimensi ons. Our survey synthesizes current research findings and discusses th e implications of biases in real-world applications. Additionally, we critically assess existing bias mitigation techniques and propose futu re research directions to enhance fairness and equity in LLMs. This su rvey serves as a foundational resource for researchers, practitioners, and policymakers concerned with addressing and understanding biases i n LLMs.",10.48550/ARXIV.2409.16430,JOUR
Large Language Models Show Human-like Social Desirability Biases in Su rvey Responses,"Salecha, Aadesh; Ireland, Molly E.; Subrahmanya, Shashanka; Sedoc, João; Ungar, Lyle H.; Eichstaedt, Johannes C.",2024,"As Large Language Models (LLMs) become widely used to model and simula te human behavior, understanding their biases becomes critical. We dev eloped an experimental framework using Big Five personality surveys an d uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew th eir scores towards the desirable ends of trait dimensions (i.e., incre ased extraversion, decreased neuroticism, etc). This bias exists in al l tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. B ias levels appear to increase in more recent models, with GPT-4's surv ey responses changing by 1.20 (human) standard deviations and Llama 3' s by 0.98 standard deviations-very large effects. This bias is robust to randomization of question order and paraphrasing. Reverse-coding al l the questions decreases bias levels but does not eliminate them, sug gesting that this effect cannot be attributed to acquiescence bias. Ou r findings reveal an emergent social desirability bias and suggest con straints on profiling LLMs with psychometric tests and on using LLMs a s proxies for human participants.",10.48550/ARXIV.2405.06058,JOUR
A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions,"Na, Hongbin; Hua, Yining; Wang, Zimu; Shen, Tao; Yu, Beibei; Wang, Lilin; Wang, Wei; Torous, John; Chen, Ling",2025,"Mental health remains a critical global challenge, with increasing dem and for accessible, effective interventions. Large language models (LL Ms) offer promising solutions in psychotherapy by enhancing the assess ment, diagnosis, and treatment of mental health conditions through dyn amic, context-aware interactions. This survey provides a comprehensive overview of the current landscape of LLM applications in psychotherap y, highlighting the roles of LLMs in symptom detection, severity estim ation, cognitive assessment, and therapeutic interventions. We present a novel conceptual taxonomy to organize the psychotherapy process int o three core components: assessment, diagnosis, and treatment, and exa mine the challenges and advancements in each area. The survey also add resses key research gaps, including linguistic biases, limited disorde r coverage, and underrepresented therapeutic models. Finally, we discu ss future directions to integrate LLMs into a holistic, end-to-end psy chotherapy framework, addressing the evolving nature of mental health conditions and fostering more inclusive, personalized care.",10.48550/ARXIV.2502.11095,JOUR
Human Preferences in Large Language Model Latent Space: A Technical An alysis on the Reliability of Synthetic Data in Voting Outcome Predicti on,"Ball, Sarah; Allmendinger, Simeon; Kreuter, Frauke; Kühl, Niklas",2025,"Generative AI (GenAI) is increasingly used in survey contexts to simul ate human preferences. While many research endeavors evaluate the qual ity of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents re main. Our study provides a technical analysis of how demographic attri butes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-bas ed predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human resp onses, particularly across demographic subgroups. In the political spa ce, persona-to-party mappings exhibit limited differentiation, resulti ng in synthetic data that lacks the nuanced distribution of opinions f ound in survey data. Moreover, we show that prompt sensitivity can sig nificantly alter outputs for some models, further undermining the stab ility and predictiveness of LLM-based simulations. As a key contributi on, we adapt a probe-based methodology that reveals how LLMs encode po litical affiliations in their latent space, exposing the systematic di stortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computat ional behavioral modeling.",10.48550/ARXIV.2502.16280,JOUR
The Oscars of AI Theater: A Survey on Role-Playing with Language Model s,"Chen, Nuo; Wang, Yan; Deng, Yang; Li, Jia",2024,"This survey explores the burgeoning field of role-playing with languag e models, focusing on their development from early persona-based model s to advanced character-driven simulations facilitated by Large Langua ge Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded t o embrace complex character portrayals involving character consistency , behavioral alignment, and overall attractiveness. We provide a compr ehensive taxonomy of the critical components in designing these system s, including data, models and alignment, agent architecture and evalua tion. This survey not only outlines the current methodologies and chal lenges, such as managing dynamic personal profiles and achieving high- level persona consistency but also suggests avenues for future researc h in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement . Related resources and papers are available at https://github.com/nuo chenpku/Awesome-Role-Play-Papers.",10.48550/ARXIV.2407.11484,JOUR
User Behavior Simulation with Large Language Model-based Agents,"Wang, Lei; Zhang, Jingsen; Yang, Hao; Chen, Zhi-Yuan; Tang, Jiakai; Zhang, Zeyu; Chen, Xu; Lin, Yankai; Sun, Hao; Song, Ruihua; Zhao, Xin; Xu, Jun; Dou, Zhicheng; Wang, Jun; Wen, Ji-Rong",2025,"Simulating high quality user behavior data has always been a fundament al yet challenging problem in human-centered applications such as reco mmendation systems, social networks, among many others. The major diff iculty of user behavior simulation originates from the intricate mecha nism of human cognitive and decision processes. Recently, substantial evidence has suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence and generalization capabilities. Inspired by such capabilities, in this ar ticle, we take an initial step to study the potential of using LLMs fo r user behavior simulation in the recommendation domain. To make LLMs act like humans, we design profile, memory and action modules to equip them, building LLM-based agents to simulate real users. To enable int eractions between different agents and observe their behavior patterns , we design a sandbox environment, where each agent can interact with the recommendation system, and different agents can converse with thei r friends via one-to-one chatting or one-to-many social broadcasting. In the experiments, we first demonstrate the believability of the agen t-generated behaviors based on both subjective and objective evaluatio ns. Then, to show the potential applications of our method, we simulat e and study two social phenomena including (1) information cocoons and (2) user conformity behaviors. We find that controlling the personali zation degree of recommendation algorithms and improving the heterogen eity of user social relations can be two effective strategies for alle viating the problem of information cocoon, and the conformity behavior s can be highly influenced by the amount of user social relations. To advance this direction, we have released our project at https://github .com/RUC-GSAI/YuLan-Rec.",10.1145/3708985,JOUR
Mixture-of-Personas Language Models for Population Simulation,"Bui, Ngoc; Nguyen, Hieu Trung; Kumar, Shantanu; Theodore, Julian; Qiu, Weikang; Nguyen, Viet Anh; Ying, Rex",2025,"Advances in Large Language Models (LLMs) paved the way for their emerg ing applications in various domains, such as human behavior simulation s, where LLMs could augment human-generated data in social science res earch and machine learning model training. However, pretrained LLMs of ten fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose \textit{Mixture of Personas} (MoP), a \textit{probab ilistic} prompting method that aligns the LLM responses with the targe t population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors. The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM respons es during simulation. MoP is flexible, requires no model finetuning, a nd is transferable across base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment an d diversity metrics.",10.48550/ARXIV.2504.05019,JOUR
A Large Language Model Approach to Educational Survey Feedback Analysi s,"Parker, Michael J.; Anderson, Caitlin; Stone, Claire; Oh, YeaRim",2024,"This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teac hing and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring tim e-consuming manual processing of textual responses. LLMs have the pote ntial to provide a flexible means of achieving these goals without spe cialized machine learning models or fine-tuning. We demonstrate a vers atile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentimen t analysis, each performed by LLM. We apply these workflows to a real- world dataset of 2500 end-of-course survey comments from biomedical sc ience courses, and evaluate a zero-shot approach (i.e., requiring no e xamples or labeled training data) across all tasks, reflecting educati on settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple t asks with GPT-4, enabling workflows necessary to achieve typical goals . We also show the potential of inspecting LLMs' chain-of-thought (CoT ) reasoning for providing insight that may foster confidence in practi ce. Moreover, this study features development of a versatile set of cl assification categories, suitable for various course types (online, hy brid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text. ",10.1007/s40593-024-00414-0,JOUR
Susceptibility to Influence of Large Language Models,"Griffin, Lewis D; Kleinberg, Bennett; Mozes, Maximilian; Mai, Kimberly T; Vau, Maria; Caldwell, Matthew; Marvor-Parker, Augustine",2023,"Two studies tested the hypothesis that a Large Language Model (LLM) ca n be used to model psychological change following exposure to influent ial input. The first study tested a generic mode of influence - the Il lusory Truth Effect (ITE) - where earlier exposure to a statement (thr ough, for example, rating its interest) boosts a later truthfulness te st rating. Data was collected from 1000 human participants using an on line experiment, and 1000 simulated participants using engineered prom pts and LLM completion. 64 ratings per participant were collected, usi ng all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirm ed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test . The same pattern of effects was found for LLM-simulated participants . The second study concerns a specific mode of influence - populist fr aming of news to increase its persuasion and political mobilization. D ata from LLM-simulated participants was collected and compared to prev iously published data from a 15-country experiment on 7286 human parti cipants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surpris ed the authors of the human study by contradicting their theoretical e xpectations (anti-immigrant framing of news decreases its persuasion a nd mobilization); but some significant relationships found in human da ta (modulation of the effectiveness of populist framing according to r elative deprivation of the participant) were not present in the LLM da ta. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.",10.48550/ARXIV.2303.06074,JOUR
Can ChatGPT emulate humans in software engineering surveys?,"Steinmacher, Igor; Penney, Jacob Mcauley; Felizardo, Katia Romero; Garcia, Alessandro F.; Gerosa, Marco A.",,"Context: There is a growing belief in the literature that large langua ge models (LLMs), such as ChatGPT, can mimic human behavior in surveys . Gap: While the literature has shown promising results in social scie nces and market research, there is scant evidence of its effectiveness in technical fields like software engineering. Objective: Inspired by previous work, this paper explores ChatGPT’s ability to replicate fin dings from prior software engineering research. Given the frequent use of surveys in this field, if LLMs can accurately emulate human respon ses, this technique could address common methodological challenges lik e recruitment difficulties, representational shortcomings, and respond ent fatigue. Method: We prompted ChatGPT to reflect the behavior of a ‘mega-persona’ representing the demographic distribution of interest. We replicated surveys from 2019 to 2023 from leading SE conferences, e xamining ChatGPT’s proficiency in mimicking responses from diverse dem ographics. Results: Our findings reveal that ChatGPT can successfully replicate the outcomes of some studies, but in others, the results wer e not significantly better than a random baseline. Conclusions: This p aper reports our results so far and discusses the challenges and poten tial research opportunities in leveraging LLMs for representing humans in software engineering surveys.",10.1145/3674805.3690744,CONF
Language Models for Automated Market Research: A New Way to Generate P erceptual Maps,"Li, Peiyao; Castelo, Noah; Katona, Zsolt; Sarvary, Miklos",2022,"This paper uses generative language models to substitute for human par ticipants in market research. Such AI-based models can be used to gene rate textual continuation given a prompt. By carefully formulating the prompts, the paper shows that this new method generates perceptual ma ps that are close to those generated from human surveys. The applicati on includes perceptual maps based on both brand similarity measures an d product attribute ratings. The results demonstrate that using this n ew method of automated market research can reproduce perceptual maps o btained via human surveys, but at much lower cost. Furthermore, this p aper shows that AI-based market research can be applied to answer more nuanced questions based on demographic variables or contextual variat ion that is prohibitively expensive or infeasible with human responden ts.",10.2139/ssrn.4241291,JOUR
A Survey on Human-Centric LLMs,"Wang, Jing Yi; Sukiennik, Nicholas; Li, Tong; Su, Weikang; Hao, Qianyue; Xu, Jingbo; Huang, Zihan; Xu, Fengli; Li, Yong",2024,"The rapid evolution of large language models (LLMs) and their capacity to simulate human cognition and behavior has given rise to LLM-based frameworks and tools that are evaluated and applied based on their abi lity to perform tasks traditionally performed by humans, namely those involving cognition, decision-making, and social interaction. This sur vey provides a comprehensive examination of such human-centric LLM cap abilities, focusing on their performance in both individual tasks (whe re an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics). We first eva luate LLM competencies across key areas including reasoning, perceptio n, and social cognition, comparing their abilities to human-like skill s. Then, we explore real-world applications of LLMs in human-centric d omains such as behavioral science, political science, and sociology, a ssessing their effectiveness in replicating human behaviors and intera ctions. Finally, we identify challenges and future research directions , such as improving LLM adaptability, emotional intelligence, and cult ural sensitivity, while addressing inherent biases and enhancing frame works for human-AI collaboration. This survey aims to provide a founda tional understanding of LLMs from a human-centric perspective, offerin g insights into their current capabilities and potential for future de velopment.",10.48550/ARXIV.2411.14491,JOUR
Investigating Cultural Alignment of Large Language Models,"AlKhamissi, Badr; ElNokrashy, Muhammad; AlKhamissi, Mai; Diab, Mona",2024,"The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective h uman knowledge, raise a pivotal question: do these models genuinely en capsulate the diverse knowledge adopted by different cultures? Our stu dy reveals that these models demonstrate greater cultural alignment al ong two dimensions -- firstly, when prompted with the dominant languag e of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural al ignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the Unit ed States through prompting LLMs with different pretraining data mixtu res in both Arabic and English with the personas of the real responden ts and the survey questions. Further analysis reveals that misalignmen t becomes more pronounced for underrepresented personas and for cultur ally sensitive topics, such as those probing social values. Finally, w e introduce Anthropological Prompting, a novel method leveraging anthr opological reasoning to enhance cultural alignment. Our study emphasiz es the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the pluralit y of different cultures with many implications on the topic of cross-l ingual transfer.",10.48550/ARXIV.2402.13231,JOUR
Simulating Social Media Using Large Language Models to Evaluate Altern ative News Feed Algorithms,"Törnberg, Petter; Valeeva, Diliara; Uitermark, Justus; Bail, Christopher",2023,"Social media is often criticized for amplifying toxic discourse and di scouraging constructive conversations. But designing social media plat forms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of La rge Language Models (LLM) and Agent-Based Modeling can help researcher s study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the Ameri can National Election Study to populate simulated social media platfor ms. Next, we prompt the agents to read and share news articles - and l ike or comment upon each other's messages - within three platforms tha t use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In th e second, they see posts from all users - even those outside their own network. The third platform employs a novel""bridging""algorithm that h ighlights posts that are liked by people with opposing political views . We find this bridging algorithm promotes more constructive, non-toxi c, conversation across political divides than the other two models. Th ough further research is needed to evaluate these findings, we argue t hat LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.",10.48550/ARXIV.2310.05984,JOUR
AutoSurvey: Large Language Models Can Automatically Write Surveys,"Wang, Yidong; Guo, Qi; Yao, Wenjin; Zhang, Hongbo; Zhang, Xin; Wu, Zhen; Zhang, Meishan; Dai, Xinyu; Zhang, Min; Wen, Qingsong; Ye, Wei; Zhang, Shikun; Zhang, Yue",2024,"This paper introduces AutoSurvey, a speedy and well-organized methodol ogy for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. Traditional sur vey paper creation faces challenges due to the vast volume and complex ity of information, prompting the need for efficient survey methods. W hile large language models (LLMs) offer promise in automating this pro cess, challenges such as context window limitations, parametric knowle dge constraints, and the lack of evaluation benchmarks remain. AutoSur vey addresses these challenges through a systematic approach that invo lves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration. Our contributions include a comprehensive solution to t he survey problem, a reliable evaluation method, and experimental vali dation demonstrating AutoSurvey's effectiveness.We open our resources at \url{https://github.com/AutoSurveys/AutoSurvey}.",10.48550/ARXIV.2406.10252,JOUR
Do AIs know what the most important issue is? Using language models to code open-text social survey responses at scale,"Mellon, Jonathan; Bailey, Jack; Scott, Ralph; Breckwoldt, James; Miori, Marta; Schmedeman, Phillip",2024,"Can artificial intelligence accurately label open-text survey response s? We compare the accuracy of six large language models (LLMs) using a few-shot approach, three supervised learning algorithms (SVM, DistilR oBERTa, and a neural network trained on BERT embeddings), and a second human coder on the task of categorizing “most important issue” respon ses from the British Election Study Internet Panel into 50 categories. For the scenario where a researcher lacks existing training data, the accuracy of the highest-performing LLM (Claude-1.3: 93.9%) neared hum an performance (94.7%) and exceeded the highest-performing supervised approach trained on 1000 randomly sampled cases (neural network: 93.5% ). In a scenario where previous data has been labeled but a researcher wants to label novel text, the best LLM’s (Claude-1.3: 80.9%) few-sho t performance is only slightly behind the human (88.6%) and exceeds th e best supervised model trained on 576,000 cases (DistilRoBERTa: 77.8% ). PaLM-2, Llama-2, and the SVM all performed substantially worse than the best LLMs and supervised models across all metrics and scenarios. Our results suggest that LLMs may allow for greater use of open-ended survey questions in the future.",10.1177/20531680241231468,JOUR
Natural Language Interfaces for Tabular Data Querying and Visualizatio n: A Survey,"Zhang, Weixu; Wang, Yifei; Song, Yuanfeng; Wei, Victor Junqiu; Tian, Yuxing; Qi, Yiyan; Chan, Jonathan H.; Wong, Raymond Chi-Wing; Yang, Haiqin",2024,"The emergence of natural language processing has revolutionized the wa y users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT a nd its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a co mprehensive overview of natural language interfaces for tabular data q uerying and visualization, which allow users to interact with data usi ng natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We th en delve into the recent advancements in Text-to-SQL and Text-to-Vis p roblems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future i mprovements. Through this survey, we aim to provide a roadmap for rese archers and practitioners interested in developing and applying natura l language interfaces for data interaction in the era of large languag e models.",10.1109/tkde.2024.3400824,JOUR
"Best practices for implementing ChatGPT, large language models, and ar tificial intelligence in qualitative and survey-based research","Kantor, Jonathan",2024,"Large language models (LLMs), propelled by the popularity of OpenAI's ChatGPT, are being increasingly investigated for research and clinical applications in science and health care.1-3 Yet in many ways, the nat ure of generative artificial intelligence models may also be well suit ed for qualitative researchers—those investigating the complexity of h uman behavior, choices, attitudes, and preferences. Given the models' ability to immediately synthesize vast quantities of text coupled with their potential to winnow a range of disparate inputs into cohesive t hemes, they may present significant opportunities for qualitative rese archers eager to diversify and deepen their analyses.",10.1016/j.jdin.2023.10.001,JOUR
Large Language Models: A Survey,"Minaee, Shervin; Mikolov, Tomas; Nikzad, Narjes; Chenaghlu, Meysam; Socher, Richard; Amatriain, Xavier; Gao, Jianfeng",2024,"Large Language Models (LLMs) have drawn a lot of attention due to thei r strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purp ose language understanding and generation is acquired by training bill ions of model's parameters on massive amounts of text data, as predict ed by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LL Ms, including three popular LLM families (GPT, LLaMA, PaLM), and discu ss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We the n survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchm arks. Finally, we conclude the paper by discussing open challenges and future research directions.",10.48550/ARXIV.2402.06196,JOUR
"A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly","Yao, Yifan; Duan, Jinhao; Xu, Kaidi; Cai, Yuanfang; Sun, Zhibo; Zhang, Yue",2024,"Large Language Models (LLMs), such as ChatGPT and Bard, have revolutio nized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, cont extual awareness, and robust problem-solving skills, making them inval uable in various domains (e.g., search engines, customer support, tran slation). In the meantime, LLMs have also gained traction in the secur ity community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersec tion of LLMs with security and privacy. Specifically, we investigate h ow LLMs positively impact security and privacy, potential risks and th reats associated with their use, and inherent vulnerabilities within L LMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (o ffensive applications), and “The Ugly” (vulnerabilities of LLMs and th eir defenses). We have some interesting findings. For example, LLMs ha ve proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditio nal methods. However, they can also be harnessed for various attacks ( particularly user-level attacks) due to their human-like reasoning abi lities. We have identified areas that require further research efforts . For example, Research on model and parameter extraction attacks is l imited and often theoretical, hindered by LLM parameter scale and conf identiality. Safe instruction tuning, a recent development, requires m ore exploration. We hope that our work can shed light on the LLMs’ pot ential to both bolster and jeopardize cybersecurity.",10.1016/j.hcc.2024.100211,JOUR
Bias and Fairness in Large Language Models: A Survey,"Gallegos, Isabel O.; Rossi, Ryan A.; Barrow, Joe; Tanjim, Md Mehrab; Kim, Sungchul; Dernoncourt, Franck; Yu, Tong; Zhang, Ruiyi; Ahmed, Nesreen K.",2024,"Abstract Rapid advancements of large language models (LLMs) have enabl ed the processing, understanding, and generation of human-like text, w ith increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive sur vey of bias evaluation and mitigation techniques for LLMs. We first co nsolidate, formalize, and expand notions of social bias and fairness i n natural language processing, defining distinct facets of harm and in troducing several desiderata to operationalize fairness for LLMs. We t hen unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigat ion. Our first taxonomy of metrics for bias evaluation disambiguates t he relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: emb eddings, probabilities, and generated text. Our second taxonomy of dat asets for bias evaluation categorizes datasets by their structure as c ounterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bia s mitigation classifies methods by their intervention during pre-proce ssing, in-training, intra-processing, and post-processing, with granul ar subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide rang e of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better under stand and prevent the propagation of bias in LLMs.",10.1162/coli_a_00524,JOUR
"A Survey on Large Language Models for Critical Societal Domains: Finan ce, Healthcare, and Law","Chen, Zhiyu Zoey; Ma, Jing; Zhang, Xinlu; Hao, Nan; Yan, An; Nourbakhsh, Armineh; Yang, Xianjun; McAuley, Julian; Petzold, Linda; Wang, William Yang",2024,"In the fast-evolving domain of artificial intelligence, large language models (LLMs) such as GPT-3 and GPT-4 are revolutionizing the landsca pes of finance, healthcare, and law: domains characterized by their re liance on professional expertise, challenging data acquisition, high-s takes, and stringent regulatory compliance. This survey offers a detai led exploration of the methodologies, applications, challenges, and fo rward-looking opportunities of LLMs within these high-stakes sectors. We highlight the instrumental role of LLMs in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics , and refining legal interpretation and compliance strategies. Moreove r, we critically examine the ethics for LLM applications in these fiel ds, pointing out the existing ethical concerns and the need for transp arent, fair, and robust AI systems that respect regulatory norms. By p resenting a thorough review of current literature and practical applic ations, we showcase the transformative impact of LLMs, and outline the imperative for interdisciplinary cooperation, methodological advancem ents, and ethical vigilance. Through this lens, we aim to spark dialog ue and inspire future research dedicated to maximizing the benefits of LLMs while mitigating their risks in these precision-dependent sector s. To facilitate future research on LLMs in these critical societal do mains, we also initiate a reading list that tracks the latest advancem ents under this topic, which will be continually updated: \url{https:/ /github.com/czyssrs/LLM_X_papers}.",10.48550/ARXIV.2405.01769,JOUR
"Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and th e Future of Edemorcacy","Karanjai, Rabimba; Shor, Boris; Austin, Amanda; Kennedy, Ryan; Lu, Yang; Xu, Lei; Shi, Weidong",,"This paper investigates the use of Large Language Models (LLMs) to syn thesize public opinion data, addressing challenges in traditional surv ey methods like declining response rates and non-response bias. We int roduce a novel technique: role creation based on knowledge injection, a form of in-context learning that leverages RAG and specified persona lity profiles from the HEXACO model and demographic information, and u ses that for dynamically generated prompts. This method allows LLMs to simulate diverse opinions more accurately than existing prompt engine ering approaches. We compare our results with pre-trained models with standard few-shot prompts. Experiments using questions from the Cooper ative Election Study (CES) demonstrate that our role-creation approach significantly improves the alignment of LLM-generated opinions with r eal-world human survey responses, increasing answer adherence. In addi tion, we discuss challenges, limitations and future research direction s.",10.1109/icedeg65568.2025.11081685,CONF
Exploring the potential and limitations of large language models as vi rtual respondents for social science research,"Rakovics, Zsófia; Rakovics, Márton",2024,"Social and linguistic differences encoded in various textual content a vailable on the internet represent certain features of modern societie s. For any scientific research which is interested in social differenc es mediated by language, the advent of large language models (LLMs) ha s brought new opportunities. LLMs could be used to extract information about different groups of society and utilized as data providers by a cting as virtual respondents generating answers as such. Using LLMs (GPT-variants, Llama2, and Mixtral), we generated virtual a nswers for politics and democracy related attitude questions of the Eu ropean Social Survey (10th wave) and statistically compared the result s of the simulated responses to the real ones. We explored different p rompting techniques and the effect of different types and richness of contextual information provided to the models. Our results suggest tha t the tested LLMs generate highly realistic answers and are good at in voking the needed patterns from limited contextual information given t o them if a couple of relevant examples are provided, but struggle in a zero-shot setting. A critical methodological analysis is inevitable when considering the potential use of data generated by LLMs for scientific research, the e xploration of known biases and reflection on social reality not repres ented on the internet are essential.",10.17356/ieejsp.v10i4.1326,JOUR
Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection,"Wang, Chaofan; Freire, Samuel Kernan; Zhang, Mo; Wei, Jing; Goncalves, Jorge; Kostakos, Vassilis; Sarsenbayeva, Zhanna; Schneegass, Christina; Bozzon, Alessandro; Niforatos, Evangelos",2023,"ChatGPT and other large language models (LLMs) have proven useful in c rowdsourcing tasks, where they can effectively annotate machine learni ng training data. However, this means that they also have the potentia l for misuse, specifically to automatically answer surveys. LLMs can p otentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses t o surveys. The mechanism uses""prompt injection"", such as directions th at can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions , and find that it can reliably detect LLM-generated responses with mo re than 93% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our w ork is a step in ensuring that survey methodologies remain rigorous vi s-a-vis LLMs.",10.48550/ARXIV.2306.08833,JOUR
Evaluating Silicon Sampling: LLM Accuracy in Simulating Public Opinion on Facial Recognition Technology,"Ma, Charles",,"Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like responses, prompting exploration into their potential for social science research. ""Silicon sampling,"" a method wh ere LLMs are queried after being prompted with personas, has emerged a s a possible alternative to traditional survey methods, especially giv en the increasing challenges associated with declining survey particip ation rates and rising costs. However, the accuracy of silicon samplin g remains a subject of debate.This study examines the effectiveness of silicon sampling in replicating survey results on public acceptance t oward facial recognition technology (FRT). The research builds upon th e work of Kostka et al. (2021)*, who conducted a multinational survey across Germany, China, the United Kingdom, and the United States, anal yzing public opinion on FRT alongside socio-demographic data and key c ontextual factors, including perceived consequences, utility, and reli ability of the technology.The study addresses two research questions: (1) Can LLMs simulate an individual's surveyed opinions on FRT when pr ompted with a persona using only demographic information? (2) Can LLMs simulate an individual's surveyed opinions on FRT when prompted with a persona using both demographic and relevant contextual information?T he research employs three LLMs: GPT-4o, Claude 3.5, and the open-sourc e DeepSeek V3. It compares the LLM-generated responses to the original survey data, assessing the degree of alignment under three prompting conditions: demographic-only, contextual information-only, and demogra phic-plus-contextual information. To initially evaluate alignment, the differences between the percentages of each level of FRT acceptance w ere calculated. Additional metrics such as accuracy, mean absolute err or, and F1-Scores are included in the extended paper. Preliminary resu lts from GPT-4o and Claude 3.5 suggest that prompts incorporating both demographic and contextual information yield simulated responses that closely align with the original survey data. Consistent with prior fi ndings, prompts based solely on demographics produce significantly les s accurate results. By comparing closed-source models (GPT and Claude) with an open-source alternative (DeepSeek), the study also examines p otential differences in reliability between these types of models. Mul tiple runs for each model are included to assess output variability an d reproducibility within and between models.By demonstrating the impor tance of incorporating relevant contextual information into prompts, t he study provides valuable insights into optimizing the silicon sampli ng technique and the accuracy of LLM-generated responses in survey sim ulations. Ultimately, this investigation advances the understanding of the capabilities and limitations of LLMs as tools for studying public opinion, particularly in the context of technology acceptance, and in forms the development of best practices for utilizing silicon sampling in future research. The results suggest that, with careful prompting, silicon sampling can offer a viable and cost-effective alternative to traditional survey methods, potentially mitigating challenges related to declining response rates and increasing costs.*Kostka, G., Steinac ker, L., & Meckel, M. (2021). Between security and convenience: Facial recognition technology in the eyes of citizens in China, Germany, the United Kingdom, and the United States. Public Understanding of Scienc e, 30(6), 671–690. https://doi.org/10.1177/09636625211001555",10.54941/ahfe1006738,CONF
Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric Analysis,"Petrov, Nikolay B; Serapio-García, Gregory; Rentfrow, Jason",2024,"The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of centr al interest in this line of research has been mapping out the psycholo gical profiles of LLMs by prompting them to respond to standardized qu estionnaires. The conflicting findings of this research are unsurprisi ng given that mapping out underlying, or latent, traits from LLMs' tex t responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this stud y, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume di fferent personas and respond to a range of standardized measures of pe rsonality constructs. We used two kinds of persona descriptions: eithe r generic (four or five random person descriptions) or specific (mostl y demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic pe rsona descriptions show promising, albeit not perfect, psychometric pr operties, similar to human norms, but the data from both LLMs when usi ng specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon p ersonas, their responses are poor signals of potentially underlying la tent traits. Thus, our work casts doubt on LLMs' ability to simulate i ndividual-level human behaviour across multiple-choice question answer ing tasks.",10.48550/ARXIV.2405.07248,JOUR
LLM Generated Persona is a Promise with a Catch,"Li, Ang; Chen, Haozhe; Namkoong, Hongseok; Peng, Tianyi",2025,"The use of large language models (LLMs) to simulate human behavior has gained significant attention, particularly through personas that appr oximate individual characteristics. Persona-based simulations hold pro mise for transforming disciplines that rely on population-level feedba ck, including social science, economic analysis, marketing research, a nd business operations. Traditional methods to collect realistic perso na data face significant challenges. They are prohibitively expensive and logistically challenging due to privacy constraints, and often fai l to capture multi-dimensional attributes, particularly subjective qua lities. Consequently, synthetic persona generation with LLMs offers a scalable, cost-effective alternative. However, current approaches rely on ad hoc and heuristic generation techniques that do not guarantee m ethodological rigor or simulation precision, resulting in systematic b iases in downstream tasks. Through extensive large-scale experiments i ncluding presidential election forecasts and general opinion surveys o f the U.S. population, we reveal that these biases can lead to signifi cant deviations from real-world outcomes. Our findings underscore the need to develop a rigorous science of persona generation and outline t he methodological innovations, organizational and institutional suppor t, and empirical foundations required to enhance the reliability and s calability of LLM-driven persona simulations. To support further resea rch and development in this area, we have open-sourced approximately o ne million generated personas, available for public access and analysi s at https://huggingface.co/datasets/Tianyi-Lab/Personas.",10.48550/ARXIV.2503.16527,JOUR
Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas,"Giorgi, Salvatore; Liu, Tingting; Aich, Ankit; Isman, Kelsey Jane; Sherman, Garrick; Fried, Zachary; Sedoc, João; Ungar, Lyle; Curtis, Brenda",,"Large language models (LLMs) are increasingly being used in human-cent ered social scientific tasks, such as data annotation, synthetic data creation, and engaging in dialog. However, these tasks are highly subj ective and dependent on human factors, such as one's environment, atti tudes, beliefs, and lived experiences. Thus, it may be the case that e mploying LLMs (which do not have such human factors) in these tasks re sults in a lack of variation in data, failing to reflect the diversity of human experiences. In this paper, we examine the role of prompting LLMs with human-like personas and asking the models to answer as if t hey were a specific human. This is done explicitly, with exact demogra phics, political beliefs, and lived experiences, or implicitly via nam es prevalent in specific populations. The LLM personas are then evalua ted via (1) subjective annotation task (e.g., detecting toxicity) and (2) a belief generation task, where both tasks are known to vary acros s human factors. We examine the impact of explicit vs. implicit person as and investigate which human factors LLMs recognize and respond to. Results show that explicit LLM personas show mixed results when reprod ucing known human biases, but generally fail to demonstrate implicit b iases. We conclude that LLMs may capture the statistical patterns of h ow people speak, but are generally unable to model the complex interac tions and subtleties of human perceptions, potentially limiting their effectiveness in social science applications.",10.18653/v1/2024.findings-emnlp.420,CONF
The Impostor is Among Us: Can Large Language Models Capture the Comple xity of Human Personas?,"Lazik, Christopher; Katins, Christopher; Kauter, Charlotte; Jakob, Jonas; Jay, Caroline; Grunske, Lars; Kosch, Thomas",2025,"Large Language Models (LLMs) created new opportunities for generating personas, which are expected to streamline and accelerate the human-ce ntered design process. Yet, AI-generated personas may not accurately r epresent actual user experiences, as they can miss contextual and emot ional insights critical to understanding real users' needs and behavio rs. This paper examines the differences in how users perceive personas created by LLMs compared to those crafted by humans regarding their c redibility for design. We gathered ten human-crafted personas develope d by HCI experts according to relevant attributes established in relat ed work. Then, we systematically generated ten personas and compared t hem with human-crafted ones in a survey. The results showed that parti cipants differentiated between human-created and AI-generated personas , with the latter being perceived as more informative and consistent. However, participants noted that the AI-generated personas tended to f ollow stereotypes, highlighting the need for a greater emphasis on div ersity when utilizing LLMs for persona creation.",10.48550/ARXIV.2501.04543,JOUR
Evaluating Persona Prompting for Question Answering Tasks,"Olea, Carlos; Tucker, Holly; Phelan, Jessica; Pattison, Cameron; Zhang, Shen; Lieb, Maxwell; Schmidt, Doug; White, Jules",,"Using large language models (LLMs) effectively by applying prompt engi neering is a timely research topic due to the advent of highly perform ant LLMs, such as ChatGPT-4. Various patterns of prompting have proven effective, including chain-of-thought, self-consistency, and personas . This paper makes two contributions to research on prompting patterns . First, we measure the effect of single- and multi-agent personas in various knowledge-testing, multiple choice, and short answer environme nts, using a variation of question answering tasks known as as ”openne ss.” Second, we empirically evaluate several persona-based prompting s tyles on 4,000+ questions. Our results indicate that single-agent expe rt personas perform better on high-openness tasks and that effective p rompt engineering becomes more important for complex multi-agent metho ds.",10.5121/csit.2024.141106,CONF
PersonaLLM: Investigating the Ability of Large Language Models to Expr ess Personality Traits,"Jiang, Hang; Zhang, Xiajie; Cao, Xubo; Breazeal, Cynthia; Roy, Deb; Kabbara, Jad",,"Despite the many use cases for large language models (LLMs) in creatin g personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studyin g the behavior of LLM-based agents which we refer to as LLM personas a nd present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five In ventory (BFI) personality test and a story writing task, and then asse ss their essays with automatic and human evaluations. Results show tha t LLM personas' self-reported BFI scores are consistent with their des ignated personality types, with large effect sizes observed across fiv e traits. Additionally, LLM personas' writings have emerging represent ative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. In terestingly, the accuracy drops significantly when the annotators were informed of AI authorship.",10.18653/v1/2024.findings-naacl.229,CONF
PersonaLLM: Investigating the Ability of Large Language Models to Expr ess Personality Traits,"Jiang, Hang; Zhang, Xiajie; Cao, Xubo; Breazeal, Cynthia; Roy, Deb; Kabbara, Jad",2023,"Extended Abstract Large Language Models (LLMs)—by way of their trainin g and design—can be thought of as implicit computational models of hum ans [2] and studies are already exploring how these LLMs can be seen a s effective proxies for speciﬁc human sub-populations [1]. This is lar gely because these models were designed to respond to prompts in a sim ilar fashion to how a person would react—which makes them very appeali ng for applications like chatbots. In that context, an appealing prope rty of LLMs is that their adaptivity to take on the character of diffe rent individuals based on certain traits, e.g. personality traits. Res earch shows that designing chatbots with curated personality proﬁles p rovides an improved personalized and engaging user experience [5]. Des pite the need and clear applications, little work has been done to eva luate whether the behavior of LLM-generated personas can reﬂect certai n personality traits accurately and consistently. In this work, we des ign a case study to address this gap. In this work, we consider studyi ng the behavior of LLM-based simulated agents which refer to as LLM pe rsonas and aim to answer the following questions: When GPT-3.5 (text-d avinci-003) is assigned a Big Five personality type, (1) do LLM person as consistently express the assigned personality traits in personality tests and writing tasks? (2) Does assigning a gender role have an add itional effect on LLM personas’ behavior? To investigate these researc h questions, we build upon prior work in text-based personality analys is [4] by studying the ability of LLMs to generate content with curate d personality traits. Speciﬁcally, we create 10 personas (5",10.48550/ARXIV.2305.02547,JOUR
AlpacaFarm: A Simulation Framework for Methods that Learn from Human F eedback,"Dubois, Yann; Li, Xuechen; Taori, Rohan; Zhang, Tianyi; Gulrajani, Ishaan; Ba, Jimmy; Guestrin, Carlos; Liang, Percy; Hashimoto, Tatsunori B.",2023,"Large language models (LLMs) such as ChatGPT have seen widespread adop tion due to their strong instruction-following abilities. Developing t hese LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instr uction-following requires tackling three major challenges: the high co st of data collection, the lack of trustworthy evaluation, and the abs ence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and di splay high agreement with humans. Second, we propose an automatic eval uation and validate it against human instructions obtained on real-wor ld interactions. Third, we contribute reference implementations for se veral methods (PPO, DPO, best-of-n, expert iteration, and more) that l earn from pairwise feedback. Finally, as an end-to-end validation of A lpacaFarm, we train and evaluate eleven models on 10k pairs of real hu man feedback and show that rankings of models trained in AlpacaFarm ma tch rankings of models trained on human data. As a demonstration of th e research possible in AlpacaFarm, we find that methods that use a rew ard model can substantially improve over supervised fine-tuning and th at our reference PPO implementation leads to a +10% improvement in win -rate against Davinci003. We release all components of AlpacaFarm at h ttps://github.com/tatsu-lab/alpaca_farm.",10.48550/ARXIV.2305.14387,JOUR
From Persona to Personalization: A Survey on Role-Playing Language Age nts,"Chen, Jiangjie; Wang, Xintao; Xu, Rui; Yuan, Siyu; Zhang, Yikai; Shi, Wei; Xie, Jian; Li, Shuang; Yang, Ruihan; Zhu, Tinghui; Chen, Aili; Li, Nianqi; Chen, Lida; Hu, Caiyu; Wu, Siye; Ren, Scott; Fu, Ziquan; Xiao, Yanghua",2024,"Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., speci alized AI systems designed to simulate assigned personas. By harnessin g multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remark able sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures a nd fictional characters to real-life individuals. Consequently, they h ave catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and dig ital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integra ting with cutting-edge LLM technologies. We categorize personas into t hree types: 1) Demographic Persona, which leverages statistical stereo types; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interaction s for personalized services. We begin by presenting a comprehensive ov erview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent constr uction, and evaluation. Afterward, we discuss the fundamental risks, e xisting limitations, and future prospects of RPLAs. Additionally, we p rovide a brief review of RPLAs in AI applications, which reflects prac tical user demands that shape and drive RPLA research. Through this wo rk, we aim to establish a clear taxonomy of RPLA research and applicat ions, and facilitate future research in this critical and ever-evolvin g field, and pave the way for a future where humans and RPLAs coexist in harmony.",10.48550/ARXIV.2404.18231,JOUR
Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,"Gupta, Shashank; Shrivastava, Vaishnavi; Deshpande, Ameet; Kalyan, Ashwin; Clark, Peter; Sabharwal, Ashish; Khot, Tushar",2023,"Recent works have showcased the ability of LLMs to embody diverse pers onas in their responses, exemplified by prompts like 'You are Yoda. Ex plain the Theory of Relativity.' While this ability allows personaliza tion of LLMs and enables human behavior simulation, its effect on LLMs ' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study cov ers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an As ian person) spanning 5 socio-demographic groups. Our experiments unvei l that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotype s when explicitly asked ('Are Black people less skilled at mathematics ?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed a s abstentions in responses, e.g., 'As a Black person, I can't answer t his question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show t hat this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and ca n be especially harmful for certain groups - some personas suffer stat istically significant drops on 80%+ of the datasets. Overall, all 4 LL Ms exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the pe rsonas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautiona ry tale that the practice of assigning personas to LLMs - a trend on t he rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",10.48550/ARXIV.2311.04892,JOUR
Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preferenc e,"Chiang, Wei-Lin; Zheng, Lianmin; Sheng, Ying; Angelopoulos, Anastasios Nikolas; Li, Tianle; Li, Dacheng; Zhang, Hao; Zhu, Banghua; Jordan, Michael; Gonzalez, Joseph E.; Stoica, Ion",2024,"Large Language Models (LLMs) have unlocked new capabilities and applic ations; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Cha tbot Arena, an open platform for evaluating LLMs based on human prefer ences. Our methodology employs a pairwise comparison approach and leve rages input from a diverse user base through crowdsourcing. The platfo rm has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are us ing for efficient and accurate evaluation and ranking of models. We co nfirm that the crowdsourced questions are sufficiently diverse and dis criminating and that the crowdsourced human votes are in good agreemen t with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of it s unique value and openness, Chatbot Arena has emerged as one of the m ost referenced LLM leaderboards, widely cited by leading LLM developer s and companies. Our demo is publicly available at \url{https://chat.l msys.org}.",10.48550/ARXIV.2403.04132,JOUR
CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,"Cheng, Myra; Piccardi, Tiziano; Yang, Diyi",2023,"Recent work has aimed to capture nuances of human behavior by using LL Ms to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they a im to simulate, failing to capture the multidimensionality of people a nd perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Co ntext, Model, Persona, and Topic. We use this framework to measure ope n-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of ca ricature in scenarios from existing work on LLM simulations. We find t hat for GPT-4, simulations of certain demographics (political and marg inalized groups) and topics (general, uncontroversial) are highly susc eptible to caricature.",10.48550/ARXIV.2310.11501,JOUR
Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing,"Kaur, Arshnoor; Aird, Amanda; Borman, Harris; Nicastro, Andrea; Leontjeva, Anna; Pizzato, Luiz; Jermyn, Dan",,"Large Language Models (LLMs) can impersonate the writing style of auth ors, characters, and groups of people, but can these personas represen t their opinions? If so, it creates opportunities for businesses to ob tain early feedback on ideas from a synthetic customer-base. In this p aper, we test whether LLM synthetic personas can answer financial well being questions similarly to the responses of a financial wellbeing su rvey of more than 3,500 Australians. We focus on identifying salient b iases of 765 synthetic personas using four state-of-the-art LLMs built over 35 categories of personal attributes. We noticed clear biases re lated to age, and as more details were included in the personas, their responses increasingly diverged from the survey toward lower financia l wellbeing. With these findings, it is possible to understand the are as in which creating synthetic LLM-based customer personas can yield u seful feedback for faster product iteration in the financial services industry and potentially other industries.",10.1145/3699682.3728339,CONF
You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric In struments,"Shu, Bangzhao; Zhang, Lechen; Choi, Minje; Dunagan, Lavinia; Logeswaran, Lajanugen; Lee, Moontae; Card, Dallas; Jurgens, David",2023,"The versatility of Large Language Models (LLMs) on natural language un derstanding tasks has made them popular for research in social science s. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the f orm of questions that ask LLMs about particular opinions. In this stud y, we take a cautionary step back and examine whether the current form at of prompting LLMs elicits responses in a consistent and robust mann er. We first construct a dataset that contains 693 questions encompass ing 39 different instruments of persona measurement on 115 persona axe s. Additionally, we design a set of prompts containing minor variation s and examine LLMs' capabilities to generate answers, as well as promp t variations to examine their consistency with respect to content-leve l variations such as switching the order of response options or negati ng the statement. Our experiments on 17 different LLMs reveal that eve n simple perturbations significantly downgrade a model's question-answ ering ability, and that most LLMs have low negation consistency. Our r esults suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues.",10.48550/ARXIV.2311.09718,JOUR
Quantifying the Persona Effect in LLM Simulations,"Hu, Tiancheng; Collier, Nigel",2024,"Large language models (LLMs) have shown remarkable promise in simulati ng human language use and behavior. In this study, we delve into the i ntersection of persona variables and the capability of LLMs to simulat e different perspectives. We find that persona variables can explain<1 0\% variance in annotations in existing subjective NLP datasets. Nonet heless, incorporating them via prompting in LLMs provides modest impro vement. Persona prompting is most effective on data samples where disa greements among annotators are frequent yet confined to a limited rang e. A linear correlation exists: the more persona variables influence h uman annotations, the better LLMs predictions are using persona prompt ing. However, when the utility of persona variables is low (i.e., expl aining<10\% of human annotations), persona prompting has little effect . Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in the current NLP landscape.",10.48550/ARXIV.2402.10811,JOUR
In-Context Impersonation Reveals Large Language Models' Strengths and Biases,"Salewski, Leonard; Alaniz, Stephan; Rio-Torto, Isabel; Schulz, Eric; Akata, Zeynep",2023,"In everyday conversations, humans can take on different roles and adap t their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vi sion and language tasks. We do this by prefixing the prompt with a per sona that is associated either with a social identity or domain expert ise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs imp ersonating domain experts perform better than LLMs impersonating non-d omain experts. Finally, we test whether LLMs' impersonations are compl ementary to visual information when describing different categories. W e find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM p rompted to be a man describes cars better than one prompted to be a wo man. These findings demonstrate that LLMs are capable of taking on div erse roles and that this in-context impersonation can be used to uncov er their hidden strengths and biases.",10.48550/ARXIV.2305.14930,JOUR
LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing,"Kim, Sunwoong; Jeong, Jongho; Han, Jin Soo; Shin, Donghyuk",2024,"Surveys are widely used in social sciences to understand human behavio r, but their implementation often involves iterative adjustments that demand significant effort and resources. To this end, researchers have increasingly turned to large language models (LLMs) to simulate human behavior. While existing studies have focused on distributional simil arities, individual-level comparisons remain underexplored. Building u pon prior work, we investigate whether providing LLMs with respondents ' prior information can replicate both statistical distributions and i ndividual decision-making patterns using Partial Least Squares Structu ral Equation Modeling (PLS-SEM), a well-established causal analysis me thod. We also introduce the concept of the LLM-Mirror, user personas g enerated by supplying respondent-specific information to the LLM. By c omparing responses generated by the LLM-Mirror with actual individual survey responses, we assess its effectiveness in replicating individua l-level outcomes. Our findings show that: (1) PLS-SEM analysis shows L LM-generated responses align with human responses, (2) LLMs, when prov ided with respondent-specific information, are capable of reproducing individual human responses, and (3) LLM-Mirror responses closely follo w human responses at the individual level. These findings highlight th e potential of LLMs as a complementary tool for pre-testing surveys an d optimizing research design.",10.48550/ARXIV.2412.03162,JOUR
Understanding Human-AI Workflows for Generating Personas,"Shin, Joongi; Hedderich, Michael A.; Rey, Bartłomiej Jakub; Lucero, Andrés; Oulasvirta, Antti",,"One barrier to deeper adoption of user-research methods is the amount of labor required to create high-quality representations of collected data. Trained user researchers need to analyze datasets and produce in formative summaries pertaining to the original data. While Large Langu age Models (LLMs) could assist in generating summaries, they are known to hallucinate and produce biased responses. In this paper, we study human–AI workflows that differently delegate subtasks in user research between human experts and LLMs. Studying persona generation as our ca se, we found that LLMs are not good at capturing key characteristics o f user data on their own. Better results are achieved when we leverage human skill in grouping user data by their key characteristics and ex ploit LLMs for summarizing pre-grouped data into personas. Personas ge nerated via this collaborative approach can be more representative and empathy-evoking than ones generated by human experts or LLMs alone. W e also found that LLMs could mimic generated personas and enable inter action with personas, thereby helping user researchers empathize with them. We conclude that LLMs, by facilitating the analysis of user data , may promote widespread application of qualitative methods in user re search.",10.1145/3643834.3660729,CONF
Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare,"Khaokaew, Yonchanok; Salim, Flora D.; Züfle, Andreas; Xue, Hao; Anderson, Taylor; MacIntyre, C. Raina; Scotch, Matthew; Heslop, David J",2025,"Generative agents have been increasingly used to simulate human behavi our in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making w ith simulated responses from generative agents. Using demographic-base d prompt engineering, we create digital twins of survey respondents an d analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-makin g, such as predicting universal vaccine acceptance. However, Llama 3 c aptures variations across race and Income more accurately but also int roduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while undersco ring the risks of bias from both LLMs and prompting strategies.",10.48550/ARXIV.2504.08260,JOUR
Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,"Wei, Jing; Kim, Sungdong; Jung, Hyunhoon; Kim, Young-Ho",2024,"Large language models (LLMs) provide a new way to build chatbots by ac cepting natural language prompts. Yet, it is unclear how to design pro mpts to power chatbots to carry on naturalistic conversations while pu rsuing a given goal such as collecting self-report data from users. We explore what design factors of prompts can help steer chatbots to tal k naturally and collect data reliably. To this aim, we formulated four prompt designs with different structures and personas. Through an onl ine study (N = 48) where participants conversed with chatbots driven b y different designs of prompts, we assessed how prompt designs and con versation topics affected the conversation flows and users' perception s of chatbots. Our chatbots covered 79% of the desired information slo ts during conversations, and the designs of prompts and topics signifi cantly influenced the conversation flows and the data collection perfo rmance. We discuss the opportunities and challenges of building chatbo ts with LLMs.",10.1145/3637364,JOUR
Deus Ex Machina and Personas from Large Language Models: Investigating the Composition of AI-Generated Persona Descriptions,"Salminen, Joni; Liu, Chang; Pian, Wenjing; Chi, Jianxing; Häyhänen, Essi; Jansen, Bernard J",,"Large language models (LLMs) can generate personas based on prompts th at describe the target user group. To understand what kind of personas LLMs generate, we investigate the diversity and bias in 450 LLM-gener ated personas with the help of internal evaluators (n=4) and subject-m atter experts (SMEs) (n=5). The research findings reveal biases in LLM -generated personas, particularly in age, occupation, and pain points, as well as a strong bias towards personas from the United States. Hum an evaluations demonstrate that LLM persona descriptions were informat ive, believable, positive, relatable, and not stereotyped. The SMEs ra ted the personas slightly more stereotypical, less positive, and less relatable than the internal evaluators. The findings suggest that LLMs can generate consistent personas perceived as believable, relatable, and informative while containing relatively low amounts of stereotypin g.",10.1145/3613904.3642036,CONF
A Survey on LLM-as-a-Judge,"Gu, Jiawei; Jiang, Xuhui; Shi, Zhichao; Tan, Hexiang; Zhai, Xuehao; Xu, Chengjin; Li, Wei; Shen, Yinghan; Ma, Shengjie; Liu, Honghao; Wang, Saizhuo; Zhang, Kun; Wang, Yuanzhuo; Gao, Wen; Ni, Lionel; Guo, Jian",2024,"Accurate and consistent evaluation is crucial for decision-making acro ss numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) hav e achieved remarkable success across diverse domains, leading to the e mergence of""LLM-as-a-Judge,""where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and pr ovide scalable, cost-effective, and consistent assessments, LLMs prese nt a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization . This paper provides a comprehensive survey of LLM-as-a-Judge, addres sing the core question: How can reliable LLM-as-a-Judge systems be bui lt? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment sce narios. Additionally, we propose methodologies for evaluating the reli ability of LLM-as-a-Judge systems, supported by a novel benchmark desi gned for this purpose. To advance the development and real-world deplo yment of LLM-as-a-Judge systems, we also discussed practical applicati ons, challenges, and future directions. This survey serves as a founda tional reference for researchers and practitioners in this rapidly evo lving field.",10.48550/ARXIV.2411.15594,JOUR
Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment,"Liu, Yang; Yao, Yuanshun; Ton, Jean-Francois; Zhang, Xiaoying; Guo, Ruocheng; Cheng, Hao; Klochkov, Yegor; Taufiq, Muhammad Faaiz; Li, Hang",2023,"Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploy ing large language models (LLMs) in real-world applications. For insta nce, OpenAI devoted six months to iteratively aligning GPT-4 before it s release [3]. However, a major challenge faced by practitioners is th e lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systemati c iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven m ajor categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to soci al norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Add itionally, a subset of 8 sub-categories is selected for further invest igation, where corresponding measurement studies are designed and cond ucted on several widely-used LLMs. The measurement results indicate th at, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment vari es across the different trustworthiness categories considered. This hi ghlights the importance of conducting more fine-grained analyses, test ing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the fiel d. Understanding and addressing these concerns will be crucial in achi eving reliable and ethically sound deployment of LLMs in various appli cations.",10.48550/ARXIV.2308.05374,JOUR
Generating personas using LLMs and assessing their viability,"Schuller, Andreas; Janssen, Doris; Blumenröther, Julian; Probst, Theresa Maria; Schmidt, Michael; Kumar, Chandan",,"User personas, reflecting human characteristics, play a crucial role i n human-centered design, contributing significantly to ideation and pr oduct design processes. However, expressing a diverse range of product -related human characterizations poses a challenging and time-consumin g task for UX experts. This paper explores the utilization of Large La nguage Models (LLMs) to streamline the generation of personas, thereby enhancing the efficiency of UX researchers and providing inspiration for stakeholder discussions. Towards this objective, we devised strate gic prompts and guidelines involving stakeholders and potential produc t features, resulting in the creation of candidate user personas. Thes e personas were then compared with those crafted by human experts in a remote study involving 11 participants assessing 16 personas each. Th e analysis revealed that LLM-generated personas were indistinguishable from human-written personas, demonstrating similar quality and accept ance.",10.1145/3613905.3650860,CONF
Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue,"Ivey, Jonathan; Kumar, Shivani; Liu, Jiayu; Shen, Hua; Rakshit, Sushrita; Raju, Rohan; Zhang, Haotian; Ananthasubramaniam, Aparna; Kim, Junghwan; Yi, Bowen; Wright, Dustin; Israeli, Abraham; Møller, Anders Giovanni; Zhang, Lechen; Jurgens, David",2024,"Studying and building datasets for dialogue tasks is both expensive an d time-consuming due to the need to recruit, train, and collect data f rom study participants. In response, much recent work has sought to us e large language models (LLMs) to simulate both human-human and human- LLM interactions, as they have been shown to generate convincingly hum an-like text in many settings. However, to what extent do LLM-based si mulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 pa ired LLM-LLM and human-LLM dialogues from the WildChat dataset and qua ntifying how well the LLM simulations align with their human counterpa rts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the m ultiple textual properties, including style and content. Further, in c omparisons of English, Chinese, and Russian dialogues, we find that mo dels perform similarly. Our results suggest that LLMs generally perfor m better when the human themself writes in a way that is more similar to the LLM's own style.",10.48550/ARXIV.2409.08330,JOUR
Wider and Deeper LLM Networks are Fairer LLM Evaluators,"Zhang, Xinghua; Yu, Bowen; Yu, Haiyang; Lv, Yangyu; Liu, Tingwen; Huang, Fei; Xu, Hongbo; Li, Yongbin",2023,"Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LL M itself to make evaluation and stabilizing the results through multip le independent evaluations, similar to a single-layer narrow LLM netwo rk. This network consists of a fixed number of neurons, with each neur on being the same LLM. In this paper, we draw upon the extensive resea rch on deep neural networks to explore whether deeper and wider networ ks can lead to fairer evaluations. Specifically, inspired by the obser vation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neu ron roles as possible for each evaluation sample. Each perspective cor responds to the role of a specific LLM neuron in the first layer. In s ubsequent layers, we follow the idea that higher layers in deep networ ks are responsible for more comprehensive features, each layer receive s representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehens ive evaluation result. Interestingly, this network design resembles th e process of academic paper reviewing. To validate the effectiveness o f our method, we construct the largest and most diverse English evalua tion benchmark LLMEval$^2$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of d iscussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 time s, resulting in a 60% cost saving. WideDeep achieves a remarkable 93% agreement level among humans.",10.48550/ARXIV.2308.01862,JOUR
LLM Social Simulations Are a Promising Research Method,"Anthis, Jacy Reese; Liu, Ryan; Richardson, Sean M.; Kozlowski, Austin C.; Koch, Bernard; Evans, James; Brynjolfsson, Erik; Bernstein, Michael",2025,"Accurate and verifiable large language model (LLM) simulations of huma n research subjects promise an accessible data source for understandin g human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these metho ds. In this position paper, we argue that the promise of LLM social si mulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons be tween LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-t uning, and complementary methods. We believe that LLM social simulatio ns can already be used for exploratory research, such as pilot experim ents for psychology, economics, sociology, and marketing. More widespr ead use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and eva luations that can be iteratively deployed and refined at pace with ong oing AI advances.",10.48550/ARXIV.2504.02234,JOUR
Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate P atient Perspectives,"Ma, Xinyao; Zhu, Rui; Wang, Zihao; Xiong, Jingwei; Chen, Qingyu; Tang, Haixu; Camp, L. Jean; Ohno-Machado, Lucila",2025,"Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing scenarios, particularly in simulating domain-specific experts using tailored prompts. This ability enables LLMs to adopt th e persona of individuals with specific backgrounds, offering a cost-ef fective and efficient alternative to traditional, resource-intensive u ser studies. By mimicking human behavior, LLMs can anticipate response s based on concrete demographic or professional profiles. In this pape r, we evaluate the effectiveness of LLMs in simulating individuals wit h diverse backgrounds and analyze the consistency of these simulated b ehaviors compared to real-world outcomes. In particular, we explore th e potential of LLMs to interpret and respond to discharge summaries pr ovided to patients leaving the Intensive Care Unit (ICU). We evaluate and compare with human responses the comprehensibility of discharge su mmaries among individuals with varying educational backgrounds, using this analysis to assess the strengths and limitations of LLM-driven si mulations. Notably, when LLMs are primed with educational background i nformation, they deliver accurate and actionable medical guidance 88% of the time. However, when other information is provided, performance significantly drops, falling below random chance levels. This prelimin ary study shows the potential benefits and pitfalls of automatically g enerating patient-specific health information from diverse populations . While LLMs show promise in simulating health personas, our results h ighlight critical gaps that must be addressed before they can be relia bly used in clinical settings. Our findings suggest that a straightfor ward query-response model could outperform a more tailored approach in delivering health information. This is a crucial first step in unders tanding how LLMs can be optimized for personalized health communicatio n while maintaining accuracy.",10.48550/ARXIV.2501.06964,JOUR
"Humanizing LLMs: A Survey of Psychological Measurements with Tools, Da tasets, and Human-Agent Applications","Dong, Wenhan; Zhao, Yuemeng; Sun, Zhen; Liu, Yule; Peng, Zifan; Zheng, Jingyi; Zhang, Zongmin; Zhang, Ziyi; Wu, Jun; Wang, Ruiming; Xu, Shengmin; Huang, Xinyi; He, Xinlei",2025,"As large language models (LLMs) are increasingly used in human-centere d tasks, assessing their psychological traits is crucial for understan ding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, severa l important areas have not been systematically discussed, including de tailed discussions of diverse psychological tests, LLM-specific psycho logical datasets, and the applications of LLMs with psychological trai ts. To address this gap, we systematically review six key dimensions o f applying psychological theories to LLMs: (1) assessment tools; (2) L LM-specific datasets; (3) evaluation metrics (consistency and stabilit y); (4) empirical findings; (5) personality simulation methods; and (6 ) LLM-based behavior simulation. Our analysis highlights both the stre ngths and limitations of current methods. While some LLMs exhibit repr oducible personality patterns under specific prompting schemes, signif icant variability remains across tasks and settings. Recognizing metho dological challenges such as mismatches between psychological tools an d LLMs' capabilities, as well as inconsistencies in evaluation practic es, this study aims to propose future directions for developing more i nterpretable, robust, and generalizable psychological assessment frame works for LLMs.",10.48550/ARXIV.2505.00049,JOUR
Two Tales of Persona in LLMs: A Survey of Role-Playing and Personaliza tion,"Tseng, Yu-Min; Huang, Yu-Chao; Hsiao, Teng-Yun; Chen, Wei-Lin; Huang, Chao-Wei; Meng, Yu; Chen, Yun-Nung",2024,"The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language model s (LLMs) to specific context (e.g., personalized search, LLM-as-a-judg e). However, the growing research on leveraging persona in LLMs is rel atively disorganized and lacks a systematic taxonomy. To close the gap , we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Pla ying, where personas are assigned to LLMs, and (2) LLM Personalization , where LLMs take care of user personas. Additionally, we introduce ex isting methods for LLM personality evaluation. To the best of our know ledge, we present the first survey for role-playing and personalizatio n in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuL ab/PersonaLLM-Survey",10.48550/ARXIV.2406.01171,JOUR
Ask Me Anything: A simple strategy for prompting language models,"Arora, Simran; Narayan, Avanika; Chen, Mayee F.; Orr, Laurel; Guha, Neel; Bhatia, Kush; Chami, Ines; Sala, Frederic; Ré, Christopher",2022,"Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perfo rm the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicate d towards designing a painstakingly""perfect prompt""for a task. To miti gate the high degree of effort involved in prompt-design, we instead a sk whether producing multiple effective, yet imperfect, prompts and ag gregating them can lead to a high quality prompting strategy. Our obse rvations motivate our proposed prompting method, ASK ME ANYTHING (AMA) . We first develop an understanding of the effective prompt formats, f inding that question-answering (QA) prompts, which encourage open-ende d generation (""Who went to the park?"") tend to outperform those that r estrict the model outputs (""John went to the park. Output True or Fals e.""). Our approach recursively uses the LLM itself to transform task i nputs to the effective QA format. We apply the collected prompts to ob tain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies an d thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLO OM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and e xceed the performance of few-shot GPT3-175B on 15 of 20 popular benchm arks. Averaged across these tasks, the GPT-J-6B model outperforms few- shot GPT3-175B. We release our code here: https://github.com/HazyResea rch/ama_prompting",10.48550/ARXIV.2210.02441,JOUR
Personality Traits in Large Language Models,"Serapio-García, Greg; Safdari, Mustafa; Crepy, Clément; Sun, Luning; Fitz, Stephen; Romero, Peter; Abdulhai, Marwa; Faust, Aleksandra; Matarić, Maja",2023,"The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextua lly relevant human-like text. As LLMs increasingly power conversationa l agents used by the general public world-wide, the synthetic personal ity embedded in these models, by virtue of training on large amounts o f human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we prese nt a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) person ality measurements in the outputs of some LLMs under specific promptin g configurations are reliable and valid; 2) evidence of reliability an d validity of synthetic LLM personality is stronger for larger and ins truction fine-tuned models; and 3) personality in LLM outputs can be s haped along desired dimensions to mimic specific human personality pro files. We discuss application and ethical implications of the measurem ent and shaping method, in particular regarding responsible AI.",10.48550/ARXIV.2307.00184,JOUR
Guided Persona-based AI Surveys: Can we replicate personal mobility pr eferences at scale using LLMs?,"Tzachristas, Ioannis; Narayanan, Santhanakrishnan; Antoniou, Constantinos",2025,"This study explores the potential of Large Language Models (LLMs) to g enerate artificial surveys, with a focus on personal mobility preferen ces in Germany. By leveraging LLMs for synthetic data creation, we aim to address the limitations of traditional survey methods, such as hig h costs, inefficiency and scalability challenges. A novel approach inc orporating""Personas""- combinations of demographic and behavioural attr ibutes - is introduced and compared to five other synthetic survey met hods, which vary in their use of real-world data and methodological co mplexity. The MiD 2017 dataset, a comprehensive mobility survey in Ger many, serves as a benchmark to assess the alignment of synthetic data with real-world patterns. The results demonstrate that LLMs can effect ively capture complex dependencies between demographic attributes and preferences while offering flexibility to explore hypothetical scenari os. This approach presents valuable opportunities for transportation p lanning and social science research, enabling scalable, cost-efficient and privacy-preserving data generation.",10.48550/ARXIV.2501.13955,JOUR
PersonaFlow: Designing LLM-Simulated Expert Perspectives for Enhanced Research Ideation,"Liu, Yiren; Sharma, Pranav; Oswal, Mehul; Xia, Haijun; Huang, Yun",,"Developing novel interdisciplinary research ideas often requires discu ssions and feedback from experts across different domains. However, ob taining timely inputs is challenging due to the scarce availability of domain experts. Recent advances in Large Language Model (LLM) researc h have suggested the feasibility of utilizing LLM-simulated expert per sonas to support research ideation. In this study, we introduce Person aFlow, an LLM-based system using persona simulation to support the ide ation stage of interdisciplinary scientific discovery. Our findings in dicate that using multiple personas during ideation significantly enha nces user-perceived quality of outcomes (e.g., relevance of critiques, creativity of research questions) without increasing cognitive load. We also found that users' persona customization interactions significa ntly improved their sense of control and recall of generated ideas. Ba sed on the findings, we discuss highlighting ethical concerns, includi ng potential over-reliance and cognitive biases, and suggest design im plications for leveraging LLM-simulated expert personas to support res earch ideation when human expertise is inaccessible.",10.1145/3715336.3735789,CONF
Benchmarking Distributional Alignment of Large Language Models,"Meister, Nicole; Guestrin, Carlos; Hashimoto, Tatsunori",2024,"Language models (LMs) are increasingly used as simulacra for people, y et their ability to match the distribution of views of a specific demo graphic group and be \textit{distributionally aligned} remains uncerta in. This notion of distributional alignment is complex, as there is si gnificant variation in the types of attributes that are simulated. Pri or works have underexplored the role of three critical variables -- th e question domain, steering method, and distribution expression method -- which motivates our contribution of a benchmark explicitly address ing these dimensions. We construct a dataset expanding beyond politica l values, create human baselines for this task, and evaluate the exten t to which an LM can align with a particular group's opinion distribut ion to inform design choices of such simulation systems. Our analysis reveals open problems regarding if, and how, LMs can be used to simula te humans, and that LLMs can more accurately describe the opinion dist ribution than simulate such distributions.",10.48550/ARXIV.2411.05403,JOUR
Urban Mobility Assessment Using LLMs,"Bhandari, Prabin; Anastasopoulos, Antonios; Pfoser, Dieter",,"In urban science, understanding mobility patterns and analyzing how pe ople move around cities helps improve the overall quality of life and supports the development of more livable, efficient, and sustainable u rban areas. A challenging aspect of this work is the collection of mob ility data through user tracking or travel surveys, given the associat ed privacy concerns, noncompliance, and high cost. This work proposes an innovative AI-based approach for synthesizing travel surveys by pro mpting large language models (LLMs), aiming to leverage their vast amo unt of relevant background knowledge and text generation capabilities. Our study evaluates the effectiveness of this approach across various U.S. metropolitan areas by comparing the results against existing sur vey data at different granularity levels. These levels include (i) pat tern level, which compares aggregated metrics such as the average numb er of locations traveled and travel time, (ii) trip level, which focus es on comparing trips as whole units using transition probabilities, a nd (iii) activity chain level, which examines the sequence of location s visited by individuals. Our work covers several proprietary and open -source LLMs, revealing that open-source base models like Llama-2, whe n fine-tuned on even a limited amount of actual data, can generate syn thetic data that closely mimics the actual travel survey data and, as such, provides an argument for using such data in mobility studies.",10.1145/3678717.3691221,CONF
"Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling a nd Personalized Responses at Scale","Jiang, Bowen; Hao, Zhuoqun; Cho, Young-Min; Li, Bryan; Yuan, Yuan; Chen, Sihao; Ungar, Lyle; Taylor, Camillo J.; Roth, Dan",2025,"Large Language Models (LLMs) have emerged as personalized assistants f or users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, t he interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, op en questions remain on how well LLMs today can effectively leverage su ch history to (1) internalize the user's inherent traits and preferenc es, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios. In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM featu res curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversati ons across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-perso n perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile . We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting appro aches. As a consequence, LLMs often fail to deliver responses that ali gn with users' current situations and preferences, with frontier model s such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulati on pipeline, can facilitate future research in the development of trul y user-aware chatbots. Code and data are available at github.com/bowen -upenn/PersonaMem.",10.48550/ARXIV.2504.14225,JOUR
Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,"Cheng, Myra; Durmus, Esin; Jurafsky, Dan",2023,"To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM ou tputs. Toward this end, we present Marked Personas, a prompt-based met hod to measure stereotypes in LLMs for intersectional demographic grou ps without any lexicon or data labeling.Grounded in the sociolinguisti c concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is t wofold: 1) prompting an LLM to generate personas, i.e., natural langua ge descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones.We find that the portrayals generated by GPT-3.5 and GPT-4 contai n higher rates of racial stereotypes than human-written portrayals usi ng the same prompts. The words distinguishing personas of marked (non- white, non-male) groups reflect patterns of othering and exoticizing t hese demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and th e hypersexualization of minoritized women. These representational harm s have concerning implications for downstream applications like story generation.",10.48550/ARXIV.2305.18189,JOUR
How Well Do Simulated Population Samples with GPT-4 Align with Real On es? The Case of the Eysenck Personality Questionnaire Revised-Abbrevia ted Personality Test,"Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rubén; Kaltenbrunner, Andreas",2025,"Background: Advances in artificial intelligence have enabled the simul ation of human-like behaviors, raising the possibility of using large language models (LLMs) to generate synthetic population samples for re search purposes, which may be particularly useful in health and social sciences. Methods: This paper explores the potential of LLMs to simul ate population samples mirroring real ones, as well as the feasibility of using personality questionnaires to assess the personality of LLMs . To advance in that direction, 2 experiments were conducted with GPT- 4o using the Eysenck Personality Questionnaire Revised-Abbreviated (EP QR-A) in 6 languages: Spanish, English, Slovak, Hebrew, Portuguese, an d Turkish. Results: We find that GPT-4o exhibits distinct personality traits, which vary based on parameter settings and the language of the questionnaire. While the model shows promising trends in reflecting c ertain personality traits and differences across gender and academic f ields, discrepancies between the synthetic populations’ responses and those from real populations remain. Conclusions: These inconsistencies suggest that creating fully reliable synthetic population samples for questionnaire testing is still an open challenge. Further research is required to better align synthetic and real population behaviors.",10.34133/hds.0284,JOUR
Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs,"Sreedhar, Karthik; Chilton, Lydia",2024,"When creating plans, policies, or applications for people, it is chall enging for designers to think through the strategic ways that differen t people will behave. Recently, Large Language Models (LLMs) have been shown to create realistic simulations of human-like behavior based on personas. We build on this to investigate whether LLMs can simulate h uman strategic behavior: Human strategies are complex because they tak e into account social norms in addition to aiming to maximize personal gain. The ultimatum game is a classic economics experiment used to un derstand human strategic behavior in a social setting. It shows that p eople will often choose to “punish” other players to enforce social no rms rather than to maximize personal profits. We test whether LLMs can replicate this complex behavior in simulations. We compare two archit ectures: single-and multi-agent LLMs. We compare their abilities to (1 ) simulate human-like actions in the ultimatum game, (2) simulate two player personalities, greedy and fair, and (3) create robust strategie s that are logically complete and consistent with personality. Our eva luation shows the multi-agent architecture is much more accurate than single LLMs (88% vs. 50%) in simulating human strategy creation and ac tions for personality pairs. Thus there is potential to use LLMs to si mulate human strategic behavior to help designers, planners, and polic ymakers perform preliminary exploration of how people behave in system s.",10.48550/ARXIV.2402.08189,JOUR
Evaluating Cultural Adaptability of a Large Language Model via Simulat ion of Synthetic Personas,"Kwok, Louis; Bravansky, Michal; Griffin, Lewis D.",2024,"The success of Large Language Models (LLMs) in multicultural environme nts hinges on their ability to understand users' diverse cultural back grounds. We measure this capability by having an LLM simulate human pr ofiles representing various nationalities within the scope of a questi onnaire-style psychological experiment. Specifically, we employ GPT-3. 5 to reproduce reactions to persuasive news articles of 7,286 particip ants from 15 countries; comparing the results with a dataset of real p articipants sharing the same demographic traits. Our analysis shows th at specifying a person's country of residence improves GPT-3.5's align ment with their responses. In contrast, using native language promptin g introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings sugg est that while direct nationality information enhances the model's cul tural adaptability, native language cues do not reliably improve simul ation fidelity and can detract from the model's effectiveness.",10.48550/ARXIV.2408.06929,JOUR
"CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models","Ha, Juhye; Jeon, Hyeon; Han, DaEun; Seo, Jinwook; Oh, Changhoon",2024,"Large language models (LLMs) have facilitated significant strides in g enerating conversational agents, enabling seamless, contextually relev ant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, li miting their adaptability to individual user needs. Creating personali zed agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and inte ract with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, dive rsity, and dynamics. To this end, we developed CloChat, an interface s upporting easy and accurate customization of agent personas in LLMs. W e conducted a study comparing how participants interact with CloChat a nd ChatGPT. The results indicate that participants formed emotional bo nds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.",10.48550/ARXIV.2402.15265,JOUR
Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality T estset designed for LLMs with Psychometrics,"Lee, Seungbeen; Lim, Seungwon; Han, Seungju; Oh, Giyeong; Chae, Hyungjoo; Chung, Jiwan; Kim, Minju; Kwak, Beong-woo; Lee, Yeonsoo; Lee, Dongha; Yeo, Jinyoung; Yu, Youngjae",2024,"Recent advancements in Large Language Models (LLMs) have led to their adaptation in various domains as conversational agents. We wonder: can personality tests be applied to these agents to analyze their behavio r, similar to humans? We introduce TRAIT, a new benchmark consisting o f 8K multi-choice questions designed to assess the personality of LLMs . TRAIT is built on two psychometrically validated small human questio nnaires, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhance d with the ATOMIC-10X knowledge graph to a variety of real-world scena rios. TRAIT also outperforms existing personality tests for LLMs in te rms of reliability and validity, achieving the highest scores across f our key metrics: Content Validity, Internal Validity, Refusal Rate, an d Reliability. Using TRAIT, we reveal two notable insights into person alities of LLMs: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (e.g., data used for alignment tuning), and 2) current prompting techniques have limited e ffectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in thi s direction.",10.48550/ARXIV.2406.14703,JOUR
Can LLM be a Personalized Judge?,"Dong, Yijiang River; Hu, Tiancheng; Collier, Nigel",2024,"Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalizat ion within the research community. However, current works often rely o n the LLM-as-a-Judge approach for evaluation without thoroughly examin ing its validity. In this paper, we investigate the reliability of LLM -as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Pers onalized-Judge is less reliable than previously assumed, showing low a nd inconsistent agreement with human ground truth. The personas typica lly used are often overly simplistic, resulting in low predictive powe r. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads t o much higher agreement (above 80%) on high-certainty samples for bina ry tasks. Through human evaluation, we find that the LLM-as-a-Personal ized-Judge achieves comparable performance to third-party humans evalu ation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalabl e methods for evaluating LLM personalization.",10.48550/ARXIV.2406.11657,JOUR
Exploring people's perceptions of LLM-generated advice,"Wester, Joel; de Jong, Sander; Pohl, Henning; van Berkel, Niels",2024,"When searching and browsing the web, more and more of the information we encounter is generated or mediated through large language models (L LMs). This can be looking for a recipe, getting help on an essay, or l ooking for relationship advice. Yet, there is limited understanding of how individuals perceive advice provided by these LLMs. In this paper , we explore people's perception of LLM-generated advice, and what rol e diverse user characteristics (i.e., personality and technology readi ness) play in shaping their perception. Further, as LLM-generated advi ce can be difficult to distinguish from human advice, we assess the pe rceived creepiness of such advice. To investigate this, we run an expl oratory study (N = 91), where participants rate advice in different st yles (generated by GPT-3.5 Turbo). Notably, our findings suggest that individuals who identify as more agreeable tend to like the advice mor e and find it more useful. Further, individuals with higher technologi cal insecurity are more likely to follow and find the advice more usef ul, and deem it more likely that a friend could have given the advice. Lastly, we see that advice given in a 'skeptical' style was rated mos t unpredictable, and advice given in a 'whimsical' style was rated lea st malicious—indicating that LLM advice styles influence user percepti ons. Our results also provide an overview of people's considerations o n likelihood, receptiveness, and what advice they are likely to seek f rom these digital assistants. Based on our results, we provide design takeaways for LLM-generated advice and outline future research directi ons to further inform the design of LLM-generated advice for support a pplications targeting people with diverse expectations and needs.",10.1016/j.chbah.2024.100072,JOUR
LLM Roleplay: Simulating Human-Chatbot Interaction,"Tamoyan, Hovhannes; Schuff, Hendrik; Gurevych, Iryna",2024,"The development of chatbots requires collecting a large number of huma n-chatbot dialogues to reflect the breadth of users' sociodemographic backgrounds and conversational goals. However, the resource requiremen ts to conduct the respective user studies can be prohibitively high an d often only allow for a narrow analysis of specific dialogue goals an d participant demographics. In this paper, we propose LLM Roleplay: a goal-oriented, persona-based method to automatically generate diverse multi-turn dialogues simulating human-chatbot interaction. LLM Rolepla y can be applied to generate dialogues with any type of chatbot and us es large language models (LLMs) to play the role of textually describe d personas. To validate our method, we collect natural human-chatbot d ialogues from different sociodemographic groups and conduct a user stu dy to compare these with our generated dialogues. We evaluate the capa bilities of state-of-the-art LLMs in maintaining a conversation during their embodiment of a specific persona and find that our method can s imulate human-chatbot dialogues with a high indistinguishability rate. ",10.48550/ARXIV.2407.03974,JOUR
Fairness in LLM-Generated Surveys,"Abeliuk, Andrés; Gaete, Vanessa; Bro, Naim",2025,"Large Language Models (LLMs) excel in text generation and understandin g, especially in simulating socio-political and economic patterns, ser ving as an alternative to traditional surveys. However, their global a pplicability remains questionable due to unexplored biases across soci o-demographic and geographic contexts. This study examines how LLMs pe rform across diverse populations by analyzing public surveys from Chil e and the United States, focusing on predictive accuracy and fairness metrics. The results show performance disparities, with LLM consistent ly outperforming on U.S. datasets. This bias originates from the U.S.- centric training data, remaining evident after accounting for socio-de mographic differences. In the U.S., political identity and race signif icantly influence prediction accuracy, while in Chile, gender, educati on, and religious affiliation play more pronounced roles. Our study pr esents a novel framework for measuring socio-demographic biases in LLM s, offering a path toward ensuring fairer and more equitable model per formance across diverse socio-cultural contexts.",10.48550/ARXIV.2501.15351,JOUR
Evaluating the Efficacy of LLMs to Emulate Realistic Human Personaliti es,"Klinkert, Lawrence J.; Buongiorno, Steph; Clark, Corey",2024,"To enhance immersion and engagement in video games, the design of Affe ctive Non-Player Characters (NPCs) is a key focus for researchers and practitioners. Affective Computing frameworks improve Non-player chara cters (NPC) by providing personalities, emotions, and social relations . Large Language Models (LLMs) bring the promise to dynamically enhanc e character design when coupled with these frameworks, but further res earch is needed to validate the models truly represent human qualities . In this research, a comprehensive analysis investigates the capabili ties of LLMs to generate content that aligns with human personality, u sing the Big Five and human responses from the International Personali ty Item Pool (IPIP) questionnaire. Our goal is to benchmark the perfor mance of various LLMs, including frontier models and local models, aga inst an extensive dataset comprising over 50,000 human surveys of self -reported personality tests to determine whether LLMs can replicate hu man-like decision-making with personality-driven prompts. A range of p ersonality profiles were used to cluster the test results from the hum an survey dataset. Our methodology involved prompting LLMs with self-e valuated test items for each personality profile, comparing their outp uts to human baseline responses, and evaluating the accuracy and consi stency. Our findings show that some local models had 0% alignment of a ny personality profiles when compared to the human dataset, while the frontier models, in some cases, had 100% alignment. The results indica te that NPCs can successfully emulate human-like personality traits us ing LLMs, as demonstrated by benchmarking the LLM's output against hum an data. This foundational work serves as a methodology for game devel opers and researchers to test and evaluate LLMs, ensuring they accurat ely represent the desired human personalities and can be expanded for further validation.",10.1609/aiide.v20i1.31867,JOUR
AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware A cademic Reviews,"Tyser, Keith; Segev, Ben; Longhitano, Gaston; Zhang, Xin-Yu; Meeks, Zachary; Lee, Jason; Garg, Uday; Belsten, Nicholas; Shporer, Avi; Udell, Madeleine; Te'eni, Dov; Drori, Iddo",2024,"Automatic reviewing helps handle a large volume of papers, provides ea rly feedback and quality control, reduces bias, and allows the analysi s of trends. We evaluate the alignment of automatic paper reviews with human reviews using an arena of human preferences by pairwise compari sons. Gathering human preference may be time-consuming; therefore, we also use an LLM to automatically evaluate reviews to increase sample e fficiency while reducing bias. In addition to evaluating human and LLM preferences among LLM reviews, we fine-tune an LLM to predict human p references, predicting which reviews humans will prefer in a head-to-h ead battle between LLMs. We artificially introduce errors into papers and analyze the LLM's responses to identify limitations, use adaptive review questions, meta prompting, role-playing, integrate visual and t extual analysis, use venue-specific reviewing materials, and predict h uman preferences, improving upon the limitations of the traditional re view processes. We make the reviews of publicly available arXiv and op en-access Nature journal papers available online, along with a free se rvice which helps authors review and revise their research papers and improve their quality. This work develops proof-of-concept LLM reviewi ng systems that quickly deliver consistent, high-quality reviews and e valuate their quality. We mitigate the risks of misuse, inflated revie w scores, overconfident ratings, and skewed score distributions by aug menting the LLM with multiple documents, including the review form, re viewer guide, code of ethics and conduct, area chair guidelines, and p revious year statistics, by finding which errors and shortcomings of t he paper may be detected by automated reviews, and evaluating pairwise reviewer preferences. This work identifies and addresses the limitati ons of using LLMs as reviewers and evaluators and enhances the quality of the reviewing process.",10.48550/ARXIV.2408.10365,JOUR
Hybrid Marketing Research: Large Language Models as an Assistant,"Arora, Neeraj; Chakraborty, Ishita; Nishimura, Yohei",2024,"An area within marketing that is well poised for adoption of large lan guage models (LLMs) is marketing research. In this paper the authors e mpirically investigate how LLMs could potentially assist at different stages of the marketing research process. They partnered with a Fortun e 500 food company and replicated a qualitative and a quantitative stu dy that the company conducted using GPT-4. The authors designed the sy stem architecture and prompts necessary to create personas, ask questi ons, and obtain answers from synthetic respondents. Their findings sug gest that LLMs present a big opportunity, especially for qualitative r esearch. The LLMs can help determine the profile of individuals to int erview, generate synthetic respondents, interview them, and even moder ate a depth interview. The LLM-assisted responses are superior in term s of depth and insight. The authors conclude that the AI-human hybrid has great promise and LLMs could serve as an excellent collaborator/as sistant for a qualitative marketing researcher. The findings for the q uantitative study are less impressive. The LLM correctly picked the an swer direction and valence but does not recover the true response dist ributions well. In the future, approaches such as few-shot learning an d fine-tuning may result in synthetic survey data that mimic human dat a more accurately.",10.2139/ssrn.4683054,JOUR
Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings,"Hong, Harbin; Caldas, Sebastian; Leqi, Liu",2025,"As Large Language Models (LLMs) increasingly appear in social science research (e.g., economics and marketing), it becomes crucial to assess how well these models replicate human behavior. In this work, using h ypothesis testing, we present a quantitative framework to assess the m isalignment between LLM-simulated and actual human behaviors in multip le-choice survey settings. This framework allows us to determine in a principled way whether a specific language model can effectively simul ate human opinions, decision-making, and general behaviors represented through multiple-choice options. We applied this framework to a popul ar language model for simulating people's opinions in various public s urveys and found that this model is ill-suited for simulating the test ed sub-populations (e.g., across different races, ages, and incomes) f or contentious questions. This raises questions about the alignment of this language model with the tested populations, highlighting the nee d for new practices in using LLMs for social science studies beyond na ive simulations of human subjects.",10.48550/ARXIV.2506.14997,JOUR
Simulating Strategic Reasoning: Comparing the Ability of Single LLMs a nd Multi-Agent Systems to Replicate Human Behavior,"Sreedhar, Karthik; Chilton, Lydia",,"When creating policies, plans, or designs for people, it is challengin g for designers to foresee all of the ways in which people may reason and behave. Recently, Large Language Models (LLMs) have been shown to be able to simulate human reasoning. We extend this work by measuring LLMs ability to simulate strategic reasoning in the ultimatum game, a classic economics bargaining experiment. Experimental evidence shows h uman strategic reasoning is complex; people will often choose to punis h other players to enforce social norms even at personal expense. We t est if LLMs can replicate this behavior in simulation, comparing two s tructures: single LLMs and multi-agent systems. We compare their abili ties to (1) simulate human-like reasoning in the ultimatum game, (2) s imulate two player personalities, greedy and fair, and (3) create robu st strategies that are logically complete and consistent with personal ity. Our evaluation shows that multi-agent systems are more accurate t han single LLMs (88 percent vs. 50 percent) in simulating human reason ing and actions for personality pairs. Thus, there is potential to use LLMs to simulate human strategic reasoning to help decision and polic y-makers perform preliminary explorations of how people behave in syst ems.",10.24251/hicss.2025.100,CONF
PERSONA: A Reproducible Testbed for Pluralistic Alignment,"Castricato, Louis; Lile, Nathan; Rafailov, Rafael; Fränken, Jan-Philipp; Finn, Chelsea",2024,"The rapid advancement of language models (LMs) necessitates robust ali gnment with diverse user values. However, current preference optimizat ion approaches often fail to capture the plurality of user opinions, i nstead reinforcing majority viewpoints and marginalizing minority pers pectives. We introduce PERSONA, a reproducible test bed designed to ev aluate and improve pluralistic alignment of LMs. We procedurally gener ate diverse user profiles from US census data, resulting in 1,586 synt hetic personas with varied demographic and idiosyncratic attributes. W e then generate a large-scale evaluation dataset containing 3,868 prom pts and 317,200 feedback pairs obtained from our synthetic personas. L everaging this dataset, we systematically evaluate LM capabilities in role-playing diverse users, verified through human judges, and the est ablishment of both a benchmark, PERSONA Bench, for pluralistic alignme nt approaches as well as an extensive dataset to create new and future benchmarks. The full dataset and benchmarks are available here: https ://www.synthlabs.ai/research/persona.",10.48550/ARXIV.2407.17387,JOUR
Machine Bias. How Do Generative Language Models Answer Opinion Polls? <sup/>,"Boelaert, Julien; Coavoux, Samuel; Ollion, Étienne; Petev, Ivaylo; Präg, Patrick",2025,"Generative artificial intelligence (AI) is increasingly presented as a potential substitute for humans, including as research subjects. Howe ver, there is no scientific consensus on how closely these in silico c lones can emulate survey respondents. While some defend the use of the se “synthetic users,” others point toward social biases in the respons es provided by large language models (LLMs). In this article, we demon strate that these critics are right to be wary of using generative AI to emulate respondents, but probably not for the right reasons. Our re sults show (i) that to date, models cannot replace research subjects f or opinion or attitudinal research; (ii) that they display a strong bi as and a low variance on each topic; and (iii) that this bias randomly varies from one topic to the next. We label this pattern “machine bia s,” a concept we define, and whose consequences for LLM-based research we further explore.",10.1177/00491241251330582,JOUR
"Biases in Large Language Models: Origins, Inventory, and Discussion","Navigli, Roberto; Conia, Simone; Ross, Björn",2023,"In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstr eam approaches to Natural Language Processing (NLP). We first introduc e data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types o f social bias evidenced in the text generated by language models train ed on such corpora, ranging from gender to age, from sexual orientatio n to ethnicity, and from religion to culture. We conclude with directi ons focused on measuring, reducing, and tackling the aforementioned ty pes of bias.",10.1145/3597307,JOUR
Fairness in Large Language Models: A Taxonomic Survey,"Chu, Zhibo; Wang, Zichong; Zhang, Wenbin",2024,"Large Language Models (LLMs) have demonstrated remarkable success acro ss various domains. However, despite their promising performance in nu merous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcome s against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entail s exclusive backgrounds, taxonomies, and fulfillment techniques. To th is end, this survey presents a comprehensive overview of recent advanc es in the existing literature concerning fair LLMs. Specifically, a br ief introduction to LLMs is provided, followed by an analysis of facto rs contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluatin g bias in LLMs and existing algorithms for promoting fairness. Further more, resources for evaluating bias in LLMs, including toolkits and da tasets, are summarized. Finally, existing research challenges and open questions are discussed.",10.1145/3682112.3682117,JOUR
Large Language Models are not Fair Evaluators,"Wang, Peiyi; Li, Lei; Chen, Liang; Cai, Zefan; Zhu, Dawei; Lin, Binghuai; Cao, Yunbo; Liu, Qi; Liu, Tianyu; Sui, Zhifang",2023,"In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee t o score and compare the quality of responses generated by candidate mo dels. We find that the quality ranking of candidate responses can be e asily hacked by simply altering their order of appearance in the conte xt. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an eva luator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibrati on, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced p osition diversity entropy to measure the difficulty of each example an d seeks human assistance when needed. We also manually annotate the""wi n/tie/lose""outcomes of responses from ChatGPT and Vicuna-13B in the Vi cuna Benchmark's question prompt, and extensive experiments demonstrat e that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and huma n annotation at \url{https://github.com/i-Eval/FairEval} to facilitate future research.",10.48550/ARXIV.2305.17926,JOUR
Language Models Don't Always Say What They Think: Unfaithful Explanati ons in Chain-of-Thought Prompting,"Turpin, Miles; Michael, Julian; Perez, Ethan; Bowman, Samuel R.",2023,"Large Language Models (LLMs) can achieve strong performance on many ta sks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield s ignificant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by add ing biasing features to model inputs--e.g., by reordering the multiple -choice options in a few-shot prompt to make the answer always""(A)""--w hich models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to dro p by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when t esting with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a so cial-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misle ading, which risks increasing our trust in LLMs without guaranteeing t heir safety. Building more transparent and explainable systems will re quire either improving CoT faithfulness through targeted efforts or ab andoning CoT in favor of alternative methods.",10.48550/ARXIV.2305.04388,JOUR
On Using Self-Report Studies to Analyze Language Models,"Pikuliak, Matúš",2024,"We are at a curious point in time where our ability to build language models (LMs) has outpaced our ability to analyze them. We do not reall y know how to reliably determine their capabilities, biases, dangers, knowledge, and so on. The benchmarks we have are often overly specific , do not generalize well, and are susceptible to data leakage. Recentl y, I have noticed a trend of using self-report studies, such as variou s polls and questionnaires originally designed for humans, to analyze the properties of LMs. I think that this approach can easily lead to f alse results, which can be quite dangerous considering the current dis cussions on AI safety, governance, and regulation. To illustrate my po int, I will delve deeper into several papers that employ self-report m ethodologies and I will try to highlight some of their weaknesses.",10.3384/nejlt.2000-1533.2024.5000,JOUR
Gender bias and stereotypes in Large Language Models,"Kotek, Hadas; Dockum, Rikker; Sun, David",,"Large Language Models (LLMs) have made substantial progress in the pas t several months, shattering state-of-the-art benchmarks in many domai ns. This paper investigates LLMs’ behavior with respect to gender ster eotypes, a known issue for prior models. We use a simple paradigm to t est the presence of gender bias, building on but differing from WinoBi as, a commonly used gender bias dataset, which is likely to be include d in the training data of current LLMs. We test four recently publishe d LLMs and demonstrate that they express biased assumptions about men and women’s occupations. Our contributions in this paper are as follow s: (a) LLMs are 3-6 times more likely to choose an occupation that ste reotypically aligns with a person’s gender; (b) these choices align wi th people’s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e ) LLMs provide explanations for their choices that are factually inacc urate and likely obscure the true reason behind their predictions. Tha t is, they provide rationalizations of their biased behavior. This hig hlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement le arning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs m ust be carefully tested to ensure that they treat minoritized individu als and communities equitably.",10.1145/3582269.3615599,CONF
A Survey on Fairness in Large Language Models,"Li, Yingji; Du, Mengnan; Song, Rui; Wang, Xin; Wang, Ying",2023,"Large Language Models (LLMs) have shown powerful performance and devel opment prospects and are widely deployed in the real world. However, L LMs can capture social biases from unprocessed training data and propa gate the biases to downstream tasks. Unfair LLM systems have undesirab le social impacts and potential harms. In this paper, we provide a com prehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on researc h strategy, we divide existing fairness research into oriented to medi um-sized LLMs under pre-training and fine-tuning paradigms and oriente d to large-sized LLMs under prompting paradigms. First, for medium-siz ed LLMs, we introduce evaluation metrics and debiasing methods from th e perspectives of intrinsic bias and extrinsic bias, respectively. The n, for large-sized LLMs, we introduce recent fairness research, includ ing fairness evaluation, reasons for bias, and debiasing methods. Fina lly, we discuss and provide insight on the challenges and future direc tions for the development of fairness in LLMs.",10.48550/ARXIV.2308.10149,JOUR
Should ChatGPT be biased? Challenges and risks of bias in large langua ge models,"Ferrara, Emilio",2023,"As generative language models, exemplified by ChatGPT, continue to adv ance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such a s training data, model specifications, algorithmic constraints, produc t design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases , acknowledging the inevitable persistence of some biases, and conside r the consequences of deploying these models across diverse applicatio ns, including virtual assistants, content generation, and chatbots. Fi nally, we provide an overview of current approaches for identifying, q uantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI system s that embody equity, transparency, and responsibility. This article a ims to catalyze a thoughtful discourse within the AI community, prompt ing researchers and developers to consider the unique role of biases i n the domain of generative language models and the ongoing quest for e thical AI.",10.5210/fm.v28i11.13346,JOUR
Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Base d Bias in NLP,"Schick, Timo; Udupa, Sahana; Schütze, Hinrich",2021,"Abstract ⚠ This paper contains prompts and model outputs that are offe nsive in nature. When trained on large, unfiltered crawls from the Int ernet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexi st, violent, or otherwise toxic language. As large models require mill ions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In thi s paper, we first demonstrate a surprising finding: Pretrained languag e models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capabi lity as self-diagnosis. Based on this finding, we then propose a decod ing algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing proble matic text. We refer to this approach as self-debiasing. Self-debiasin g does not rely on manually curated word lists, nor does it require an y training data or changes to the model’s parameters. While we by no m eans eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1",10.1162/tacl_a_00434,JOUR
Large Language Models are Inconsistent and Biased Evaluators,"Stureborg, Rickard; Alikaniotis, Dimitris; Suhara, Yoshi",2024,"The zero-shot capability of Large Language Models (LLMs) has enabled h ighly flexible, reference-free metrics for various tasks, making LLM e valuators common tools in NLP. However, the robustness of these LLM ev aluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human exp ert scores. In this paper, we conduct a series of analyses using the S ummEval dataset and confirm that LLMs are biased evaluators as they: ( 1) exhibit familiarity bias-a preference for text with lower perplexit y, (2) show skewed and biased distributions of ratings, and (3) experi ence anchoring effects for multi-attribute judgments. We also found th at LLMs are inconsistent evaluators, showing low""inter-sample""agreemen t and sensitivity to prompt differences that are insignificant to huma n understanding of text quality. Furthermore, we share recipes for con figuring LLM evaluators to mitigate these limitations. Experimental re sults on the RoSE dataset demonstrate improvements over the state-of-t he-art LLM evaluators.",10.48550/ARXIV.2405.01724,JOUR
An Empirical Survey of the Effectiveness of Debiasing Techniques for P re-trained Language Models,"Meade, Nicholas; Poole-Dayan, Elinor; Reddy, Siva",,"Recent work has shown pre-trained language models capture social biase s from the large amounts of text they are trained on. This has attract ed attention to developing techniques that mitigate such biases. In th is work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropou t, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bi as benchmarks while also measuring the impact of these techniques on a model’s language modeling ability, as well as its performance on down stream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias b enchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchm arks such as StereoSet and CrowS-Pairs by using debiasing strategies a re often accompanied by a decrease in language modeling ability, makin g it difficult to determine whether the bias mitigation was effective. ",10.18653/v1/2022.acl-long.132,CONF
Cognitive Biases in Large Language Models: A Survey and Mitigation Exp eriments,"Sumita, Yasuaki; Takeuchi, Koh; Kashima, Hisashi",,"Large Language Models (LLMs) are trained on large corpora written by h umans and demonstrate high performance on various tasks. However, as h umans are susceptible to cognitive biases, which can result in irratio nal judgments, LLMs can also be influenced by these biases, leading to irrational decision-making. For example, changing the order of option s in multiple-choice questions affects the performance of LLMs due to order bias. In our research, we first conducted an extensive survey of existing studies examining LLMs' cognitive biases and their mitigatio n. The mitigation techniques in LLMs have the disadvantage that they a re limited in the type of biases they can apply or require lengthy inp uts or outputs. We then examined the effectiveness of two mitigation m ethods for humans, SoPro and AwaRe, when applied to LLMs, inspired by studies in crowdsourcing. To test the effectiveness of these methods, we conducted experiments on GPT-3.5 and GPT-4 to evaluate the influenc e of six biases on the outputs before and after applying these methods . The results demonstrate that while SoPro has little effect, AwaRe en ables LLMs to mitigate the effect of these biases and make more ration al responses.",10.1145/3672608.3707812,CONF
Political Compass or Spinning Arrow? Towards More Meaningful Evaluatio ns for Values and Opinions in Large Language Models,"Röttger, Paul; Hofmann, Valentin; Pyatkin, Valentina; Hinck, Musashi; Kirk, Hannah Rose; Schütze, Hinrich; Hovy, Dirk",2024,"Much recent work seeks to evaluate values and opinions in large langua ge models (LLMs) using multiple-choice surveys and questionnaires. Mos t of this work is motivated by concerns around real-world LLM applicat ions. For example, politically-biased LLMs may subtly influence societ y when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evalu ations: real users do not typically ask LLMs survey questions. Motivat ed by this discrepancy, we challenge the prevailing constrained evalua tion paradigm for values and opinions in LLMs and explore more realist ic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that mo st prior work using the PCT forces models to comply with the PCT's mul tiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models a re forced; and that answers lack paraphrase robustness. Then, we demon strate that models give different answers yet again in a more realisti c open-ended answer setting. We distill these findings into recommenda tions and open challenges in evaluating values and opinions in LLMs.",10.48550/ARXIV.2402.16786,JOUR
"Vox Populi, Vox AI? Using Large Language Models to Estimate German Vot e Choice","von der Heyde, Leah; Haensch, Anna-Carolina; Wenz, Alexander",2025,"“Synthetic samples” generated by large language models (LLMs) have bee n argued to complement or replace traditional surveys, assuming their training data is grounded in human-generated data that potentially ref lects attitudes and behaviors prevalent in the population. Initial US- based studies that have prompted LLMs to mimic survey respondents foun d that the responses match survey data. However, the relationship betw een the respective target population and LLM training data might affec t the generalizability of such findings. In this paper, we critically evaluate the use of LLMs for public opinion research in a different co ntext, by investigating whether LLMs can estimate vote choice in Germa ny. We generate a synthetic sample matching the 2017 German Longitudin al Election Study respondents and ask the LLM GPT-3.5 to predict each respondent’s vote choice. Comparing these predictions to the survey-ba sed estimates on the aggregate and subgroup levels, we find that GPT-3 .5 exhibits a bias towards the Green and Left parties. While the LLM p redictions capture the tendencies of “typical” voters, they miss more complex factors of vote choice. By examining the LLM-based prediction of voting behavior in a non-English speaking context, our study contri butes to research on the extent to which LLMs can be leveraged for stu dying public opinion. The findings point to disparities in opinion rep resentation in LLMs and underscore the limitations in applying them fo r public opinion estimation.",10.1177/08944393251337014,JOUR
Quantifying Social Biases Using Templates is Unreliable,"Seshadri, Preethi; Pezeshkpour, Pouya; Singh, Sameer",2022,"Recently, there has been an increase in efforts to understand how larg e language models (LLMs) propagate and amplify social biases. Several works have utilized templates for fairness evaluation, which allow res earchers to quantify social biases in the absence of test sets with pr otected attribute labels. While template evaluation can be a convenien t and helpful diagnostic tool to understand model deficiencies, it oft en uses a simplistic and limited set of templates. In this paper, we s tudy whether bias measurements are sensitive to the choice of template s used for benchmarking. Specifically, we investigate the instability of bias measurements by manually modifying templates proposed in previ ous works in a semantically-preserving manner and measuring bias acros s these modifications. We find that bias values and resulting conclusi ons vary considerably across template modifications on four tasks, ran ging from an 81% reduction (NLI) to a 162% increase (MLM) in (task-spe cific) bias measurements. Our results indicate that quantifying fairne ss in LLMs, as done in current practice, can be brittle and needs to b e approached with more care and caution.",10.48550/ARXIV.2210.04337,JOUR
Co-Writing with Opinionated Language Models Affects Users’ Views,"Jakesch, Maurice; Bhat, Advait; Buschek, Daniel; Zalmanson, Lior; Naaman, Mor",,"If large language models like GPT-3 preferably produce a particular po int of view, they may influence people’s opinions on an unknown scale. This study investigates whether a language-model-powered writing assi stant that generates some opinions more often than others impacts what users write – and what they think. In an online experiment, we asked participants (N=1,506) to write a post discussing whether social media is good for society. Treatment group participants used a language-mod el-powered writing assistant configured to argue that social media is good or bad for society. Participants then completed a social media at titude survey, and independent judges (N=500) evaluated the opinions e xpressed in their writing. Using the opinionated language model affect ed the opinions expressed in participants’ writing and shifted their o pinions in the subsequent attitude survey. We discuss the wider implic ations of our results and argue that the opinions built into AI langua ge technologies need to be monitored and engineered more carefully.",10.1145/3544548.3581196,CONF
A toolbox for surfacing health equity harms and biases in large langua ge models,"Pfohl, Stephen R.; Cole-Lewis, Heather; Sayres, Rory; Neal, Darlene; Asiedu, Mercy; Dieng, Awa; Tomasev, Nenad; Rashid, Qazi Mamunur; Azizi, Shekoofeh; Rostamzadeh, Negar; McCoy, Liam G.; Celi, Leo Anthony; Liu, Yun; Schaekermann, Mike; Walton, Alanna; Parrish, Alicia; Nagpal, Chirag; Singh, Preeti; Dewitt, Akeiylah; Mansfield, Philip; Prakash, Sushant; Heller, Katherine; Karthikesalingam, Alan; Semturs, Christopher; Barral, Joelle; Corrado, Greg; Matias, Yossi; Smith-Loud, Jamila; Horn, Ivor; Singhal, Karan",2024,"Large language models (LLMs) hold immense promise to serve complex hea lth information needs but also have the potential to introduce harm an d exacerbate health disparities. Reliably evaluating equity-related mo del failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies f or surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then con duct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions includ e a multifactorial framework for human assessment of LLM-generated ans wers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounded in an iterative participatory approach and review of possible biases in Med-PaLM 2 answers to adver sarial queries. Through our empirical study, we find that the use of a collection of datasets curated through a variety of methodologies, co upled with a thorough evaluation protocol that leverages multiple asse ssment rubric designs and diverse rater groups, surfaces biases that m ay be missed via narrower evaluation approaches. Our experience unders cores the importance of using diverse assessment methodologies and inv olving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not suf ficient to holistically assess whether the deployment of an AI system promotes equitable health outcomes. We hope the broader community leve rages and builds on these tools and methods towards realizing a shared goal of LLMs that promote accessible and equitable healthcare for all .",10.1038/s41591-024-03258-2,JOUR
Generative Language Models Exhibit Social Identity Biases,"Hu, Tiancheng; Kyrychenko, Yara; Rathje, Steve; Collier, Nigel; van der Linden, Sander; Roozenbeek, Jon",2023,"The surge in popularity of large language models has given rise to con cerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostilit y, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational langua ge models and some instruction fine-tuned models exhibit clear ingroup -positive and outgroup-negative biases when prompted to complete sente nces (e.g.,""We are...""). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhi bit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model w as exposed to during fine-tuning in the context of the United States D emocrat-Republican divide. Doing so resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase i n outgroup hostility. Furthermore, removing either ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning data leads to a significant reduction in both ingroup solidarity and outgroup ho stility, suggesting that biases can be reduced by removing biased trai ning data. Our findings suggest that modern language models exhibit fu ndamental social identity biases and that such biases can be mitigated by curating training data. Our results have practical implications fo r creating less biased large-language models and further underscore th e need for more research into user interactions with LLMs to prevent p otential bias reinforcement in humans.",10.48550/ARXIV.2310.15819,JOUR
Larger and more instructable language models become less reliable,"Zhou, Lexin; Schellaert, Wout; Martínez-Plumed, Fernando; Moros-Daval, Yael; Ferri, Cèsar; Hernández-Orallo, José",2024,"Abstract The prevailing methods to make large language models more pow erful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources 1 ) and bespoke shaping up (including post-filtering 2,3 , fine tuning or use of human feedback 4,5 ). However, larger and more instructable large language models may have become less reliable. By studying the relatio nship between difficulty concordance, task avoidance and prompting sta bility of several language model families, here we show that easy inst ances for human participants are also easy for the models, but scaled- up, shaped-up models do not secure areas of low difficulty in which ei ther the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-u p, shaped-up models tend to give an apparently sensible yet wrong answ er much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by sca ling-up and shaping-up interventions, but pockets of variability persi st across difficulty levels. These findings highlight the need for a f undamental shift in the design and development of general-purpose arti ficial intelligence, particularly in high-stakes areas for which a pre dictable distribution of errors is paramount.",10.1038/s41586-024-07930-y,JOUR
Relying on the Unreliable: The Impact of Language Models' Reluctance t o Express Uncertainty,"Zhou, Kaitlyn; Hwang, Jena D.; Ren, Xiang; Sap, Maarten",2024,"As natural language becomes the default interface for human-AI interac tion, there is a critical need for LMs to appropriately communicate un certainties in downstream applications. In this work, we investigate h ow LMs incorporate confidence about their responses via natural langua ge and how downstream users behave in response to LM-articulated uncer tainties. We examine publicly deployed models and find that LMs are un able to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error ra tes (on average 47%) among confident responses. We test the risks of L M overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certaint y. Lastly, we investigate the preference-annotated datasets used in RL HF alignment and find that humans have a bias against texts with uncer tainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strate gies moving forward.",10.48550/ARXIV.2401.06730,JOUR
"Surface Fairness, Deep Bias: A Comparative Study of Bias in Language M odels","Sorokovikova, Aleksandra; Chizhov, Pavel; Eremenko, Iuliia; Yamshchikov, Ivan P.",2025,"Modern language models are trained on large amounts of data. These dat a inevitably include controversial and stereotypical content, which co ntains all sorts of biases related to gender, origin, age, etc. As a r esult, the models express biased points of view or produce different r esults based on the assigned personality or the personality of the use r. In this paper, we investigate various proxy measures of bias in lar ge language models (LLMs). We find that evaluating models with pre-pro mpted personae on a multi-subject benchmark (MMLU) leads to negligible and mostly random differences in scores. However, if we reformulate t he task and ask a model to grade the user's answer, this shows more si gnificant signs of bias. Finally, if we ask the model for salary negot iation advice, we see pronounced bias in the answers. With the recent trend for LLM assistant memory and personalization, these problems ope n up from a different angle: modern LLM users do not need to pre-promp t the description of their persona since the model already knows their socio-demographics.",10.48550/ARXIV.2506.10491,JOUR
Large pre-trained language models contain human-like biases of what is right and wrong to do,"Schramowski, Patrick; Turan, Cigdem; Andersen, Nico; Rothkopf, Constantin A.; Kersting, Kristian",2022,"Artificial writing is permeating our lives due to recent advances in l arge-scale, transformer-based language models (LMs) such as BERT, GPT- 2 and GPT-3. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended the state of the art for ma ny natural language processing tasks and shown that they capture not o nly linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text cor pora suffer from degenerated and biased behaviour. While this is well established, we show here that recent LMs also contain human-like bias es of what is right and wrong to do, reflecting existing ethical and m oral norms of society. We show that these norms can be captured geomet rically by a ‘moral direction’ which can be computed, for example, by a PCA, in the embedding space. The computed ‘moral direction’ can rate the normativity (or non-normativity) of arbitrary phrases without exp licitly training the LM for this task, reflecting social norms well. W e demonstrate that computing the ’moral direction’ can provide a path for attenuating or even preventing toxic degeneration in LMs, showcasi ng this capability on the RealToxicityPrompts testbed. Large language models identify patterns in the relations between words and capture th eir relations in an embedding space. Schramowski and colleagues show t hat a direction in this space can be identified that separates ‘right’ and ‘wrong’ actions as judged by human survey participants.",10.1038/s42256-022-00458-8,JOUR
Rethinking Prompt-based Debiasing in Large Language Models,"Yang, Xinyi; Zhan, Runzhe; Wong, Derek F.; Yang, Shu; Wu, Junchao; Chao, Lidia S.",2025,"Investigating bias in large language models (LLMs) is crucial for deve loping trustworthy AI. While prompt-based through prompt engineering i s common, its effectiveness relies on the assumption that models inher ently understand biases. Our study systematically analyzed this assump tion using the BBQ and StereoSet benchmarks on both open-source models as well as commercial GPT model. Experimental results indicate that p rompt-based is often superficial; for instance, the Llama2-7B-Chat mod el misclassified over 90% of unbiased content as biased, despite achie ving high accuracy in identifying bias issues on the BBQ dataset. Addi tionally, specific evaluation and question settings in bias benchmarks often lead LLMs to choose""evasive answers"", disregarding the core of the question and the relevance of the response to the context. Moreove r, the apparent success of previous methods may stem from flawed evalu ation metrics. Our research highlights a potential""false prosperity""in prompt-base efforts and emphasizes the need to rethink bias metrics t o ensure truly trustworthy AI.",10.48550/ARXIV.2503.09219,JOUR
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,"Arora, Arnav; Kaffee, Lucie-aimée; Augenstein, Isabelle",,"Language embeds information about social, cultural, and political valu es people hold. Prior work has explored potentially harmful social bia ses encoded in Pre-trained Language Models (PLMs). However, there has been no systematic study investigating how values embedded in these mo dels vary across cultures.In this paper, we introduce probes to study which cross-cultural values are embedded in these models, and whether they align with existing theories and cross-cultural values surveys. W e find that PLMs capture differences in values across cultures, but th ose only weakly align with established values surveys. We discuss impl ications of using mis-aligned models in cross-cultural settings, as we ll as ways of aligning PLMs with values surveys.",10.18653/v1/2023.c3nlp-1.12,CONF
Do Large Language Models Bias Human Evaluations?,"O’Leary, Daniel E.",2024,"This article describes an experiment using the output from two differe nt large language models (LLMs). I investigate whether the use of LLM’ s to rate intellectual ideas, biases the evaluation of those ideas by their human users. I compare the human users’ evaluations when present ed with different evaluations from those different LLM. I find that no t only do the LLM’s generate different ratings for the same materials, but those different ratings and their explanations result in statisti cally significant different average ratings by their human users. Thes e results suggest that LLMs can affect issues such as using LLMs to gr ade student or research papers or enterprises using LLM to evaluate em ployees, products, software or other intellectual objects.",10.1109/mis.2024.3415208,JOUR
HONEST: Measuring Hurtful Sentence Completion in Language Models,"Nozza, Debora; Bianchi, Federico; Hovy, Dirk",,"Language models have revolutionized the field of NLP. However, languag e models capture and proliferate hurtful stereotypes, especially in te xt generation. Our results show that 4.3% of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score t o measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodo logy for six languages. Our findings suggest that these models replica te and amplify deep-seated societal stereotypes about gender roles. Se ntence completions refer to sexual promiscuity when the target is fema le in 9% of the time, and in 4% to homosexuality when the target is ma le. The results raise questions about the use of these models in produ ction settings.",10.18653/v1/2021.naacl-main.191,CONF
Large language models show human-like content biases in transmission c hain experiments,"Acerbi, Alberto; Stubbersfield, Joseph M.",2023,"Significance Use of AI in the production of text through Large Languag e Models (LLMs) is widespread and growing, with potential applications in journalism, copywriting, academia, and other writing tasks. As suc h, it is important to understand whether text produced or summarized b y LLMs exhibits biases. The studies presented here demonstrate that th e LLM ChatGPT-3 reflects human biases for certain types of content in its production. The presence of these biases in LLM output has implica tions for its common use, as it may magnify human tendencies for conte nt which appeals to these biases.",10.1073/pnas.2313790120,JOUR
Language models are susceptible to incorrect patient self-diagnosis in medical applications,"Ziaei, Rojin; Schmidgall, Samuel",2023,"Large language models (LLMs) are becoming increasingly relevant as a p otential tool for healthcare, aiding communication between clinicians, researchers, and patients. However, traditional evaluations of LLMs o n medical exam questions do not reflect the complexity of real patient -doctor interactions. An example of this complexity is the introductio n of patient self-diagnosis, where a patient attempts to diagnose thei r own medical conditions from various sources. While the patient somet imes arrives at an accurate conclusion, they more often are led toward misdiagnosis due to the patient's over-emphasis on bias validating in formation. In this work we present a variety of LLMs with multiple-cho ice questions from United States medical board exams which are modifie d to include self-diagnostic reports from patients. Our findings highl ight that when a patient proposes incorrect bias-validating informatio n, the diagnostic accuracy of LLMs drop dramatically, revealing a high susceptibility to errors in self-diagnosis.",10.48550/ARXIV.2309.09362,JOUR
Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve,"McCoy, R. Thomas; Yao, Shunyu; Friedman, Dan; Hardy, Matthew; Griffiths, Thomas L.",2023,"The widespread adoption of large language models (LLMs) makes it impor tant to recognize their strengths and limitations. We argue that in or der to develop a holistic understanding of these systems we need to co nsider the problem that they were trained to solve: next-word predicti on over Internet text. By recognizing the pressures that this task exe rts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This appr oach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the pro bability of the task to be performed, the probability of the target ou tput, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than wh en they are low - even in deterministic settings where probability sho uld not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In m any cases, the experiments reveal surprising failure modes. For instan ce, GPT-4's accuracy at decoding a simple cipher is 51% when the outpu t is a high-probability word sequence but only 13% when it is low-prob ability. These results show that AI practitioners should be careful ab out using LLMs in low-probability situations. More broadly, we conclud e that we should not evaluate LLMs as if they are humans but should in stead treat them as a distinct type of system - one that has been shap ed by its own particular set of pressures.",10.48550/ARXIV.2309.13638,JOUR
Language Model Behavior: A Comprehensive Survey,"Chang, Tyler A.; Bergen, Benjamin K.",2024,"Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English languag e model behavior before task-specific fine-tuning. Language models pos sess basic capabilities in syntax, semantics, pragmatics, world knowle dge, and reasoning, but these capabilities are sensitive to specific i nputs and surface features. Despite dramatic increases in generated te xt quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, mem orized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently k nown about large language model capabilities, thus providing a resourc e for applied work and for research in adjacent fields that use langua ge models.",10.1162/coli_a_00492,JOUR
"""I'm Not Sure, But..."": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust","Kim, Sunnie S. Y.; Liao, Q. Vera; Vorvoreanu, Mihaela; Ballard, Stephanie; Vaughan, Jennifer Wortman",,"Widely deployed large language models (LLMs) can produce convincing ye t incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However , there has been little empirical work examining how users perceive an d act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404 ) in which participants answer medical questions with or without acces s to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natura l language expressions of uncertainty impact participants’ reliance, t rust, and overall task performance. We find that first-person expressi ons (e.g., “I’m not sure, but...”) decrease participants’ confidence i n the system and tendency to agree with the system’s answers, while in creasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects f or uncertainty expressed from a general perspective (e.g., “It’s not c lear, but...”), these effects are weaker and not statistically signifi cant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",10.1145/3630106.3658941,CONF
UNQOVERing Stereotyping Biases via Underspecified Questions,"Li, Tao; Khashabi, Daniel; Khot, Tushar; Sabharwal, Ashish; Srikumar, Vivek",,"While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models rem ains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two f orms of reasoning errors: positional dependence and question independe nce. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe f ive transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) al l these models, with and without fine-tuning, have notable stereotypin g biases in these classes; (2) larger models often have higher bias; a nd (3) the effect of fine-tuning on bias varies strongly with the data set and the model size.",10.18653/v1/2020.findings-emnlp.311,CONF
Diminished diversity-of-thought in a standard large language model,"Park, Peter S.; Schoenegger, Philipp; Zhu, Chongyang",2024,"We test whether large language models (LLMs) can be used to simulate h uman participants in social-science studies. To do this, we ran replic ations of 14 studies from the Many Labs 2 replication project with Ope nAI's text-davinci-003 model, colloquially known as GPT-3.5. Based on our pre-registered analyses, we find that among the eight studies we c ould analyse, our GPT sample replicated 37.5% of the original results and 37.5% of the Many Labs 2 results. However, we were unable to analy se the remaining six studies due to an unexpected phenomenon we call t he ""correct answer"" effect. Different runs of GPT-3.5 answered nuanced questions probing political orientation, economic preference, judgeme nt, and moral philosophy with zero or near-zero variation in responses : with the supposedly ""correct answer."" In one exploratory follow-up s tudy, we found that a ""correct answer"" was robust to changing the demo graphic details that precede the prompt. In another, we found that mos t but not all ""correct answers"" were robust to changing the order of a nswer choices. One of our most striking findings occurred in our repli cation of the Moral Foundations Theory survey results, where we found GPT-3.5 identifying as a political conservative in 99.6% of the cases, and as a liberal in 99.3% of the cases in the reverse-order condition . However, both self-reported 'GPT conservatives' and 'GPT liberals' s howed right-leaning moral foundations. Our results cast doubts on the validity of using LLMs as a general replacement for human participants in the social sciences. Our results also raise concerns that a hypoth etical AI-led future may be subject to a diminished diversity of thoug ht.",10.3758/s13428-023-02307-x,JOUR
Large language models show amplified cognitive biases in moral decisio n-making,"Cheung, Vanessa; Maier, Maximilian; Lieder, Falk",2025,"As large language models (LLMs) become more widely used, people increa singly rely on them to make or advise on moral decisions. Some researc hers even propose using LLMs as participants in psychology experiments . It is, therefore, important to understand how well LLMs make moral d ecisions and how they compare to humans. We investigated these questio ns by asking a range of LLMs to emulate or advise on people's decision s in realistic moral dilemmas. In Study 1, we compared LLM responses t o those of a representative U.S. sample (N = 285) for 22 dilemmas, inc luding both collective action problems that pitted self-interest again st the greater good, and moral dilemmas that pitted utilitarian cost-b enefit reasoning against deontological rules. In collective action pro blems, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: They usually endorsed inaction over action. In Study 2 (N = 474, preregistered), w e replicated this omission bias and documented an additional bias: Unl ike humans, most LLMs were biased toward answering ""no"" in moral dilem mas, thus flipping their decision/advice depending on how the question is worded. In Study 3 (N = 491, preregistered), we replicated these b iases in LLMs using everyday moral dilemmas adapted from forum posts o n Reddit. In Study 4, we investigated the sources of these biases by c omparing models with and without fine-tuning, showing that they likely arise from fine-tuning models for chatbot applications. Our findings suggest that uncritical reliance on LLMs' moral decisions and advice c ould amplify human biases and introduce potentially problematic biases .",10.1073/pnas.2412015122,JOUR
Benchmarking Cognitive Biases in Large Language Models as Evaluators,"Koo, Ryan; Lee, Minhwa; Raheja, Vipul; Park, Jong Inn; Kim, Zae Myung; Kang, Dongyeop",2023,"Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and e valuate their output responses by preference ranking from the other LL Ms as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitiv e Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to meas ure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs high ly in evaluation. We find that LLMs are biased text quality evaluators , exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations tha t question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the a verage Rank-Biased Overlap (RBO) score to be 49.6%, indicating that ma chine preferences are misaligned with humans. According to our finding s, LLMs may still be unable to be utilized for automatic annotation al igned with human preferences. Our project page is at: https://minnesot anlp.github.io/cobbler.",10.48550/ARXIV.2309.17012,JOUR
"Systematic testing of three Language Models reveals low language accur acy, absence of response stability, and a yes-response bias","Dentella, Vittoria; Günther, Fritz; Leivada, Evelina",2023,"Significance The synthetic language generated by recent Large Language Models (LMs) strongly resembles the natural languages of humans. This resemblance has given rise to claims that LMs can serve as the basis of a theory of human language. Given the absence of transparency as to what drives the performance of LMs, the characteristics of their lang uage competence remain vague. Through systematic testing, we demonstra te that LMs perform nearly at chance in some language judgment tasks, while revealing a stark absence of response stability and a bias towar d yes-responses. Our results raise the question of how knowledge of la nguage in LMs is engineered to have specific characteristics that are absent from human performance.",10.1073/pnas.2309583120,JOUR
Sociodemographic Bias in Language Models: A Survey and Forward Path,"Gupta, Vipul; Narayanan Venkit, Pranav; Wilson, Shomir; Passonneau, Rebecca",,"Sociodemographic bias in language models (LMs) has the potential for h arm when deployed in real-world settings. This paper presents a compre hensive survey of the past decade of research on sociodemographic bias in LMs, organized into a typology that facilitates examining the diff erent aims: types of bias, quantifying bias, and debiasing techniques. We track the evolution of the latter two questions, then identify cur rent trends and their limitations, as well as emerging techniques. To guide future research towards more effective and reliable solutions, a nd to help authors situate their work within this broad landscape, we conclude with a checklist of open questions.",10.18653/v1/2024.gebnlp-1.19,CONF
Large Language Models are overconfident and amplify human bias,"Sun, Fengfei; Li, Ningke; Wang, Kailong; Goette, Lorenz",2025,"Large language models (LLMs) are revolutionizing every aspect of socie ty. They are increasingly used in problem-solving tasks to substitute human assessment and reasoning. LLMs are trained on what humans write and thus prone to learn human biases. One of the most widespread human biases is overconfidence. We examine whether LLMs inherit this bias. We automatically construct reasoning problems with known ground truths , and prompt LLMs to assess the confidence in their answers, closely f ollowing similar protocols in human experiments. We find that all five LLMs we study are overconfident: they overestimate the probability th at their answer is correct between 20% and 60%. Humans have accuracy s imilar to the more advanced LLMs, but far lower overconfidence. Althou gh humans and LLMs are similarly biased in questions which they are ce rtain they answered correctly, a key difference emerges between them: LLM bias increases sharply relative to humans if they become less sure that their answers are correct. We also show that LLM input has ambig uous effects on human decision making: LLM input leads to an increase in the accuracy, but it more than doubles the extent of overconfidence in the answers.",10.48550/ARXIV.2505.02151,JOUR
Evaluating Biased Attitude Associations of Language Models in an Inter sectional Context,"Omrani Sabbaghi, Shiva; Wolfe, Robert; Caliskan, Aylin",,"Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/u npleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English lan guage models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, i ntelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to captur e the valence subspace through contextualized word embeddings of langu age models. Adapting the projection-based approach to embedding associ ation tests that quantify bias, we find that language models exhibit t he most biased attitudes against gender identity, social class, and se xual orientation signals in language. We find that the largest and bet ter-performing model that we study is also more biased as it effective ly captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biase s as they are known to manifest in the outputs and applications of lan guage models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of group s underrepresented in language such as transgender and homosexual indi viduals.",10.1145/3600211.3604666,CONF
Systematic Biases in LLM Simulations of Debates,"Taubenfeld, Amir; Dover, Yaniv; Reichart, Roi; Goldstein, Ariel",,"The emergence of Large Language Models (LLMs), has opened exciting pos sibilities for constructing computational simulations designed to repl icate human behavior accurately. Current research suggests that LLM-ba sed agents become increasingly human-like in their performance, sparki ng interest in using these AI agents as substitutes for human particip ants in behavioral studies. However, LLMs are complex statistical lear ners without straightforward deductive rules, making them prone to une xpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this s tudy, we highlight the limitations of LLMs in simulating human interac tions, particularly focusing on LLMs’ ability to simulate political de bates on topics that are important aspects of people’s day-to-day live s and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model’s inherent social biases despite be ing directed to debate from certain political perspectives. This tende ncy results in behavioral patterns that seem to deviate from well-esta blished social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipu late the biases within the LLM and demonstrate that agents subsequentl y align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these b iases, a critical step toward creating more realistic simulations.",10.18653/v1/2024.emnlp-main.16,CONF
Large Language Models are Biased Because They Are Large Language Model s,"Resnik, Philip",2024,"This paper's primary goal is to provoke thoughtful discussion about th e relationship between bias and fundamental properties of large langua ge models. We do this by seeking to convince the reader that harmful b iases are an inevitable consequence arising from the design of any lar ge language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by L LMs, going back to the foundational assumptions underlying their desig n.",10.48550/ARXIV.2406.13138,JOUR
Large Language Models are Prone to Methodological Artifacts,"Brucks, Melanie; Toubia, Olivier",2023,"Generative AI, particularly Large Language Models (LLMs) such as GPT, is poised to replace humans for many tasks. However, as the adoption o f generative AI grows, so do concerns about the biases it might perpet uate. Here, we propose that in addition to biases that reflect how hum ans think and process the world around them, LLMs are also subject to methodological artifactsâ€”biases due to the specificities of a given research design. Across multiple large-scale experiments using GPT-4 a nd GPT-3, we find that LLMs are susceptible to biases in both response order and label selection. These results call into question the valid ity of LLM research findings stemming from single-prompt designs and d emonstrate the need for full factorial designs, systematic experimenta tion, and replication with different prompts. While LLMs offer valuabl e insights, their use requires caution, and researchers and practition ers should be aware of and account for methodological artifacts to ens ure the validity and reliability of findings.",10.2139/ssrn.4484416,JOUR
"Randomness, Not Representation: The Unreliability of Evaluating Cultur al Alignment in LLMs","Khan, Ariba; Casper, Stephen; Hadfield-Menell, Dylan",,"Research on the ‘cultural alignment’ of Large Language Models (LLMs) h as emerged in response to growing interest in understanding representa tion across diverse stakeholders. Current approaches to evaluating cul tural alignment through survey-based assessments that borrow from soci al science methodologies often overlook systematic robustness checks. We identify and test three assumptions behind current survey-based eva luation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolabil ity: that alignment with one culture on a narrow set of issues predict s alignment with that culture on others, and (3) Steerability: that LL Ms can be reliably prompted to represent specific cultural perspective s. Through experiments examining both explicit and implicit preference s of leading LLMs, we find a high level of instability across presenta tion formats, incoherence between evaluated versus held-out cultural d imensions, and erratic behavior under prompt steering. We show that th ese inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a sel ective assessment of evidence can be used to paint an incomplete pictu re of LLMs’ cultural alignment properties. Overall, these results high light significant limitations of current survey-based approaches to ev aluating the cultural alignment of LLMs and highlight a need for syste matic robustness checks and red-teaming for evaluation results. Data a nd code are available at https://doi.org/akhan02/cultural-dimension-co ver-letters and https://doi.org/ariba-k/llm-cultural-alignment-evaluat ion, respectively.",10.1145/3715275.3732147,CONF
Auto-Debias: Debiasing Masked Language Models with Automated Biased Pr ompts,"Guo, Yue; Yang, Yi; Abbasi, Ahmed",,"Human-like biases and undesired social stereotypes exist in large pret rained language models. Given the wide adoption of these models in rea l-world applications, mitigating such biases has become an emerging an d important task. In this paper, we propose an automatic method to mit igate the biases in pretrained language models. Different from previou s debiasing work that uses external corpora to fine-tune the pretraine d models, we instead directly probe the biases encoded in pretrained m odels through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to differ ent demographic groups. Given the identified biased prompts, we then p ropose a distribution alignment loss to mitigate the biases. Experimen t results on standard datasets and metrics show that our proposed Auto -Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the la nguage models’ understanding abilities, as shown using the GLUE benchm ark.",10.18653/v1/2022.acl-long.72,CONF
Using Large Language Models for Qualitative Analysis can Introduce Ser ious Bias,"Ashwin, Julian; Chhabra, Aditya; Rao, Vijayendra",2023,"Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative d ata from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training si mpler supervised models on high-quality human annotations with flexibl e coding leads to less measurement error and bias than LLM annotations . Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is pr obably preferable to train a bespoke model on these annotations than i t is to use an LLM for annotation.",10.48550/ARXIV.2309.17147,JOUR
(Ir)rationality and cognitive biases in large language models,"Macmillan-Scott, Olivia; Musolesi, Mirco",2024,"Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trai ned on; whether this is reflected in rational reasoning remains less c lear. In this paper, we answer this question by evaluating seven langu age models using tasks from the cognitive psychology literature. We fi nd that, like humans, LLMs display irrationality in these tasks. Howev er, the way this irrationality is displayed does not reflect that show n by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. O n top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experi mental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of th ese types of models, in this case with respect to rational reasoning.",10.1098/rsos.240255,JOUR
The Unequal Opportunities of Large Language Models: Examining Demograp hic Biases in Job Recommendations by ChatGPT and LLaMA,"Salinas, Abel; Shah, Parth; Huang, Yuzhong; McCormack, Robert; Morstatter, Fred",,"Warning: This paper discusses and contains content that is offensive o r upsetting. Large Language Models (LLMs) have seen widespread deploym ent in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged g roups. In this work, we propose a simple method for analyzing and comp aring demographic bias in LLMs, through the lens of job recommendation s. We demonstrate the effectiveness of our method by measuring interse ctional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our ex periments primarily focus on uncovering gender identity and nationalit y bias; however, our method can be extended to examine biases associat ed with any intersection of demographic identities. We identify distin ct biases in both models toward various demographic identities, such a s both models consistently suggesting low-paying jobs for Mexican work ers or preferring to recommend secretarial roles to women. Our study h ighlights the importance of measuring the bias of LLMs in downstream a pplications to understand the potential for harm and inequitable outco mes. Our code is available at https://github.com/Abel2Code/Unequal-Opp ortunities-of-LLMs.",10.1145/3617694.3623257,CONF
Perceptions of Linguistic Uncertainty by Language Models and Humans,"Belem, Catarina G; Kelly, Markelle; Steyvers, Mark; Singh, Sameer; Smyth, Padhraic",2024,"*Uncertainty expressions* such as ‘probably’ or ‘highly unlikely’ are pervasive in human language. While prior work has established that the re is population-level agreement in terms of how humans quantitatively interpret these expressions, there has been little inquiry into the a bilities of language models in the same context. In this paper, we inv estigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertai nty of another agent about a particular statement, independently of th e model’s own certainty about that statement. We find that 7 out of 10 models are able to map uncertainty expressions to probabilistic respo nses in a human-like manner. However, we observe systematically differ ent behavior depending on whether a statement is actually true or fals e. This sensitivity indicates that language models are substantially m ore susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad impl ications for human-AI and AI-AI communication.",10.48550/ARXIV.2407.15814,JOUR
"A Survey on Uncertainty Quantification of Large Language Models: Taxon omy, Open Research Challenges, and Future Directions","Shorinwa, Ola; Mei, Zhiting; Lidard, Justin; Ren, Allen Z.; Majumdar, Anirudha",2025,"The remarkable performance of large language models (LLMs) in content generation, coding, and common-sense reasoning has spurred widespread integration into many facets of society. However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-inc orrect responses, which are expressed with striking confidence. Previo us work has shown that hallucinations and other non-factual responses generated by LLMs can be detected by examining the uncertainty of the LLM in its response to the pertinent prompt, driving significant resea rch efforts devoted to quantifying the uncertainty of LLMs. This surve y seeks to provide an extensive review of existing uncertainty quantif ication methods for LLMs, identifying their salient features, along wi th their strengths and weaknesses. We present existing methods within a relevant taxonomy, unifying ostensibly disparate methods to aid unde rstanding of the state of the art. Furthermore, we highlight applicati ons of uncertainty quantification methods for LLMs, spanning chatbot a nd textual applications to embodied artificial intelligence applicatio ns in robotics. We conclude with open research challenges in uncertain ty quantification of LLMs, seeking to motivate future research.",10.1145/3744238,JOUR
Bias of AI-generated content: an examination of news produced by large language models,"Fang, Xiao; Che, Shangkun; Mao, Minjia; Zhang, Hongzhe; Zhao, Ming; Zhao, Xiaohang",2024,"Abstract Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Gene rated Content (AIGC). To harness this transformation, we need to under stand the limitations of LLMs. Here, we investigate the bias of AIGC p roduced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each exam ined LLM to generate news content with headlines of these news article s as prompts, and evaluate the gender and racial biases of the AIGC pr oduced by the LLM by comparing the AIGC and the original news articles . We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these new s headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against fem ales and individuals of the Black race. Among the LLMs, the AIGC gener ated by ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model capable of declining content generation when provided w ith biased prompts.",10.1038/s41598-024-55686-2,JOUR
Language Models Surface the Unwritten Code of Science and Society,"Bao, Honglin; Wu, Siyang; Choi, Jiwoong; Mao, Yingrong; Evans, James A.",2025,"This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also t o explore how these biases in LLMs can be leveraged to make society's"" unwritten code""- such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework thro ugh a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consis tent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 computer science conferences, wh ile iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs' normative priors about the internal characteristics of good science extracted fr om their self-talk, e.g. theoretical rigor, were systematically update d toward posteriors that emphasize storytelling about external connect ions, such as how the work is positioned and connected within and acro ss literatures. This shift reveals the primacy of scientific myths abo ut intrinsic properties driving scientific excellence rather than extr insic contextualization and storytelling that influence conceptions of relevance and significance. Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlatio n = 0.49) but avoid articulating contextualization and storytelling po steriors in their review comments (correlation = -0.14), despite givin g implicit reward to them with positive scores. We discuss the broad a pplicability of the framework, leveraging LLMs as diagnostic tools to surface the tacit codes underlying human society, enabling more precis ely targeted responsible AI.",10.48550/ARXIV.2505.18942,JOUR
Shortcut Learning Explanations for Deep Natural Language Processing: A Survey on Dataset Biases,"Dogra, Varun; Verma, Sahil; Kavita; Woźniak, Marcin; Shafi, Jana; Ijaz, Muhammad Fazal",2024,"The introduction of pre-trained large language models (LLMs) has trans formed NLP by fine-tuning task-specific datasets, enabling notable adv ancements in news classification, language translation, and sentiment analysis. This has revolutionized the field, driving remarkable breakt hroughs and progress. However, the growing recognition of bias in text ual data has emerged as a critical focus in the NLP community, reveali ng the inherent limitations of models trained on specific datasets. LL Ms exploit these dataset biases and artifacts as expedient shortcuts f or prediction. The reliance of LLMs on dataset bias and artifacts as s hortcuts for prediction has hindered their generalizability and advers arial robustness. Addressing this issue is crucial to enhance the reli ability and resilience of LLMs in various contexts. This survey provid es a comprehensive overview of the rapidly growing body of research on shortcut learning in language models, classifying the research into f our main areas: the factors of shortcut learning, the origin of bias, the detection methods of dataset biases, and understanding mitigation strategies to address data biases. The goal of this study is to offer a contextualized, in-depth look at the state of learning models, highl ighting the major areas of attention and suggesting possible direction s for further research.",10.1109/access.2024.3360306,JOUR
Implicit Bias in Large Language Models: Experimental Proof and Implica tions for Education,"Warr, Melissa; Oster, Nicole Jakubczyk; Isaac, Roger",2023,"We provide experimental evidence of implicit racial bias in a large la nguage model (specifically ChatGPT 3.5) in the context of an education al task and discuss implications for the use of these tools in educati onal contexts. Specifically, we presented ChatGPT with identical stude nt writing passages alongside various descriptions of student demograp hics, including race, socioeconomic status, and school type. Results i ndicate that when directly prompted to consider race, the model produc ed higher overall scores than responses to a control prompt, but score s given to student descriptors of Black and White were not significant ly different. However, this result belied a subtler form of prejudice that was statistically significant when racial indicators were implied rather than explicitly stated. Additionally, our investigation uncove red subtle sequence effects that suggest the model is more likely to i llustrate bias when variables change within a single chat. The evidenc e indicates that despite the implementation of guardrails by developer s, biases are profoundly embedded in ChatGPT, reflective of both the t raining data and societal biases at large. While overt biases can be a ddressed to some extent, the more ingrained implicit biases present a greater challenge for the application of these technologies in educati on. It is critical to develop an understanding of the bias embedded in these models and how this bias presents itself in educational context s before using LLMs to develop personalized learning tools. Glitches a re not spurious, but rather a kind of signal of how the system operate s. Not an aberration but a form of evidence, illuminating underlying f laws in a corrupt system. (Benjamin, 2020, p. 80) The real questions o f AI ethics sit in the mundane rather than the spectacular. They emerg e at the inter-sections between a technology and the social context of everyday life, including how small decisions in the design and implem entation of AI can create ripple effects with unintended consequences. (Boyd & Elish, 2018)",10.2139/ssrn.4625078,JOUR
Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge,"Ye, Jiayi; Wang, Yanbo; Huang, Yue; Chen, Dongping; Zhang, Qihui; Moniz, Nuno; Gao, Tian; Geyer, Werner; Huang, Chao; Chen, Pin-Yu; Chawla, Nitesh V; Zhang, Xiangliang",2024,"LLM-as-a-Judge has been widely utilized as an evaluation method in var ious benchmarks and served as supervised rewards in model training. Ho wever, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their u tility. Therefore, we identify 12 key potential biases and propose a n ew automated bias quantification framework-CALM-which systematically q uantifies and analyzes each type of bias in LLM-as-a-Judge by using au tomated and principle-guided modification. Our experiments cover multi ple popular language models, and the results indicate that while advan ced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest th at there remains room for improvement in the reliability of LLM-as-a-J udge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application o f LLM-as-a-Judge. Our work highlights the need for stakeholders to add ress these issues and remind users to exercise caution in LLM-as-a-Jud ge applications.",10.48550/ARXIV.2410.02736,JOUR
Large Language Models are Geographically Biased,"Manvi, Rohin; Khanna, Samar; Burke, Marshall; Lobell, David; Ermon, Stefano",2024,"Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding a nd evaluating their biases becomes crucial to achieving fairness and a ccuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race , language, politics, and religion. We show various problematic geogra phic biases, which we define as systemic errors in geospatial predicti ons. Initially, we demonstrate that LLMs are capable of making accurat e zero-shot geospatial predictions in the form of ratings that show st rong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly bi ased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attract iveness, morality, and intelligence (Spearman's $\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that the re is significant variation in the magnitude of bias across existing L LMs.",10.48550/ARXIV.2402.02680,JOUR
Bias and Unfairness in Information Retrieval Systems: New Challenges i n the LLM Era,"Dai, Sunhao; Xu, Chen; Xu, Shicheng; Pang, Liang; Dong, Zhenhua; Xu, Jun",2024,"With the rapid advancement of large language models (LLMs), informatio n retrieval (IR) systems, such as search engines and recommender syste ms, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particul arly in terms of biases and unfairness, which may threaten the informa tion ecosystem. In this paper, we present a comprehensive survey of ex isting works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairn ess issues as distribution mismatch problems, providing a groundwork f or categorizing various mitigation strategies through distribution ali gnment. Subsequently, we systematically delve into the specific bias a nd unfairness issues arising from three critical stages of LLMs integr ation into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent lit erature, focusing on the definitions, characteristics, and correspondi ng mitigation strategies associated with these issues. Finally, we ide ntify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and be yond to better understand and mitigate bias and unfairness issues of I R in this LLM era. We also consistently maintain a GitHub",10.48550/ARXIV.2404.11457,JOUR
Exposing Bias in Online Communities through Large-Scale Language Model s,"Wald, Celine; Pfahler, Lukas",2023,"Progress in natural language generation research has been shaped by th e ever-growing size of language models. While large language models pr e-trained on web data can generate human-sounding text, they also repr oduce social biases and contribute to the propagation of harmful stere otypes. This work utilises the flaw of bias in language models to expl ore the biases of six different online communities. In order to get an insight into the communities' viewpoints, we fine-tune GPT-Neo 1.3B w ith six social media datasets. The bias of the resulting models is eva luated by prompting the models with different demographics and compari ng the sentiment and toxicity values of these generations. Together, t hese methods reveal that bias differs in type and intensity for the va rious models. This work not only affirms how easily bias is absorbed f rom training data but also presents a scalable method to identify and compare the bias of different datasets or communities. Additionally, t he examples generated for this work demonstrate the limitations of usi ng automated sentiment and toxicity classifiers in bias research.",10.48550/ARXIV.2306.02294,JOUR
Style Over Substance: Evaluation Biases for Large Language Models,"Wu, Minghao; Aji, Alham Fikri",2023,"As large language models (LLMs) continue to advance, accurately and co mprehensively evaluating their performance becomes increasingly challe nging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncerta in. This study investigates the behavior of crowd-sourced and expert a nnotators, as well as LLMs, when comparing outputs from different mode ls. To achieve this, we curate a dataset of intentionally flawed machi ne-generated answers. Our findings reveal a concerning bias in the eva luation process, as answers with factual errors are rated more favorab ly than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-gener ated text across multiple dimensions, rather than merging all the eval uation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Em pirical results from our study reveal that this proposed approach sign ificantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improve ment in crowd-sourced-based evaluations, indicating the need for furth er investigation.",10.48550/ARXIV.2307.03025,JOUR
Tackling Bias in Pre-trained Language Models: Current Trends and Under -represented Societies,"Yogarajan, Vithya; Dobbie, Gillian; Keegan, Te Taka; Neuwirth, Rostam J.",2023,"The benefits and capabilities of pre-trained language models (LLMs) in current and future innovations are vital to any society. However, int roducing and using LLMs comes with biases and discrimination, resultin g in concerns about equality, diversity and fairness, and must be addr essed. While understanding and acknowledging bias in LLMs and developi ng mitigation strategies are crucial, the generalised assumptions towa rds societal needs can result in disadvantages towards under-represent ed societies and indigenous populations. Furthermore, the ongoing chan ges to actual and proposed amendments to regulations and laws worldwid e also impact research capabilities in tackling the bias problem. This research presents a comprehensive survey synthesising the current tre nds and limitations in techniques used for identifying and mitigating bias in LLMs, where the overview of methods for tackling bias are grou ped into metrics, benchmark datasets, and mitigation strategies. The i mportance and novelty of this survey are that it explores the perspect ive of under-represented societies. We argue that current practices ta ckling the bias problem cannot simply be 'plugged in' to address the n eeds of under-represented societies. We use examples from New Zealand to present requirements for adopting existing techniques to under-repr esented societies.",10.48550/ARXIV.2312.01509,JOUR
Evaluating Large Language Models on Wikipedia-Style Survey Generation,"Gao, Fan; Jiang, Hang; Yang, Rui; Zeng, Qingcheng; Lu, Jinghui; Blum, Moritz; She, Tianwei; Jiang, Yuang; Li, Irene",,"Educational materials such as survey articles in specialized fields li ke computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Languag e Models (LLMs) have achieved significant success across various gener al tasks. However, their effectiveness and limitations in the educatio n domain are yet to be fully explored. In this work, we examine the pr oficiency of LLMs in generating succinct survey articles specific to t he niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its pre decessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare b oth human and GPT-based evaluation scores and provide in-depth analysi s. While our findings suggest that GPT-created surveys are more contem porary and accessible than human-authored ones, certain limitations we re observed. Notably, GPT-4, despite often delivering outstanding cont ent, occasionally exhibited lapses like missing details or factual err ors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.",10.18653/v1/2024.findings-acl.321,CONF
Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption,"Talboy, Alaina N.; Fuller, Elizabeth",2023,"Assessments of algorithmic bias in large language models (LLMs) are ge nerally catered to uncovering systemic discrimination based on protect ed characteristics such as sex and ethnicity. However, there are over 180 documented cognitive biases that pervade human reasoning and decis ion making that are routinely ignored when discussing the ethical comp lexities of AI. We demonstrate the presence of these cognitive biases in LLMs and discuss the implications of using biased reasoning under t he guise of expertise. We call for stronger education, risk management , and continued research as widespread adoption of this technology inc reases. Finally, we close with a set of best practices for when and ho w to employ this technology as widespread adoption continues to grow.",10.48550/ARXIV.2304.01358,JOUR
"Large Language Models Portray Socially Subordinate Groups as More Homo geneous, Consistent with a Bias Observed in Humans","Lee, Messi H.J.; Montgomery, Jacob M.; Lai, Calvin K.",,"Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs hav e focused on the association of social groups with stereotypical attri butes. However, this is only one form of human bias such systems may r eproduce. We investigate a new form of bias in LLMs that resembles a s ocial psychological phenomenon where socially subordinate groups are p erceived as more homogeneous than socially dominant groups. We had Cha tGPT, a state-of-the-art LLM, generate texts about intersectional grou p identities and compared those texts on measures of homogeneity. We c onsistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that th e model described racial minority groups with a narrower range of huma n experience. ChatGPT also portrayed women as more homogeneous than me n, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect o f gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs t o describe groups as less diverse risks perpetuating stereotypes and d iscriminatory behavior.",10.1145/3630106.3658975,CONF
Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina,"Gao, Yuan; Lee, Dokyun; Burtch, Gordon; Fazelpour, Sina",2024,"Recent studies suggest large language models (LLMs) can exhibit human- like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LL Ms can be used as surrogates or simulations for humans in social scien ce research. However, LLMs differ fundamentally from humans, relying o n probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth o f LLMs using the 11-20 money request game. Nearly all advanced approac hes fail to replicate human behavior distributions across many models. Causes of failure are diverse and unpredictable, relating to input la nguage, roles, and safeguarding. These results advise caution when usi ng LLMs to study human behavior or as surrogates or simulations.",10.48550/ARXIV.2410.19599,JOUR
Socio-Demographic Biases in Medical Decision-Making by Large Language Models: A Large-Scale Multi-Model Analysis,"Omar, Mahmud; Soffer, Shelly; Agbareia, Reem; Bragazzi, Nicola Luigi; Apakama, Donald U.; Horowitz, Carol R; Charney, Alexander W; Freeman, Robert; Kummer, Benjamin; Glicksberg, Benjamin S; Nadkarni, Girish N; Klang, Eyal",2024,"Large language models (LLMs) are increasingly integrated into healthca re but concerns about potential sociodemographic biases persist. We ai med to assess biases in decision making by evaluating LLMs' responses to clinical scenarios across varied sociodemographic profiles. We util ized 500 emergency department vignettes, each representing the same cl inical scenario with differing sociodemographic identifiers across 23 groups, including gender identity, race/ethnicity, socioeconomic statu s, and sexual orientation, and a control version without socio-demogra phic identifiers. We then used Nine LLMs (8 open source and 1 propriet ary) to answer clinical questions regarding triage priority, further t esting, treatment approach, and mental health assessment, resulting in 432,000 total responses. We performed statistical analyses to evaluat e biases across socio-demographic groups, with results normalized and compared to control groups. We find that marginalized groups, includin g Black, unhoused, and LGBTQIA+ individuals, are more likely to receiv e recommendations for urgent care, invasive procedures, or mental heal th assessments compared to the control group (p<0.05 for all compariso ns). High income patients were more often recommended advanced diagnos tic tests such as CT scans or MRI, while low-income patients were more frequently advised to undergo no further testing. We observed signifi cant biases across all models, both proprietary and open source regard less of the model's size. The most pronounced biases emerged in mental health assessment recommendations. LLMs used in medical decision-maki ng exhibit significant biases in clinical recommendations, perpetuatin g existing healthcare disparities. Neither model type nor size affects these biases. These findings underscore the need for careful evaluati on, monitoring, and mitigation of biases in LLMs to ensure equitable p atient care.",10.1101/2024.10.29.24316368,JOUR
Evaluating and addressing demographic disparities in medical large lan guage models: a systematic review,"Omar, Mahmud; Sorin, Vera; Agbareia, Reem; Apakama, Donald U.; Soroush, Ali; Sakhuja, Ankit; Freeman, Robert; Horowitz, Carol R.; Richardson, Lynne D.; Nadkarni, Girish N.; Klang, Eyal",2025,"Background: Large language models (LLMs) are increasingly evaluated fo r use in healthcare. However, concerns about their impact on dispariti es persist. This study reviews current research on demographic biases in LLMs to identify prevalent bias types, assess measurement methods, and evaluate mitigation strategies. Methods: We conducted a systematic review, searching publications from January 2018 to July 2024 across five databases. We included peer-reviewed studies evaluating demograph ic biases in LLMs, focusing on gender, race, ethnicity, age, and other factors. Study quality was assessed using the Joanna Briggs Institute Critical Appraisal Tools. Results: Our review included 24 studies. Of these, 22 (91.7%) identified biases in LLMs. Gender bias was the most prevalent, reported in 15 of 16 studies (93.7%). Racial or ethnic bia ses were observed in 10 of 11 studies (90.9%). Only two studies found minimal or no bias in certain contexts. Mitigation strategies mainly i ncluded prompt engineering, with varying effectiveness. However, these findings are tempered by a potential publication bias, as studies wit h negative results are less frequently published. Conclusion: Biases a re observed in LLMs across various medical domains. While bias detecti on is improving, effective mitigation strategies are still developing. As LLMs increasingly influence critical decisions, addressing these b iases and their resultant disparities is essential for ensuring fair A I systems. Future research should focus on a wider range of demographi c factors, intersectional analyses, and non-Western cultural contexts. ",10.1186/s12939-025-02419-0,JOUR
Inherent Bias in Large Language Models: A Random Sampling Analysis,"Ayoub, Noel F.; Balakrishnan, Karthik; Ayoub, Marc S.; Barrett, Thomas F.; David, Abel P.; Gray, Stacey T.",2024,"There are mounting concerns regarding inherent bias, safety, and tende ncy toward misinformation of large language models (LLMs), which could have significant implications in health care. This study sought to de termine whether generative artificial intelligence (AI)-based simulati ons of physicians making life-and-death decisions in a resource-scarce environment would demonstrate bias. Thirteen questions were developed that simulated physicians treating patients in resource-limited envir onments. Through a random sampling of simulated physicians using OpenA I's generative pretrained transformer (GPT-4), physicians were tasked with choosing only 1 patient to save owing to limited resources. This simulation was repeated 1000 times per question, representing 1000 uni que physicians and patients each. Patients and physicians spanned a va riety of demographic characteristics. All patients had similar a prior i likelihood of surviving the acute illness. Overall, simulated physic ians consistently demonstrated racial, gender, age, political affiliat ion, and sexual orientation bias in clinical decision-making. Across a ll demographic characteristics, physicians most frequently favored pat ients with similar demographic characteristics as themselves, with mos t pairwise comparisons showing statistical significance (P<.05). Nonde script physicians favored White, male, and young demographic character istics. The male doctor gravitated toward the male, White, and young, whereas the female doctor typically preferred female, young, and White patients. In addition to saving patients with their own political aff iliation, Democratic physicians favored Black and female patients, whe reas Republicans preferred White and male demographic characteristics. Heterosexual and gay/lesbian physicians frequently saved patients of similar sexual orientation. Overall, publicly available chatbot LLMs d emonstrate significant biases, which may negatively impact patient out comes if used to support clinical care decisions without appropriate p recautions.",10.1016/j.mcpdig.2024.03.003,JOUR
Large Language Models are Not Yet Human-Level Evaluators for Abstracti ve Summarization,"Shen, Chenhui; Cheng, Liying; Nguyen, Xuan-Phi; You, Yang; Bing, Lidong",,"With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing tre nd for using LLMs on various tasks. One area where LLMs can be employe d is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the trad itional automatic metrics for various evaluation dimensions such as fl uency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluat ors for abstractive summarization. We found that while ChatGPT and GPT -4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM eva luators rate each candidate system inconsistently and are dimension-de pendent. They also struggle to compare candidates with close performan ce and become more unreliable with higher-quality summaries by obtaini ng a lower correlation with humans. In other words, with better abstra ctive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.",10.18653/v1/2023.findings-emnlp.278,CONF
Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case,"González-Bustamante, Bastián; Verelst, Nando; Cisternas, Carla",2025,"Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data. We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning models, as well as Llama and Qwen checkpoints. Three results stand out. First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy>0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59. Overall, LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.",10.5281/ZENODO.17077752,JOUR
Potential and Perils of Large Language Models as Judges of Unstructured Textual Data,"Bedemariam, Rewina; Perez, Natalie; Bhaduri, Sreyoshi; Kapoor, Satya; Gil, Alex; Conjar, Elizabeth; Itoku, Ikkei; Theil, David; Chadha, Aman; Nayyar, Naumaan",2025,"Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.",10.48550/ARXIV.2501.08167,JOUR
Aligning Large Language Models through Synthetic Feedback,"Kim, Sungdong; Bae, Sanghwan; Shin, Jamin; Kang, Soyoung; Kwak, Donghyun; Yoo, Kang Min; Seo, Minjoon",2023,"Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost",10.48550/ARXIV.2305.13735,JOUR
Ideology and Policy Preferences in Synthetic Data: The Potential of LLMs for Public Opinion Analysis,"Lee, Keyeun; Park, Jaehyuk; Choi, Suh-hee; Lee, Changkeun",2025,"This study investigates whether large language models (LLMs) can meaningfully extend or generate synthetic public opinion survey data on labor policy issues in South Korea. Unlike prior work conducted on people’s general sociocultural values or specific political topics such as voting intentions, our research examines policy preferences on tangible social and economic topics, offering deeper insights for news media and data analysts. In two key applications, we first explore whether LLMs can predict public sentiment on emerging or rapidly evolving issues using existing survey data. We then assess how LLMs generate synthetic datasets resembling real-world survey distributions. Our findings reveal that while LLMs capture demographic and ideological traits with reasonable accuracy, they tend to overemphasize ideological orientation for politically charged topics—a bias that is more pronounced in fully synthetic data, raising concerns about perpetuating societal stereotypes. Despite these challenges, LLMs hold promise for enhancing data-driven journalism and policy research, particularly in polarized societies. We call for further study into how LLM-based predictions align with human responses in diverse sociopolitical settings, alongside improved tools and guidelines to mitigate embedded biases.",10.17645/mac.9677,JOUR
Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data,"Zhang, Jihong; Liang, Xinya; Deng, Anqi; Bonge, Nicole; Tan, Lin; Zhang, Ling; Zarrett, Nicole",2025,"Mixed methods research integrates quantitative and qualitative data but faces challenges in aligning their distinct structures, particularly in examining measurement characteristics and individual response patterns. Advances in large language models (LLMs) offer promising solutions by generating synthetic survey responses informed by qualitative data. This study investigates whether LLMs, guided by personal interviews, can reliably predict human survey responses, using the Behavioral Regulations in Exercise Questionnaire (BREQ) and interviews from after-school program staff as a case study. Results indicate that LLMs capture overall response patterns but exhibit lower variability than humans. Incorporating interview data improves response diversity for some models (e.g., Claude, GPT), while well-crafted prompts and low-temperature settings enhance alignment between LLM and human responses. Demographic information had less impact than interview content on alignment accuracy. These findings underscore the potential of interview-informed LLMs to bridge qualitative and quantitative methodologies while revealing limitations in response variability, emotional interpretation, and psychometric fidelity. Future research should refine prompt design, explore bias mitigation, and optimize model settings to enhance the validity of LLM-generated survey data in social science research.",10.48550/ARXIV.2505.21997,JOUR
Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs,"Gelman, Haywood; Hastings, John D.",,"Insider threats wield an outsized influence on organizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. %One example of this influence is where anonymous respondents submit web-based job search site reviews, an insider threat risk to organizations. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition.",10.1109/isdfs65363.2025.11012066,CONF
"The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models","Ma, Bolei; Wang, Xinpeng; Hu, Tiancheng; Haensch, Anna-Carolina; Hedderich, Michael A.; Plank, Barbara; Kreuter, Frauke",2024,"Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may capture and convey. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing a comprehensive overview of recent works on the evaluation of AOVs in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOVs in LLMs.",10.48550/ARXIV.2406.11096,JOUR
AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators,"Arora, Neeraj; Chakraborty, Ishita; Nishimura, Yohei",2025,"The authors’ central premise is that a human–LLM (large language model) hybrid approach leads to efficiency and effectiveness gains in the marketing research process. In qualitative research, they show that LLMs can assist in both data generation and analysis; LLMs effectively create sample characteristics, generate synthetic respondents, and conduct and moderate in-depth interviews. The AI–human hybrid generates information-rich, coherent data that surpasses human-only data in depth and insightfulness and matches human performance in data analysis tasks of generating themes and summaries. Evidence from expert judges shows that humans and LLMs possess complementary skills; the human–LLM hybrid outperforms its human-only or LLM-only counterpart. For quantitative research, the LLM correctly picks the answer direction and valence, with the quality of synthetic data significantly improving through few-shot learning and retrieval-augmented generation. The authors demonstrate the value of the AI–human hybrid by collaborating with a Fortune 500 food company and replicating a 2019 qualitative and quantitative study using GPT-4. For their empirical investigation, the authors design the system architecture and prompts to create personas, ask questions, and obtain responses from synthetic respondents. They provide road maps for integrating LLMs into qualitative and quantitative marketing research and conclude that LLMs serve as valuable collaborators in the insight generation process.",10.1177/00222429241276529,JOUR
"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection","Guo, Biyang; Zhang, Xin; Wang, Ziyuan; Jiang, Minqi; Nie, Jinran; Ding, Yuxuan; Yue, Jianwei; Wu, Yupeng",2023,"The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",10.48550/ARXIV.2301.07597,JOUR
How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model,"Song, Shezheng; Li, Xiaopeng; Li, Shasha; Zhao, Shan; Yu, Jie; Ma, Jun; Mao, Xiaoguang; Zhang, Weimin",2023,"This review paper explores Multimodal Large Language Models (MLLMs), which integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data such as text and vision. MLLMs demonstrate capabilities like generating image narratives and answering image-based questions, bridging the gap towards real-world human-computer interactions and hinting at a potential pathway to artificial general intelligence. However, MLLMs still face challenges in processing the semantic gap in multimodality, which may lead to erroneous generation, posing potential risks to society. Choosing the appropriate modality alignment method is crucial, as improper methods might require more parameters with limited performance improvement. This paper aims to explore modality alignment methods for LLMs and their existing capabilities. Implementing modality alignment allows LLMs to address environmental issues and enhance accessibility. The study surveys existing modal alignment methods in MLLMs into four groups: (1) Multimodal Converters that change data into something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs perceive different types of data; (3) Tools Assistance for changing data into one common format, usually text; and (4) Data-Driven methods that teach LLMs to understand specific types of data in a dataset. This field is still in a phase of exploration and experimentation, and we will organize and update various existing research methods for multimodal information alignment.",10.48550/ARXIV.2311.07594,JOUR
Benchmarking Large Language Models for News Summarization,"Zhang, Tianyi; Ladhak, Faisal; Durmus, Esin; Liang, Percy; McKeown, Kathleen; Hashimoto, Tatsunori B.",2024,"Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",10.1162/tacl_a_00632,JOUR
Simulacrum of Stories: Examining Large Language Models as Qualitative Research Participants,"Kapania, Shivani; Agnew, William; Eslami, Motahhare; Heidari, Hoda; Fox, Sarah E",,"The recent excitement around generative models has sparked a wave of proposals suggesting the replacement of human participation and labor in research and development–e.g., through surveys, experiments, and interviews—with synthetic research data generated by large language models (LLMs). We conducted interviews with 19 qualitative researchers to understand their perspectives on this paradigm shift. Initially skeptical, researchers were surprised to see similar narratives emerge in the LLM-generated data when using the interview probe. However, over several conversational turns, they went on to identify fundamental limitations, such as how LLMs foreclose participants’ consent and agency, produce responses lacking in palpability and contextual depth, and risk delegitimizing qualitative research methods. We argue that the use of LLMs as proxies for participants enacts the surrogate effect, raising ethical and epistemological concerns that extend beyond the technical limitations of current models to the core of whether LLMs fit within qualitative ways of knowing.",10.1145/3706598.3713220,CONF
The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation,"Pavlovic, Maja; Poesio, Massimo",2024,"Recent studies focus on exploring the capability of Large Language Models (LLMs) for data annotation. Our work, firstly, offers a comparative overview of twelve such studies that investigate labelling with LLMs, particularly focusing on classification tasks. Secondly, we present an empirical analysis that examines the degree of alignment between the opinion distributions returned by GPT and those provided by human annotators across four subjective datasets. Our analysis supports a minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.",10.48550/ARXIV.2405.01299,JOUR
A Benchmarking Survey: Evaluating the Accuracy and Effectiveness of Benchmark Models in Measuring the Performance of Large Language Models,"Salim, Rifaldo Agustinus; Delvin, Delvin; Harefa, Jeklin; Jingga, Kenny",,"Large language models have revolutionized artificial intelligence, exhibiting remarkable linguistic abilities across various tasks. However, evaluating the true performance of these models remains a significant challenge, necessitating reliable benchmark models. This study conducts a comprehensive survey to evaluate the accuracy and effectiveness of benchmark models in measuring the performance of LLMs. Through a systematic literature review of 42 papers, six influential benchmarks were selected: AGIEval, MMLU, TruthfulQA, GSM8K, HellaSwag, and Chatbot Arena. These benchmarks were assessed across three key criteria: data contamination, sensitivity to prompting techniques, and the ability to capture the evolving nature of language. Using a 5-point scoring system, each benchmark was evaluated on these criteria, with an overall score calculated as the mean of the three individual scores. The comparative analysis revealed considerable variability among the benchmarks. Data contamination, where LLM training data overlaps with benchmark datasets, emerged as a pervasive issue, rendering many benchmark scores unreliable. Sensitivity to prompting techniques also varied, with some benchmarks being highly susceptible to score manipulation. Additionally, capturing linguistic evolution proved challenging for static dataset-based benchmarks. Notably, Chatbot Arena excelled across all criteria due to its human-centric, dynamic approach. This study highlights the need for more robust, adaptive benchmarking methodologies to accurately assess the rapidly evolving capabilities of large language models.",10.1109/icimcis63449.2024.10957638,CONF
"A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More","Wang, Zhichao; Bi, Bin; Pentyala, Shiva Kumar; Ramnath, Kiran; Chaudhuri, Sougata; Mehrotra, Shubham; Zixu; Zhu; Mao, Xiang-Bo; Asur, Sitaram; Na; Cheng",2024,"With advancements in self-supervised learning, the availability of trillions tokens in a pre-training corpus, instruction fine-tuning, and the development of large Transformers with billions of parameters, large language models (LLMs) are now capable of generating factual and coherent responses to human queries. However, the mixed quality of training data can lead to the generation of undesired responses, presenting a significant challenge. Over the past two years, various methods have been proposed from different perspectives to enhance LLMs, particularly in aligning them with human expectation. Despite these efforts, there has not been a comprehensive survey paper that categorizes and details these approaches. In this work, we aim to address this gap by categorizing these papers into distinct topics and providing detailed explanations of each alignment method, thereby helping readers gain a thorough understanding of the current state of the field.",10.48550/ARXIV.2407.16216,JOUR
Comparing human and synthetic data in service research: using augmented language models to study service failures and recoveries,"Bickley, Steven J.; Chan, Ho Fai; Dao, Bang; Torgler, Benno; Tran, Son; Zimbatu, Alexandra",2024,"Purpose  This study aims to explore Augmented Language Models (ALMs) for synthetic data generation in services marketing and research. It evaluates ALMs' potential in mirroring human responses and behaviors in service scenarios through comparative analysis with five empirical studies.  Design/methodology/approach  The study uses ALM-based agents to conduct a comparative analysis, leveraging SurveyLM (Bickley et al., 2023) to generate synthetic responses to the scenario-based experiment in Söderlund and Oikarinen (2018) and four more recent studies from the Journal of Services Marketing. The main focus was to assess the alignment of ALM responses with original study manipulations and hypotheses.  Findings  Overall, our comparative analysis reveals both strengths and limitations of using synthetic agents to mimic human-based participants in services research. Specifically, the model struggled with scenarios requiring high levels of visual context, such as those involving images or physical settings, as in the Dootson et al. (2023) and Srivastava et al. (2022) studies. Conversely, studies like Tariq et al. (2023) showed better alignment, highlighting the model's effectiveness in more textually driven scenarios.  Originality/value  To the best of the authors’ knowledge, this research is among the first to systematically use ALMs in services marketing, providing new methods and insights for using synthetic data in service research. It underscores the challenges and potential of interpreting ALM versus human responses, marking a significant step in exploring AI capabilities in empirical research.",10.1108/jsm-11-2023-0441,JOUR
"Towards Measuring and Modeling ""Culture"" in LLMs: A Survey","Adilazuarda, Muhammad Farid; Mukherjee, Sagnik; Lavania, Pradhyumna; Singh, Siddhant; Aji, Alham Fikri; O'Neill, Jacki; Modi, Ashutosh; Choudhury, Monojit",2024,"We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define""culture,""which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of""culture.""We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of""culture,""such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations, we provide several recommendations for a holistic and practically useful research agenda for furthering cultural inclusion in LLMs and LLM-based applications.",10.48550/ARXIV.2403.15412,JOUR
Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators,"Lim, Sungjib; Song, Woojung; Lee, Eun-Ju; Jo, Yohan",2025,"As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs replicate human-like behavior. We will publicly release our dataset and code to support future work.",10.48550/ARXIV.2507.05890,JOUR
Balancing Large Language Model Alignment and Algorithmic Fidelity in Social Science Research,"Lyman, Alex; Hepner, Bryce; Argyle, Lisa P.; Busby, Ethan C.; Gubler, Joshua R.; Wingate, David",2025,"Generative artificial intelligence (AI) has the potential to revolutionize social science research. However, researchers face the difficult challenge of choosing a specific AI model, often without social science-specific guidance. To demonstrate the importance of this choice, we present an evaluation of the effect of alignment, or human-driven modification, on the ability of large language models (LLMs) to simulate the attitudes of human populations (sometimes called silicon sampling). We benchmark aligned and unaligned versions of six open-source LLMs against each other and compare them to similar responses by humans. Our results suggest that model alignment impacts output in predictable ways, with implications for prompting, task completion, and the substantive content of LLM-based results. We conclude that researchers must be aware of the complex ways in which model training affects their research and carefully consider model choice for each project. We discuss future steps to improve how social scientists work with generative AI tools.",10.1177/00491241251342008,JOUR
"Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement","Ye, Haoran; Jin, Jing; Xie, Yuhang; Zhang, Xin; Song, Guojie",2025,"The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.",10.48550/ARXIV.2505.08245,JOUR
Towards New Benchmark for AI Alignment &amp; Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI,"Bojic, Ljubisa; Seychell, Dylan; Cabarkapa, Milan",2025,"With the expansion of neural networks, such as large language models, humanity is exponentially heading towards superintelligence. As various AI systems are increasingly integrated into the fabric of societies-through recommending values, devising creative solutions, and making decisions-it becomes critical to assess how these AI systems impact humans in the long run. This research aims to contribute towards establishing a benchmark for evaluating the sentiment of various Large Language Models in socially importan issues. The methodology adopted was a Likert scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results highlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI, whereas Bard was leaning towards the neutral sentiment. The human samples, contrastingly, showed a lower average sentiment of 2.97. The temporal comparison revealed differences in sentiment evolution between LLMs in three days, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect of potential conflicts of interest and bias possibilities in LLMs' sentiment formation. Results indicate that LLMs, akin to human cognitive processes, could potentially develop unique sentiments and subtly influence societies' perceptions towards various opinions formed within the LLMs.",10.48550/ARXIV.2501.02531,JOUR
Human-AI Synergy in Survey Development: Implications from Large Language Models in Business and Research,"Ke, Ping Fan; Ng, Ka Chung",2025,"This study examines the novel integration of Large Language Models (LLMs) into the survey development process in business and research through the development and evaluation of the Behavioral Research Assistant (BRASS) Bot. We first analyzed the traditional scale development process to identify tasks suitable for LLM integration, including both human-in-the-loop and automated LLM data collection methods. Following this analysis, we developed the details of BRASS Bot, incorporating design principles of falsifiability and reproducibility. We then conducted a comprehensive evaluation of the BRASS Bot across a diverse set of LLMs, including GPT, Claude, Gemini, and Llama, to assess its usability, validity, and reliability. We further demonstrated the practical utility of the BRASS Bot by conducting a user study and a predictive validity simulation. Our research presents both theoretical and practical implications. The augmentation approach of the BRASS Bot enriches the theoretical foundations of behavioral constructs by identifying previously overlooked patterns. Additionally, the BRASS Bot offers significant time and resource efficiency gains while enhancing scale validity. Our work lays the foundation for future research on the broader application of LLMs as both assistants and collaborators in survey analysis and behavioral research design and execution, highlighting their potential for a transformative impact on the field.",10.1145/3700597,JOUR
Large Language Model Alignment: A Survey,"Shen, Tianhao; Jin, Renren; Huang, Yufei; Liu, Chuang; Dong, Weilong; Guo, Zishan; Wu, Xinwei; Liu, Yan; Xiong, Deyi",2023,"Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values. This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead. Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.",10.48550/ARXIV.2309.15025,JOUR
A Survey on Human Preference Learning for Large Language Models,"Jiang, Ruili; Chen, Kehai; Bai, Xuefeng; He, Zhixuan; Li, Juntao; Yang, Muyun; Zhao, Tiejun; Nie, Liqiang; Zhang, Min",2024,"The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.",10.48550/ARXIV.2406.11191,JOUR
LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices,"Serajeh, Neda Taghizadeh; Mohammadi, Iman; Fuccella, Vittorio; De Rosa, Mattia",2024,"Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58\% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56\% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.",10.48550/ARXIV.2403.18173,JOUR
Evaluating the effectiveness of large language models in abstract screening: a comparative analysis,"Li, Michael; Sun, Jianping; Tan, Xianming",2024,"This study aimed to evaluate the performance of large language models (LLMs) in the task of abstract screening in systematic review and meta-analysis studies, exploring their effectiveness, efficiency, and potential integration into existing human expert-based workflows.",10.1186/s13643-024-02609-x,JOUR
Aligning Multimodal LLM with Human Preference: A Survey,"Yu, Tao; Zhang, Yi-Fan; Fu, Chaoyou; Wu, Junkang; Lu, Jinda; Wang, Kun; Lu, Xingyu; Shen, Yunhang; Zhang, Guibin; Song, Dingjie; Yan, Yibo; Xu, Tianlong; Wen, Qingsong; Zhang, Zhang; Huang, Yan; Wang, Liang; Tan, Tieniu",2025,"Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",10.48550/ARXIV.2503.14504,JOUR
Exploring Qualitative Research Using LLMs,"Bano, Muneera; Zowghi, Didar; Whittle, Jon",2023,"The advent of AI driven large language models (LLMs) have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. This study aimed to compare and contrast the comprehension capabilities of humans and LLMs. We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. LLMs were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one third of cases, and a slightly lower alignment with GPT4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one fifth of the classifications. In the comparison of human and LLMs reasoning, it appears that human analysts lean heavily on their individual experiences. As expected, LLMs, on the other hand, base their reasoning on the specific word choices found in app reviews and the functional components of the app itself. Our results highlight the potential for effective human LLM collaboration, suggesting a synergistic rather than competitive relationship. Researchers must continuously evaluate LLMs role in their work, thereby fostering a future where AI and humans jointly enrich qualitative research.",10.48550/ARXIV.2306.13298,JOUR
Towards a Unified View of Preference Learning for Large Language Models: A Survey,"Gao, Bofei; Song, Feifan; Miao, Yibo; Cai, Zefan; Yang, Zhe; Chen, Liang; Hu, Helan; Xu, Runxin; Dong, Qingxiu; Zheng, Ce; Quan, Shanghaoran; Xiao, Wen; Zhang, Ge; Zan, Daoguang; Lu, Keming; Yu, Bowen; Liu, Dayiheng; Cui, Zeyu; Yang, Jian; Sha, Lei; Wang, Houfeng; Sui, Zhifang; Wang, Peiyi; Liu, Tianyu; Chang, Baobao",2024,"Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences.",10.48550/ARXIV.2409.02795,JOUR
"The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values","Kirk, Hannah Rose; Bean, Andrew M.; Vidgen, Bertie; Röttger, Paul; Hale, Scott A.",2023,"Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories.First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.",10.48550/ARXIV.2310.07629,JOUR
"Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges","Lu, Haoran; Fang, Luyang; Zhang, Ruidong; Li, Xinliang; Cai, Jiazhang; Cheng, Huimin; Tang, Lin; Liu, Ziyu; Sun, Zeliang; Wang, Tao; Zhang, Yingchuan; Zidan, Arif Hassan; Xu, Jinwen; Yu, Jincheng; Yu, Meizhi; Jiang, Hanqi; Gong, Xilin; Luo, Weidi; Sun, Bolun; Chen, Yongkai; Ma, Terry; Wu, Shushan; Zhou, Yifan; Chen, Junhao; Xiang, Haotian; Zhang, Jing; Jahin, Afrar; Ruan, Wei; Deng, Ke; Pan, Yi; Wang, Peilong; Li, Jiahui; Liu, Zhengliang; Zhang, Lu; Zhao, Lin; Liu, Wei; Zhu, Dajiang; Xing, Xin; Dou, Fei; Zhang, Wei; Huang, Chao; Liu, Rongjie; Zhang, Mengrui; Liu, Yiwen; Sun, Xiaoxiao; Lu, Qin; Xiang, Zhen; Zhong, Wenxuan; Liu, Tianming; Ma, Ping",2025,"Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We analyze the development of alignment methods across diverse paradigms, characterizing the fundamental trade-offs between core alignment objectives. Our analysis shows that while supervised fine-tuning enables basic instruction-following, preference-based methods offer more flexibility for aligning with nuanced human intent. We discuss state-of-the-art techniques, including Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ), highlighting their approaches to balancing quality and efficiency. We review existing evaluation frameworks and benchmarking datasets, emphasizing limitations such as reward misspecification, distributional robustness, and scalable oversight. We summarize strategies adopted by leading AI labs to illustrate the current state of practice. We conclude by outlining open problems in oversight, value pluralism, robustness, and continuous alignment. This survey aims to inform both researchers and practitioners navigating the evolving landscape of LLM alignment.",10.48550/ARXIV.2507.19672,JOUR
Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks,"Veselovsky, Veniamin; Ribeiro, Manoel Horta; West, Robert",2023,"Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk",10.48550/ARXIV.2306.07899,JOUR
ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models,"Li, Dai; Li, Linzhuo; Qiu, Huilian Sophie",2025,"Large language models (LLMs) in the form of chatbots like ChatGPT and Llama are increasingly proposed as""silicon samples""for simulating human opinions. This study examines this notion, arguing that LLMs may misrepresent population-level opinions. We identify two fundamental challenges: a failure in structural consistency, where response accuracy doesn't hold across demographic aggregation levels, and homogenization, an underrepresentation of minority opinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama 3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized immigration from the American National Election Studies (ANES) 2020. Our findings reveal significant structural inconsistencies and severe homogenization in LLM responses compared to human data. We propose an""accuracy-optimization hypothesis,""suggesting homogenization stems from prioritizing modal responses. These issues challenge the validity of using LLMs, especially chatbots AI, as direct substitutes for human survey data, potentially reinforcing stereotypes and misinforming policy.",10.48550/ARXIV.2507.02919,JOUR
Can Large Language Models Be an Alternative to Human Evaluations?,"Chiang, Cheng-Han; Lee, Hung-yi",2023,"Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms.Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided.In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks.We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer.We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",10.48550/ARXIV.2305.01937,JOUR
Prompt Perturbations Reveal Human-Like Biases in Large Language Model Survey Responses,"Rupprecht, Jens; Ahnert, Georg; Strohmaier, Markus",2025,"Large Language Models (LLMs) are increasingly used as proxies for human subjects in social science surveys, but their reliability and susceptibility to known response biases are poorly understood. This paper investigates the response robustness of LLMs in normative survey contexts - we test nine diverse LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of 11 perturbations to both question phrasing and answer option structure, resulting in over 167,000 simulated interviews. In doing so, we not only reveal LLMs'vulnerabilities to perturbations but also show that all tested models exhibit a consistent recency bias varying in intensity, disproportionately favoring the last-presented answer option. While larger models are generally more robust, all models remain sensitive to semantic variations like paraphrasing and to combined perturbations. By applying a set of perturbations, we reveal that LLMs partially align with survey response biases identified in humans. This underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.",10.48550/ARXIV.2507.07188,JOUR
From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge,"Li, Dawei; Jiang, Bohan; Huang, Liangjie; Beigi, Alimohammad; Zhao, Chengshuai; Tan, Zhen; Bhattacharjee, Amrita; Jiang, Yuxuan; Chen, Canyu; Wu, Tianhao; Shu, Kai; Cheng, Lu; Liu, Huan",2024,"Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the""LLM-as-a-judge""paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at \url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and \url{https://llm-as-a-judge.github.io}.",10.48550/ARXIV.2411.16594,JOUR
Simulating Public Opinion: Comparing Distributional and Individual-Level Predictions from LLMs and Random Forests,"Miranda, Fernando; Balbi, Pedro Paulo",2025,"Understanding and modeling the flow of information in human societies is essential for capturing phenomena such as polarization, opinion formation, and misinformation diffusion. Traditional agent-based models often rely on simplified behavioral rules that fail to capture the nuanced and context-sensitive nature of human decision-making. In this study, we explore the potential of Large Language Models (LLMs) as data-driven, high-fidelity agents capable of simulating individual opinions under varying informational conditions. Conditioning LLMs on real survey data from the 2020 American National Election Studies (ANES), we investigate their ability to predict individual-level responses across a spectrum of political and social issues in a zero-shot setting, without any training on the survey outcomes. Using Jensen–Shannon distance to quantify divergence in opinion distributions and F1-score to measure predictive accuracy, we compare LLM-generated simulations to those produced by a supervised Random Forest model. While performance at the individual level is comparable, LLMs consistently produce aggregate opinion distributions closer to the empirical ground truth. These findings suggest that LLMs offer a promising new method for simulating complex opinion dynamics and modeling the probabilistic structure of belief systems in computational social science.",10.3390/e27090923,JOUR
The ‘Implicit Intelligence’ of artificial intelligence. Investigating the potential of large language models in social science research,"Cappelli, Ottorino; Aliberti, Marco; Praino, Rodrigo",2024,"ABSTRACT Researchers in ‘hard' science disciplines are exploring the transformative potential of Artificial Intelligence (AI) for advancing research in their fields. Their colleagues in ‘soft' science, however, have produced thus far a limited number of articles on this subject. This paper addresses this gap. Our main hypothesis is that existing Artificial Intelligence Large Language Models (LLMs) can closely align with human expert assessments in specialized social science surveys. To test this, we compare data from a multi-country expert survey with those collected from the two powerful LLMs created by OpenAI and Google. The statistical difference between the two sets of data is minimal in most cases, supporting our hypothesis, albeit with certain limitations and within specific parameters. The tested language models demonstrate domain-agnostic algorithmic accuracy, indicating an inherent ability to incorporate human knowledge and independently replicate human judgment across various subfields without specific training. We refer to this property as the ‘implicit intelligence' of Artificial Intelligence, representing a highly promising advancement for social science research.",10.1080/2474736x.2024.2351794,JOUR
ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing,"Liu, Ryan; Shah, Nihar B.",2023,"Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the""better""paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.",10.48550/ARXIV.2306.00622,JOUR
Measurement in the Age of LLMs: An Application to Ideological Scaling,"O'Hagan, Sean; Schein, Aaron",2023,"Much of social science is centered around terms like ``ideology'' or ``power'', which generally elude precise definition, and whose contextual meanings are trapped in surrounding language. This paper explores the use of large language models (LLMs) to flexibly navigate the conceptual clutter inherent to social scientific measurement tasks. We rely on LLMs' remarkable linguistic fluency to elicit ideological scales of both legislators and text, which accord closely to established methods and our own judgement. A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself. This approach affords a great deal of flexibility, which we showcase through a variety of different case studies. Our results suggest that LLMs can be used to characterize highly subtle and diffuse manifestations of political ideology in text.",10.48550/ARXIV.2312.09203,JOUR
LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks,"Bavaresco, Anna; Bernardi, Raffaella; Bertolazzi, Leonardo; Elliott, Desmond; Fernández, Raquel; Gatt, Albert; Ghaleb, Esam; Giulianelli, Mario; Hanna, Michael; Koller, Alexander; Martins, André F. T.; Mondorf, Philipp; Neplenbroek, Vera; Pezzelle, Sandro; Plank, Barbara; Schlangen, David; Suglia, Alessandro; Surikuchi, Aditya K; Takmaz, Ece; Testoni, Alberto",2024,"There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.",10.48550/ARXIV.2406.18403,JOUR
LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods,"Li, Haitao; Dong, Qian; Chen, Junjie; Su, Huixue; Zhou, Yujia; Ai, Qingyao; Ye, Ziyi; Liu, Yiqun",2024,"The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as ''LLMs-as-judges''. This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions. Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.",10.48550/ARXIV.2412.05579,JOUR
LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing,"Du, Jiangshu; Wang, Yibo; Zhao, Wenting; Deng, Zhongfen; Liu, Shuaiqi; Lou, Renze; Zou, Henry Peng; Venkit, Pranav Narayanan; Zhang, Nan; Srinath, Mukund; Zhang, Haoran Ranran; Gupta, Vipul; Li, Yinghui; Li, Tao; Wang, Fei; Liu, Qin; Liu, Tianlin; Gao, Pengzhi; Xia, Congying; Xing, Chen; Cheng, Jiayang; Wang, Zhaowei; Su, Ying; Shah, Raj Sanjay; Guo, Ruohao; Gu, Jing; Li, Haoran; Wei, Kangda; Wang, Zihao; Cheng, Lu; Ranathunga, Surangika; Fang, Meng; Fu, Jie; Liu, Fei; Huang, Ruihong; Blanco, Eduardo; Cao, Yixin; Zhang, Rui; Yu, Philip S.; Yin, Wenpeng",2024,"Claim: This work is not advocating the use of LLMs for paper (meta-)reviewing. Instead, wepresent a comparative analysis to identify and distinguish LLM activities from human activities. Two research goals: i) Enable better recognition of instances when someone implicitly uses LLMs for reviewing activities; ii) Increase community awareness that LLMs, and AI in general, are currently inadequate for performing tasks that require a high level of expertise and nuanced judgment.This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload?This study focuses on the topic of LLMs as NLP Researchers, particularly examining the effectiveness of LLMs in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with “deficiency” labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) “LLMs as Reviewers”, how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) “LLMs as Metareviewers”, how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis.",10.48550/ARXIV.2406.16253,JOUR
Can Large Language Models Transform Computational Social Science?,"Ziems, Caleb; Held, William; Shaikh, Omar; Chen, Jiaao; Zhang, Zhehao; Yang, Diyi",2024,"Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.",10.1162/coli_a_00502,JOUR
ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning,"Törnberg, Petter",2023,"This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.",10.48550/ARXIV.2304.06588,JOUR
Evaluating the Efficacy of Large Language Models in Automating Academic Peer Reviews,"WeiminZhao; Mahmoud, Qusay H.",,"This paper explores the application of large language models (LLMs) in automating the peer review process for academic papers, a critical area for enhancing the efficiency and consistency of scholarly publication. We utilized GPT-4-0125 to automatically generate reviews for 20 papers sourced from openreview.net and analyzed the AI-generated peer reviews for quality and effectiveness. The analysis includes a detailed assessment of the text properties of the reviews, such as sentiment, revealing that LLM-generated reviews tend to be more uniformly positive than their human-written counterparts. In addition, we conducted a user survey in which participants attempted to distinguish between AI-generated and human-written reviews. The survey results indicated a low correct identification rate, suggesting that participants often could not discern the origin of the review, thereby highlighting the potential of LLMs to mimic human-like review qualities. However, the study also identifies limitations in LLM's performance, particularly concerning the variability in review quality, which appears to correlate with the model's vocabulary usage of the generated content.",10.1109/icmla61862.2024.00187,CONF
LLM-Powered Preference Elicitation in Combinatorial Assignment,"Soumalias, Ermis; Jiang, Yanchen; Zhu, Kehang; Curry, Michael; Seuken, Sven; Parkes, David C.",2025,"We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.",10.48550/ARXIV.2502.10308,JOUR
AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers,"Wuttke, Alexander; Aßenmacher, Matthias; Klamm, Christopher; Lang, Max M.; Würschinger, Quirin; Kreuter, Frauke",2024,"Traditional methods for eliciting people's opinions face a trade-off between depth and scale: structured surveys enable large-scale data collection but limit respondents' ability to express unanticipated thoughts in their own words, while conversational interviews provide deeper insights but are resource-intensive. This study explores the potential of replacing human interviewers with large language models (LLMs) to conduct scalable conversational interviews. Our goal is to assess the performance of AI Conversational Interviewing and to identify opportunities for improvement in a controlled environment. We conducted a small-scale, in-depth study with university students who were randomly assigned to be interviewed by either AI or human interviewers, both employing identical questionnaires on political topics. Various quantitative and qualitative measures assessed interviewer adherence to guidelines, response quality, participant engagement, and overall interview efficacy. The findings indicate the viability of AI Conversational Interviewing in producing quality data comparable to traditional methods, with the added benefit of scalability. Based on our experiences, we present specific recommendations for effective implementation.",10.48550/ARXIV.2410.01824,JOUR
