TY  - JOUR
TI  - Out of One, Many: Using Language Models to Simulate Human Samples
AU  - Argyle, Lisa P.
AU  - Busby, Ethan C.
AU  - Fulda, Nancy
AU  - Gubler, Joshua R.
AU  - Rytting, Christopher
AU  - Wingate, David
T2  - Political Analysis
AB  - We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.
DA  - 2023/07//
PY  - 2023
DO  - 10.1017/pan.2023.2
VL  - 31
IS  - 3
SP  - 337
EP  - 351
SN  - 1047-1987, 1476-4989
ST  - Out of One, Many
UR  - https://www.cambridge.org/core/journals/political-analysis/article/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49
Y2  - 2025/05/08/
KW  - machine learning
KW  - public opinion
KW  - artificial intelligence
KW  - computational social science
ER  - 

TY  - GEN
TI  - Examining the Feasibility of Large Language Models as Survey Respondents
AU  - Kitadai, Ayato
AU  - Ogawa, Kazuhito
AU  - Nishino, Nariaki
AB  - This study examines the potential of large language models (LLMs) to substitute for human respondents in survey research. Surveys serve as essential tools in fields like social science, marketing, and policy-making; however, traditional methods often require considerable time and costs. LLMs present a promising alternative to mitigate these burdens, though their reliability—particularly outside of U.S. contexts—remains uncertain. This study focuses on surveys conducted in Japan, comparing the responses generated by LLMs to those of actual Japanese participants. Our analysis reveals notable discrepancies due to inherent biases in LLMs, though adjusting the models to better align with specific personas can partially enhance the accuracy of simulated responses. We emphasize the need for further research to fully understand the capabilities and limitations of LLMs, aiming to refine their application in diverse areas such as social sciences, marketing, and policy decision-making.
DA  - 2024/12//
PY  - 2024
DO  - 10.1109/BigData62323.2024.10825497
DP  - IEEE Xplore
UR  - https://ieeexplore.ieee.org/document/10825497
Y2  - 2025/08/09/18:52:20
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10825497&ref=
KW  - Social sciences
KW  - Surveys
KW  - Large language models
KW  - Analytical models
KW  - Big Data
KW  - Decision making
KW  - Ecosystems
KW  - Faces
KW  - Focusing
KW  - large language models
KW  - questionnaire survey
KW  - Reliability
KW  - simulation
ER  - 

TY  - GEN
TI  - Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models
AU  - Jiang, Shapeng
AU  - Wei, Lijia
AU  - Zhang, Chen
AB  - In recent years, large language models (LLMs) have attracted attention due to their ability to generate human-like text. As surveys and opinion polls remain key tools for gauging public attitudes, there is increasing interest in assessing whether LLMs can accurately replicate human responses. This study examines the potential of LLMs, specifically ChatGPT-4o, to replicate human responses in large-scale surveys and to predict election outcomes based on demographic data. Employing data from the World Values Survey (WVS) and the American National Election Studies (ANES), we assess the LLM's performance in two key tasks: simulating human responses and forecasting U.S. election results. In simulations, the LLM was tasked with generating synthetic responses for various socio-cultural and trust-related questions, demonstrating notable alignment with human response patterns across U.S.-China samples, though with some limitations on value-sensitive topics. In prediction tasks, the LLM was used to simulate voting behavior in past U.S. elections and predict the 2024 election outcome. Our findings show that the LLM replicates cultural differences effectively, exhibits in-sample predictive validity, and provides plausible out-of-sample forecasts, suggesting potential as a cost-effective supplement for survey-based research.
DA  - 2025/02/11/
PY  - 2025
DO  - 10.48550/arXiv.2411.01582
DP  - arXiv.org
PB  - arXiv
ST  - Donald Trumps in the Virtual Polls
UR  - http://arxiv.org/abs/2411.01582
Y2  - 2025/08/09/18:55:43
L1  - http://arxiv.org/pdf/2411.01582v2
L2  - http://arxiv.org/abs/2411.01582
KW  - Economics - General Economics
KW  - Quantitative Finance - Economics
ER  - 

TY  - GEN
TI  - Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys
AU  - Geng, Mingmeng
AU  - He, Sihong
AU  - Trotta, Roberto
AB  - Can large language models (LLMs) simulate social surveys? To answer this question, we conducted millions of simulations in which LLMs were asked to answer subjective questions. A comparison of different LLM responses with the European Social Survey (ESS) data suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. We further discussed statistical methods for measuring the difference between LLM answers and survey data and proposed a novel measure inspired by Jaccard similarity, as LLM-generated responses are likely to have a smaller variance. Our experiments also reveal that it is important to analyze the robustness and variability of prompts before using LLMs to simulate social surveys, as their imitation abilities are approximate at best.
DA  - 2024/10/21/
PY  - 2024
DO  - 10.48550/arXiv.2405.19323
DP  - arXiv.org
PB  - arXiv
ST  - Are Large Language Models Chameleons?
UR  - http://arxiv.org/abs/2405.19323
Y2  - 2025/08/09/18:58:24
L1  - http://arxiv.org/pdf/2405.19323v2
L2  - http://arxiv.org/abs/2405.19323
N1  - Comment: 17 pages
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computation and Language
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - GEN
TI  - Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses
AU  - Vogelsmeier, Leonie V. D. E.
AU  - Oliveira, Eduardo
AU  - Misiejuk, Kamila
AU  - López-Pernas, Sonsoles
AU  - Saqr, Mohammed
AB  - Large language models (LLMs) offer the potential to simulate human-like responses and behaviors, creating new opportunities for psychological science. In the context of self-regulated learning (SRL), if LLMs can reliably simulate survey responses at scale and speed, they could be used to test intervention scenarios, refine theoretical models, augment sparse datasets, and represent hard-to-reach populations. However, the validity of LLM-generated survey responses remains uncertain, with limited research focused on SRL and existing studies beyond SRL yielding mixed results. Therefore, in this study, we examined LLM-generated responses to the 44-item Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich \& De Groot, 1990), a widely used instrument assessing students' learning strategies and academic motivation. Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large. We analyzed item distributions, the psychological network of the theoretical SRL dimensions, and psychometric validity based on the latent factor structure. Our results suggest that Gemini 2 Flash was the most promising LLM, showing considerable sampling variability and producing underlying dimensions and theoretical relationships that align with prior theory and empirical findings. At the same time, we observed discrepancies and limitations, underscoring both the potential and current constraints of using LLMs for simulating psychological survey data and applying it in educational contexts.
DA  - 2025/06/16/
PY  - 2025
DO  - 10.48550/arXiv.2506.13384
DP  - arXiv.org
PB  - arXiv
ST  - Delving Into the Psychology of Machines
UR  - http://arxiv.org/abs/2506.13384
Y2  - 2025/08/09/18:58:29
L1  - http://arxiv.org/pdf/2506.13384v1
L2  - http://arxiv.org/abs/2506.13384
KW  - Computer Science - Computers and Society
KW  - Computer Science - Artificial Intelligence
KW  - Statistics - Methodology
KW  - Statistics - Other Statistics
ER  - 

TY  - JOUR
TI  - Synthetic Replacements for Human Survey Data? The Perils of Large Language Models
AU  - Bisbee, James
AU  - Clinton, Joshua D.
AU  - Dorff, Cassy
AU  - Kenkel, Brenton
AU  - Larson, Jennifer M.
T2  - Political Analysis
AB  - Large language models (LLMs) offer new research possibilities for social scientists, but their potential as “synthetic data” is still largely unknown. In this paper, we investigate how accurately the popular LLM ChatGPT can recover public opinion, prompting the LLM to adopt different “personas” and then provide feeling thermometer scores for 11 sociopolitical groups. The average scores generated by ChatGPT correspond closely to the averages in our baseline survey, the 2016–2020 American National Election Study (ANES). Nevertheless, sampling by ChatGPT is not reliable for statistical inference: there is less variation in responses than in the real surveys, and regression coefficients often differ significantly from equivalent estimates obtained using ANES data. We also document how the distribution of synthetic responses varies with minor changes in prompt wording, and we show how the same prompt yields significantly different results over a 3-month period. Altogether, our findings raise serious concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.
DA  - 2024/10//
PY  - 2024
DO  - 10.1017/pan.2024.5
DP  - Cambridge University Press
VL  - 32
IS  - 4
SP  - 401
EP  - 416
LA  - en
SN  - 1047-1987, 1476-4989
ST  - Synthetic Replacements for Human Survey Data?
UR  - https://www.cambridge.org/core/journals/political-analysis/article/synthetic-replacements-for-human-survey-data-the-perils-of-large-language-models/B92267DC26195C7F36E63EA04A47D2FE?utm_source=chatgpt.com
Y2  - 2025/08/09/18:58:32
L1  - https://www.cambridge.org/core/services/aop-cambridge-core/content/view/B92267DC26195C7F36E63EA04A47D2FE/S1047198724000056a.pdf/div-class-title-synthetic-replacements-for-human-survey-data-the-perils-of-large-language-models-div.pdf
KW  - ChatGPT
KW  - public opinion
KW  - research ethics
KW  - synthetic data
ER  - 

TY  - GEN
TI  - Questioning the Survey Responses of Large Language Models
AU  - Dominguez-Olmedo, Ricardo
AU  - Hardt, Moritz
AU  - Mendler-Dünner, Celestine
AB  - Surveys have recently gained popularity as a tool to study large language models. By comparing models’ survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter “A”. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.
DA  - 2024/11/06/
PY  - 2024
DO  - 10.48550/arXiv.2306.07951
DP  - openreview.net
LA  - en
UR  - https://openreview.net/forum?id=Oo7dlLgqQX&utm_source=chatgpt.com
Y2  - 2025/08/09/18:58:34
L1  - https://openreview.net/pdf?id=Oo7dlLgqQX
ER  - 

TY  - CONF
TI  - Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing
AU  - Kaur, Arshnoor
AU  - Aird, Amanda
AU  - Borman, Harris
AU  - Nicastro, Andrea
AU  - Leontjeva, Anna
AU  - Pizzato, Luiz
AU  - Jermyn, Dan
T3  - UMAP '25
AB  - Large Language Models (LLMs) can impersonate the writing style of authors, characters, and groups of people, but can these personas represent their opinions? If so, it creates opportunities for businesses to obtain early feedback on ideas from a synthetic customer-base. In this paper, we test whether LLM synthetic personas can answer financial wellbeing questions similarly to the responses of a financial wellbeing survey of more than 3,500 Australians. We focus on identifying salient biases of 765 synthetic personas using four state-of-the-art LLMs built over 35 categories of personal attributes. We noticed clear biases related to age, and as more details were included in the personas, their responses increasingly diverged from the survey toward lower financial wellbeing. With these findings, it is possible to understand the areas in which creating synthetic LLM-based customer personas can yield useful feedback for faster product iteration in the financial services industry and potentially other industries.
C1  - New York, NY, USA
C3  - Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization
DA  - 2025/06/13/
PY  - 2025
DO  - 10.1145/3699682.3728339
DP  - ACM Digital Library
SP  - 185
EP  - 193
PB  - Association for Computing Machinery
SN  - 979-8-4007-1313-2
ST  - Synthetic Voices
UR  - https://dl.acm.org/doi/10.1145/3699682.3728339
Y2  - 2025/09/01/
L1  - https://dl.acm.org/doi/pdf/10.1145/3699682.3728339
ER  - 

TY  - GEN
TI  - Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices
AU  - Wang, Han
AU  - Pawlak, Jacek
AU  - Sivakumar, Aruna
AB  - Stated preference (SP) surveys are a key method to research how individuals make trade-offs in hypothetical, also futuristic, scenarios. In energy context this includes key decarbonisation enablement contexts, such as low-carbon technologies, distributed renewable energy generation, and demand-side response [1,2]. However, they tend to be costly, time-consuming, and can be affected by respondent fatigue and ethical constraints. Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like textual responses, prompting growing interest in their application to survey research. This study investigates the use of LLMs to simulate consumer choices in energy-related SP surveys and explores their integration into data analysis workflows. A series of test scenarios were designed to systematically assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and DeepSeek-R1) at both individual and aggregated levels, considering contexts factors such as prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, LLM types, integration with traditional choice models, and potential biases. Cloud-based LLMs do not consistently outperform smaller local models. In this study, the reasoning model DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Across models, systematic biases are observed against the gas boiler and no-retrofit options, with a preference for more energy-efficient alternatives. The findings suggest that previous SP choices are the most effective input factor, while longer prompts with additional factors and varied formats can cause LLMs to lose focus, reducing accuracy.
DA  - 2025/08/22/
PY  - 2025
DO  - 10.48550/arXiv.2503.10652
DP  - arXiv.org
PB  - arXiv
ST  - Can Large Language Models Simulate Human Responses?
UR  - http://arxiv.org/abs/2503.10652
Y2  - 2025/09/05/14:43:07
L1  - http://arxiv.org/pdf/2503.10652v3
L2  - http://arxiv.org/abs/2503.10652
KW  - Computer Science - Computers and Society
KW  - Computer Science - Computation and Language
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - JOUR
TI  - LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations
AU  - Cho, Suhyun
AU  - Kim, Jaeyun
AU  - Kim, Jang Hyun
T2  - IEEE Access
AB  - This study explores whether large language models (LLMs) can learn a person’s opinions from their speech and act based on that knowledge. It also proposes the potential for utilizing such trained models in survey research. Traditional survey research collects information through standardized questions. However, surveys require repeated administration with new participants each time, which involves significant costs and time. With the recent advancements in LLMs, artificial intelligence (AI) has shown remarkable capabilities, often surpassing humans in tasks that require natural language understanding (NLU) and natural language generation (NLG). Despite this, research on whether AI can replicate human thought processes in tasks such as text interpretation or question-answering remains insufficient. This study proposes a Surveyed LLM, specialized for survey tasks, and a Doppelganger LLM that mimics human thought processes. It tests to what extent the Doppelganger model can replicate human judgment. Furthermore, it suggests the possibility of mimicking not only group distributions but also individual opinions.
DA  - 2024///
PY  - 2024
DO  - 10.1109/ACCESS.2024.3502219
DP  - IEEE Xplore
VL  - 12
SP  - 178917
EP  - 178927
SN  - 2169-3536
ST  - LLM-Based Doppelgänger Models
UR  - https://ieeexplore.ieee.org/document/10758652
Y2  - 2025/09/05/14:43:18
L1  - https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=10758652&ref=
KW  - Surveys
KW  - synthetic data
KW  - Data models
KW  - LLM
KW  - NLP
KW  - Predictive models
KW  - Adaptation models
KW  - Artificial intelligence
KW  - Computational modeling
KW  - Costs
KW  - Mathematical models
KW  - NLU
KW  - Oral communication
KW  - survey research
KW  - Training
ER  - 

TY  - GEN
TI  - Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour
AU  - Holtdirk, Tobias
AU  - Assenmacher, Dennis
AU  - Bleier, Arnim
AU  - Wagner, Claudia
AB  - A major challenge for survey researchers is dealing with missing data, which restricts the scope of analysis and the reliability of inferences that can be drawn. Recently, researchers have started investigating the potential of Large Language Models (LLMs) to role-play a pre-defined set of ``characters'' and simulate their survey responses with little or no additional training data and costs. Previous research has mostly focused on zero-shot LLM predictions. However, often other survey responses are at least partially available. This work investigates the viability and robustness of supervised fine-tuning on these responses to simulate systematic and random item-level non-responses in the context of German voting behaviour.
Our results show when systematic item non-responses are present, fine-tuned LLMs outperform traditional classification approaches on survey data. Fine-tuned LLMs also seem to be more robust to changes in the set of features that the model can use to make predictions. Finally, we see that fine-tuned LLMs match the performance of traditional classification methods when survey responses are missing completely at random.
DA  - 2025/02/27/
PY  - 2025
DO  - 10.31219/osf.io/udz28_v2
DP  - OSF Preprints
LA  - en-us
PB  - OSF
ST  - Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models
UR  - https://osf.io/udz28_v2
Y2  - 2025/09/05/14:43:45
ER  - 

TY  - GEN
TI  - AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction
AU  - Kim, Junsol
AU  - Lee, Byungkyu
AB  - Large language models (LLMs) that produce human-like responses have begun to revolutionize research practices in the social sciences. We develop a novel methodological framework that fine-tunes LLMs with repeated cross-sectional surveys to incorporate the meaning of survey questions, individual beliefs, and temporal contexts for opinion prediction. We introduce two new emerging applications of the AI-augmented survey: retrodiction (i.e., predict year-level missing responses) and unasked opinion prediction (i.e., predict entirely missing responses). Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in retrodiction (AUC = 0.86 for personal opinion prediction, $\rho$ = 0.98 for public opinion prediction). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. On the other hand, our fine-tuned Alpaca-7b models show modest success in unasked opinion prediction (AUC = 0.73, $\rho$ = 0.67). We discuss practical constraints and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction. Our study demonstrates that LLMs and surveys can mutually enhance each other's capabilities: LLMs can broaden survey potential, while surveys can improve the alignment of LLMs.
DA  - 2024/04/07/
PY  - 2024
DO  - 10.48550/arXiv.2305.09620
DP  - arXiv.org
PB  - arXiv
ST  - AI-Augmented Surveys
UR  - http://arxiv.org/abs/2305.09620
Y2  - 2025/09/08/00:03:14
L1  - http://arxiv.org/pdf/2305.09620v3
L2  - http://arxiv.org/abs/2305.09620
KW  - Computer Science - Computation and Language
KW  - Computer Science - Artificial Intelligence
KW  - Computer Science - Machine Learning
ER  - 

TY  - JOUR
TI  - Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models
AU  - Ahnert, Georg
AU  - Pellert, Max
AU  - Garcia, David
AU  - Strohmaier, Markus
T2  - Proceedings of the International AAAI Conference on Web and Social Media
AB  - This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We focus our analysis on the beginning of the COVID-19 pandemic that had a strong impact on public opinion and collective emotions. We validate our estimates against representative British survey data and find strong positive and significant correlations for several collective emotions. The estimates obtained are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. We demonstrate the flexibility of our method on questions of public opinion for which no pre-trained classifier is available. Our work extends the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. It enables flexible and new approaches to the longitudinal analysis of social media data.
DA  - 2025/06/07/
PY  - 2025
DO  - 10.1609/icwsm.v19i1.35801
DP  - ojs.aaai.org
VL  - 19
SP  - 15
EP  - 36
LA  - en
SN  - 2334-0770
UR  - https://ojs.aaai.org/index.php/ICWSM/article/view/35801
Y2  - 2025/09/08/10:01:46
L1  - https://ojs.aaai.org/index.php/ICWSM/article/download/35801/37955
KW  - LLM as Aggregrated Proxies
ER  - 

TY  - JOUR
TI  - AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators
AU  - Arora, Neeraj
AU  - Chakraborty, Ishita
AU  - Nishimura, Yohei
T2  - Journal of Marketing
AB  - The authors’ central premise is that a human–LLM (large language model) hybrid approach leads to efficiency and effectiveness gains in the marketing research process. In qualitative research, they show that LLMs can assist in both data generation and analysis; LLMs effectively create sample characteristics, generate synthetic respondents, and conduct and moderate in-depth interviews. The AI–human hybrid generates information-rich, coherent data that surpasses human-only data in depth and insightfulness and matches human performance in data analysis tasks of generating themes and summaries. Evidence from expert judges shows that humans and LLMs possess complementary skills; the human–LLM hybrid outperforms its human-only or LLM-only counterpart. For quantitative research, the LLM correctly picks the answer direction and valence, with the quality of synthetic data significantly improving through few-shot learning and retrieval-augmented generation. The authors demonstrate the value of the AI–human hybrid by collaborating with a Fortune 500 food company and replicating a 2019 qualitative and quantitative study using GPT-4. For their empirical investigation, the authors design the system architecture and prompts to create personas, ask questions, and obtain responses from synthetic respondents. They provide road maps for integrating LLMs into qualitative and quantitative marketing research and conclude that LLMs serve as valuable collaborators in the insight generation process.
DA  - 2025/03/01/
PY  - 2025
DO  - 10.1177/00222429241276529
DP  - SAGE Journals
VL  - 89
IS  - 2
SP  - 43
EP  - 70
LA  - EN
SN  - 0022-2429
ST  - AI–Human Hybrids for Marketing Research
UR  - https://doi.org/10.1177/00222429241276529
Y2  - 2025/09/09/11:20:03
L1  - https://journals.sagepub.com/doi/pdf/10.1177/00222429241276529
ER  - 

TY  - JOUR
TI  - Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response
AU  - Ji, Junyung
AU  - Kim, Jiwoo
AU  - Kim, Younghoon
T2  - Future Internet
AB  - Survey data play a crucial role in various research fields, including economics, education, and healthcare, by providing insights into human behavior and opinions. However, item non-response, where respondents fail to answer specific questions, presents a significant challenge by creating incomplete datasets that undermine data integrity and can hinder or even prevent accurate analysis. Traditional methods for addressing missing data, such as statistical imputation techniques and deep learning models, often fall short when dealing with the rich linguistic content of survey data. These approaches are also hampered by high time complexity for training and the need for extensive preprocessing or feature selection. In this paper, we introduce an approach that leverages Large Language Models (LLMs) through prompt engineering for predicting item non-responses in survey data. Our method combines the strengths of both traditional imputation techniques and deep learning methods with the advanced linguistic understanding of LLMs. By integrating respondent similarities, question relevance, and linguistic semantics, our approach enhances the accuracy and comprehensiveness of survey data analysis. The proposed method bypasses the need for complex preprocessing and additional training, making it adaptable, scalable, and capable of generating explainable predictions in natural language. We evaluated the effectiveness of our LLM-based approach through a series of experiments, demonstrating its competitive performance against established methods such as Multivariate Imputation by Chained Equations (MICE), MissForest, and deep learning models like TabTransformer. The results show that our approach not only matches but, in some cases, exceeds the performance of these methods while significantly reducing the time required for data processing.
DA  - 2024/10//
PY  - 2024
DO  - 10.3390/fi16100351
DP  - www.mdpi.com
VL  - 16
IS  - 10
SP  - 351
LA  - en
SN  - 1999-5903
UR  - https://www.mdpi.com/1999-5903/16/10/351
Y2  - 2025/09/09/12:54:27
L1  - https://www.mdpi.com/1999-5903/16/10/351/pdf?version=1727429624
KW  - large language models
KW  - item non-response
KW  - prompt engineering
KW  - survey data
ER  - 

TY  - JOUR
TI  - Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice
AU  - von der Heyde, Leah
AU  - Haensch, Anna-Carolina
AU  - Wenz, Alexander
T2  - Social Science Computer Review
AB  - “Synthetic samples” generated by large language models (LLMs) have been argued to complement or replace traditional surveys, assuming their training data is grounded in human-generated data that potentially reflects attitudes and behaviors prevalent in the population. Initial US-based studies that have prompted LLMs to mimic survey respondents found that the responses match survey data. However, the relationship between the respective target population and LLM training data might affect the generalizability of such findings. In this paper, we critically evaluate the use of LLMs for public opinion research in a different context, by investigating whether LLMs can estimate vote choice in Germany. We generate a synthetic sample matching the 2017 German Longitudinal Election Study respondents and ask the LLM GPT-3.5 to predict each respondent’s vote choice. Comparing these predictions to the survey-based estimates on the aggregate and subgroup levels, we find that GPT-3.5 exhibits a bias towards the Green and Left parties. While the LLM predictions capture the tendencies of “typical” voters, they miss more complex factors of vote choice. By examining the LLM-based prediction of voting behavior in a non-English speaking context, our study contributes to research on the extent to which LLMs can be leveraged for studying public opinion. The findings point to disparities in opinion representation in LLMs and underscore the limitations in applying them for public opinion estimation.
DA  - 2025/04/26/
PY  - 2025
DO  - 10.1177/08944393251337014
DP  - SAGE Journals
SP  - 08944393251337014
LA  - EN
SN  - 0894-4393
ST  - Vox Populi, Vox AI?
UR  - https://doi.org/10.1177/08944393251337014
Y2  - 2025/09/09/15:55:09
L1  - https://journals.sagepub.com/doi/pdf/10.1177/08944393251337014
ER  - 

TY  - CONF
TI  - The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models
AU  - Ma, Bolei
AU  - Wang, Xinpeng
AU  - Hu, Tiancheng
AU  - Haensch, Anna-Carolina
AU  - Hedderich, Michael A.
AU  - Plank, Barbara
AU  - Kreuter, Frauke
T2  - Findings 2024
A2  - Al-Onaizan, Yaser
A2  - Bansal, Mohit
A2  - Chen, Yun-Nung
T3  - Pure Systematic Review
AB  - Recent advances in Large Language Models (LLMs) have sparked wide interest in validating and comprehending the human-like cognitive-behavioral traits LLMs may capture and convey. These cognitive-behavioral traits include typically Attitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within LLMs remains opaque, and different evaluation methods may yield different results. This has led to a lack of clarity on how different studies are related to each other and how they can be interpreted. This paper aims to bridge this gap by providing a comprehensive overview of recent works on the evaluation of AOVs in LLMs. Moreover, we survey related approaches in different stages of the evaluation pipeline in these works. By doing so, we address the potential and challenges with respect to understanding the model, human-AI alignment, and downstream application in social sciences. Finally, we provide practical insights into evaluation methods, model enhancement, and interdisciplinary collaboration, thereby contributing to the evolving landscape of evaluating AOVs in LLMs.
C1  - Miami, Florida, USA
C3  - Findings of the Association for Computational Linguistics: EMNLP 2024
DA  - 2024/11//
PY  - 2024
DO  - 10.18653/v1/2024.findings-emnlp.513
DP  - ACLWeb
SP  - 8783
EP  - 8805
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2024.findings-emnlp.513/
Y2  - 2025/09/09/15:56:27
L1  - https://aclanthology.org/2024.findings-emnlp.513.pdf
ER  - 

TY  - GEN
TI  - Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information
AU  - Sun, Seungjong
AU  - Lee, Eungu
AU  - Nan, Dongyan
AU  - Zhao, Xiangying
AU  - Lee, Wonbyung
AU  - Jansen, Bernard J.
AU  - Kim, Jang Hyun
AB  - Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose "random silicon sampling," a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.
DA  - 2024/02/28/
PY  - 2024
DO  - 10.48550/arXiv.2402.18144
DP  - arXiv.org
PB  - arXiv
ST  - Random Silicon Sampling
UR  - http://arxiv.org/abs/2402.18144
Y2  - 2025/09/09/16:21:42
L1  - http://arxiv.org/pdf/2402.18144v1
N1  - Comment: 25 pages, 4 figures, 19 Tables
KW  - Computer Science - Computers and Society
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - CONF
TI  - Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study
AU  - Ma, Bolei
AU  - Yoztyurk, Berk
AU  - Haensch, Anna-Carolina
AU  - Wang, Xinpeng
AU  - Herklotz, Markus
AU  - Kreuter, Frauke
AU  - Plank, Barbara
AU  - Aßenmacher, Matthias
T2  - ACL 2025
A2  - Che, Wanxiang
A2  - Nabende, Joyce
A2  - Shutova, Ekaterina
A2  - Pilehvar, Mohammad Taher
AB  - In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD. Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions. These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.
C1  - Vienna, Austria
C3  - Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2025/07//
PY  - 2025
DO  - 10.18653/v1/2025.acl-long.90
DP  - ACLWeb
SP  - 1785
EP  - 1809
PB  - Association for Computational Linguistics
SN  - 979-8-89176-251-0
ST  - Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions
UR  - https://aclanthology.org/2025.acl-long.90/
Y2  - 2025/09/10/10:00:37
L1  - https://aclanthology.org/2025.acl-long.90.pdf
ER  - 

TY  - CONF
TI  - Knowledge of cultural moral norms in large language models
AU  - Ramezani, Aida
AU  - Xu, Yang
T2  - ACL 2023
A2  - Rogers, Anna
A2  - Boyd-Graber, Jordan
A2  - Okazaki, Naoaki
AB  - Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as “homosexuality” and “divorce”; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.
C1  - Toronto, Canada
C3  - Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
DA  - 2023/07//
PY  - 2023
DO  - 10.18653/v1/2023.acl-long.26
DP  - ACLWeb
SP  - 428
EP  - 446
PB  - Association for Computational Linguistics
UR  - https://aclanthology.org/2023.acl-long.26/
Y2  - 2025/09/10/14:13:45
L1  - https://aclanthology.org/2023.acl-long.26.pdf
KW  - LLM as Aggregrated Proxies
ER  - 

TY  - GEN
TI  - More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research
AU  - Buskirk, Trent D.
AU  - Keusch, Florian
AU  - Heyde, Leah von der
AU  - Eck, Adam
AB  - Survey research has a long-standing history of being a human-powered field, but one that embraces various technologies for the collection, processing, and analysis of various behavioral, political, and social outcomes of interest, among others. At the same time, Large Language Models (LLMs) bring new technological challenges and prerequisites in order to fully harness their potential. In this paper, we report work-in-progress on a systematic literature review based on keyword searches from multiple large-scale databases as well as citation networks that assesses how LLMs are currently being applied within the survey research process. We synthesize and organize our findings according to the survey research process to include examples of LLM usage across three broad phases: pre-data collection, data collection, and post-data collection. We discuss selected examples of potential use cases for LLMs as well as its pitfalls based on examples from existing literature. Considering survey research has rich experience and history regarding data quality, we discuss some opportunities and describe future outlooks for survey research to contribute to the continued development and refinement of LLMs.
DA  - 2025/09/03/
PY  - 2025
DO  - 10.48550/arXiv.2509.03391
DP  - arXiv.org
PB  - arXiv
ST  - More Parameters Than Populations
UR  - http://arxiv.org/abs/2509.03391
Y2  - 2025/09/11/11:23:47
L1  - http://arxiv.org/pdf/2509.03391v1
L2  - http://arxiv.org/abs/2509.03391
KW  - Computer Science - Computers and Society
KW  - Computer Science - Digital Libraries
ER  - 

TY  - GEN
TI  - The threat of analytic flexibility in using large language models to simulate human data: A call to attention
AU  - Cummins, Jamie
AB  - Social scientists are now using large language models to create "silicon samples" - synthetic datasets intended to stand in for human respondents, aimed at revolutionising human subjects research. However, there are many analytic choices which must be made to produce these samples. Though many of these choices are defensible, their impact on sample quality is poorly understood. I map out these analytic choices and demonstrate how a very small number of decisions can dramatically change the correspondence between silicon samples and human data. Configurations (N = 252) varied substantially in their capacity to estimate (i) rank ordering of participants, (ii) response distributions, and (iii) between-scale correlations. Most critically, configurations were not consistent in quality: those that performed well on one dimension often performed poorly on another, implying that there is no "one-size-fits-all" configuration that optimises the accuracy of these samples. I call for greater attention to the threat of analytic flexibility in using silicon samples.
DA  - 2025/09/16/
PY  - 2025
DO  - 10.48550/arXiv.2509.13397
DP  - arXiv.org
PB  - arXiv
ST  - The threat of analytic flexibility in using large language models to simulate human data
UR  - http://arxiv.org/abs/2509.13397
Y2  - 2025/09/18/13:54:43
L1  - http://arxiv.org/pdf/2509.13397v1
L2  - http://arxiv.org/abs/2509.13397
N1  - Comment: 11 pages, 3 figures
KW  - Computer Science - Computers and Society
KW  - Computer Science - Artificial Intelligence
ER  - 

TY  - JOUR
TI  - Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias
AU  - Lee, Sanguk
AU  - Peng, Tai-Quan
AU  - Goldberg, Matthew H.
AU  - Rosenthal, Seth A.
AU  - Kotcher, John E.
AU  - Maibach, Edward W.
AU  - Leiserowitz, Anthony
T2  - PLOS Climate
AB  - Large language models (LLMs) can be used to estimate human attitudes and behavior, including measures of public opinion, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs in estimating public opinion about global warming. LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. Findings indicate that LLMs can effectively reproduce presidential voting behaviors but not global warming opinions unless the issue relevant covariates are included. When conditioned on both demographic and covariates, GPT-4 demonstrates improved accuracy, ranging from 53% to 91%, in predicting beliefs and attitudes about global warming. Additionally, we find an algorithmic bias that underestimates the global warming opinions of Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation.
DA  - 2024/08/07/
PY  - 2024
DO  - 10.1371/journal.pclm.0000429
DP  - PLoS Journals
VL  - 3
IS  - 8
SP  - e0000429
J2  - PLOS Climate
LA  - en
SN  - 2767-3200
ST  - Can large language models estimate public opinion about global warming?
UR  - https://journals.plos.org/climate/article?id=10.1371/journal.pclm.0000429
Y2  - 2025/09/30/16:28:09
L1  - https://journals.plos.org/climate/article/file?id=10.1371/journal.pclm.0000429&type=printable
KW  - Social sciences
KW  - Surveys
KW  - Survey research
KW  - Algorithms
KW  - Conditioned response
KW  - Global warming
KW  - Political parties
KW  - Psychological attitudes
ER  - 

TY  - CONF
TI  - SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies
AU  - Choube, Akshat
AU  - Swain, Vedant Das
AU  - Mishra, Varun
T2  - 2024 12th International Conference on Affective Computing and Intelligent Interaction (ACII)
AB  - Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning models. However, the continuous need to self-report various internal states adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behav-ioral dispositions, enabling LLMs to construct mental models of participants to simulate their responses on psychological scales. As part of the framework, we provide four evaluation metrics to assess the effectiveness of the simulated responses. We demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses on one scale using responses from another as behavioral information. We use SeSaMe's evaluation metrics to assess the alignment between human and SeSaMe-simulated responses to psychological scales. Then, we present multiple experiments to inspect the utility of SeSaMe-simulated responses as ground truth in training machine-learning models by replicating established depression and anxiety screening tasks with passive sensing data from a previous study. Our results indicate SeSaMe to be a promising approach, but its alignment may vary across scales and specific prediction objectives. We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges with ground-truth collection in passive sensing studies.
DA  - 2024/09/01/
PY  - 2024
DO  - 10.1109/ACII63134.2024.00031
DP  - www.computer.org
SP  - 228
EP  - 237
LA  - English
PB  - IEEE Computer Society
SN  - 979-8-3315-1643-7
ST  - SeSaMe
UR  - https://www.computer.org/csdl/proceedings-article/acii/2024/164300a228/26aTZgMz31e
Y2  - 2025/09/30/16:28:14
ER  - 

TY  - JOUR
TI  - Performance and biases of Large Language Models in public opinion simulation
AU  - Qu, Yao
AU  - Wang, Jue
T2  - Humanities and Social Sciences Communications
AB  - The rise of Large Language Models (LLMs) like ChatGPT marks a pivotal advancement in artificial intelligence, reshaping the landscape of data analysis and processing. By simulating public opinion, ChatGPT shows promise in facilitating public policy development. However, challenges persist regarding its worldwide applicability and bias across demographics and themes. Our research employs socio-demographic data from the World Values Survey to evaluate ChatGPT’s performance in diverse contexts. Findings indicate significant performance disparities, especially when comparing countries. Models perform better in Western, English-speaking, and developed nations, notably the United States, in comparison to others. Disparities also manifest across demographic groups, showing biases related to gender, ethnicity, age, education, and social class. The study further uncovers thematic biases in political and environmental simulations. These results highlight the need to enhance LLMs’ representativeness and address biases, ensuring their equitable and effective integration into public opinion research alongside conventional methodologies.
DA  - 2024/08/28/
PY  - 2024
DO  - 10.1057/s41599-024-03609-x
DP  - www.nature.com
VL  - 11
IS  - 1
SP  - 1095
J2  - Humanit Soc Sci Commun
LA  - en
SN  - 2662-9992
UR  - https://www.nature.com/articles/s41599-024-03609-x
Y2  - 2025/09/30/16:28:38
L1  - https://www.nature.com/articles/s41599-024-03609-x.pdf
KW  - Politics and international relations
KW  - Science
KW  - technology and society
ER  - 

