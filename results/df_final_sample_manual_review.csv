paperId,title,year,authors,abstract,doi,publicationTypes,researchAreas,source
88a390141104b22981d4fd8fe891b42f84670057,Generative AI Increases Cybersecurity Risks for Seniors,2024,Zahm Siyed,"We evaluate how generative AI exacerbates the cyber risks faced by senior citizens. We assess the risk that powerful LLMs can easily be misconfigured to serve a malicious purpose, and that platforms such as HackGPT or WormGPT can facilitate low-skilled script kiddies to replicate the effectiveness of high-skilled threat actors. We surveyed 85 seniors and found that the combination of loneliness and low cyber literacy places 87% of them at high risk of being hacked. Our survey further revealed that 67% of seniors have already been exposed to potentially exploitable digital intrusions and only 22% of seniors have sufficient awareness of risks to ask techno-literate for remedial assistance. Our risk analysis suggests that existing attack vectors can be augmented with AI to create highly personalized and believable digital exploits that are extremely difficult for seniors to distinguish from legitimate interactions. Technological advances allow for the replication of familiar voices, live digital reconstruction of faces, personalized targeting, and falsification of records. Once an attack vector is identified, certain generative polymorphic capabilities allow rapid mutation and obfuscation to deliver unique payloads. Both inbound and outbound risks exist. In addition to inbound attempts by individual threat actors, seniors are vulnerable to outbound attacks through poisoned LLMs, such as Threat GPT or PoisonGPT. Generative AI can maliciously alter databases to provide incorrect information or compromised instructions to gullible seniors seeking outbound digital guidance. By analyzing the extent to which senior citizens are at risk of exploitation through new developments in AI, the paper will contribute to the development of effective strategies to safeguard this vulnerable population.",10.5539/cis.v17n2p39,"['JournalArticle', 'Review']",Computer Science,Semantic Scholar
eae23f2d833ef1b63c144e5d59e749e313597f8f,Reimagining Personal Data: Unlocking the Potential of AI-Generated Images in Personal Data Meaning-Making,2025,"Soobin Park, Hankyung Kim, Youn-kyung Lim","Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-based application as a probe that represents personal data through generative images utilizing Open AI’s GPT-4 model and DALL-E 3. We then conducted a 21-day diary study and interviews using the probe with 16 participants to investigate users’ in-depth experiences with images generated by AI in everyday lives. Our findings reveal new qualities of experiences in users’ engagement with data, highlighting how participants constructed personal meaning from their data through imagination and speculation on AI-generated images. We conclude by discussing the potential and concerns of leveraging image-generative AI for personal data meaning-making.",10.1145/3706598.3713722,"['JournalArticle', 'Book', 'Conference']",Computer Science,Semantic Scholar
224153bd0306ca73647ddaef890aad0899874b4c,Open-Ended NPC Dialogue Favors Casual Players: A Pilot Comparison of Three LLM-Driven Dialogue Systems,2025,"Rasmus Ploug, Emil Rimer, Anthon Kristian Skov Petersen, Marco Scirea","Non-player character (NPC) dialogue plays a crucial role in shaping the player experience in narrativedriven video games, influencing agency, immersion and story engagement. Despite the recent advancements in large language models (LLMs) for dynamic dialogue generation, few empirical studies have compared their impact across different dialogue system designs. This pilot study explores how LLM-driven dialogue systems affect the player experience using a custom-developed role-playing game (RPG) featuring four different dialogue designs; static control (CV), rephrase (A), hybrid (B) and fully open-ended (C). Behavioral data and post-game questionnaires were collected from 64 participants. Results indicate that fully open-ended dialogues led to significantly longer dialogue interactions and higher overall engagement, particularly among casual players, with the survey feedback highlighting its immersive and natural tone. These findings suggest that fully open-ended LLM-based dialogue in video games can enhance narrative depth and player involvement.",10.1109/CoG64752.2025.11114150,"['JournalArticle', 'Conference', 'Review']",Computer Science,Semantic Scholar
WOS:001517190200239,Assessing Gender and Age Influences on Moral Decision Making in Autonomous Vehicles Using Large Language Models,2025,"Rathore, Heena; Chowdary, Pranay Jasti; Griffith, Henry","Autonomous systems, especially those in safety-critical applications like Autonomous Vehicles (AVs), require human-like reasoning capabilities to make ethical decisions. Large Language Models (LLMs) have shown potential in simulating diverse human moral responses, offering insights into how different moral frameworks, such as utilitarianism and deontological ethics, could enhance decision-making algorithms in AVs. Existing research indicates that LLMs tend to align with commonsense morality in morally unambiguous cases, but face challenges in providing detailed justifications for their choices. Studies leveraging frameworks like the Moral Machine and Moral Foundations Theory have explored how LLMs simulate human preferences. Despite this progress, a significant gap remains in understanding how gender and age impact moral preferences when decisions are influenced by LLMs in autonomous systems. This paper addresses this gap by investigating how LLM-based systems can reflect and adapt to moral preferences across gender and age groups, while ensuring that these systems offer transparent explanations that align with human moral intuitions in high-stakes AV decision-making scenarios. It was found that LLM models demonstrated diverse tendencies: some leaned towards favoring younger individuals over older ones, while others displayed a subtle preference for males in decision-making situations, highlighting differences in how the models prioritized age and gender.",10.1109/CCNC54725.2025.10976190,Proceedings Paper,Computer Science; Engineering; Telecommunications,Web of Science
5cfbaa9828d1cf447aaf9cb39f8ccadf3fd1de88,AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society,2025,"J. Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, Yong Li","Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.",10.48550/arXiv.2502.08691,"['JournalArticle', 'Review']",Computer Science; Sociology,Semantic Scholar
58c7539843595a9913551518f3e2f3fc39069bec,Driving Generative Agents With Their Personality,2024,"Lawrence J. Klinkert, Stephanie Buongiorno, Corey Clark","This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.",10.48550/arXiv.2402.14879,['JournalArticle'],Computer Science; Psychology,Semantic Scholar
5a13127f6ac6c486b7888f6ad22bf58656d63ee6,"Practicing Stress Relief for the Everyday: Designing Social Simulation Using VR, AR, and LLMs",2024,"Anna Fang, Hriday Chhabria, Alekhya Maram, Haiyi Zhu","Stress is an inevitable part of day-to-day life yet many find themselves unable to manage it themselves, particularly when professional or peer support are not always readily available. As self-care becomes increasingly vital for mental well-being, this paper explores the potential of social simulation as a safe, virtual environment for practicing stress relief for everyday situations. Leveraging the immersive capabilities of VR, AR, and LLMs, we developed eight interactive prototypes for various everyday stressful scenarios (e.g. public speaking) then conducted prototype-driven semi-structured interviews with 19 participants. We reveal that people currently lack effective means to support themselves through everyday stress and found that social simulation fills a gap for simulating real environments for training mental health practices. We outline key considerations for future development of simulation for self-care, including risks of trauma from hyper-realism, distrust of LLM-recommended timing for mental health recommendations, and the value of accessibility for self-care interventions.",10.48550/arXiv.2410.01672,['JournalArticle'],Computer Science; Psychology,Semantic Scholar
8eed06168bac98b2497f7408f1253614c68ccece,LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert Networks,2025,"Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng","The centralization of Large Language Models (LLMs) development has created significant barriers to AI advancement, limiting the democratization of these powerful technologies. This centralization, coupled with the scarcity of high-quality training data and mounting complexity of maintaining comprehensive expertise across rapidly expanding knowledge domains, poses critical challenges to the continued growth of LLMs. While solutions like Retrieval-Augmented Generation (RAG) offer potential remedies, maintaining up-to-date expert knowledge across diverse domains remains a significant challenge, particularly given the exponential growth of specialized information. This paper introduces LLMs Networks (LLM-Net), a blockchain-based framework that democratizes LLMs-as-a-Service through a decentralized network of specialized LLM providers. By leveraging collective computational resources and distributed domain expertise, LLM-Net incorporates fine-tuned expert models for various specific domains, ensuring sustained knowledge growth while maintaining service quality through collaborative prompting mechanisms. The framework's robust design includes blockchain technology for transparent transaction and performance validation, establishing an immutable record of service delivery. Our simulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet, Llama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the reputation-based mechanism in maintaining service quality by selecting high-performing respondents (LLM providers). Thereby it demonstrates the potential of LLM-Net to sustain AI advancement through the integration of decentralized expertise and blockchain-based accountability.",10.48550/arXiv.2501.07288,['JournalArticle'],Computer Science,Semantic Scholar
1ada7526a9f763ae5523ee843e932657f4caca86,Leveraging Computer Vision and Visual LLMs for Cost-Effective and Consistent Street Food Safety Assessment in Kolkata India,2025,"Alexey Chernikov, Klaus Ackermann, Caitlin Brown, Denni Tommasi","Ensuring street food safety in developing countries is crucial due to the high prevalence of foodborne illnesses. Traditional methods of food safety assessments face challenges such as resource constraints, logistical issues, and subjective biases influenced by surveyors personal lived experiences, particularly when interacting with local communities. For instance, a local food safety inspector may inadvertently overrate the quality of infrastructure due to prior familiarity or past purchases, thereby compromising objective assessment. This subjectivity highlights the necessity for technologies that reduce human biases and enhance the accuracy of survey data across various domains.
This paper proposes a novel approach based on a combination of Computer Vision and a lightweight Visual Large Language Model (VLLM) to automate the detection and analysis of critical food safety infrastructure in street food vendor environments at a field experiment in Kolkata, India. The system utilises a three-stage object extraction pipeline from the video to identify, extract and select unique representations of critical elements such as hand-washing stations, dishwashing areas, garbage bins, and water tanks. These four infrastructure items are crucial for maintaining safe food practices, irrespective of the specific methods employed by the vendors. A VLLM then analyses the extracted representations to assess compliance with food safety standards. Notably, over half of the pipeline can be processed using a user's smartphone, significantly reducing government server workload. By leveraging this decentralised approach, the proposed system decreases the analysis cost by many orders of magnitude compared to alternatives like ChatGPT or Claude 3.5. Additionally, processing data on local government servers provides better privacy and security than cloud platforms, addressing critical ethical considerations. This automated approach significantly improves efficiency, consistency, and scalability, providing a robust solution to enhance public health outcomes in developing regions.",10.1609/aaai.v39i27.35008,"['JournalArticle', 'Conference', 'Review']",Computer Science; Environmental Science,Semantic Scholar
a2be2060a0e2e7b5f420be97b5bb3c018b718a09,An AI-Powered Platform for Personalized Mock Interviews Using LLMs and Modern Full-Stack Web Technologies,2025,"Mr. K Kiran Babu, CH. Srikaran, G. Chandrashekar, D. Akshara, K. A. Reddy",,10.55248/gengpi.6.0625.2380,['JournalArticle'],Computer Science,Semantic Scholar
ea16041e6b898fa335fea54c4102b6f941e2bbe4,Generative Artificial Intelligence: Transforming the Future,2024,Kaviyaraj R,"We are at the dawn of a new area of AI: Generative Artificial Intelligence (AI), which holds the potential to automate and generate in ways never before possible and can be applied across countless disciplines from natural language processing (NLP) to image synthesis, interactive simulations, and more. Generative models like Generative Adversarial Networks and transformers, as well as cutting edge AI tools such as GitHub Copilot, DALL-E, Gemini, and Bing Copilot have made content generation a different game. From realizing intelligent code generation, realistic image synthesis, or personalized search, these tools are transforming industries to software development and digital art, to search engines. In this paper we present a comprehensive survey of generative AI technologies and the methodologies behind these technologies, including attention mechanisms, variational autoencoders, diffusion models, and transformer-based models such as GPT and BERT. The paper also discusses the implementation of generative AI across Augmented Reality and Virtual Reality, highlighting how the use of AI to generate content is revolutionizing the interactive experiences in gaming, education, and training. In the study we also perform performance evaluations of generative AI models on benchmark datasets using BLEU, ROUGE scores etc. We discuss some of the challenges facing generative AI on ethical, computational, and too-often opaque lines and predict that generative AI can transform industries by creating autonomous creativity and personalized learning.",10.1109/EmergIN63207.2024.10961802,"['Conference', 'Review']",Art; Computer Science,Semantic Scholar
5ea440a2ff384518a222989205d4729f0363a80f,Machine Learners Should Acknowledge the Legal Implications of Large Language Models as Personal Data,2025,"Henrik Nolte, Michele Finck, Kristof Meding","Does GPT know you? The answer depends on your level of public recognition; however, if your information was available on a website, the answer could be yes. Most Large Language Models (LLMs) memorize training data to some extent. Thus, even when an LLM memorizes only a small amount of personal data, it typically falls within the scope of data protection laws. If a person is identified or identifiable, the implications are far-reaching. The LLM is subject to EU General Data Protection Regulation requirements even after the training phase is concluded. To back our arguments: (1.) We reiterate that LLMs output training data at inference time, be it verbatim or in generalized form. (2.) We show that some LLMs can thus be considered personal data on their own. This triggers a cascade of data protection implications such as data subject rights, including rights to access, rectification, or erasure. These rights extend to the information embedded within the AI model. (3.) This paper argues that machine learning researchers must acknowledge the legal implications of LLMs as personal data throughout the full ML development lifecycle, from data collection and curation to model provision on e.g., GitHub or Hugging Face. (4.) We propose different ways for the ML research community to deal with these legal implications. Our paper serves as a starting point for improving the alignment between data protection law and the technical capabilities of LLMs. Our findings underscore the need for more interaction between the legal domain and the ML community.",10.48550/arXiv.2503.01630,['JournalArticle'],Computer Science; Law,Semantic Scholar
7c5d5e621ff3faa4860a9d6fd42d2c316c63194e,Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,2024,"Mucong Ding, Chenghao Deng, Jocelyn Choo, Zichu Wu, Aakriti Agrawal, Avi Schwarzschild, Tianyi Zhou, Tom Goldstein, John Langford, Anima Anandkumar, Furong Huang","While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",10.48550/arXiv.2409.18433,['JournalArticle'],Computer Science; Mathematics,Semantic Scholar
979ce0b348d1cb6151c7c521d3f6c48aedf2d3ae,Deep Learning and Transformers Accuracy in Rumor Detection on Social Media,2025,"Long Yu, Jiarui Dai, Jiaqi Dai, Yanan Wang","The increasing popularity of social media platforms has revolutionized how news and information are shared. While these social platforms facilitate rapid dissemination, they also provide fertile ground for the proliferation of rumors and unverified information. False information spreads as quickly as accurate news, often influencing public opinion and decision-making processes. Identifying rumors early is critical to limiting their potential harm and mitigating negative consequences. This study evaluates the practical application and scalability of transformer-based models, specifically GPT-2, in detecting rumors on social media platforms alongside traditional deep learning (DL) models. We explore various deep learning models such as Long Short-Term Memory (LSTM), Convolutional Neural Networks (CNN), ALBERT, and GPT-2. Performance was assessed using standard evaluation metrics, including accuracy, precision, recall, F1-score, and analysis of Receiver Operating Characteristic (ROC) curves. The comparative results reveal that transformer-based approaches significantly outperform traditional DL models in detecting rumors with higher accuracy and reliability. Among the evaluated models, GPT-2 achieved the highest scores across all performance metrics, demonstrating exceptional capability in identifying and predicting rumor-laden content. This study introduces key innovations, including a direct comparative analysis of transformer-based and traditional DL models, highlighting GPT-2’s advanced attention mechanisms that capture nuanced linguistic and contextual features. Additionally, it underscores GPT-2’s scalability for real-world misinformation mitigation and critically examines dataset biases and adaptability challenges. Future advancements, such as multimodal approaches integrating text, images, and videos, as well as hybrid models combining transformers with traditional DL techniques, are proposed to enhance detection accuracy and efficiency. These findings underline the transformative potential of advanced AI techniques in combating misinformation on social media platforms. The research emphasizes the potential for scalable and practical implementation of GPT-2-based tools in mitigating false information dissemination, contributing to a more reliable and resilient digital ecosystem. This work advances the understanding of AI's role in mitigating the spread of false information.",10.33166/aetic.2025.03.003,['JournalArticle'],Computer Science,Semantic Scholar
b960b6d4b6555fcf78f000de86f52b9f7605489d,Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation,2025,"Zilin Huang, Zihao Sheng, Zhengyang Wan, Yansong Qu, Yuhao Luo, Boyue Wang, Pei Li, Yen-Jung Chen, Jiancong Chen, Keke Long, Jiayi Meng, Yue Leng, Sikai Chen","Recent advances in autonomous system simulation platforms have significantly enhanced the safe and scalable testing of driving policies. However, existing simulators do not yet fully meet the needs of future transportation research-particularly in enabling effective human-AI collaboration and modeling socially-aware driving agents. This paper introduces Sky-Drive, a novel distributed multi-agent simulation platform that addresses these limitations through four key innovations: (a) a distributed architecture for synchronized simulation across multiple terminals; (b) a multi-modal human-in-the-loop framework integrating diverse sensors to collect rich behavioral data; (c) a human-AI collaboration mechanism supporting continuous and adaptive knowledge exchange; and (d) a digital twin framework for constructing high-fidelity virtual replicas of real-world transportation environments. Sky-Drive supports diverse applications such as autonomous vehicle-human road users interaction modeling, human-in-the-loop training, socially-aware reinforcement learning, personalized driving development, and customized scenario generation. Future extensions will incorporate foundation models for context-aware decision support and hardware-in-the-loop testing for real-world validation. By bridging scenario generation, data collection, algorithm training, and hardware integration, Sky-Drive has the potential to become a foundational platform for the next generation of human-centered and socially-aware autonomous transportation systems research. The demo video and code are available at:https://sky-lab-uw.github.io/Sky-Drive-website/",10.48550/arXiv.2504.18010,['JournalArticle'],Computer Science; Engineering,Semantic Scholar
55c3095681acc82780508b0e484dba0c30cf1caa,Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation,2024,"Gauthier Guinet, Behrooz Omidvar-Tehrani, Anoop Deoras, Laurent Callot","We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",10.48550/arXiv.2405.13622,"['JournalArticle', 'Conference']",Computer Science,Semantic Scholar
WOS:001255317903026,OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs,2024,"Li, Jiahao Nick; Xu, Yan; Grossman, Tovi; Santosa, Stephanie; Li, Michelle","The progression to Pervasive Augmented Reality envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users' context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to diferent types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multi-modal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classifcation, in-context learning and fnetuning) and identifed the most efective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.",10.1145/3613904.3642068,Proceedings Paper,Computer Science,Web of Science
