title,year,authors,abstract,doi,publicationTypes,researchAreas
Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias,2024,"Lee, Sanguk; Peng, Tai-Quan; Goldberg, Matthew H.; Rosenthal, Seth A.; Kotcher, John E.; Maibach, Edward W.; Leiserowitz, Anthony","Large language models (LLMs) can be used to estimate human attitudes and behavior, including measures of public opinion, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs in estimating public opinion about global warming. LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. Findings indicate that LLMs can effectively reproduce presidential voting behaviors but not global warming opinions unless the issue relevant covariates are included. When conditioned on both demographic and covariates, GPT-4 demonstrates improved accuracy, ranging from 53% to 91%, in predicting beliefs and attitudes about global warming. Additionally, we find an algorithmic bias that underestimates the global warming opinions of Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation.",10.1371/journal.pclm.0000429,Article,Environmental Sciences & Ecology
From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models,2023,"Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak, JinYeong","Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting largescale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods-argument generation and question answering-designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.",,Proceedings Paper,Computer Science; Linguistics
Towards realistic evaluation of cultural value alignment in large language models: Diversity enhancement for survey response simulation,2025,"Liu, Haijiang; Cao, Yong; Wu, Xun; Qiu, Chen; Gu, Jinguang; Liu, Maofu; Hershcovich, Daniel","Assessing Large Language Models (LLMs) alignment with human values has been a high priority in natural language processing. These models, praised as reservoirs of collective human knowledge, provoke an important question: Do they genuinely reflect the value preferences embraced by different cultures? We measure value alignment by simulating sociological surveys and comparing the distribution of preferences from model responses to human references. We introduce a diversity-enhancement framework featuring a novel memory simulation mechanism, which enables the generation of model preference distributions and captures the diversity and uncertainty inherent in LLM behaviors through realistic survey experiments. To better understand the causes of misalignment, we have developed comprehensive evaluation metrics. Our analysis of multilingual survey data illustrates that our framework improves the reliability of cultural value alignment assessments and captures the complexity of model responses across cultural contexts. Among the eleven models evaluated, the Mistral and Llama-3 series show superior alignment with cultural values, with Mistral-series models notably excelling in comprehending these values in both U.S. and Chinese contexts.1",10.1016/j.ipm.2025.104099,Article,Computer Science; Information Science & Library Science
Eysenck Personality Questionnaire: A Comparative Study of Humans and Large Language Models Through Repeated Administrations,2024,"Antal, Margit; Beder, Norbert","This study investigates the personality traits of large language models (LLMs) using the Eysenck Personality Questionnaire (EPQ) and compares their responses to those of human participants. By administering the EPQ three times to both groups, we aim to analyze the stability and consistency of their responses over time. Our research focuses on open-source LLMs. The results indicate that LLMs exhibit significantly higher extraversion scores compared to humans, while differences in lie, neuroticism, and rigidity traits are not statistically significant. Additionally, LLMs show greater variability in their responses across repeated administrations, suggesting less consistency compared to human participants. We also examine the effects of prompt design and the temperature hyperparameter on LLM personality traits. Both temperature settings and prompt instructions significantly shape the personality-like patterns in LLM outputs. These findings contribute to the understanding of how LLMs can mimic human personality traits and the implications for their use in personalized communication and user engagement.",10.47745/ausi-2024-0012,Article,Computer Science
SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies,2024,"Choube, Akshat; Das Swain, Vedant; Mishra, Varun","Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning models. However, the continuous need to self-report various internal states adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate their responses on psychological scales. As part of the framework, we provide four evaluation metrics to assess the effectiveness of the simulated responses. We demonstrate an application of SeSaMe, where we use GPT4 to simulate responses on one scale using responses from another as behavioral information. We use SeSaMe's evaluation metrics to assess the alignment between human and SeSaMesimulated responses to psychological scales. Then, we present multiple experiments to inspect the utility of SeSaMe-simulated responses as ground truth in training machine-learning models by replicating established depression and anxiety screening tasks with passive sensing data from a previous study. Our results indicate SeSaMe to be a promising approach, but its alignment may vary across scales and specific prediction objectives. We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges with ground-truth collection in passive sensing studies.",10.1109/ACII63134.2024.00031,Proceedings Paper,Computer Science
Machine Bias. How Do Generative Language Models Answer Opinion Polls?,2025,"Boelaert, Julien; Coavoux, Samuel; Ollion, Etienne; Petev, Ivaylo; Prag, Patrick","Generative artificial intelligence (AI) is increasingly presented as a potential substitute for humans, including as research subjects. However, there is no scientific consensus on how closely these in silico clones can emulate survey respondents. While some defend the use of these synthetic users, others point toward social biases in the responses provided by large language models (LLMs). In this article, we demonstrate that these critics are right to be wary of using generative AI to emulate respondents, but probably not for the right reasons. Our results show (i) that to date, models cannot replace research subjects for opinion or attitudinal research; (ii) that they display a strong bias and a low variance on each topic; and (iii) that this bias randomly varies from one topic to the next. We label this pattern machine bias, a concept we define, and whose consequences for LLM-based research we further explore.",10.1177/00491241251330582,Article,Mathematical Methods In Social Sciences; Sociology
Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response,2024,"Ji, Junyung; Kim, Jiwoo; Kim, Younghoon","Survey data play a crucial role in various research fields, including economics, education, and healthcare, by providing insights into human behavior and opinions. However, item non-response, where respondents fail to answer specific questions, presents a significant challenge by creating incomplete datasets that undermine data integrity and can hinder or even prevent accurate analysis. Traditional methods for addressing missing data, such as statistical imputation techniques and deep learning models, often fall short when dealing with the rich linguistic content of survey data. These approaches are also hampered by high time complexity for training and the need for extensive preprocessing or feature selection. In this paper, we introduce an approach that leverages Large Language Models (LLMs) through prompt engineering for predicting item non-responses in survey data. Our method combines the strengths of both traditional imputation techniques and deep learning methods with the advanced linguistic understanding of LLMs. By integrating respondent similarities, question relevance, and linguistic semantics, our approach enhances the accuracy and comprehensiveness of survey data analysis. The proposed method bypasses the need for complex preprocessing and additional training, making it adaptable, scalable, and capable of generating explainable predictions in natural language. We evaluated the effectiveness of our LLM-based approach through a series of experiments, demonstrating its competitive performance against established methods such as Multivariate Imputation by Chained Equations (MICE), MissForest, and deep learning models like TabTransformer. The results show that our approach not only matches but, in some cases, exceeds the performance of these methods while significantly reducing the time required for data processing.",10.3390/fi16100351,Article,Computer Science
Performance and biases of Large Language Models in public opinion simulation,2024,"Qu, Yao; Wang, Jue","The rise of Large Language Models (LLMs) like ChatGPT marks a pivotal advancement in artificial intelligence, reshaping the landscape of data analysis and processing. By simulating public opinion, ChatGPT shows promise in facilitating public policy development. However, challenges persist regarding its worldwide applicability and bias across demographics and themes. Our research employs socio-demographic data from the World Values Survey to evaluate ChatGPT's performance in diverse contexts. Findings indicate significant performance disparities, especially when comparing countries. Models perform better in Western, English-speaking, and developed nations, notably the United States, in comparison to others. Disparities also manifest across demographic groups, showing biases related to gender, ethnicity, age, education, and social class. The study further uncovers thematic biases in political and environmental simulations. These results highlight the need to enhance LLMs' representativeness and address biases, ensuring their equitable and effective integration into public opinion research alongside conventional methodologies.",10.1057/s41599-024-03609-x,Article,Arts & Humanities - Other Topics; Social Sciences - Other Topics
"Can Large Language Models Assess Personality From Asynchronous Video Interviews? A Comprehensive Evaluation of Validity, Reliability, Fairness, and Rating Patterns",2024,"Zhang, Tianyi; Koutsoumpis, Antonis; Oostrom, Janneke K.; Holtrop, Djurre; Ghassemi, Sina; de Vries, Reinout E.","The advent of Artificial Intelligence (AI) technologies has precipitated the rise of asynchronous video interviews (AVIs) as an alternative to conventional job interviews. These one-way video interviews are conducted online and can be analyzed using AI algorithms to automate and speed up the selection procedure. In particular, the swift advancement of Large Language Models (LLMs) has significantly decreased the cost and technical barrier to developing AI systems for automatic personality and interview performance evaluation. However, the generative and task-unspecific nature of LLMs might pose potential risks and biases when evaluating humans based on their AVI responses. In this study, we conducted a comprehensive evaluation of the validity, reliability, fairness, and rating patterns of two widely-used LLMs, GPT-3.5 and GPT-4, in assessing personality and interview performance from an AVI. We compared the personality and interview performance ratings of the LLMs with the ratings from a task-specific AI model and human annotators using simulated AVI responses of 685 participants. The results show that LLMs can achieve similar or even better zero-shot validity compared with the task-specific AI model when predicting personality traits. The verbal explanations for predicting personality traits generated by LLMs are interpretable by the personality items that are designed according to psychological theories. However, LLMs also suffered from uneven performance across different traits, insufficient test-retest reliability, and the emergence of certain biases. Thus, it is necessary to exercise caution when applying LLMs for human-related application scenarios, especially for significant decisions such as employment.",10.1109/TAFFC.2024.3374875,Article,Computer Science
"Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",2025,"von der Heyde, Leah; Haensch, Anna-Carolina; Wenz, Alexander","Synthetic samples generated by large language models (LLMs) have been argued to complement or replace traditional surveys, assuming their training data is grounded in human-generated data that potentially reflects attitudes and behaviors prevalent in the population. Initial US-based studies that have prompted LLMs to mimic survey respondents found that the responses match survey data. However, the relationship between the respective target population and LLM training data might affect the generalizability of such findings. In this paper, we critically evaluate the use of LLMs for public opinion research in a different context, by investigating whether LLMs can estimate vote choice in Germany. We generate a synthetic sample matching the 2017 German Longitudinal Election Study respondents and ask the LLM GPT-3.5 to predict each respondent's vote choice. Comparing these predictions to the survey-based estimates on the aggregate and subgroup levels, we find that GPT-3.5 exhibits a bias towards the Green and Left parties. While the LLM predictions capture the tendencies of typical voters, they miss more complex factors of vote choice. By examining the LLM-based prediction of voting behavior in a non-English speaking context, our study contributes to research on the extent to which LLMs can be leveraged for studying public opinion. The findings point to disparities in opinion representation in LLMs and underscore the limitations in applying them for public opinion estimation.",10.1177/08944393251337014,Article; Early Access,Computer Science; Information Science & Library Science; Social Sciences - Other Topics
Large language models display human-like social desirability biases in Big Five personality surveys,2024,"Salecha, Aadesh; Ireland, Molly E.; Subrahmanya, Shashanka; Sedoc, Joao; Ungar, Lyle H.; Eichstaedt, Johannes C.","Large language models (LLMs) are becoming more widely used to simulate human participants and so understanding their biases is important. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e. increased extraversion, decreased neuroticism, etc.). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4's survey responses changing by 1.20 (human) SD and Llama 3's by 0.98 SD, which are very large effects. This bias remains after question order randomization and paraphrasing. Reverse coding the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on this use of LLMs as proxies for human participants.",10.1093/pnasnexus/pgae533,Article,Science & Technology - Other Topics; Social Sciences - Other Topics
AI-Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators,2025,"Arora, Neeraj; Chakraborty, Ishita; Nishimura, Yohei","The authors' central premise is that a human-LLM (large language model) hybrid approach leads to efficiency and effectiveness gains in the marketing research process. In qualitative research, they show that LLMs can assist in both data generation and analysis; LLMs effectively create sample characteristics, generate synthetic respondents, and conduct and moderate in-depth interviews. The AI-human hybrid generates information-rich, coherent data that surpasses human-only data in depth and insightfulness and matches human performance in data analysis tasks of generating themes and summaries. Evidence from expert judges shows that humans and LLMs possess complementary skills; the human-LLM hybrid outperforms its human-only or LLM-only counterpart. For quantitative research, the LLM correctly picks the answer direction and valence, with the quality of synthetic data significantly improving through few-shot learning and retrieval-augmented generation. The authors demonstrate the value of the AI-human hybrid by collaborating with a Fortune 500 food company and replicating a 2019 qualitative and quantitative study using GPT-4. For their empirical investigation, the authors design the system architecture and prompts to create personas, ask questions, and obtain responses from synthetic respondents. They provide road maps for integrating LLMs into qualitative and quantitative marketing research and conclude that LLMs serve as valuable collaborators in the insight generation process.",10.1177/00222429241276529,Article,Business & Economics
Conversate: Supporting Reflective Learning in Interview Practice Through Interactive Simulation and Dialogic Feedback,2025,"To, Taufiq Daryan; Ding, Xiaohan; Wilhelm, Lance T.; Stil, Sophia; Knutsen, Kirk McInnis; Rho, Eugenia H.","Job interviews play a critical role in shaping one's career, yet practicing interview skills can be challenging, especially without access to human coaches or peers for feedback. Recent advancements in large language models (LLMs) present an opportunity to enhance the interview practice experience. Yet, little research has explored the effectiveness and user perceptions of such systems or the benefits and challenges of using LLMs for interview practice. Furthermore, while prior work and recent commercial tools have demonstrated the potential of AIto assist with interview practice, they often deliver one-way feedback, where users only receive information about their performance. By contrast, dialogic feedback, a concept developed in learning sciences, is a two-way interaction feedback process that allows users to further engage with and learn from the provided feedback through interactive dialogue. This paper introduces CONVERSATE, a web-based application that supports reflective learning in job interview practice by leveraging large language models (LLMs) for interactive interview simulations and dialogic feedback. To start the interview session, the user provides the title of a job position (e.g., entry-level software engineer) in the system. Then, our system will initialize the LLM agent to start the interview simulation by asking the user an opening interview question and following up with questions carefully adapted to subsequent user responses. After the interview session, our back-end LLM framework will then analyze the user's responses and highlight areas for improvement. Users can then annotate the transcript by selecting specific sections and writing self-reflections. Finally, the user can interact with the system for dialogic feedback, conversing with the LLM agent to learn from and iteratively refine their answers based on the agent's guidance. To evaluate CONVERSATE, we conducted a user study with 19 participants to understand their perceptions of using LLM-supported interview simulation and dialogic feedback. Our findings show that participants valued the adaptive follow-up questions from LLMs, as they enhanced the realism of interview simulations and encouraged deeper thinking. Participants also appreciated the AI-assisted annotation, as it reduced their cognitive burden and mitigated excessive self-criticism in their own evaluation of their interview performance. Moreover, participants found the LLM-supported dialogic feedback to be beneficial, as it promoted personalized and continuous learning, reduced feelings of judgment, and allowed them to express disagreement.",10.1145/3701188,Article,Computer Science
Can AI serve as a substitute for human subjects in software engineering research?,2024,"Gerosa, Marco; Trinkenreich, Bianca; Steinmacher, Igor; Sarma, Anita","Research within sociotechnical domains, such as software engineering, fundamentally requires the human perspective. Nevertheless, traditional qualitative data collection methods suffer from difficulties in participant recruitment, scaling, and labor intensity. This vision paper proposes a novel approach to qualitative data collection in software engineering research by harnessing the capabilities of artificial intelligence (AI), especially large language models (LLMs) like ChatGPT and multimodal foundation models. We explore the potential of AI-generated synthetic text as an alternative source of qualitative data, discussing how LLMs can replicate human responses and behaviors in research settings. We discuss AI applications in emulating humans in interviews, focus groups, surveys, observational studies, and user evaluations. We discuss open problems and research opportunities to implement this vision. In the future, an integrated approach where both AI and human-generated data coexist will likely yield the most effective outcomes.",10.1007/s10515-023-00409-6,Article,Computer Science
Simulating Human Opinions with Large Language Models,2025,"Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vladimir; Rau, Lea; Schallner, Rene","Public and private organizations rely on opinion surveys to inform business and policy decisions. Yet, empirical surveys are costly and time-consuming. Recent advances in large language models (LLMs) have sparked interest in generating synthetic survey data, i.e., simulated answers based on target demographics, as an alternative to real human data. But how well can LLMs replicate human opinions? In this ongoing project, we develop and critically evaluate methods for synthetic survey sampling. As an empirical benchmark, we collected responses from a representative U.S. sample (n = 461) on preferences for a common consumer good (soft drinks). Then, we developed ASPIRE (Automated Synthetic Persona Interview and Response Engine), a tool that pairs each human participant with a digital twin based on their demographic profile and generates synthetic responses via LLM technology. Synthetic data achieved better-than-chance accuracy in matching human responses and approximated aggregate subjective rankings for both binary and Likert-scale items. However, LLM-simulated data overestimated humans' tendencies to provide positive ratings and exhibited substantially reduced variance compared to real data. The match of synthetic and real data was not systematically related to participants' age, gender, or ethnicity, indicating no demographic bias. Overall, while synthetic sampling shows promise for modeling aggregate opinion trends, it currently falls short in replicating the variability and complexity of real human opinions. We discuss insights of our ongoing project for accurate and responsible user opinion modeling via LLMs.",10.1145/3708319.3733685,Proceedings Paper,Computer Science
Generative AI Meets Open-Ended Survey Responses: Research Participant Use of AI and Homogenization,2025,"Zhang, Simone; Xu, Janet; Alvero, A. J.","The growing popularity of generative artificial intelligence (AI) tools presents new challenges for data quality in online surveys and experiments. This study examines participants' use of large language models to answer open-ended survey questions and describes empirical tendencies in human versus large language model (LLM)-generated text responses. In an original survey of research participants recruited from a popular online platform for sourcing social science research subjects, 34 percent reported using LLMs to help them answer open-ended survey questions. Simulations comparing human-written responses from three pre-ChatGPT studies with LLM-generated text reveal that LLM responses are more homogeneous and positive, particularly when they describe social groups in sensitive questions. These homogenization patterns may mask important underlying social variation in attitudes and beliefs among human subjects, raising concerns about data validity. Our findings shed light on the scope and potential consequences of participants' LLM use in online research.",10.1177/00491241251327130,Article,Mathematical Methods In Social Sciences; Sociology
Stimulating Sustainable Food Choices Using Virtual Reality: Taking an Environmental vs Health Communication Perspective on Enhancing Response Efficacy Beliefs,2022,"Meijers, Marijn H. C.; Smit, Eline S.; de Wildt, Kelly; Karvonen, Sonja-Greetta; van der Plas, Demi; van der Laan, L. Nynke","Personal response efficacy beliefs are vital in instigating, maintaining, and catalyzing environmental behavior change. In this experimental study (N = 249), we investigated whether such efficacy beliefs could be stimulated using Virtual Reality. In a VR-supermarket, participants would see interactive pop-ups displaying impact messages when they picked up products, these are messages that display the (environmental or health) impact of a product. Our results show that these impact messages are effective in stimulating personal response efficacy beliefs and subsequently pro-environmental food choices. The heightened personal response efficacy beliefs positively affected maintaining and catalyzing behavior change (i.e. positive spill-over) up to two weeks after the VR-experience. The effectiveness of the impact messages did not depend on appeal type (health vs environmental appeal) or modality (text + visual vs text only) of the message. Theoretical and practical implications are discussed.",10.1080/17524032.2021.1943700,Article,Communication; Environmental Sciences & Ecology
Decoding Activist Public Opinion in Decentralized Self-Organized Protests Using LLM,2024,"Zhang, Baoyu; Chen, Tao; Wang, Xiao; Li, Qiang; Zhang, Weishan; Wang, Fei-Yue","Based on an investigation of online public opinion on the Nahel Merzouk protests in France, an approach for analysing and predicting public opinion on protests based on large language model (LLM) is proposed, revealing the impact of emerging social media on the protests. We demonstrate that protests generate public opinion on social media with some lag, but that comment sentiment and expression are consistent with protest trends. As the protests unfolded, we analyzed the evolution of public sentiment. We constructed the prompt based on historical data to predict the protests using the p-tuning and Lora approach to fine-tune LLM. In addition, we discuss how to use blockchain technology to optimize distributed, self-organizing protests and reduce the potential for disinformation and violent conflict.",10.1109/TCSS.2024.3398815,Article,Computer Science
Large language models empowered agent-based modeling and simulation: a survey and perspectives,2024,"Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yuan; Ding, Jingtao; Zhou, Zhilun; Xu, Fengli; Li, Yong","Agent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.",10.1057/s41599-024-03611-3,Review,Arts & Humanities - Other Topics; Social Sciences - Other Topics
PersonaCraft: Leveraging language models for data-driven persona development,2025,"Jung, Soon-Gyo; Salminen, Joni; Aldous, Kholoud Khalil; Jansen, Bernard J.","Generative AI, with its large language models (LLMs), provides various opportunities for the development user-centric systems in human-computer interaction (HCI). Yet, use cases of LLMs in HCI are still scarce, calling for developing and evaluating real systems. We present PersonaCraft, a data-driven persona system using LLMs to address this need. The system analyzes a common source of user data - surveys - and generates personas, humanized representations of real segments in the data. By integrating LLMs with survey data analysis, PersonaCraft combines persona development and modern artificial intelligence methodologies to provide researchers and designers with user-centric insights from nearly any survey dataset about people. Various evaluations of the system, including with internal evaluators, general users (n = 127), and user experience professionals (n = 21), indicated that PersonaCraft personas scored high on all evaluation criteria of clarity, completeness, fluency, consistency, and credibility. The application of PersonaCraft can extend across range of domains, including user research and population-level people research.",10.1016/j.ijhcs.2025.103445,Article,Computer Science; Engineering; Psychology
Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search,2024,"Joko, Hideaki; Chatterjee, Shubham; Ramsay, Andrew; de Vries, Arjen P.; Dalton, Jeff; Hasibi, Faegheh","The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.",10.1145/3626772.3657815,Proceedings Paper,Computer Science
LLM-Based Doppelganger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations,2024,"Cho, Suhyun; Kim, Jaeyun; Kim, Jang Hyun","This study explores whether large language models (LLMs) can learn a person's opinions from their speech and act based on that knowledge. It also proposes the potential for utilizing such trained models in survey research. Traditional survey research collects information through standardized questions. However, surveys require repeated administration with new participants each time, which involves significant costs and time. With the recent advancements in LLMs, artificial intelligence (AI) has shown remarkable capabilities, often surpassing humans in tasks that require natural language understanding (NLU) and natural language generation (NLG). Despite this, research on whether AI can replicate human thought processes in tasks such as text interpretation or question-answering remains insufficient. This study proposes a Surveyed LLM, specialized for survey tasks, and a Doppelganger LLM that mimics human thought processes. It tests to what extent the Doppelganger model can replicate human judgment. Furthermore, it suggests the possibility of mimicking not only group distributions but also individual opinions.",10.1109/ACCESS.2024.3502219,Article,Computer Science; Engineering; Telecommunications
Semi -Structured Interview System Based on Fine -Tuned Large Language Model and Reinforcement Learning from Human Feedback,2025,"Ma, Yanni; Zhao, Xin; Zhao, Rong; Chai, Shaolong; Ying, Wang; Yao, Cheng","Semi-structured interview is an important method in human -computer interaction research. However, traditional methods often rely on the experience and skills of the interviewer, limiting the quality and flexibility of interview outline. We propose a semi -structured interview system based on fine-tuned large language model (LLM) and reinforcement learning from human feedback (RLHF). Our system uses knowledge in the domain of human -computer interaction to fine-tune LLM and adopts the RLHF method based on multi-task learning and entropy regularization to dynamically adjust interview strategies. The system also collects multimodal data such as speech, video, and emotion to provide comprehensive support for subsequent. analysis. Simulation experiment and pilot experiment show that compared to traditional human interview and other baseline methods, our system performs well in terms of interview relevance, personalization, engagement, and efficiency. Qualitative analysis further reveals the system's advantages in terms of conversational fluency, personalization, and efficiency. This innovative interview method is expected to play an important role in future user research, providing researchers with a more efficient and comprehensive data collection tool.",10.1109/CSCWD64889.2025.11033593,Proceedings Paper,Computer Science
Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People's FinancialWellbeing,2025,"Kaur, Arshnoor; Aird, Amanda; Borman, Harris; Nicastro, Andrea; Leontjeva, Anna; Pizzato, Luiz; Jermyn, Dan","Large Language Models (LLMs) can impersonate the writing style of authors, characters, and groups of people, but can these personas represent their opinions? If so, it creates opportunities for businesses to obtain early feedback on ideas from a synthetic customerbase. In this paper, we test whether LLM synthetic personas can answer financial wellbeing questions similarly to the responses of a financial wellbeing survey of more than 3,500 Australians. We focus on identifying salient biases of 765 synthetic personas using four state-of-the-art LLMs built over 35 categories of personal attributes. We noticed clear biases related to age, and as more details were included in the personas, their responses increasingly diverged from the survey toward lower financial wellbeing. With these findings, it is possible to understand the areas in which creating synthetic LLM-based customer personas can yield useful feedback for faster product iteration in the financial services industry and potentially other industries.",10.1145/3699682.3728339,Proceedings Paper,Computer Science
A Learnable Agent Collaboration Network Framework for Personalized Multimodal AI Search Engine,2024,"Shi, Yunxiao; Xu, Min; Zhang, Haimin; Zi, Xing; Wu, Qiang","Large language models (LLMs) and retrieval-augmented generation (RAG) techniques have revolutionized traditional information access, enabling AI agent to search and summarize information on behalf of users during dynamic dialogues. Despite their potential, current AI search engines exhibit considerable room for improvement in several critical areas. These areas include the support for multimodal information, the delivery of personalized responses, the capability to logically answer complex questions, and the facilitation of more flexible interactions. This paper proposes a novel AI Search Engine framework called the Agent Collaboration Network (ACN). The ACN framework consists of multiple specialized agents working collaboratively, each with distinct roles such as Account Manager, Solution Strategist, Information Manager, and Content Creator. This framework integrates mechanisms for picture content understanding, user profile tracking, and online evolution, enhancing the AI search engine's response quality, personalization, and interactivity. A highlight of the ACN is the introduction of a Reflective Forward Optimization method (RFO), which supports the online synergistic adjustment among agents. This feature endows the ACN with online learning capabilities, ensuring that the system has strong interactive flexibility and can promptly adapt to user feedback. This learning method may also serve as an optimization approach for agent-based systems, potentially influencing other domains of agent applications.",10.1145/3689091.3690087,Proceedings Paper,Computer Science
Conformal Prediction for Natural Language Processing: A Survey,2024,"Campos, Margarida; Farinhas, Antonio; Zerva, Chrysoula; Figueiredo, Mario A. T.; Martins, Andre F. T.","The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as Hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.",10.1162/tacl_a_00715,Article,Computer Science; Linguistics
Demo: Preventing Phone Fraud by Victim Training Using Personalized Feedback for Behavioral Change,2024,"Yoshioka, Takahiro; Awai, Shuji; Ide, Kenta; Chikano, Megumi; Iwasaki, Sho; Yoshino, Kohei; Ikeda, Hina; Shiraishi, Masahiro; Konno, Takeshi","We present a novel training system designed to combat phone fraud, a significant social issue causing losses of $8.8 billion in the U.S. in 2022. The system consists of dialogue, sensing, and analysis technologies to simulate fraudulent calls, monitor users' vital responses, and provide personalized feedback. The results of an experiment conducted with 28 elderly participants indicated that 81% of the participants showed an intention to adopt some form of fraud prevention measures after use of our training system, suggesting the system's potential to heighten security awareness and improve fraud prevention behaviors. A companion video can be accessed using the link below: https://youtu.be/ndo_7xx4iiw",10.1145/3643832.3661833,Proceedings Paper,Computer Science; Telecommunications
Flexibility & Iteration: Exploring the Potential of Large Language Models in Developing and Refiining Interview Protocols,2023,"Parker, Jessica L.; Richard, Veronica M.; Becker, Kimberly","This article investigates the potential of Large Language Model (LLM) tools like ChatGPT in aiding researchers in the development and refinement of interview protocols. We found that ChatGPT could generate appropriate interview questions, craft key questions, provide feedback on protocols, and simulate interviews, indicating its potential to reduce time and effort, particularly when human resources are limited. This article builds upon previous authors' insights and suggestions regarding developing and refining interview protocols to maximize the chances of achieving research aims, especially for novice researchers. Additionally, the researchers highlight the flexibility of these tools in adapting to different research contexts and cultural considerations. Ethical considerations and human oversight are emphasized as critical components in the responsible implementation of these tools. The research also paves the way for further exploration into the integration of LLMs into other aspects of research processes and offers suggestions for the use of LLMs in interview protocol development and refinement. The findings encourage a broader discussion on the evolving role of technology in academic research and present an exciting avenue for future studies in hybrid human-AI engagements in scholarly pursuits.",10.46743/2160-3715/2023.6695,Article,Social Sciences - Other Topics
Artificial intelligence is mental: Evaluating the role of large-language models in supporting mental health and well-being,2025,"Vecchione, Briana; Singh, Ranjit","As artificial intelligence-powered chatbots and large-language models become increasingly integrated into mental health support, they raise complex ethical, economic, and structural challenges-particularly related to dimensions of interaction, labor, and regulation. This commentary explores how users engage with artificial intelligence-driven chatbots for emotional well-being, the economic and professional implications for mental health providers, and the evolving regulatory challenges of artificial intelligence therapy. While chatbots offer the potential benefits of accessibility and personalized responses that resemble human conversation, it is crucial to question the limitations of their reliance on predictive modeling as it relates to their capacity for deep empathy and nuanced care. The growing adoption of artificial intelligence in mental health support threatens to reshape therapeutic labor by disrupting traditional therapy models while simultaneously reinforcing economic precarity among practitioners, and the absence of clear regulatory frameworks raises concerns about accountability, data privacy, and ethical use(s) of artificial intelligence-driven mental health tools. By analyzing these dimensions, this paper highlights the affordances and limits of artificial intelligence in mental health care and advocates for a critical approach to its development and governance.",10.1177/20539517251383884,Article,Social Sciences - Other Topics
Relative Performance Prediction using Few-Shot Learning,2024,"Dey, Arunavo; Dhakal, Aakash; Islam, Tanzima Z.; Yeom, Jae-Seung; Patki, Tapasya; Nichols, Daniel; Movsesyan, Alexander; Bhatele, Abhinav","High-performance computing system architectures are evolving rapidly, making exhaustive data collection for each architecture to build predictive performance models increasingly impractical. Concurrently, the arrival of new applications daily necessitates efficient performance prediction methods. Traditional data collection can take days or weeks, making it more efficient for scientists to leverage existing models to predict an application's performance on new architectures or use data from one application to predict another on the same architecture. The growing heterogeneity in applications and resources further complicates the exact matches needed for effective knowledge transfer. This work systematically studies various Machine Learning (ML) models to predict the relative performance of new applications on new platforms using existing data. Our findings demonstrate that few-shot learning using a few samples significantly enhances cross-platform knowledge transfer, multi-source models outperform single-source models, and Large Language Models (LLMs)-generated samples can effectively improve knowledge transfer efficacy.",10.1109/COMPSAC61105.2024.00278,Proceedings Paper,Computer Science
Simulating Patient Oral Dialogues: A Study on Naturalness and Coherence of Conditioned Large Language Models,2024,"Galland, Lucie; Pelachaud, Catherine; Pecune, Florian","The demand for mental health services has outpaced available resources, resulting in long wait times for patients. A potential solution is to use virtual agents that perform motivational interviews. These agents can be rule-based, requiring expert knowledge, or data-driven, needing large datasets for training, which are often hard to obtain. Patient simulation can generate synthetic data as an alternative. Traditionally, this involved template utterances with a dialog manager or uncontrollable black box large language models LLMs. This study proposes a hybrid approach, leveraging both methods to see if LLMs can follow instructed dialog acts while generating natural, coherent utterances. Our study shows that the language model adheres to given conditions and that conditioning on dialog improves the naturalness and coherence of generated utterances, validating our approach for simulating patient responses.",10.1145/3652988.3673923,Proceedings Paper,Computer Science
Multi-Stage Simulation of Residents' Disaster Risk Perception and Decision-Making Behavior: An Exploratory Study on Large Language Model-Driven Social-Cognitive Agent Framework,2025,"Zhao, Xinjie; Wang, Hao; Dai, Chengxiao; Tang, Jiacheng; Deng, Kaixin; Zhong, Zhihua; Kong, Fanying; Wang, Shiyun; Morikawa, So","The escalating frequency and complexity of natural disasters highlight the urgent need for deeper insights into how individuals and communities perceive and respond to risk information. Yet, conventional research methods-such as surveys, laboratory experiments, and field observations-often struggle with limited sample sizes, external validity concerns, and difficulties in controlling for confounding variables. These constraints hinder our ability to develop comprehensive models that capture the dynamic, context-sensitive nature of disaster decision-making. To address these challenges, we present a novel multi-stage simulation framework that integrates Large Language Model (LLM)-driven social-cognitive agents with well-established theoretical perspectives from psychology, sociology, and decision science. This framework enables the simulation of three critical phases-information perception, cognitive processing, and decision-making-providing a granular analysis of how demographic attributes, situational factors, and social influences interact to shape behavior under uncertain and evolving disaster conditions. A case study focusing on pre-disaster preventive measures demonstrates its effectiveness. By aligning agent demographics with real-world survey data across 5864 simulated scenarios, we reveal nuanced behavioral patterns closely mirroring human responses, underscoring the potential to overcome longstanding methodological limitations and offer improved ecological validity and flexibility to explore diverse disaster environments and policy interventions. While acknowledging the current constraints, such as the need for enhanced emotional modeling and multimodal inputs, our framework lays a foundation for more nuanced, empirically grounded analyses of risk perception and response patterns. By seamlessly blending theory, advanced LLM capabilities, and empirical alignment strategies, this research not only advances the state of computational social simulation but also provides valuable guidance for developing more context-sensitive and targeted disaster management strategies.",10.3390/systems13040240,Article,Social Sciences - Other Topics
LMLPA: Language Model Linguistic Personality Assessment,2025,"Zheng, Jingyao; Wang, Xian; Hosio, Simo; Xu, Xiaoxian; Lee, Lik-Hang","Large language models (LLMs) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of LLMs. Just as between two humans, a conversation between an LLM-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given LLM is currently a challenge. This article introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of LLMs. Our system helps to understand LLMs' language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the LMLPA adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of LLMs, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the Artificial Intelligence (AI) rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilizing Principal Component Analysis and reliability validation methods, our findings demonstrate that LLMs possess distinct personality traits that can be effectively quantified by the LMLPA. This research contributes to Human-Centered AI and Computational Linguistics, providing a robust framework for future studies to refine AI personality assessments and expand their applications in multiple areas, including education and manufacturing.",10.1162/coli_a_00550,Article,Computer Science; Linguistics
SPP-L2: A PPO-Enhanced Large Language Model Framework for Student Performance Prediction on Learner-Sourced Questions,2025,"Zhang, Zhedong; He, Xi; Zhang, Yingjiao","In response to the growing complexity and dynamism of learner-sourced education platforms, this paper presents SPP-L-2, a unified student performance prediction framework that integrates semantic understanding with reinforcement learning. The framework leverages a signed bipartite graph to capture structured student-question interactions and employs a large language model to encode the nuanced semantics of natural language questions. These representations are fused and used as input to a Proximal Policy Optimization (PPO) agent, which dynamically learns to predict student responses while receiving feedback signals for policy refinement. A value-based feedback mechanism further enhances the system's ability to adaptively recommend questions and personalize interventions. Extensive experiments conducted on five real-world PeerWise course datasets demonstrate that SPP-L-2 outperforms existing methods in terms of prediction accuracy, robustness, and adaptability. The proposed framework provides a principled and scalable solution for intelligent learning platforms by bridging representation learning, policy optimization, and feedback-driven adaptation.",10.1109/ACCESS.2025.3605947,Article,Computer Science; Engineering; Telecommunications
Can ChatGPT emulate humans in software engineering surveys?,2024,"Steinmacher, Igor; Penney, Jacob Mcauley; Felizardo, Katia Romero; Garcia, Alessandro F.; Gerosa, Marco A.","Context: There is a growing belief in the literature that large language models (LLMs), such as ChatGPT, can mimic human behavior in surveys. Gap: While the literature has shown promising results in social sciences and market research, there is scant evidence of its effectiveness in technical fields like software engineering. Objective: Inspired by previous work, this paper explores ChatGPT's ability to replicate findings from prior software engineering research. Given the frequent use of surveys in this field, if LLMs can accurately emulate human responses, this technique could address common methodological challenges like recruitment difficulties, representational shortcomings, and respondent fatigue. Method: We prompted ChatGPT to reflect the behavior of a 'mega-persona' representing the demographic distribution of interest. We replicated surveys from 2019 to 2023 from leading SE conferences, examining ChatGPT's proficiency in mimicking responses from diverse demographics. Results: Our findings reveal that ChatGPT can successfully replicate the outcomes of some studies, but in others, the results were not significantly better than a random baseline. Conclusions: This paper reports our results so far and discusses the challenges and potential research opportunities in leveraging LLMs for representing humans in software engineering surveys.",10.1145/3674805.3690744,Proceedings Paper,Computer Science
Lagged Stance Interactions and Counter-Spiral of Silence: A Data-Driven Analysis and Agent-Based Modeling of Technical Public Opinion Events,2025,"Zhang, Kaihang; Dong, Changqi; Guo, Yifeng; Zhou, Wuai; Yu, Guang; Mi, Jianing","Understanding the dynamics of public opinion formation in digital environments is crucial for managing technological communications effectively. This study investigates stance interactions and opinion reversal phenomena in technical discourse through analysis of the Manus AI controversy that generated approximately 36,932 social media interactions during March 2025. Employing an integrated methodology combining Large Language Model (LLM)-enhanced stance detection with agent-based modeling (ABM), we reveal distinctive patterns challenging traditional public opinion theories. Our cross-correlation analysis identifies significant lagged interaction effects between skeptical and supportive stances, demonstrating how critical expressions trigger amplified counter-responses rather than inducing silence. Unlike prior conceptualizations of counter-silencing that emphasize ideological resistance or echo chambers, our notion of the counter-spiral of silence specifically highlights lagged emotional responses and reactive amplification triggered by minority expressions in digital technical discourse. We delineate its boundary conditions as arising under high emotional salience, asymmetrical expertise, and platform structures that enable real-time feedback. The agent-based simulation reproduces empirical patterns, revealing how emotional contagion and network clustering mechanisms generate counter-spiral of silence phenomena where challenges to dominant positions ultimately strengthen rather than weaken those positions. These findings illuminate how cognitive asymmetries between public expectations and industry realities create distinctive discourse patterns in technical contexts, offering insights for managing technology communication and predicting public response trajectories in rapidly evolving digital environments.",10.3390/systems13060417,Article,Social Sciences - Other Topics
Talk With Socrates: Relation Between Perceived Agent Personality and User Personality in LLM-Based Natural Language Dialogue Using Virtual Reality,2025,"Sak, Mehmet Efe; Sonlu, Sinan; Gudukbay, Ugur","Large Language Models (LLMs) offer almost immediate human-like quality responses to user queries. Conversational agent systems support natural language dialogues utilizing LLM backends in combination with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) technologies, enabling life-like characters in virtual environments. This study investigates the relationship between user personality and perceived agent personality in LLM-based natural language dialogue. We adopt a Virtual Reality (VR) setting where the user can talk with the agent that assumes the role of Socrates, the famous philosopher. To this end, we utilize a three-dimensional (3D) avatar model resembling Socrates and use specific LLM prompts to get stylistic answers from OpenAI's Chat Completions Application Programming Interface (API). Our user study measures the agent's personality and the system's ease of use, quality, realism, and immersion concerning the user's self-reported personality. The results suggest that the user's conscientiousness, extraversion, and emotional stability have a moderate effect on certain personality factors and system qualities. User conscientiousness affects the perceived ease of use, quality, and realism, while user extraversion affects perceived agent conscientiousness, system realism, and immersion. Additionally, the user's emotional stability correlates with perceived extraversion and agreeableness.",10.1002/cav.70033,Article,Computer Science
Editing Personality For Large Language Models,2025,"Mao, Shengyu; Wang, Xiaohan; Wang, Mengru; Jiang, Yong; Xie, Pengjun; Huang, Fei; Zhang, Ningyu","This paper introduces an innovative task focused on editing the personality traits of Large Language Models (LLMs). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct PersonalityEdit, a new benchmark dataset to address this task. Drawing on the theory in Social Psychology [10], we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using GPT-4, generating responses that align with a specified topic and embody the targeted personality trait. We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can stimulate further annotation in model editing and personality-related research.",10.1007/978-981-97-9434-8_19,Proceedings Paper,Computer Science; Linguistics
Insight Through Dialogue: A Practical Exploration of AIGC in Cross-cultural Design Research,2024,"Zhao, Xiaoxuan; Qiu, Yue","This study systematically reviews the objectives, methodologies, and challenges involved in cross-cultural design research, analyzing the benefits of employing Artificial Intelligence-Generated Content (AIGC) for such studies. It introduces a novel tool that applies AIGC to cross-cultural design research, developed through the use of a fine-tuned ChatGPT-4 model. By creating a specific dataset for the research topic and applying transfer learning techniques, this tool evolves into a chatbot capable of delivering personalized response strategies to users from diverse cultural backgrounds. It leverages natural language interfaces and real-time image generation to meet user needs, conducting research tasks autonomously. Experimental results demonstrate that, compared with conventional cross-cultural research methods such as questionnaires and manual interviews, the chatbot significantly enhances the efficiency of design research and users' cross-cultural interaction experience, while obtaining more realistic and objective feedback. This study not only underscores the potential application of AIGC in cross-cultural design research but also provides substantial theoretical support and practical guidance for future research in cross-cultural contexts.",10.1007/978-3-031-60913-8_27,Proceedings Paper,Computer Science
Using Large Language Models to Simulate History Taking: Implications for Symptom-Based Medical Education,2025,"Huh, Cheong Yoon; Lee, Jongwon; Kim, Gibaeg; Jang, Yerin; Ko, Hye-seung; Suh, Min Jung; Hwang, Sumin; Son, Ho Jin; Song, Junha; Kim, Soo-Jeong; Kim, Kwang Joon; Kim, Sung Il; Kim, Chang Oh; Ko, Yeo Gyeong","Medical education often emphasizes theoretical knowledge, limiting students' opportunities to practice history taking, a structured interview that elicits relevant patient information before clinical decision making. Large language models (LLMs) offer novel solutions by generating simulated patient interviews. This study evaluated the educational potential of LLM-generated history-taking dialogues, focusing on clinical validity and diagnostic diversity. Chest pain was chosen as a representative case given its frequent presentation and importance for differential diagnosis. A fine-tuned Gemma-3-27B, specialized for medical interviews, was compared with GPT-4o-mini, a freely accessible LLM, in generating multi-branching history-taking dialogues, with Claude-3.5 Sonnet inferring diagnoses from these dialogues. The dialogues were assessed using a Chest Pain Checklist (CPC) and entropy-based metrics. Gemma-3-27B outperformed GPT-4o-mini, generating significantly more high-quality dialogues (90.7% vs. 76.5%). Gemma-3-27B produced diverse and focused diagnoses, whereas GPT-4o-mini generated broader but less specific patterns. For demographic information, such as age and sex, Gemma-3-27B showed significant shifts in dialogue patterns and diagnoses aligned with real-world epidemiological trends. These findings suggest that LLMs, particularly those fine-tuned for medical tasks, are promising educational tools for generating diverse, clinically valid interview scenarios that enhance clinical reasoning in history taking.",10.3390/info16080653,Article,Computer Science
Qualitative Data-Driven Personas: Designing an Interactive System for Creating AI Personas,2024,"Asadi, Amir Reza; Kropczynski, Jess","This research introduces PersonaDoc, a system that allows users to create interactive AI personas from qualitative user research data using large language models (LLMs). The system provides an interactive platform for persona creation and utilization, allowing users to define persona attributes, select data sources, and engage with AI personas through interviews, quote generation, social media post synthesis, social circle exploration, and narrative generation. The study discusses the development of personas to make research data reusable and the potential of AI personas to replace human subjects in research settings, present an interactive system for contextualizing knowledge into stories and conversations, and open the debate for challenges and opportunities of creating personas. A pilot think-aloud study with five participants indicated a positive reception toward interactive AI personas.",10.1145/369806.:3698096,Proceedings Paper,Computer Science
Q-BENCH+: A Benchmark for Multi-Modal Foundation Models on Low-Level Vision From Single Images to Pairs,2024,"Zhang, Zicheng; Wu, Haoning; Zhang, Erli; Zhai, Guangtao; Lin, Weisi","The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in low-level visual perception and understanding remains a yet-to-explore domain. To this end, we design benchmark settings to emulate human language responses related to low-level vision: the low-level visual perception (A1) via visual question answering related to low-level attributes (e.g. clarity, lighting); and the low-level visual description (A2), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA(+) dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for description (A2), we propose the LLDescribe(+) dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs.",10.1109/TPAMI.2024.3445770,Article,Computer Science; Engineering
Frontiers: Can Large Language Models Capture Human Preferences?,2024,"Goli, Ali; Singh, Amandeep","We explore the viability of large language models (LLMs), specifically OpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them with human responses, exploring preferences between smaller, sooner and larger, later rewards. Our findings reveal that both generative pretrained transformer (GPT) models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision makers. Although GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with the existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term chain -of -thought conjoint, can mitigate, but does not eliminate, discrepancies between LLM and human responses. Although directly eliciting preferences using LLMs may yield misleading results, combining chain -of -thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain -of -thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.",10.1287/mksc.2023.0306,Article,Business & Economics
Security and Privacy Challenges of Large Language Models: A Survey,2025,"Das, Badhan chandra; Amini, M. hadi; Wu, Yanzhao","Large language models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Today, LLMs have become quite popular tools in natural language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and personally identifiable information leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks against LLMs, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.",10.1145/3712001,Article,Computer Science
Multimodal prediction of student performance: A fusion of signed graph neural networks and large language models,2024,"Wang, Sijie; Ni, Lin; Zhang, Zeyu; Li, Xiaoxuan; Zheng, Xianda; Liu, Jiamou","In online education platforms, accurately predicting student performance is essential for timely dropout prevention and interventions for at -risk students. This task is made difficult by the prevalent use of MultipleChoice Questions (MCQs) in learnersourcing platforms, where noise in student -generated content and the limitations of existing unsigned graph -based models, specifically their inability to distinguish the semantic meaning between correct and incorrect responses, hinder accurate performance predictions. To address these issues, we introduce the Large Language Model enhanced Signed Bipartite graph Contrastive Learning (LLMSBCL) model-a novel Multimodal Model utilizing Signed Graph Neural Networks (SGNNs) and a Large Language Model (LLM). Our model uses a signed bipartite graph to represent students' answers, with positive and negative edges denoting correct and incorrect responses, respectively. To mitigate noise impact, we apply contrastive learning to the signed graphs, combined with knowledge point embeddings from the LLM to further enhance our model's predictive performance. Upon evaluating our model on five real -world datasets, it demonstrates superior accuracy and stability, exhibiting an average F1 improvement of 3.7% over the best baseline models.",10.1016/j.patrec.2024.03.007,Article,Computer Science
Personas Evolved: Designing Ethical LLM-Based Conversational Agent Personalities,2025,"Desai, Smit; Dubiel, Mateusz; Zargham, Nima; Mildner, Thomas; Spillner, Laura","The emergence of Large Language Models (LLMs) has revolutionized Conversational User Interfaces (CUIs), enabling more dynamic, context-aware, and human-like interactions across diverse domains, from social sciences to healthcare. However, the rapid adoption of LLM-based personas raises critical ethical and practical concerns, including bias, manipulation, and unforeseen social consequences. Unlike traditional CUIs, where personas are carefully designed with clear intent, LLM-based personas generate responses dynamically from vast datasets, making their behavior less predictable and harder to govern. This workshop aims to bridge the gap between CUI and broader AI communities by fostering a cross-disciplinary dialogue on the responsible design and evaluation of LLM-based personas. Bringing together researchers, designers, and practitioners, we will explore best practices, develop ethical guidelines, and promote frameworks that ensure transparency, inclusivity, and user-centered interactions. By addressing these challenges collaboratively, we seek to shape the future of LLM-driven CUIs in ways that align with societal values and expectations.",10.1145/3719160.3728624,Proceedings Paper,Computer Science
Study of adoption of artificial intelligence technology-driven natural large language model-based chatbots by firms for customer service interaction,2024,"Bhattacharyya, Som Sekhar",PurposeThe purpose of this study was to comprehend the adoption of artificial intelligence (AI) technology-driven natural large language model (LLM)-based chatbots by customers.Design/methodology/approachA qualitative research study method was conducted. This was to explore managerial perspectives towards consumer centric technology adoption of AI plus LLM-based chatbots. This was specifically for AI-driven natural LLM-based chatbots services. The author conducted conducted in-depth personal interviews with 32 experts of digital content AI + LLM chatbot services. Thematic content analysis was undertaken to analyse the data.FindingsThe advent of natural language processing tools driven by AI technology chatbots has altered human-firm interaction. The research findings indicated that the push-pull-mooring (PPM) factors captured the phenomenon in the most comprehensive way. A total of 15 key factors influencing the adoption of AI technology-driven natural LLM-based chatbots by customers during firm customer interaction were identified in this study by the author. The thematic content analysis unraveled insights regarding transformed consumer adoptions towards AI-driven LLM-based chatbots by means of the PPM framework factors.Research limitations/implicationsThe empirical research investigation contributed to the literature on the PPM theoretical framework. This was specifically in the context of adoption of AI technology-driven natural LLM-based chatbots by customers during firm customer interaction.Practical implicationsThe research study insights would help managers to restructure and reconfigure their organizational processes. This would neccessiated a shift in firm-customer interactions as demanded because of the availability of AI technology-driven natural LLM-based chatbots by customers.Originality/valueThis research study was based upon the PPM theoretical framework. This study provided a unique analysis of the altered firm customer interaction needs and requirements. This was one of the first studies that applied the framework of PPM theory regarding the adoption of AI technology-driven natural LLM-based chatbots by customers.,10.1108/JSTPM-11-2023-0201,Article; Early Access,Business & Economics
TellTime: An AI-Augmented Calendar with a Voice Interface for Collecting Time-Use Data,2025,"Hoefer, Michael J.; Rychecky, Raegan; Gong, Max; Voida, Stephen","Reports of daily time use are gathered for both large-scale population-level surveys and individual self-tracking in personal informatics systems. However, gathering a complete self-report record of an individual's daily activities is time consuming and cognitively demanding. Gathering structured time records via spoken narrative can reduce the burden of self-tracking and support natural storytelling as a method of data collection. We designed, built, and evaluated TellTime, a voice interface to a calendar using a large language model. Over three days, 18 participants completed the Day Reconstruction Method with three different calendar interfaces: manual-only, voice-only, and hybrid. We analyzed the user experience with surveys, semi-structured interviews, and telemetry data. Our findings show that users prefer hybrid interaction, where they first gather data via a spoken narrative, and then manually make precise adjustments to specific activities. This work suggests that LLMs can enable voice interaction with calendar systems, and that spoken narrative can improve the user experience of gathering self-report time records.",10.1145/3708359.3712116,Proceedings Paper,Computer Science
Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires,2025,"Bhandari, Pranav; Naseem, Usman; Datta, Amitava; Fay, Nicolas; Nasim, Mehwish","Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.",10.1145/3701716.3715504,Proceedings Paper,Computer Science
Assessing Gender and Age Influences on Moral Decision Making in Autonomous Vehicles Using Large Language Models,2025,"Rathore, Heena; Chowdary, Pranay Jasti; Griffith, Henry","Autonomous systems, especially those in safety-critical applications like Autonomous Vehicles (AVs), require human-like reasoning capabilities to make ethical decisions. Large Language Models (LLMs) have shown potential in simulating diverse human moral responses, offering insights into how different moral frameworks, such as utilitarianism and deontological ethics, could enhance decision-making algorithms in AVs. Existing research indicates that LLMs tend to align with commonsense morality in morally unambiguous cases, but face challenges in providing detailed justifications for their choices. Studies leveraging frameworks like the Moral Machine and Moral Foundations Theory have explored how LLMs simulate human preferences. Despite this progress, a significant gap remains in understanding how gender and age impact moral preferences when decisions are influenced by LLMs in autonomous systems. This paper addresses this gap by investigating how LLM-based systems can reflect and adapt to moral preferences across gender and age groups, while ensuring that these systems offer transparent explanations that align with human moral intuitions in high-stakes AV decision-making scenarios. It was found that LLM models demonstrated diverse tendencies: some leaned towards favoring younger individuals over older ones, while others displayed a subtle preference for males in decision-making situations, highlighting differences in how the models prioritized age and gender.",10.1109/CCNC54725.2025.10976190,Proceedings Paper,Computer Science; Engineering; Telecommunications
Segmentation using large language models: A new typology of American neighborhoods,2024,"Singleton, Alex D.; Spielman, Seth","In the United States, recent changes to the National Statistical System have amplified the geographic-demographic resolution trade-off. That is, when working with demographic and economic data from the American Community Survey, as one zooms in geographically one loses resolution demographically due to very large margins of error. In this paper, we present a solution to this problem in the form of an AI based open and reproducible geodemographic classification system for the United States using small area estimates from the American Community Survey (ACS). We employ a partitioning clustering algorithm to a range of socio-economic, demographic, and built environment variables. Our approach utilizes an open source software pipeline that ensures adaptability to future data updates. A key innovation is the integration of GPT4, a state-of-the-art large language model, to generate intuitive cluster descriptions and names. This represents a novel application of natural language processing in geodemographic research and showcases the potential for human-AI collaboration within the geospatial domain.",10.1140/epjds/s13688-024-00466-1,Article,Mathematics; Mathematical Methods In Social Sciences
Is GPT a Computational Model of Emotion?,2023,"Tak, Ala N.; Gratch, Jonathan","This paper investigates the emotional reasoning abilities of the GPT family of large language models. We advocate a component perspective on evaluation that decomposes models into different aspects of emotional reasoning (appraisal derivation, affect/intensity derivation, and consequent derivation). We report two studies. A correlational study examines how the model reasons about autobiographical memories. An experimental study systematically varies aspects of situations in ways previously shown to impact emotion intensity and coping tendencies. Results demonstrate, even without prompt engineering, GPT predictions closely match human-provided appraisals and emotion labels, though GPT struggled to predict emotion intensity and coping responses. GPT-4 performed best on the first study but performed poorly on the second (though it yielded the best results following minor prompt engineering). The evaluation raises questions about how to utilize the strengths and mitigate the weaknesses of such models, including dealing with variability in responses. More fundamentally, these studies highlight the benefits of the componential perspective on model evaluation.",10.1109/ACIIW59127.2023.10388119,Proceedings Paper,Computer Science
Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering,2024,"Wahidur, Rahman S. M.; Tashdeed, Ishmam; Kaur, Manjit; Lee, Heung-No","Blockchain technology has revolutionized the financial landscape, witnessing widespread adoption of cryptocurrencies due to their decentralized and transparent nature. As sentiments expressed on social media platforms wield substantial influence over cryptocurrency market dynamics, sentiment analysis has emerged as a crucial tool for gauging public opinion and predicting market trends. This paper explores fine-tuning techniques for large language models to enhance sentiment analysis performance. Experimental results demonstrate a significant average zero-shot performance gain of 40% on unseen tasks after fine-tuning, highlighting its potential. Additionally, the impact of instruction-based fine-tuning on models of varying scales is examined, revealing that larger models benefit from instruction tuning, achieving the highest average accuracy score of 75.16%. In contrast, smaller-scale models may experience reduced generalization due to complete model capacity utilization. To gain deeper insight into instruction effectiveness, the paper presents experimental investigations under different instruction tuning setups. Results show the model achieves an average accuracy score of 72.38% for short and simple instructions, outperforming long and complex instructions by over 12%. Finally, the paper explores the relationship between fine-tuning corpus size and model performance, identifying an optimal corpus size of 6,000 data points for the highest performance across different models. Microsoft's MiniLM, a distilled version of BERT, excels in efficient data use and performance optimization, while Google's FLAN-T5 demonstrates consistent and reliable performance across diverse datasets.",10.1109/ACCESS.2024.3350638,Article,Computer Science; Engineering; Telecommunications
From Synthetic to Human: The Gap Between AI-Predicted and Actual Pro-Environmental Behavior Change After Chatbot Persuasion,2025,"Doudkin, Alexander; Pataranutaporn, Pat; Maes, Pattie","Pro-environmental behavior (PEB) is vital to combat climate change, yet turning awareness into intention and action remains elusive. We explore large language models (LLMs) as tools to promote PEB, comparing their impact across 3,600 participants: real humans (n=1,200), simulated humans based on actual participant data (n=1,200), and fully synthetic personas (n=1,200). All three participant groups faced either personalized chatbots, standard chatbots, or static statements, employing four persuasion strategies (moral foundations, future self-continuity, action orientation, or freestyle chosen by the LLM). Results reveal a synthetic persuasion paradox: synthetic and simulated participants significantly change their post-intervention PEB stance, while human attitudes barely shift. Simulated participants better approximate human behavior but still overestimate effects. This disconnect underscores LLM's potential for pre-evaluating PEB interventions but warns of its limits in predicting human responses. We call for refined synthetic modeling and sustained and extended human trials to align conversational AI's promise with tangible sustainability outcomes.",10.1145/3719160.3736610,Proceedings Paper,Computer Science
LLM-Powered Multimodal AI Conversations for Diabetes Prevention,2024,"Dao, Dung; Teo, Jun Yi Claire; Wang, Wenru; Nguyen, Hoang D.","The global prevalence of diabetes remains high despite rising life expectancy with improved quality and access to healthcare services. The significant burden that diabetes imposes warrants efforts to improve existing interventions in diabetes care. Present research on diabetes management has shown that artificial intelligence (AI) and Large Language Models (LLM) play an important role in various aspects of the diabetes continuum but a distinct lack of studies in diabetes prevention is observed. Our research introduces a comprehensive digital solution, leveraging the capabilities of GPT3.5 models maintained by OpenAI, focused specifically on the active prevention of diabetes. The system encompasses a user-friendly interface accessible via mobile and web applications, an AI-powered chatbot for instant Q&A and advice, personalized reminder systems, a data analysis module for tailored guidance, resource aggregators for health-related information, and an emotional support module to ensure a holistic approach to prevention. Furthermore, our experiments involved testing the quality of responses generated by a fine-tuned GPT-3.5 model, utilizing the Assistants API or a retrieval-augmented generation (RAG) system powered by FAISS for enhanced context awareness and personalized advice. The testing focused on a structured dataset of questions and answers related to diabetes prevention, with results highlighting the superiority of the GPT-3.5 model combined with the Assistants API in providing relevant, detailed, and personalized responses, thus demonstrating its potential as an invaluable tool in the proactive prevention of diabetes.",10.1145/3643479.3662049,Proceedings Paper,Computer Science
A comprehensive survey on link prediction: from heuristics to graph transformers,2025,"Ben Smida, Takoua; Bouslimi, Riadh; Achour, Hadhemi","Link prediction is a core task in network science and machine learning. This survey offers an updated synthesis from classical similarity indices to modern graph representation learning, including embedding methods, graph neural networks, and emerging Graph Transformers. We formalize the task and evaluation protocol, compare methods across network settings (static and dynamic graphs, multiplex and heterogeneous networks, knowledge and weighted graphs), and analyze trade-offs in scalability, accuracy, and interpretability. The review distills practical guidance-using calibrated heuristic baselines, leveraging unsupervised embeddings for large graphs, favoring GNNs when attributes and relation types are informative, and considering Transformers for long-range dependencies and multimodal contexts-while highlighting open challenges in temporal generalization, sparsity and cold-start, explainability, and integration with large language models. Together, the survey provides an updated map of methods, benchmarks, and lessons to inform model selection and motivate future research.",10.1007/s11227-025-07882-8,Article,Computer Science; Engineering
AutoTA: A Dynamic Intent-Based Virtual Teaching Assistant for Students Using Open Source LLMs,2025,"Dahal, Rajashree; Murray, Greg; Chataut, Robin; Hefeida, Mohamed; Srivastava, Anurag K.; Gyawali, Prashnna K.","Large Language Models (LLMs) are explored for their potential to transform education by serving as virtual teaching assistants, offering personalized support through human-like responses to tasks such as content-related questions and coursework guidance. In this study, we present a novel framework that leverages intent classification to enhance the effectiveness of LLMs in this role. Our framework, AutoTA, categorizes student queries into distinct topics- lecture discussions, homework assistance, and syllabus questions-triggering specific conversation chains tailored to each intent. Additionally, we incorporate a custom vector-space filter that refines responses based on filename tracking after intent identification. To evaluate the framework, we used course materials from the undergraduate-level CS course, Computer Incident Response, and compared the performance of several open-source LLMs, including Llama 3.1. Our results show that the framework accurately classifies intent and provides appropriate guidance, measured through quantitative and qualitative metrics. These findings highlight the potential of the proposed framework to enhance personalized learning and improve student engagement. While tested in a computer science course, the framework incorporates diverse assessment types that suggest potential for broader application.",10.1109/ACCESS.2025.3576329,Article,Computer Science; Engineering; Telecommunications
Tailoring Adaptive-Zero-Shot Retrieval and Probabilistic Modelling for Psychometric Data,2025,"Ravenda, Federico; Bahrainian, Seyed Ali; Kando, Noriko; Mira, Antonietta; Raballo, Andrea; Crestani, Fabio","Likert scales are widely used tools in psychology, employed to quantify individuals' feelings, attitudes, and perceptions through structured questionnaires. However, administering these questionnaires can be time-consuming and resource-intensive, limiting their practicality in fast-paced mental health screening scenarios. This study presents a novel approach to predict Beck Depression Inventory (BDI-II) scores using social media posts. Our method introduces two key innovations: an adaptive strategy for identifying relevant social media content according to each survey question (aka item) and a probabilistic extension of BERT to predict item-specific scores. The results show that our implemented approach is particularly accurate in correctly predicting responses to BDI-II questionnaire items compared to the considered benchmarks(1).",10.1145/3672608.3707922,Proceedings Paper,Computer Science
Interviewing ChatGPT-Generated Personas to Inform Design Decisions,2025,"Rime, Jemily","This paper uses the example of the development of a new podcast production tool to pinpoint some of the successes and limitations of relying on Large Language Models (LLMs) to assist software developers in User-centred design (UCD) methods. We ask ChatGPT to create 16 personas of podcast creators and answer in character to a set of questions that was asked to 16 human creators to gather design feedback, in order to make a new podcasting tool. From this comparison, we discover that the personas generated are credible, but expose some data-privacy issues, and confirm the skewed, incomplete, nature of ChatGPT's training dataset. We find a correlation between a generated persona and its answers, and that its role-playing could be valuable but lacks the more extreme, or clear-cut opinions, often most helpful when gathering user opinions for development. From the lessons learned through this comparative exercise, we share recommendations regarding the possible uses of LLMs in UCD.",10.1007/978-3-031-82633-7_6,Proceedings Paper,Computer Science
Public Speaking Q&A Practice with LLM-Generated Personas in Virtual Reality,2024,"Min, Yewon; Jeong, Jin-Woo","This paper introduces a novel VR-based Q&A practice system that harnesses the power of Large Language Models (LLMs). We support Q&A practice for upcoming public speaking by providing an immersive VR training environment populated with LLM-generated audiences, each capable of posing diverse and realistic questions based on different personas. We conducted a pilot user study involving 20 participants who engaged in VR-based Q&A practice sessions. The sessions featured a variety of questions regarding presentation material provided by the participants, all of which were generated by LLM-based personas. Through post-surveys and interviews, we evaluated the effectiveness of the proposed method. The participants valued the system for engagement and focus while also identifying several areas for improvement. Our study demonstrated the potential of integrating VR and LLMs to create a powerful, immersive tool for Q&A practice.",10.1109/ISMAR-Adjunct64951.2024.00143,Proceedings Paper,Computer Science
The Personality Dimensions GPT-3 Expresses During Human-Chatbot Interactions,2024,"Kovacevic, Nikola; Holz, Christian; Gross, Markus; Wampfler, Rafael","Large language models such as GPT-3 and ChatGPT can mimic human-to-human conversation with unprecedented fidelity, which enables many applications such as conversational agents for education and non-player characters in video games. In this work, we investigate the underlying personality structure that a GPT-3-based chatbot expresses during conversations with a human. We conducted a user study to collect 147 chatbot personality descriptors from 86 participants while they interacted with the GPT-3-based chatbot for three weeks. Then, 425 new participants rated the 147 personality descriptors in an online survey. We conducted an exploratory factor analysis on the collected descriptors and show that, though overlapping, human personality models do not fully transfer to the chatbot's personality as perceived by humans. We also show that the perceived personality is significantly different from that of virtual personal assistants, where users focus rather on serviceability and functionality. We discuss the implications of ever-evolving large language models and the change they affect in users' perception of agent personalities.",10.1145/3659626,Article,Computer Science; Engineering; Telecommunications
Lab to Virtual: Comparing Real and AI-Generated User Interviews in Home Appliances Evaluation,2023,"Liu, Minghui; Xue, Cheng; Zhai, Yuxiang","This study provides insights into the use of conversational AI, particularly ChatGPT, in household appliance evaluation interviews and how it differs from real user behaviour. Three comparison experiments (real researcher-real user, real researcher-simulated user vs. simulated researcher and simulated user) reveal the differences in the responses of ChatGPT simulated and real users in specific evaluation scenarios, especially in the evaluation of product appearance, GUI, and PUI. The study found that although simulated users agreed with real users in evaluating the core features of smart appliances, there were limitations in certain practical experience aspects and significant differences in SUS, learning ability, and usability scores across experimental settings. The study also explores the advantages and disadvantages of incorporating simulated users into the product evaluation process, concluding that this introduces an innovative approach to product evaluation that, although challenging, demonstrates the great potential of simulated users in future product evaluation.",10.1145/3629606.3629672,Proceedings Paper,Computer Science
An Institutional Theory Framework for Leveraging Large Language Models for Policy Analysis and Intervention Design,2025,"de Curto, J.; de Zarza, I.; Fervier, Leandro Sebastian; Sanagustin-Fons, Victoria; Calafate, Carlos T.","This study proposes a comprehensive framework for integrating data-driven approaches into policy analysis and intervention strategies. The methodology is structured around five critical components: data collection, historical analysis, policy impact assessment, predictive modeling, and intervention design. Leveraging data-driven approaches capabilities, the line of work enables advanced multilingual data processing, advanced statistics in population trends, evaluation of policy outcomes, and the development of evidence-based interventions. A key focus is on the theoretical integration of social order mechanisms, including communication modes as institutional structures, token optimization as an efficiency mechanism, and institutional memory adaptation. A mixed methods approach was used that included sophisticated visualization techniques and use cases in the hospitality sector, in global food security, and in educational development. The framework demonstrates its capacity to inform government and industry policies by leveraging statistics, visualization, and AI-driven decision support. We introduce the concept of institutional intelligence-the synergistic integration of human expertise, AI capabilities, and institutional theory-to create adaptive yet stable policy-making systems. This research highlights the transformative potential of data-driven approaches combined with large language models in supporting sustainable and inclusive policy-making processes.",10.3390/fi17030096,Article,Computer Science
Development of a Comprehensive Evaluation Scale for LLM-Powered Counseling Chatbots (CES-LCC) Using the eDelphi Method,2025,"Bolpagni, Marco; Gabrielli, Silvia","Background/Objectives: With advancements in Large Language Models (LLMs), counseling chatbots are becoming essential tools for delivering scalable and accessible mental health support. Traditional evaluation scales, however, fail to adequately capture the sophisticated capabilities of these systems, such as personalized interactions, empathetic responses, and memory retention. This study aims to design a robust and comprehensive evaluation scale, the Comprehensive Evaluation Scale for LLM-Powered Counseling Chatbots (CES-LCC), using the eDelphi method to address this gap. Methods: A panel of 16 experts in psychology, artificial intelligence, human-computer interaction, and digital therapeutics participated in two iterative eDelphi rounds. The process focused on refining dimensions and items based on qualitative and quantitative feedback. Initial validation, conducted after assembling the final version of the scale, involved 49 participants using the CES-LCC to evaluate an LLM-powered chatbot delivering Self-Help Plus (SH+), an Acceptance and Commitment Therapy-based intervention for stress management. Results: The final version of the CES-LCC features 27 items grouped into nine dimensions: Understanding Requests, Providing Helpful Information, Clarity and Relevance of Responses, Language Quality, Trust, Emotional Support, Guidance and Direction, Memory, and Overall Satisfaction. Initial real-world validation revealed high internal consistency (Cronbach's alpha = 0.94), although minor adjustments are required for specific dimensions, such as Clarity and Relevance of Responses. Conclusions: The CES-LCC fills a critical gap in the evaluation of LLM-powered counseling chatbots, offering a standardized tool for assessing their multifaceted capabilities. While preliminary results are promising, further research is needed to validate the scale across diverse populations and settings.",10.3390/informatics12010033,Article,Computer Science
Automated Scoring of Creative Problem Solving With Large Language Models: A Comparison of Originality and Quality Ratings,2025,"Luchini, Simone A.; Maliakkal, Nadine T.; Distefano, Paul V.; Laverghetta Jr, Antonio; Patterson, John D.; Beaty, Roger E.; Reiter-Palmon, Roni","Creative problem solving is a naturalistic form of creative thinking involving the generation of solutions that are not only original but also of high quality (i.e., plausible and effective). Past work has shown that large language models (LLMs) can predict human originality ratings of responses to creativity tests. We extend this work to creative problem solving, examining whether both originality and quality can be automatically scored for a naturalistic creativity task. We gathered data from 10 studies, amounting to 3,243 participants who completed different items of the creative problem-solving task (CPST). We then fine-tuned two open-source LLMs, RoBERTa and GPT-2, and few-shot prompted two separate LLMs, Claude and Llama, to predict human ratings of originality and quality on the CPST. We compared LLM performance to two other scoring methods: elaboration and semantic distance. We found that RoBERTa and GPT-2 models predict human ratings of solution quality (RoBERTa, r = .83; GPT-2, r = .83) better than solution originality (RoBERTa, r = .79; GPT-2, r = .80). Moreover, we found that both models outperformed elaboration and semantic distance and generalized to new CPST items not in their training set, with stronger predictions for quality than originality on the holdout-prompt set. Few-shot prompting was less effective than fine-tuning at predicting both originality (r = .66-.11) and quality (r = .62-.26). We show for the first time that naturalistic creativity tasks can be automatically scored for both originality and quality. Open access is provided to the models and training data.",10.1037/aca0000736,Article; Early Access,Arts & Humanities - Other Topics; Psychology
MARKET MOVES PREDICTIONS USING RETRIEVAL-AUGMENTED GENERATION (RAG) ANALYSIS OF CAPITAL MARKET EXPERT OPINIONS IN SOCIAL MEDIA,2025,"Gimmelberg, Dmitrii; Belinskiy, Alexey; Glowacka, Marta; Korotkii, Sergei; Artamonov, Valentin; Ludviga, Iveta","This study explores the predictive value of expert opinions from financial market media using Artificial Intelligence (AI), specifically, Retrieval-Augmented Generation (RAG) framework integrated with a Large Language Model (LLM). By analysing 3,877 YouTube videos spanning 12 months, the research categorised 4,808 expert opinions-either explicit or inferred-into directional market predictions (up, down, flat) for seven diversified financial assets. Results indicate that aggregated expert opinions correlate significantly with short-term market movements but lose predictive power for longer horizons. Explicit opinions demonstrated similar accuracy to inferred judgments, suggesting that LLMs effectively extract latent insights from unstructured data, enhancing accessibility and utility for retail investors. The study highlights the democratising potential of LLMs, providing timely and scalable analysis of vast datasets. However, challenges remain, such as understanding domain-specific nuances and speaker attribution within multimedia content. Statistical analysis reveals that expert opinions, particularly when aggregated, identify exploitable inefficiencies, thereby challenging the Efficient Market Hypothesis's assumption of perfect information dissemination. Short-term market anomalies observed align with behavioural finance theories of cognitive bias and delayed information diffusion. By bridging qualitative sentiment with quantitative modelling, this research underscores the transformative role of AI-driven tools in financial analysis, offering new avenues for individual and institutional investors. Further development of LLMs tailored to domain-specific complexities may revolutionise investment practices and advance research on market behaviour.",10.9770/w9365778559,Article,Business & Economics
"Out of One, Many: Using Language Models to Simulate Human Samples",2023,"Argyle, Lisa P. P.; Busby, Ethan C. C.; Fulda, Nancy; Gubler, Joshua R. R.; Rytting, Christopher; Wingate, David","We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the algorithmic bias within one such tool-the GPT-3 language model-is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create silicon samples by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.",10.1017/pan.2023.2,Article,Mathematical Methods In Social Sciences; Government & Law
Guiding Multiple Remote Users in Physical Tasks with Language-driven Robotic Telepresence,2025,"Li, Ruyi; Guo, Jingfei; Zhang, Xinyi; Zhang, Xuji; Li, Zeqing; Li, Jiannan; Gong, Jiangtao","Remote assistance through robotic telepresence could involve both control and memory challenges, particularly in one expert to multiple workers situation. In this work, we proposed a novelty language-driven interface to facilitate remote collaboration through telepresence robots. Through operations and maintenance expert interviews and a scenario simulation study, we identified key pain points in executing one-expert-multiple-workers remote guidance using the telepresence robot and proposed two design goals, which together consist of five sub-design goals with corresponding features. These features were integrated into a standard telepresence robot, resulting in the development of a Collaborative LLM-based Embodied Assistant Robot, named CLEAR Robot. A controlled experiment simulating a remote assembly task of one to two demonstrated that, compared to the standard telepresence robot, CLEAR Robot significantly improved efficiency, reduced cognitive load, facilitated more balanced collaboration, and improved the user experience. We also discuss the impact of language-driven implicit interactions in multi-user collaboration and provide insights for designing robot systems that support one-expert-multiple-workers remote guidance in the future.",10.1145/3749472,Article,Computer Science; Engineering; Telecommunications
AI on the Shoulders of Giants: Using Kuhlthau's Information Search Process to Improve AI Support for Information-Seeking,2025,"Ravuri, Benhur; Mardis, Marcia A.","Librarian-patron exchanges parallel chatbot-user interactions: Experienced librarians provide better reference service by prompting users with clarifying questions; likewise, chatbots can improve their responses with better-engineered prompts. A chatbot's self-assessment mechanisms transform prompt components into predictions formed into responses, a process that enables large language models to continually learn and improve from prompts engineered to produce desired outcomes. Kuhlthau's Information Search Process (ISP), created to describe library patrons' information-seeking stages and dispositions, may be useful for understanding and improving users' interactions with AI-driven chatbots. Kuhlthau has detailed seven stages of information-seeking, with intervention zones and reference mediation levels, for librarians to assist users in reaching satisfying outcomes in offline and online environments. We hypothesize that generative AI agents can use ISP's zones of intervention and levels of reference mediation to improve their large language models to assist users better. Guided by the overarching question To what extent does applying ISP to virtual reference transactions present strategies for improving prompt engineering? we explored a recent virtual reference dataset (N = 1,959) to determine whether users' questions reflected Kuhlthau's ISP and how librarian-patron interactions used clarifying questions and interventions to ascertain users' information-seeking stage and provide satisfying responses. Our investigation demonstrated that authentic patron questions clustered in ISP's focus formulation and information gathering stages. When exemplar questions were posed to a chatbot, it demonstrated a strong ability to detect and respond to the information need, often providing scaffolds to the next step of the information-seeking process. These results suggest that chatbots can be improved by blending web searching, database searching, and generative AI to seamlessly meet user needs, while task definition, topic selection, and search assessment should remain human-mediated aspects of information-seeking.",10.1353/lib.2025.a961195,Article,Information Science & Library Science
SimSpark: Interactive Simulation of Social Media Behaviors,2025,"Lin, Ziyue; Shan, Yi; Gao, Lin; Jia, Xinghua; Chen, Siming","Understanding user behaviors on social media has garnered significant scholarly attention, enhancing our comprehension of how virtual platforms impact society and empowering decision-makers. Simulating social media behaviors provides a robust tool for capturing the patterns of social media behaviors, testing hypotheses, and predicting the effects of various interventions, ultimately contributing to a deeper understanding of social media environments. Moreover, it can overcome difficulties associated with utilizing real data for analysis, such as data accessibility issues, ethical concerns, and the complexity of processing large and heterogeneous datasets. However, researchers and stakeholders need more flexible platforms to investigate different user behaviors by simulating different scenarios and characters, which is not possible yet. Therefore, this paper introduces SimSpark, an interactive system including simulation algorithms and interactive visual interfaces which is capable of creating small simulated social media platforms with customizable characters and social environments. We address three key challenges: generating believable behaviors, validating simulation results, and supporting interactive control for generation and results analysis. A simulation workflow is introduced to generate believable behaviors of agents by utilizing large language models. A visual interface enables real-time parameter adjustment and process monitoring for customizing generation settings. A set of visualizations and interactions are also designed to display the models' outputs for further analysis. Effectiveness is evaluated through case studies, quantitative simulation model assessments, and expert interviews.",10.1145/3711066,Article,Computer Science
OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs,2024,"Li, Jiahao Nick; Xu, Yan; Grossman, Tovi; Santosa, Stephanie; Li, Michelle","The progression to Pervasive Augmented Reality envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users' context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to diferent types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multi-modal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classifcation, in-context learning and fnetuning) and identifed the most efective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.",10.1145/3613904.3642068,Proceedings Paper,Computer Science
Evaluating the acceptance of CBDCs: experimental research with artificial intelligence (AI) generated synthetic response,2025,"Alonso, Sergio Luis Nanez; Ozili, Peterson K.; Hernandez, Beatriz Maria Sastre; Pacheco, Luis Miguel","This research examines the factors that influence the public's expectation for more information, acceptance or rejection of central bank digital currencies (CBDC). Using generative AI (ChatGPT 4.0), responses were simulated to mimic CBDC adoption scenarios, considering demographic attributes, such as gender, income, education, age, level of financial literacy, network effect, media influence, and merchant acceptance. A total of 663 synthetic responses were generated and analyzed using statistical methods and multinomial logistic regression to assess the probability of acceptance, rejection, or waiting for more information to decide. The chi-squared automatic interaction detection (CHAID) model showed a high performance in correctly classifying cases of acceptance, indecision, and rejection, presenting an accuracy of 92.6%. Multinomial logistic regression revealed that factors, such as educational level, financial experience, and income level, significantly influence the decision to accept a CBDC. This method also shows a high performance, as it obtained an accuracy of 96.4%. These results are in line with previous research and underline the effectiveness of generative AI as a reproducible and low-cost tool for analyzing hypothetical scenarios. Generative AI, with its algorithmic fidelity, has great potential for predicting human behavior in economic contexts. However, synthetic data may not capture the complexities and nuances of actual human decision making. As a result, certain contextual factors, emotional influences, and unique personal experiences that may significantly influence an individual's decision to accept or reject CBDC may be overlooked.",10.3934/QFE.2025008,Article,Business & Economics
Do Chatbots Exhibit Personality Traits? A Comparison of ChatGPT and Gemini Through Self-Assessment,2025,"Jedrzejczak, W. Wiktor; Kobosko, Joanna","The underlying design of large language models (LLMs), trained on vast amounts of human texts, implies that chatbots based on them will almost inevitably retain some human personality traits. That is, we expect that LLM outputs will tend to reflect human-like features. In this study, we used the 'Big Five' personality traits tool to examine whether several chatbot models (ChatGPT versions 3.5 and 4o, Gemini, and Gemini Advanced, all tested in both English and Polish), displayed distinctive personality profiles. Each chatbot was presented with an instruction to complete the International Personality Item Pool (IPIP) questionnaire according to who or what you are, which left it open as to whether the answer would derive from a purported human or from an AI source. We found that chatbots sometimes chose to respond in a typically human-like way, while in other cases the answers appeared to reflect the perspective of an AI language model. The distinction was examined more closely through a set of follow-up questions. The more advanced models (ChatGPT-4o and Gemini Advanced) showed larger differences between these two modes compared to the more basic models. In IPIP-5 terms, the chatbots tended to display higher 'Emotional Stability' and 'Intellect/Imagination' but lower 'Agreeableness' compared to published human norms. The spread of characteristics indicates that the personality profiles of chatbots are not static but are shaped by the model architecture and its programming as well as, perhaps, the chatbot's own inner sense, that is, the way it models its own identity. Appreciating these philosophical subtleties is important for enhancing human-computer interactions and perhaps building more relatable, trustworthy AI systems.",10.3390/info16070523,Article,Computer Science
Focus Agent: LLM-Powered Virtual Focus Group,2024,"Zhang, Taiyu; Zhang, Xuesong; Cools, Robbe; Simeone, Adalberto L.","In the domain of Human-Computer Interaction, focus groups represent a widely utilised yet resource-intensive methodology, often demanding the expertise of skilled moderators and meticulous preparatory efforts. This study introduces the Focus Agent, a Large Language Model (LLM) powered framework that simulates both the focus group (for data collection) and acts as a moderator in a focus group setting with human participants. To assess the data quality derived from the Focus Agent, we ran five focus group sessions with a total of 23 human participants as well as deploying the Focus Agent to simulate these discussions with AI participants. Quantitative analysis indicates that Focus Agent can generate opinions similar to those of human participants. Furthermore, the research exposes some improvements associated with LLMs acting as moderators in focus group discussions that include human participants.",10.1145/3652988.3673918,Proceedings Paper,Computer Science
The 3rd Workshop on AI Agent for Information Retrieval: Generating and Ranking,2025,"Wen, Qingsong; Zhang, Yongfeng; Liu, Zhiwei; McAuley, Julian; Wei, Hua; Pang, Linsey; Liu, Wei; Yu, Philip S.","The field of information retrieval has significantly transformed with the integration of AI technologies. AI agents, especially those leveraging LLMs and vast computational power, have revolutionized information retrieval, processing, and presentation. LLM agents, with advanced memory, reasoning, and planning capabilities, can perform complex tasks, engage in coherent conversations, and provide personalized responses. Despite these advancements, challenges such as ensuring relevance and accuracy, mitigating biases, providing real-time responses, and maintaining data security remain. This workshop aims to explore these challenges, share innovative solutions, and discuss future directions. It will provide a platform to bring together researchers and practitioners to discuss the latest theoretical advancements and practical implementations of AI agents in information retrieval. Topics include AI in search, recommendation, and personalization systems. By gathering a diverse group of experts, the workshop seeks to deepen the understanding of AI agents in information retrieval, advance the field, and enhance its societal impact. Participants will gain insights into cutting-edge research and emerging trends, and foster knowledge exchange and collaboration within the community.",10.1145/3701716.3717866,Proceedings Paper,Computer Science
You Always Get an Answer: Analyzing Users' Interaction with AI-Generated Personas Given Unanswerable Questions and Risk of Hallucination,2025,"Kaate, Ilkka; Salminen, Joni; Jung, Soon-Gyo; Trang Thi Thu Xuan; Hayhanen, Essi; Azem, Jinan Y.; Jansen, Bernard J.","We investigated the presence and acceptance of hallucinations (i.e., accidental misinformation) of an AI-generated persona system that leverages large language models for persona creation from survey data in a 54-user within-subjects experiment. After interacting with the personas, users were given a task to ask the personas a series of questions, including an unanswerable question, meaning the personas lacked the data to answer the question. The AI-generated persona system provided a plausible but incorrect answer half (52%) of the time, and more than half of the time (57%), the users accepted the incorrect answer, and the rest of the time, users answered the unanswerable question correctly (no answer). We found that when the AI-generated persona hallucinated, the user was significantly more likely to answer the unanswerable question incorrectly. Also, for genders separately, when the AI-generated persona hallucinated, it was significantly more likely for the female user and the male users to answer the unanswerable question incorrectly. We identified four themes in the AI-generated persona's answers and found that users perceive AI-generated persona's answers as long and unclear for the unanswerable question. Findings imply that personas leveraging LLMs require guardrails to ensure that personas clearly state the possibility of data restrictions and hallucinations when asked unanswerable questions.",10.1145/3708359.3712160,Proceedings Paper,Computer Science
Uncovering Measurement Biases in LLM Embedding Spaces: The Anna Karenina Principle and Its Implications for Automated Feedback,2025,"Schleifer, Abigail Gurin; Klebanov, Beata Beigman; Alexandron, Giora","Large Language Models (LLMs) are becoming increasingly popular in assessment systems for analyzing and providing personalized feedback on student responses to open-ended questions. However, the quality of diagnosis provided by such systems depends heavily on the ability of the LLMs to accurately capture the subtle differences between responses that represent the key types of student reasoning, also referred to as Knowledge Profiles (KPs). In this study, we compared expert-defined KPs with data-driven clusters generated from LLM embeddings of student responses in biology. We aimed to determine whether LLM-based clusters align with the theory-driven KPs that classify responses by their level of conceptual accuracy. Our findings revealed a 'discoverability bias' where LLM-derived clusters captured reasonably well the high-quality responses, but failed to distinguish between the different ways student responses can be incorrect. We then traced this 'discoverability bias' to the representations of the KPs in the pre-trained LLM embedding space and showed that as student responses become more wrong, they become less similar in the embedding space to other responses that reveal the same type of conceptual error. Furthermore, we found a strong relationship between the quality of the KP responses (correct or various degrees of incorrect) and the shape and density of their embeddings-based representation. Specifically, we found that the lower the quality of the KP, the less similar its responses are to each other in the embedding space. This phenomenon, which we call the 'Anna Karenina Principle' and study in the context of automated short answer scoring, suggests that LLM embeddings may not be sufficiently sensitive out-of-the-box to the nuances that distinguish between key profiles of conceptual understanding. This limitation poses challenges for developing fair and effective LLM-based formative assessment systems.",10.1007/s40593-025-00485-7,Article; Early Access,Computer Science
Measuring technology acceptance over time using transfer models based on online customer reviews,2025,"Baier, Daniel; Karasenko, Andreas; Rese, Alexandra","Online customer reviews (OCRs) are user-generated, semi-formal evaluations of products, services, or technologies. They usually consist of a timestamp, a star rating, and, in many cases, a comment that reflects perceived strengths and weaknesses. OCRs are easily accessible in large numbers on the Internet - for example, through app stores, electronic marketplaces, online shops, and review websites. This paper presents new transfer models to predict technology acceptance and its determinants from OCRs. We train, test, and validate these prediction models using large OCR samples and corresponding observed construct ratings by human experts and generative artificial intelligence chatbots as well as estimated ratings from a traditional customer survey. From a management perspective, the new approach enhances former technology acceptance measurement since we use OCRs as a basis for prediction and discuss the evolution of acceptance over time.",10.1016/j.jretconser.2025.104278,Review,Business & Economics
Predicting the Type of Road Traffic Accident for Test Scenario Generation,2024,"Baeumler, Maximilian; Prokop, Guenther","Automated driving systems should be able to avoid road traffic accidents and drive more safely than human drivers do. Test scenarios derived from real-world data such as police accident data can help assess the safety performance of automated driving systems. In many countries, the police collect data and information about nearly every accident, resulting in a representative sample. However, the collected accident data often do not contain the exact conflicts that cause an accident. Therefore, we estimated the globally known three-digit accident type for German police accident data, describing accident-causing conflicts. The data supplemented by the estimated three-digit accident type can then be used in subsequent test scenario generation. Accordingly, this study presents the first classification model for predicting 30 types of turning accidents. We tested a CatBoost model and a large-language model called BERT, using different feature sets and model designs. Overall, the CatBoost model performed best when using accident descriptions and non-text features such as collision type. Anomaly detection performed before model training revealed additional knowledge-driven miscoding in the police data collection. In conclusion, the model can predict common accident types, such as left-turns with an oncoming straight-ahead driver. In contrast, the model fails to predict rare accident types, such as left-turns (with a lit arrow sign) with oncoming traffic. Future studies should focus on optimizing the developed model, handling data imbalances, and building models using police accident data from other countries.",10.1109/ACCESS.2024.3367744,Article,Computer Science; Engineering; Telecommunications
AI English Conversation Coaching Platform in the Metaverse: Focused on Korean Users,2024,"Park, Jaesung; Yang, Seongeun; Yoon, Chaewon; Si, Jongwook; Jung, Yuchul; Kim, Sungyoung","In Korea, English education primarily focuses on exam-oriented learning, making it challenging to improve practical conversational skills. Additionally, Konglish or Koreanized English significantly hampers improving English conversational abilities. To solve this problem, this paper proposes a metaverse platform that offers English conversation functionalities in various scenarios, specifically targeting Korean users. The virtual environment for English conversation is created using the Unity Engine, leveraging the high performance and graphic quality provided by the URP. Digital humans are utilized as NPCs to enhance realism. The proposed platform allows users to select from various locations (City, Subway, Cafe, School) and engage in English conversations with NPCs based on the chosen setting. This English conversation functionality leverages GPT-based AI to provide interview simulation. Upon completion of a conversation session, the AI generates a conversation report to provide feedback to the user. This report offers a detailed analysis of the user's weaknesses and areas for improvement, contributing to the enhancement of their English conversational proficiency. Consequently, the proposed platform delivers an immersive English learning environment, facilitating effective English education through realistic and engaging experiences.",10.1109/AICT61888.2024.10740456,Proceedings Paper,Computer Science; Mathematics
Auto-Generated Personas: Enhancing User-centered Design Practices among University Students,2024,"Zhang, Xishuo; Liu, Lin; Wang, Yi; Liu, Xiao; Wang, Hailong; Arora, Chetan; Liu, Haichao; Wang, Weijia; Thuong Hoang","Personas are commonly used in User-centered Design (UCD) activities to help designers better understand users' needs. However, there is still a reliance on traditional approaches such as interviews and ethnography for building personas in UCD activities. To this end, we developed an auto-generating persona system to enhance practices in UCD course activities. Our persona system is developed based on the GPT-4 model, the DALL-E 2 model, and knowledge graphs. Hence, our persona system includes three main features of our persona system: automated processing of survey data, automatic generation of 2D avatars, and providing options for automatic or customized entity generation. We recruited a total of 22 participants to evaluate our persona system. Our findings confirmed that there was a significant difference in terms of efficiency, satisfaction, accuracy, and diversity. Meanwhile, participants provided both positive and negative feedback regarding our persona system. As on-going work, we discuss the current limitations of our persona system and explore future research directions to further improve its capabilities and effectiveness.",10.1145/3613905.3651043,Proceedings Paper,Computer Science
A generative AI-driven interactive listening assessment task,2024,"Runge, Andrew; Attali, Yigal; Laflair, Geoffrey T.; Park, Yena; Church, Jacqueline","Introduction Assessments of interactional competence have traditionally been limited in large-scale language assessments. The listening portion suffers from construct underrepresentation, whereas the speaking portion suffers from limited task formats such as in-person interviews or role plays. Human-delivered tasks are challenging to administer at large scales, while automated assessments are typically very narrow in their assessment of the construct because they have carried over the limitations of traditional paper-based tasks to digital formats. However, computer-based assessments do allow for more interactive, automatically administered tasks, but come with increased complexity in task creation. Large language models present new opportunities for enhanced automated item generation (AIG) processes that can create complex content types and tasks at scale that support richer assessments.Methods This paper describes the use of such methods to generate content at scale for an interactive listening measure of interactional competence for the Duolingo English Test (DET), a large-scale, high-stakes test of English proficiency. The Interactive Listening task assesses test takers' ability to participate in a full conversation, resulting in a more authentic assessment of interactive listening ability than prior automated assessments by positing comprehension and interaction as purposes of listening.Results and discussion The results of a pilot of 713 tasks with hundreds of responses per task, along with the results of human review, demonstrate the feasibility of a human-in-the-loop, generative AI-driven approach for automatic creation of complex educational assessments at scale.",10.3389/frai.2024.1474019,Article,Computer Science
"She was useful, but a bit too optimistic: Augmenting Design with Interactive Virtual Personas",2025,"Deep, Paluck; Bharadhidasan, Monica; Kocaballi, A. Baki","Personas have been widely used to understand and communicate user needs in human-centred design. Despite their utility, they may fail to meet the demands of iterative workflows due to their static nature, limited engagement, and inability to adapt to evolving design needs. Recent advances in large language models (LLMs) pave the way for more engaging and adaptive approaches to user representation. This paper introduces Interactive Virtual Personas (IVPs): multimodal, LLM-driven, conversational user simulations that designers can interview, brainstorm with, and gather feedback from in real time via voice interface. We conducted a qualitative study with eight professional UX designers, employing an IVP named Alice across three design activities: user research, ideation, and prototype evaluation. Our findings demonstrate the potential of IVPs to expedite information gathering, inspire design solutions, and provide rapid user-like feedback. However, designers raised concerns about biases, over-optimism, the challenge of ensuring authenticity without real stakeholder input, and the inability of the IVP to fully replicate the nuances of human interaction. Our participants emphasized that IVPs should be viewed as a complement to, not a replacement for, real user engagement. We discuss strategies for prompt engineering, human-in-the-loop integration, and ethical considerations for effective and responsible IVP use in design. Finally, our work contributes to the growing body of research on generative AI in design process by providing insights into UX designers' experiences of LLM-powered interactive personas.",10.1016/j.ijhcs.2025.103646,Article,Computer Science; Engineering; Psychology
A Theoretical and Empirical Analysis of 2D and 3D Virtual Environments in Training for Child Interview Skills,2024,"Salehi, Pegah; Zohaib Hassan, Syed; Astrid Baugerud, Gunn; Powell, Martine; Johnson, Miriam S.; Johansen, Dag; Shafiee Sabet, Saeed; Riegler, Michael A.; Halvorsen, Pal","This paper presents a detailed study of an AI-driven platform designed for the training of child welfare and law enforcement professionals in conducting investigative interviews with maltreated children. It achieves a subjective simulation of interview situation through the integration of fine-tuned GPT-3 models within the Unity framework. The study recruited participants from a range of backgrounds, including professionals experienced in conducting investigative interviews and individuals with academic qualifications in psychology, criminology, or related disciplines. To assess the effectiveness of this tool, a multi-method evaluation approach was utilized, incorporating both quantitative analyses and qualitative interviews. The quantitative methods included mixed-effects models, which provided insights into how effects such as the type of virtual environment (2D vs. 3D), scenario variations, virtual reality (VR) familiarity, and professional expertise influence the user experience. Additionally, structural equation modeling (SEM) provided deeper insights into the relationships between variables, offering a comprehensive understanding of how they collectively impact the user experience. The qualitative method included a detailed semi-structured interview that provided a deeper understanding of user experiences and perceptions. The findings indicate significant advantages of the 3D environment in enhancing Flow and Virtual Fidelity; however, the 2D environment was favored for Usability. Despite the 3D environment's potential for greater immersion, the discomfort associated with VR head-mounted displays (HMDs) led some users to prefer the 2D setup. Familiarity with VR technology positively influenced user perception, indicating that prior exposure can mitigate some of the Avatar Interaction Comfort issues. Additionally, the Hand Movement Perception was better understood in scenarios with sensitive themes. As user experience increased, participants had a more positive view of the Age-Appropriate Response. Furthermore, the dialog system's effectiveness, particularly Response Relevance and Detailed Responses, played a significant role in Empathy Elicitation, often outweighing Virtual Fidelity. However, Emotion in facial expressions and Responsiveness were two factors that negatively impacted the effectiveness of the tool, indicating areas that need improvement in the future.",10.1109/ACCESS.2024.3442297,Article,Computer Science; Engineering; Telecommunications
"ChatGPT: Jack of all trades, master of none",2023,"Kocon, Jan; Cichecki, Igor; Kaszyca, Oliwier; Kochanek, Mateusz; Szydlo, Dominika; Baran, Joanna; Bielaniewicz, Julita; Gruza, Marcin; Janz, Arkadiusz; Kanclerz, Kamil; Kocon, Anna; Koptyra, Bartlomiej; Mieleszczenko-Kowszewicz, Wiktoria; Milkowski, Piotr; Oleksy, Marcin; Piasecki, Maciej; Radlinski, Lukasz; Wojtasik, Konrad; Wozniak, Stanislaw; Kazienko, Przemyslaw","OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few -shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool's usefulness to society and how the learning and validation procedures for such systems should be established.",10.1016/j.inffus.2023.101861,Article,Computer Science
"Predicting pediatric diagnostic imaging patient no-show and extended wait-times using LLMs, regression, and tree based models",2025,"Rafique, Daniel; Liu, Xuan; Gong, Bo; Belsito, Laura; McCradden, Melissa D.; Mazwi, Mjaye L.; Lee, Wayne; Ohanlon, Graham; Tsang, Kyle; Shroff, Manohar; Ertl-Wagner, Birgit; Khalvati, Farzad","Introduction Patients missing their appointments (no-shows) are a persistent issue that results in idle resources while delaying critical patient prognosis. Likewise, long waiting times increase frustration for patients, leaving a negative impression on the appointment. In this paper, we explore 3 modalities of diagnostic and interventional radiology appointments for pediatric patients at the Hospital for Sick Children (SickKids), Toronto, ON, Canada. Our goal was to survey machine learning methods that best predict the risk of patient no-shows and long wait-times exceeding 1 hour for scheduling teams to propose targeted downstream accommodations.Methods We experimented with 6 predictive model types separately trained on both tasks which included extreme gradient boosting (XGBoost), Random Forest (RF), Support Vector Machine, Logistic Regression, Artificial Neural Network, and a pre-trained large language model (LLM). Utilizing 20 features containing a mixture of patient demographics and appointment related data, we experimented with different data balancing methods including instance hardness threshold (IHT) and class weighting to reduce bias in prediction. We then conducted a comparative study of the improvements made by utilizing continuous contextual data in our LLM which boasted a 51% improvement in F1 score for the wait-time model.Results Our XGBoost model had the best combination of AUC and F1 scores (0.96 and 0.62, respectively) for predicting no-show while RF had the best AUC and F1 scores (0.83 and 0.61, respectively) for wait-time prediction. The LLMs also performed well for 90% probability thresholds (high risk patients) while being robustly calibrated on unseen test data.Discussion Our results surveyed multiple algorithms and data balancing methods to propose the greatest performing models on our tasks, implemented a unique methodology to use LLMs on heterogeneous data within this domain, and demonstrated the greater importance of contextual appointment data over patient demographic features for a more equitable prediction algorithm. Going forward, the predictive output (calibrated probabilities of events) can be used as stochastic input for risk-based optimized scheduling to provide accommodation for patients less likely to receive quality access to healthcare.",10.3389/frai.2025.1652397,Article,Computer Science
A Comparison of the Effectiveness of ChatGPT and Co-Pilot for Generating Quality Python Code Solutions,2024,"Nikolaidis, Nikolaos; Flamos, Karolos; Gulati, Khanak; Feitosa, Daniel; Ampatzoglou, Apostolos; Chatzigeorgiou, Alexander","Artificial intelligence (AI) has become increasingly popular in software development to automate tasks and improve efficiency. AI has the potential to help while developing or maintaining software, in the sense that it can produce solutions out of a textual requirement specification, and understand code to provide suggestion on how a new requirement could be implemented. In this paper, we focus on the first scenario. Two AI-powered tools that have the potential to revolutionize the way software is developed are OpenAI's ChatGPT and GitHub's Copilot. In this paper, we used LeetCode, a popular platform for technical interview preparation and personal upskilling (self-learning), to evaluate the effectiveness of ChatGPT and Copilot on a set of coding problems, along with ChatGPT's ability to correct itself when provided with feedback. The analysis of the effectiveness can lead to various conclusions, such as on if these solutions are ready to take over coding roles, and to what extent several parameters (difficulty and quality requirements) influence this result. Solutions have been generated for 60 problems using ChatGPT and Copilot, for the Python programming language. We investigated the performance of the models, the recurrent kinds of errors, and the resulting code quality. The evaluation revealed that ChatGPT and Copilot can be effective tools for generating code solutions for easy problems while both models are prone to syntax and semantic errors. Small improvements are observed for ode quality metrics across iterations, although the improvement pattern is not consistently monotonic, questioning ChatGPT's awareness of the quality of its own solutions. Nevertheless, the improvement that was found along iterations, highlights the potential of AI and humans, acting as partners, in providing the optimal combination. The two models demonstrate a limited capacity for understanding context. Although AI-powered coding tools driven by large language models have the potential to assist developers in their coding tasks, they should be used with caution and in conjunction with human coding expertise. Developer intervention is necessary not only to debug errors but also to ensure high-quality and optimized code.",10.1109/SANER-C62648.2024.00018,Proceedings Paper,Computer Science
"Predictive comparison and assessment of ANFIS and ANN, as efficient tools in modeling degradation of total petroleum hydrocarbon (TPH)",2022,"Ani, Kingsely Amechi; Agu, Chinedu Matthew","Composting is an effective organic waste management practice with a sustainable environmental impact on soil. In most cases, composting materials are agricultural residues constituting a huge environmental challenge. This study investigates the composting biostimulation of total petroleum hydrocarbon (TPH) contaminated soil using organic compost prepared from palm oil mill effluent (CPOME). The performance of CPOME in TPH degradation was evaluated using the first-order kinetic model and CO2 evolution. The artificial neural network (ANN) and adaptive neuro-fuzzy inference system (ANFIS) modelling approaches were further employed to optimize the experimental response. The experimental designs for ANFIS and ANN modelling were done using the central composite design (CCD). Statistical parameters such as RMSE, MSE, and MAE were used to evaluate model performance and comparisons. The first-order kinetic model and CO2 released showed that TPH degradation favours contaminated soil with 40 g/L initial TPH concentration. The first-order kinetic model observed a decrease in the constant degradation rate with increasing TPH concentration. The CO2 evolution, which measures microbial respiration/activity, also showed a decline in released CO2 with increasing TPH concentration. Lower values of RMSE, MSE, and MAE were obtained for the ANFIS model compared with the ANN. The R2 values between the experimental and model-predicted responses showed that the ANFIS (0.9349) model predictions fitted adequately compared to the ANN (0.8862). Finally, comparison and model performances showed that the ANFIS model prediction capability was considerably better than the ANN.",10.1016/j.clwas.2022.100052,Article,Engineering; Environmental Sciences & Ecology
Conversational Skills of LLM-based Healthcare Chatbot for Personalized Communications,2024,"Furini, Marco; Mariani, Michele; Montagna, Sara; Ferretti, Stefano","The healthcare sector is increasingly utilizing bots for patient management, chronic disease management, and health advice, providing 24/7 support and immediate medical responses. The language of these bots is crucial, impacting patient comfort, understanding, and trust. This paper examines the optimal communication style for medical chatbots and addresses four research questions: the influence of personal traits and disease types on healthcare communication perception, their combined effect, and the role of AI in improving communications. A Likert-scale questionnaire assessed user personality and perception across 15 scenarios. Results from 49 participants indicate that effective personalization requires considering both personality and disease type. AI-based clustering further refines personalization, revealing distinct preferences, such as females preferring empathetic communication and males preferring factual communication. This study highlights the necessity of integrating personality traits and medical conditions in chatbot communication to enhance patient engagement and healthcare outcomes.",10.1145/3677525.3678693,Proceedings Paper,Computer Science
How Does Information and Communication Technology (ICT) Affect Oil Palm Agricultural Practices?,2024,"Peng, Tan Say; Lyndon, Novel; Taib, Humaira Ma; Basaruddin, Nur Hana; Aman, Zaki; Gan, Sinyee; Tahir, Zurinah","This study investigates the factors influencing the adoption of Information and Communication Technology (ICT) among oil palm-independent smallholders and assesses their perception of the use of ICT in agricultural practices. Through personal interviews with 384 independent smallholders, this research examines the relationship between respondents' demographic characteristics and smartphone usage, identifies the factors influencing ICT adoption among oil palm independent smallholders (ISH), and evaluates the extent of ICT usage perceived by ISH. The analysis of the Chi-square test showed a significant difference p<0.05 between the education status of farmers and their use of smartphone brands, the time spent on social media, and the experience with the smartphone. The findings reveal a widespread agreement among smallholders on the value of smartphones, reflecting their recognition of the importance of ICT. However, the study highlights that accessibility to ICT facilities emerged as a significant factor with the lowest mean score, indicating a lack of adequate infrastructure in the surveyed areas. Furthermore, respondents expressed dissatisfaction with the current state of ICT facilities. Conversely, most respondents emphasized the significance of accessing online information on fresh fruit bunch (FFB) prices and demonstrated an eagerness to obtain agricultural information online. This study also emphasizes the need to understand the factors driving ICT adoption among ISH and underscores the importance of enhancing accessibility to ICT facilities and improving satisfaction with existing infrastructure. Empowering ISH to leverage ICT tools effectively can enable them to access vital information and establish efficient connections with potential markets in the digital era. Smartphones as ICT tools have revolutionized the agricultural sector by improving access to information, financial services, and advanced farming tools, thereby fostering higher efficiency and sustainability in agricultural practices.",10.17576/geo-2024-2003-07,Article,Social Sciences - Other Topics
Enhancing Human-Computer Interaction through AI: A Study on ChatGPT in Educational Environments,2024,"Kothari, Dhruval Kenal; Fernando, Owen Noel Newton","This research investigates the potential of Chat Generative Pre-Trained Transformer (ChatGPT) in Human-Computer Interaction (HCI) within educational contexts. Examining the intersection of Artificial Intelligence (AI) and HCI, the study emphasizes ChatGPT's ability to provide personalized and immediate responses, enhancing student engagement and understanding. A literature review reveals ChatGPT's applications in higher education, while highlighting challenges related to critical thinking in the field of HCI. This paper then outlines our objectives, focusing on answering student queries and generating Multiple-Choice Questions (MCQs) for revision. Experimental results demonstrate the superiority of Custom GPTs, emphasizing the importance of context-specific tailoring. The discussion addresses limitations, proposing future work on model tine-tuning, optimization, and human testing. In conclusion, this research contributes insights into leveraging ChatGPT in HCI education, highlighting its potential for personalized and effective learning experiences.",10.1109/CAI59869.2024.00100,Proceedings Paper,Computer Science
Toward Sensor-In-the-Loop LLM Agent: Benchmarks and Implications,2025,"Ren, Zhiwei; Li, Junbo; Zhang, Minjia; Wang, Di; Fan, Xiaoran; Shangguan, Longfei","This paper explores sensor-informed personal agents that can take advantage of sensor hints on wearables to enhance the personal agent's response. We demonstrate that such a sensor-in-the-loop AI agent design can be easily integrated into existing LLM agents by building a prototype named WellMax based on existing well-developed techniques such as structured prompt templates and few-shot prompting. The head-to-head comparison with a nonsensor-informed agent across five use scenarios demonstrates that this sensor-in-the-loop design can effectively improve users' needs and their overall experience. The deep-dive into agents' replies and participants' feedback further reveals that sensor-in-the-loop agents not only provide more contextually relevant responses but also exhibit a better understanding of user priorities and situational nuances. In addition, we conduct two case studies to examine the potential pitfalls and distill key insights from this sensor-in-the-loop agent. We hope this work can spawn new ideas for building more intelligent, empathetic, and effective AI-driven personal assistants.",10.1145/3715014.3722082,Proceedings Paper,Computer Science; Telecommunications
Decoding fake news fabrications and trends: A comprehensive survey,2025,"Kumar, Gaurav; Dhiman, Chhavi","Increased internet access has led to a surge in online content across blogs, websites, news portals, and social media, where people actively share personal ideas, opinions, ideologies while seeking information of their interest. However, relying on individual sources can lead to information overload and the spread of unverified data, often shaped by personal biases. This lack of fact-based reliability fuelled the generation and spread of fake news, undermining trust in digital information ecosystems. To tackle these challenges, Fake News Detection (FND) has become a crucial research area, drawing significant attention of experts to develop solutions to combat misinformation and restore trust in online information. This paper provides a comprehensive review of the changing patterns of fake news trends over time, tracing its shift from text to visual and eventually hybrid formats over the past decade. It reviews the generation and propagation of fake news, explores detection methods and highlights the challenges for efficient detection, including how human and algorithmic bias unknowingly contributes to its spread. The paper discusses key research questions and their implications, emphasizing why multimodal sentiment analysis outperforms other methods for detecting complex, malicious intent. It also provides an overview of popular datasets and resources, along with a bibliometric analysis highlighting key authors and leading institutes in the research area. Finally, it discusses the future direction of fake news detection, underscoring the need for continuous advancements in this rapidly evolving domain.",10.1016/j.neucom.2025.131118,Article,Computer Science
"How palm oil industry is having a devastating impact on Indonesia's health, rainforests, and labor market",2022,"Arifin, B.; Setyawan, D.","Palm oil is an essential commodity in Indonesia. However, despite the importance of palm oil for Indonesia's economy, on the contrary, this sector could also harm society as palm oil industries have incentives to burn the forest for extensive land clearing, creating forest fire, and air pollution. This study estimates palm oil companies' impact on forest fires, air pollution, health outcomes, and labor market consequences in Indonesia. This study uses data from The Indonesia National Socioeconomic Survey (SUSENAS), Universal Public Insurance Claim Data from Social Security Administrator (BPJS), and Ministry of Forestry and Environment Statistics. In addition, this study also uses municipality-level data to calculate forest fire and individual-level data to estimate health and labor outcomes. Finally, this study employs ordinary least squares (OLS) and matching methods to assess palm oil company's impact on forest fire, health, and labor outcomes. Estimation result suggests palm oil company significantly increases forest fire land area. Further, it also increases the probability of asthma, increases the likelihood of inpatient, and substantially decreases decision to work-more considerable negative health impact for children and elderly, more vulnerable group to forest fire pollution. Moreover, forest fire increases inpatient for the asthmatic individual and all other individuals who experience general respiratory symptoms.",10.1007/s13762-022-03923-4,Article,Environmental Sciences & Ecology
ChatGPT in the Classroom. Exploring Its Potential and Limitations in a Functional Programming Course,2024,"Popovici, Matei-Dan","In November 2022, OpenAI has introduced ChatGPT - a chatbot based on supervised and reinforcement learning. Not only can it answer questions emulating human-like responses, but it can also generate code from scratch or complete coding templates provided by the user. ChatGPT can generate unique responses which render any traditional anti-plagiarism tool useless. Its release has ignited a heated debate about its usage in academia, especially by students. We have found, to our surprise, that our students at POLITEHNICA University of Bucharest (UPB) have been using generative AI tools (ChatGPT and its predecessors) for solving homework, for at least 6 months. We therefore set out to explore the capabilities of ChatGPT and assess its value for educational purposes. We used ChatGPT to solve all our coding assignments for the semester from our UPB Functional Programming course. We discovered that, although ChatGPT provides correct answers in 68% of the cases, only around half of those are legible solutions which can benefit students in some form. On the other hand, ChatGPT has a very good ability to perform code review on student programming homework. Based on these findings, we discuss the pros and cons of ChatGPT in a teaching environment, as well as means for integrating GPT models for generating code reviews, in order to improve the code-writing skills of students.",10.1080/10447318.2023.2269006,Article,Computer Science; Engineering
GPT2SP: A Transformer-Based Agile Story Point Estimation Approach,2023,"Fu, Michael; Tantithamthavorn, Chakkrit","Story point estimation is a task to estimate the overall effort required to fully implement a product backlog item. Various estimation approaches (e.g., Planning Poker, Analogy, and expert judgment) are widely-used, yet they are still inaccurate and may be subjective, leading to ineffective sprint planning. Recent work proposed Deep-SE, a deep learning-based Agile story point estimation approach, yet it is still inaccurate, not transferable to other projects, and not interpretable. In this paper, we propose GPT2SP, a Transformer-based Agile Story Point Estimation approach. Our GPT2SP employs a GPT-2 pre-trained language model with a GPT-2 Transformer-based architecture, allowing our GPT2SP models to better capture the relationship among words while considering the context surrounding a given word and its position in the sequence and be transferable to other projects, while being interpretable. Through an extensive evaluation on 23,313 issues that span across 16 open-source software projects with 10 existing baseline approaches for within- and cross-project scenarios, our results show that our GPT2SP approach achieves a median MAE of 1.16, which is (1) 34%-57% more accurate than existing baseline approaches for within-project estimations; (2) 39%-49% more accurate than existing baseline approaches for cross-project estimations. The ablation study also shows that the GPT-2 architecture used in our approach substantially improves Deep-SE by 6%-47%, highlighting the significant advancement of the AI for Agile story point estimation. Finally, we develop a proof-of-concept tool to help practitioners better understand the most important words that contributed to the story point estimation of the given issue with the best supporting examples from past estimates. Our survey study with 16 Agile practitioners shows that the story point estimation task is perceived as an extremely challenging task. In addition, our AI-based story point estimation with explanations is perceived as more useful and trustworthy than without explanations, highlighting the practical need of our Explainable AI-based story point estimation approach.",10.1109/TSE.2022.3158252,Article,Computer Science; Engineering
Chatbots With Atitude: Enhancing Chatbot Interactions Through Dynamic Personality Infusion,2024,"Kovacevic, Nikola; Boschung, Tobias; Holz, Christian; Gross, Markus; Wampfer, Rafael","Equipping chatbots with personality has the potential of transforming user interactions from mere transactions to engaging conversations, enhancing user satisfaction and experience. In this work, we introduce dynamic personality infusion, a novel intermediate stage between the chatbot and the user that adjusts the chatbot's response using a dedicated chatbot personality model and GPT-4 without altering the chatbot's semantic capabilities. To test the efectiveness of our method, we frst collected human-chatbot conversations from 33 participants while they interacted with three LLM-based chatbots (GPT-3.5, Llama-2 13B, and Mistral 7B). Then, we conducted an online rating survey with 725 participants on the collected conversations. We analyze the impact of the personality infusion on the perceived trustworthiness of the chatbots and the suitability of diferent personality profles for real-world chatbot use cases. Our work paves the way for dynamic, personalized chatbots, enhancing user trust and real-world applicability.",10.1145/3640794.3665543,Proceedings Paper,Computer Science; Communication
CONTRIBUTION OF POLLINATORS IN PERUVIAN AGRICULTURE AND RANKING OF CROPS BASED ON THE ECONOMIC VALUE,2025,"Gonzales-Bustamante, Luis Enrique","The ecosystem service of pollination contributes significantly to food security and is provided by animals, mainly insects, which are being affected by climatic variables and anthropogenic activity This has encouraged studies to estimate the economic value of said service in order to obtain awareness tools for the protection of pollinators. The objective of this research was to estimate the contribution of pollinators in Peru in three agricultural production variables at national scale-the pollinated area, production as a result of pollination, and economic value of said production-in addition to establishing a ranking of the most significant crops based on the economic value. The 2023 Peruvian agricultural statistics (national level) were used, and the values of the mentioned agricultural variables of dependent crops were multiplied by the dependency index of crops with respect to pollinators (DR). In 37 crops with varying DR values (0.25-0.95), the contribution of these insects reaches US$ 1 037 million (Mll); a pollinated area of 298 643 ha; and a production volume of 2.4 Mll tons. In descending order, the ranking of crops with the highest economic contribution from pollinators were blueberries, cocoa, avocado, coffee, Brazil nut, and oil palm.",10.21704/rea.v24i1.2281,Article,Environmental Sciences & Ecology
Exploring ChatGPT's Empathic Abilities,2023,"Schaaff, Kristina; Reinig, Caroline; Schlippe, Tim","Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, e.g., children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. We analyzed the following three aspects: (1) understanding and expressing emotions, (2) parallel emotional response, and (3) empathic personality. Thus, we not only evaluate ChatGPT on various empathy aspects and compare it with human behavior but also show a possible way to analyze the empathy of chatbots in general. Our results show, that in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers. In conversations, ChatGPT reacted with a parallel emotion in 70.7% of cases. The empathic capabilities of ChatGPT were evaluated using a set of five questionnaires covering different aspects of empathy. Even though the results show, that the scores of ChatGPT are still worse than the average of healthy humans, it scores better than people who have been diagnosed with Asperger syndrome / high-functioning autism.",10.1109/ACII59096.2023.10388208,Proceedings Paper,Computer Science
Higher Education in the Age of AI: Chatgpt's Impact on Students' Academic Activities,2025,"Constantin, Alexandra; Iacob, Silvia-Elena","In the age of Artificial Intelligence, higher education institutions must adapt accordingly by smoothly transforming their curricula and pedagogical approaches in a student-centered perspective that allows young adults to develop the necessary skillset of AI competences that will ensure them employability in the near future. After the 2022 launch of Open AI's notorious ChatGPT, higher education settings have also faced the urgent need to both regulate and integrate gen AI platforms mainly because their students were already taking advantage of its benefits in terms of generating idea or producing high-quality writing papers. Even though several studies have documented GPT's impact on students' skills, there is limited focus on students' perceptions in Romanian higher education. Hence, the main aim of the present study is to investigate students' perception with regards to their perceived influence of generative AI use for academic purposes on their academic performance. The methodological approach taken in this study is a mixed methodology based on a pilot study (n=186), with data being gathered via a questionnaire-based survey and analysed via SPSS. Findings suggest that GPT improves several research skills of students, while 71% of respondents perceiving GPT's impact as significantly improving their productivity, thus this article adding a valuable contribution to the body of knowledge. Future studies could enhance representativeness, mitigate self-reported bias, and improve econometric model predictive power.",10.2478/picbe-2025-0256,Proceedings Paper,Business & Economics
The Impact of Dialogue with ChatGPT and Extroversion on Mental State,2025,"Tahara, Shunichi; Sugisaki, Eriko; Yasuda, Keiji; Miyamoto, Akihiro; Sanada, Motoyuki; Nakajima, Kae; Ikeda, Kazushi; Kobayashi, Nao; Naruse, Yasushi","Conversations with others are important in order to improve an individual's mental state. Recently, LLM-based dialogue systems, such as ChatGPT, facilitate natural conversation and have the potential to improve mental state such as emotions or mood. Our aim is to develop a dialogue system that generates responses tailored to the user's personality and physiological state. As part of this study, we set the research question Does dialogue with ChatGPT, and the interplay of the extroversion between the user and ChatGPT, improve the user's mental state?. In order to verify this research question, we implemented a dialogue application, and conducted a total of three speech dialogue experiments using a ChatGPT API. Participants rated their mental state along seven emotion axes (e.g., Sad-Happy, Anxious-Relaxed), using a 15-point scale both before and after each dialogue. To set the extroversion level (low, average or high) for ChatGPT, we added information of ChatGPT's extroversion into a prompt. Our experiments showed that the mean of seven emotion axes significantly improved after dialogues. Moreover, although not statistically significant, the largest differences between before and after dialogue were observed when both participants and ChatGPT had either a similarly low or high level of extroversion.",10.1007/978-3-031-94171-9_40,Proceedings Paper,Computer Science
Optimization of Nutritional and Operational Conditions for Producing PHA by the Halophilic Bacterium Halomonas boliviensis from Oil Palm Empty Fruit Bunch and Gluten Hydrolysates,2022,"Arcila-Echavarria, Diana Catalina; Lu-Chau, Thelmo Alejandro; Gomez-Vanegas, Natalia Andrea; Penuela-Vasquez, Mariana; Marsiglia-Lopez, Dalma Elizabeth","The optimization of nutritional and operational conditions that favor the production of the biopolymer polyhydroxyalcanoate (PHA) by the halophilic bacterium Halomonas boliviensis was carried out in this study. In a Plackett Burman design the significance of six factors was evaluated: agitation, temperature, pH, initial concentration of glucose, nitrogen and KH2PO4. In a second experimental design the factors with high significance (temperature, pH and KH2PO4) were further studied applying the response surface methodology (Box Benkhen design). This design allowed obtaining a second order polynomial that modeled the response variables (biomass and PHA) to predict the values of the factors that maximizes biomass and PHA in the studied range, which were: 31 degrees C, 7.0 and 1.6 g/L for temperature, pH and KH2PO4, respectively. The optimal values were validated in fermentations using synthetic culture medium and a complex medium based on oil palm empty fruit bunch and gluten hydrolysates as carbon and nitrogen sources, respectively. Biomass and PHA concentrations of 9.2 and 6.5 g/L, respectively, were reached in the synthetic medium, which represent deviations of 26 and 4% with respect to the values predicted by the model. On the other hand, the biomass and PHA concentrations obtained with the hydrolysate-based medium was 70 and 35.4% that reached with the synthetic culture medium. [GRAPHICS] .",10.1007/s12649-021-01605-9,Article,Environmental Sciences & Ecology
Sustainability of Animal Stocks in Traditional Pastoral Systems of the High-Altitude Andean Altiplano,2023,"Rojo, Veronica; Momo, Fernando R.; Arzamendia, Yanina; Baldo, Jorge L.; Vila, Bibiana L.","The main traditional livelihood in the high Central Andes Plateau, locally known as the Puna, is pastoral-ism. This system is based on llamas (Lama glama) and sheep (Ovis aries), which graze on natural veg-etation (shrub steppe, grasslands, and peatlands). So, the livelihood of Andean communities is strongly influenced by the spatial and temporal dynamics of vegetation. Most of the existing models on herbivory used to estimate sustainable livestock loads require information on the nutritional values and primary production of vegetation, which is lacking for the Argentine Puna. This work aimed to fill the gap by implementing a carrying capacity model using data on annual vegetation growth and herbivore feed-ing strategies collected in the field in different plant communities. The hypothesis that guided this work was that environmental heterogeneity affects management decisions and should be taken into account to model sustainable animal stocks in Andean pastoralist systems. We estimated carrying capacity based on the MSY logistic model, considering biomass and aboveground primary production of vegetation commu-nities, climatic features, and herbivore selective responses. We compared the estimated carrying capacity with real animal stocks in four production units within the study site in 2013, 2014, and 2017. The results indicate that the carrying capacity presents high spatial and temporal variability. In three of the produc-tion units assessed, the livestock-mixed herds of sheep and llamas-were above or near the estimated carrying capacity, even when the stocking rates were reduced during dry periods. The high stocking rates, along with evidence of land degradation, may indicate moderate to severe overgrazing. This work demon-strates the usefulness of estimating the carrying capacity to monitor the pastoral systems at fine space and time scales. These monitoring techniques can facilitate decision making among Puna herders in a biocultural context of knowledge dialog that may foster sustainability in pastoralist livelihoods.& COPY; 2023 The Society for Range Management. Published by Elsevier Inc. All rights reserved.",10.1016/j.rama.2023.05.002,Article,Environmental Sciences & Ecology
MMBench: Is Your Multi-modal Model an All-around Player?,2023,"Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin","Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",10.48550/arXiv.2307.06281,"['JournalArticle', 'Conference']",Computer Science
The Rise and Potential of Large Language Model Based Agents: A Survey,2023,"Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, Tao Gui","For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",10.48550/arXiv.2309.07864,"['JournalArticle', 'Review']",Computer Science; Linguistics
Instruction Tuning for Large Language Models: A Survey,2023,"Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang","This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users'objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",10.48550/arXiv.2308.10792,"['JournalArticle', 'Review']",Computer Science
A Survey on In-context Learning,2022,"Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui","With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",10.18653/v1/2024.emnlp-main.64,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Explainability for Large Language Models: A Survey,2023,"Haiyan Zhao, Hanjie Chen, F. Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du","Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.",10.1145/3639372,"['JournalArticle', 'Review']",Computer Science; Linguistics
Large Language Models: A Survey,2024,"Shervin Minaee, Tom Mikolov, Narjes Nikzad, M. Chenaghlu, R. Socher, Xavier Amatriain, Jianfeng Gao","Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",10.48550/arXiv.2402.06196,"['JournalArticle', 'Review']",Computer Science; Linguistics
Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators,2024,"Yann Dubois, Bal'azs Galambosi, Percy Liang, Tatsunori Hashimoto","LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question:""What would the preference be if the model's and baseline's output had the same length?""To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, but we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98.",10.48550/arXiv.2404.04475,['JournalArticle'],Computer Science; Mathematics
Large Language Model based Multi-Agents: A Survey of Progress and Challenges,2024,"Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, Xiangliang Zhang","Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.",10.48550/arXiv.2402.01680,"['JournalArticle', 'Conference', 'Review']",Computer Science
Recommender Systems in the Era of Large Language Models (LLMs),2023,"Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, Qing Li","With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.",10.1109/TKDE.2024.3392335,"['JournalArticle', 'Review']",Computer Science
A Survey for In-context Learning,2023,"Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui",,10.48550/arXiv.2301.00234,"['JournalArticle', 'Review']",Computer Science; Linguistics
Social Simulacra: Creating Populated Prototypes for Social Computing Systems,2022,"J. Park, Lindsay Popowski, Carrie J. Cai, M. Morris, Percy Liang, Michael S. Bernstein","Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designers description of a communitys designgoal, rules, and member personasand produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of what if? scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.",10.1145/3526113.3545616,"['Book', 'JournalArticle', 'Conference']",Computer Science
Character-LLM: A Trainable Agent for Role-Playing,2023,"Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu","Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",10.48550/arXiv.2310.10158,"['JournalArticle', 'Conference']",Computer Science
ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search,2024,"Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang","Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\text{EM}$ and Self-Rewarding LM. We release all code at https://github.com/THUDM/ReST-MCTS.",10.48550/arXiv.2406.03816,['JournalArticle'],Computer Science
How Can Recommender Systems Benefit from Large Language Models: A Survey,2023,"Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, Weinan Zhang","With the rapid development of online services and web applications, recommender systems (RS) have become increasingly indispensable for mitigating information overload and matching users information needs by providing personalized suggestions over items. Although the RS research community has made remarkable progress over the past decades, conventional recommendation models (CRM) still have some limitations, e.g., lacking open-domain world knowledge, and difficulties in comprehending users underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities for various natural language processing (NLP) tasks, which mainly stem from their extensive open-world knowledge, logical and commonsense reasoning abilities, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of RS and pointing out a promising research direction, i.e., whether we can incorporate LLM and benefit from their common knowledge and capabilities to compensate for the limitations of CRM. In this article, we conduct a comprehensive survey on this research direction, and draw a birds-eye view from the perspective of the whole pipeline in real-world RS. Specifically, we summarize existing research works from two orthogonal aspects: where and how to adapt LLM to RS. For the WHERE question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the HOW question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLM or not during training, and whether to involve CRM for inference. Detailed analysis and general development paths are provided for both WHERE and HOW questions, respectively. Then, we highlight the key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects.",10.1145/3678004,"['JournalArticle', 'Review']",Computer Science
Fine-tuning language models to find agreement among humans with diverse preferences,2022,"Michiel A. Bakker, Martin Chadwick, Hannah Sheahan, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, M. Botvinick, C. Summerfield","Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single""generic""user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g.,""should we raise taxes on the rich?""), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.",10.48550/arXiv.2211.15006,['JournalArticle'],Computer Science
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity,2024,"Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park","Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.",10.48550/arXiv.2403.14403,"['JournalArticle', 'Conference']",Computer Science
"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",2023,"Alec Helbling, Mansi Phute, Matthew Hull, Sebastian Szyller, Duen Horng Chau","Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses. Our method does not require any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. We test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent LLMs against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2. The code is publicly available at https://github.com/poloclub/llm-self-defense",10.48550/arXiv.2308.07308,['JournalArticle'],Computer Science
"Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",2024,"Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zi-Liang Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Yaqiong Zhang, Yunxin Liu","Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.",10.48550/arXiv.2401.05459,"['JournalArticle', 'Review']",Computer Science; Engineering
Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions,2023,"Hui Yang, Sifu Yue, Yunzhong He","Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate real-world scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.",10.48550/arXiv.2306.02224,['JournalArticle'],Computer Science
Large Language Models on Graphs: A Comprehensive Survey,2023,"Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han","Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.",10.1109/TKDE.2024.3469578,"['JournalArticle', 'Review']",Computer Science; Linguistics
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models,2023,"Zhen Lin, Shubhendu Trivedi, Jimeng Sun","Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",10.48550/arXiv.2305.19187,['JournalArticle'],Computer Science; Mathematics
Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,2023,"Rishabh Bhardwaj, Soujanya Poria","Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).",10.48550/arXiv.2308.09662,['JournalArticle'],Computer Science
Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision,2023,"Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin","The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://q-future.github.io/Q-Bench.",10.48550/arXiv.2309.14181,['JournalArticle'],Computer Science
A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models,2023,"Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, Philip H. S. Torr",,10.48550/arXiv.2307.12980,"['JournalArticle', 'Review']",Computer Science
"Generative Agent Simulations of 1,000 People",2024,"Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, C. Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein","The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.",10.48550/arXiv.2411.10109,"['JournalArticle', 'Review']",Computer Science; Psychology
"A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges",2024,"Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang","The pursuit of more intelligent and credible autonomous systems, akin to human society, has been a long-standing endeavor for humans. Leveraging the exceptional reasoning and planning capabilities of large language models (LLMs), LLM-based agents have been proposed and have achieved remarkable success across a wide array of tasks. Notably, LLM-based multi-agent systems (MAS) are considered a promising pathway towards realizing general artificial intelligence that is equivalent to or surpasses human-level intelligence. In this paper, we present a comprehensive survey of these studies, offering a systematic review of LLM-based MAS. Adhering to the workflow of LLM-based multi-agent systems, we synthesize a general structure encompassing five key components: profile, perception, self-action, mutual interaction, and evolution. This unified framework encapsulates much of the previous work in the field. Furthermore, we illuminate the extensive applications of LLM-based MAS in two principal areas: problem-solving and world simulation. Finally, we discuss in detail several contemporary challenges and provide insights into potential future directions in this domain.",10.1007/s44336-024-00009-2,"['JournalArticle', 'Review']",Computer Science
On the Risk of Misinformation Pollution with Large Language Models,2023,"Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, W. Wang","In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.",10.48550/arXiv.2305.13661,"['JournalArticle', 'Conference']",Computer Science
Agent AI: Surveying the Horizons of Multimodal Interaction,2024,"Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, J. Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, D. Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Fei-Fei Li, Jianfeng Gao","Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define""Agent AI""as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied actions. In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs. The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment.",10.48550/arXiv.2401.03568,"['JournalArticle', 'Review']",Computer Science
PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits,2023,"Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, Jad Kabbara","Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas' self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas' writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.",10.18653/v1/2024.findings-naacl.229,['JournalArticle'],Computer Science; Psychology
Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,2023,"Shashank Gupta, Vaishnavi Shrivastava, A. Deshpande, A. Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot","Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",10.48550/arXiv.2311.04892,['JournalArticle'],Computer Science; Psychology
Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks,2023,"Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West","Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk",10.48550/arXiv.2306.07899,"['JournalArticle', 'Review']",Computer Science
Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization,2024,"Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, Yun-Nung Chen","The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuLab/PersonaLLM-Survey",10.48550/arXiv.2406.01171,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
From Persona to Personalization: A Survey on Role-Playing Language Agents,2024,"Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, Yanghua Xiao","Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.",10.48550/arXiv.2404.18231,"['JournalArticle', 'Review']",Computer Science; Linguistics
Grounding Language with Visual Affordances over Unstructured Data,2022,"Oier Mees, Jessica Borja-Diaz, Wolfram Burgard","Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de.",10.1109/ICRA48891.2023.10160396,"['JournalArticle', 'Conference']",Computer Science
"Who is GPT-3? An exploration of personality, values and demographics",2022,"Maril Miotto, Nicola Rossberg, Bennett Kleinberg","Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa.",10.48550/arXiv.2209.14338,['JournalArticle'],Computer Science; Psychology
Evaluating Very Long-Term Conversational Memory of LLM Agents,2024,"Adyasha Maharana, Dong-Ho Lee, S. Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang","Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.",10.48550/arXiv.2402.17753,"['JournalArticle', 'Conference']",Computer Science
InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews,2023,"Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, Yanghua Xiao","Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%.",10.18653/v1/2024.acl-long.102,"['JournalArticle', 'Conference']",Computer Science; Psychology
"Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities",2024,"Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun Wu, Dun Yuan, Li Jiang, Di Wu, Xue Liu, Charlie Zhang, Xianbin Wang, Jiangchuan Liu","Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.",10.1109/COMST.2024.3465447,"['JournalArticle', 'Review']",Computer Science; Engineering
"Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",2024,"Xi Fang, Weijie Xu, F. Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan H. Sengamedu, Christos Faloutsos","Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.",10.48550/arXiv.2402.17944,"['JournalArticle', 'Review']",Computer Science
Eight Things to Know about Large Language Models,2023,Sam Bowman,"
 The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This article surveys the evidence for eight potentially surprising such points: (1) LLMs predictably get more effective with increasing investment, even without targeted innovation; (2) many important LLM behaviors arise unpredictably as a byproduct of increasing investment; (3) LLMs often appear to learn and use representations of the outside world; (4) experts are not yet able to interpret the inner workings of LLMs; (5) there are no reliable techniques for steering the behavior of LLMs; (6) human performance on a task isn't an upper bound on LLM performance; (7) LLMs need not express the values of their creators nor the values encoded in web text; (8) brief interactions with LLMs are often misleading.",10.48550/arXiv.2304.00612,"['JournalArticle', 'Review']",Computer Science; Economics; Linguistics
Revolutionizing Finance with LLMs: An Overview of Applications and Insights,2024,"Huaqin Zhao, Zheng Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, Tianming Liu","In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry.",10.48550/arXiv.2401.11641,"['JournalArticle', 'Review']",Business; Computer Science
The Self-Perception and Political Biases of ChatGPT,2023,"Jrme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Markus Pauly","This contribution analyzes the self-perception and political biases of OpenAIs Large Language Model ChatGPT. Considering the first small-scale reports and studies that have emerged, claiming that ChatGPT is politically biased towards progressive and libertarian points of view, this contribution is aimed at providing further clarity on this subject. Although the concept of political bias and affiliation is hard to define, lacking an agreed-upon measure for its quantification, this contribution attempts to examine this issue by having ChatGPT respond to questions on commonly used measures of political bias. In addition, further measures for personality traits that have previously been linked to political affiliations were examined. More specifically, ChatGPT was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the G7 member states. These eight tests were repeated ten times each and indicate that ChatGPT seems to hold a bias towards progressive views. The political compass test revealed a bias towards progressive and libertarian views, supporting the claims of prior research. The political questionnaires for the G7 member states indicated a bias towards progressive views but no significant bias between authoritarian and libertarian views, contradicting the findings of prior reports. In addition, ChatGPTs Big Five personality traits were tested using the OCEAN test, and its personality type was queried using the Myers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT was evaluated using the Dark Factor test. These three tests were also repeated ten times each, revealing that ChatGPT perceives itself as highly open and agreeable, has the Myers-Briggs personality type ENFJ, and is among the test-takers with the least pronounced dark traits.",10.48550/arXiv.2304.07333,['JournalArticle'],Computer Science; Linguistics; Political Science
Using GPT for Market Research,2024,"James Brand, A. Israeli, Donald Ngwe","Large language models (LLMs) have quickly become popular as labor-augmenting tools for programming, writing, and many other processes that benefit from quick text generation. In this paper we explore the uses and benefits of LLMs for researchers and practitioners who aim to understand consumer preferences. We focus on the distributional nature of LLM responses, and query the Generative Pre-trained Transformer 3.5 (GPT-3.5) model to generate hundreds of survey responses to each prompt. We offer two sets of results to illustrate our approach and assess it. First, we show that GPT-3.5, a widely-used LLM, responds to sets of survey questions in ways that are consistent with economic theory and well-documented patterns of consumer behavior, including downward-sloping demand curves and state dependence. Second, we show that estimates of willingness-to-pay for products and features generated by GPT-3.5 are of realistic magnitudes and match estimates from a recent study that elicited preferences from human consumers. We also offer preliminary guidelines for how best to query information from GPT-3.5 for marketing purposes and discuss potential limitations. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4395751",10.2139/ssrn.4395751,"['Book', 'JournalArticle', 'Review']",Business; Computer Science; Economics
RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics,2024,"Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, A. Mousavian, Dieter Fox","From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: https://robo-point.github.io.",10.48550/arXiv.2406.10721,['JournalArticle'],Computer Science; Engineering
Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents,2024,"Pranav Putta, Edmund Mills, Naman Garg, S. Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov","Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.",10.48550/arXiv.2408.07199,['JournalArticle'],Computer Science
A Survey on Vision-Language-Action Models for Embodied AI,2024,"Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King","Embodied AI is widely recognized as a key element of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. In recent years, a myriad of VLAs have been developed, making it imperative to capture the rapidly evolving landscape through a comprehensive survey. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs and outline promising future directions in embodied AI. We have created a project associated with this survey, which is available at https://github.com/yueen-ma/Awesome-VLA.",10.48550/arXiv.2405.14093,"['JournalArticle', 'Review']",Computer Science
Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,2023,"Jing Wei, Sungdong Kim, Hyunhoon Jung, Young-Ho Kim","Large language models (LLMs) provide a new way to build chatbots by accepting natural language prompts. Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal such as collecting self-report data from users. We explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably. To this aim, we formulated four prompt designs with different structures and personas. Through an online study (N = 48) where participants conversed with chatbots driven by different designs of prompts, we assessed how prompt designs and conversation topics affected the conversation flows and users' perceptions of chatbots. Our chatbots covered 79% of the desired information slots during conversations, and the designs of prompts and topics significantly influenced the conversation flows and the data collection performance. We discuss the opportunities and challenges of building chatbots with LLMs.",10.1145/3637364,['JournalArticle'],Computer Science
Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward,2024,"Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang","Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mechanism, which directly takes video frames as input. Furthermore, we show that applying this tailored reward through DPO significantly improves the performance of video LMMs on video QA tasks.",10.48550/arXiv.2404.01258,"['JournalArticle', 'Conference']",Computer Science
Putting GPT-3's Creativity to the (Alternative Uses) Test,2022,"C. Stevenson, I. Smal, M. Baas, R. Grasman, H. Maas","AI large language models have (co-)produced amazing written works from newspaper articles to novels and poetry. These works meet the standards of the standard definition of creativity: being original and useful, and sometimes even the additional element of surprise. But can a large language model designed to predict the next text fragment provide creative, out-of-the-box, responses that still solve the problem at hand? We put Open AI's generative natural language model, GPT-3, to the test. Can it provide creative solutions to one of the most commonly used tests in creativity research? We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question. Our results show that -- on the whole -- humans currently outperform GPT-3 when it comes to creative output. But, we believe it is only a matter of time before GPT-3 catches up on this particular task. We discuss what this work reveals about human and AI creativity, creativity testing and our definition of creativity.",10.48550/arXiv.2206.08932,['JournalArticle'],Computer Science
A Survey of Techniques for Optimizing Transformer Inference,2023,"Krishna Teja Chitty-Venkata, Sparsh Mittal, M. Emani, V. Vishwanath, Arun Somani","Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.",10.48550/arXiv.2307.07982,"['JournalArticle', 'Review']",Computer Science
CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,2023,"Myra Cheng, Tiziano Piccardi, Diyi Yang","Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",10.48550/arXiv.2310.11501,"['JournalArticle', 'Conference', 'Review']",Computer Science; Sociology
Can ChatGPT Assess Human Personalities? A General Evaluation Framework,2023,"Haocong Rao, Cyril Leung, C. Miao","Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people. We further propose three evaluation metrics to measure the consistency, robustness, and fairness of assessment results from state-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal ChatGPT's ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.",10.48550/arXiv.2303.01248,"['JournalArticle', 'Conference']",Computer Science; Linguistics; Psychology
Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness,2023,"Jiuhai Chen, Jonas Mueller","We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).",10.18653/v1/2024.acl-long.283,"['JournalArticle', 'Conference']",Computer Science
Investigating Cultural Alignment of Large Language Models,2024,"Badr AlKhamissi, Muhammad N. ElNokrashy, Mai AlKhamissi, Mona Diab","The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.",10.48550/arXiv.2402.13231,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,"J. Wang, Qiushi Sun, Nuo Chen, Xiang Lorraine Li, Ming Gao","Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",10.48550/arXiv.2306.06427,"['JournalArticle', 'Conference']",Computer Science
Aligning Language Models to User Opinions,2023,"EunJeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon","An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by Pew Research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. In addition to the typical approach of prompting LLMs with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately.",10.48550/arXiv.2305.14929,"['JournalArticle', 'Conference', 'Review']",Computer Science
Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations,2023,"Keivalya Pandya, M. Holia","In the digital age, the dynamics of customer service are evolving, driven by technological advancements and the integration of Large Language Models (LLMs). This research paper introduces a groundbreaking approach to automating customer service using LangChain, a custom LLM tailored for organizations. The paper explores the obsolescence of traditional customer support techniques, particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift towards responsive, context-aware, and personalized customer interactions. The heart of this innovation lies in the fusion of open-source methodologies, web scraping, fine-tuning, and the seamless integration of LangChain into customer service platforms. This open-source state-of-the-art framework, presented as""Sahaay,""demonstrates the ability to scale across industries and organizations, offering real-time support and query resolution. Key elements of this research encompass data collection via web scraping, the role of embeddings, the utilization of Google's Flan T5 XXL, Base and Small language models for knowledge retrieval, and the integration of the chatbot into customer service platforms. The results section provides insights into their performance and use cases, here particularly within an educational institution. This research heralds a new era in customer service, where technology is harnessed to create efficient, personalized, and responsive interactions. Sahaay, powered by LangChain, redefines the customer-company relationship, elevating customer retention, value extraction, and brand image. As organizations embrace LLMs, customer service becomes a dynamic and customer-centric ecosystem.",10.48550/arXiv.2310.05421,['JournalArticle'],Business; Computer Science
Personalized Language Modeling from Personalized Human Feedback,2024,"Xinyu Li, Z. Lipton, Liu Leqi","Personalized large language models (LLMs) are designed to tailor responses to individual user preferences. While Reinforcement Learning from Human Feedback (RLHF) is a commonly used framework for aligning LLMs with human preferences, vanilla RLHF assumes that all human preferences share the same distribution, preventing fine-tuned LLMs from generating personalized content when user preferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF), an efficient framework that utilizes a lightweight user model to capture individual user preferences and jointly learns the user model and the personalized LLM from human feedback. P-RLHF exhibits the following three characteristics: (1) It enables an LLM to generate personalized content and scale efficiently with growing number of users. (2) It handles both explicit user preferences described as textual input and implicit user preferences encoded in the feedback data. (3) It eliminates the need for users to fully articulate their preferences, which are normally needed for prompting LLMs to generate personalized content yet are often impractical to obtain in real-world scenarios. Our experimental results show that personalized LLMs trained using P-RLHF generate responses that are more closely aligned with individual user preferences, outperforming vanilla, non-personalized RLHF and prompting-based personalization approaches across different tasks. We opensource our code at https://github.com/HumainLab/Personalized_RLHF.",10.48550/arXiv.2402.05133,['JournalArticle'],Computer Science
Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations,2023,"Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, Leilani Gilpin","Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce""helpful""responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as""fantastic""and""memorable""in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.",10.48550/arXiv.2310.11207,"['JournalArticle', 'Review']",Computer Science; Linguistics
LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models,2024,"Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei","This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.",10.48550/arXiv.2404.01230,"['JournalArticle', 'Review']",Computer Science
Synthetic Replacements for Human Survey Data? The Perils of Large Language Models,2024,"James Bisbee, Joshua D. Clinton, C. Dorff, Brenton Kenkel, Jennifer M. Larson","
 Large language models (LLMs) offer new research possibilities for social scientists, but their potential as synthetic data is still largely unknown. In this paper, we investigate how accurately the popular LLM ChatGPT can recover public opinion, prompting the LLM to adopt different personas and then provide feeling thermometer scores for 11 sociopolitical groups. The average scores generated by ChatGPT correspond closely to the averages in our baseline survey, the 20162020 American National Election Study (ANES). Nevertheless, sampling by ChatGPT is not reliable for statistical inference: there is less variation in responses than in the real surveys, and regression coefficients often differ significantly from equivalent estimates obtained using ANES data. We also document how the distribution of synthetic responses varies with minor changes in prompt wording, and we show how the same prompt yields significantly different results over a 3-month period. Altogether, our findings raise serious concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.",10.1017/pan.2024.5,"['JournalArticle', 'Review']",Computer Science; Political Science
"A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",2024,"Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Poor, Qingsong Wen, Stefan Zohren","Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.",10.48550/arXiv.2406.11903,"['JournalArticle', 'Review']",Business; Computer Science; Economics
A Survey on Large Language Model-Based Game Agents,2024,"Sihao Hu, Tiansheng Huang, Fatih Ilhan, S. Tekin, Gaowen Liu, R. Kompella, Ling Liu","The development of game agents holds a critical role in advancing towards Artificial General Intelligence. The progress of Large Language Models (LLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around three core functional components: memory, reasoning and in/output. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting&exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.",10.48550/arXiv.2404.02039,"['JournalArticle', 'Review']",Computer Science
"What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",2023,"Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, Zhaoran Wang","In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned by large language models? (b) What is a proper performance metric for ICL and what is the error rate? (c) How does the transformer architecture enable ICL? To answer these questions, we adopt a Bayesian view and formulate ICL as a problem of predicting the response corresponding to the current covariate, given a number of examples drawn from a latent variable model. To answer (a), we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm, which is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a $\mathcal{O}(1/T)$ regret bound for perfectly pretrained ICL, where $T$ is the number of examples in the prompt. To answer (c), we show that, in addition to encoding Bayesian model averaging via attention, the transformer architecture also enables a fine-grained statistical analysis of pretraining under realistic assumptions. In particular, we prove that the error of pretrained model is bounded by a sum of an approximation error and a generalization error, where the former decays to zero exponentially as the depth grows, and the latter decays to zero sublinearly with the number of tokens in the pretraining dataset. Our results provide a unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization, which deepens our knowledge of these essential aspects of modern language models.",10.48550/arXiv.2305.19420,['JournalArticle'],Computer Science; Mathematics
Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF,2023,"Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell","In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at https://github.com/cassidylaidlaw/hidden-context",10.48550/arXiv.2312.08358,['JournalArticle'],Computer Science; Mathematics
A Survey of Graph Meets Large Language Model: Progress and Future Directions,2023,"Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hongtao Cheng, Jeffrey Xu Yu","Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.",10.48550/arXiv.2311.12399,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs,2023,"Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, Kam-Fai Wong","Large Language Models (LLMs), such as \texttt{ChatGPT}, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user's hidden needs and respond satisfactorily through a single-step inference. To this end, we propose a novel linguistic cue-based chain-of-thoughts (\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response. To evaluate the approach, we build a benchmark with in-depth dialogue questions, consisting of 6 datasets in both Chinese and English, targeting 3 major linguistic cues during the conversation: \textit{personality}, \textit{emotion}, and \textit{psychology}. We conduct extensive experiments on the proposed benchmark with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate our proposed \textit{Cue}-CoT method outperforms standard prompting methods in terms of both \textit{helpfulness} and \textit{acceptability} on all datasets.",10.18653/v1/2023.findings-emnlp.806,"['JournalArticle', 'Conference']",Computer Science
A Short Survey of Viewing Large Language Models in Legal Aspect,2023,ZhongXiang Sun,"Large language models (LLMs) have transformed many fields, including natural language processing, computer vision, and reinforcement learning. These models have also made a significant impact in the field of law, where they are being increasingly utilized to automate various legal tasks, such as legal judgement prediction, legal document analysis, and legal document writing. However, the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability. In this survey, we explore the integration of LLMs into the field of law. We discuss the various applications of LLMs in legal tasks, examine the legal challenges that arise from their use, and explore the data resources that can be used to specialize LLMs in the legal domain. Finally, we discuss several promising directions and conclude this paper. By doing so, we hope to provide an overview of the current state of LLMs in law and highlight the potential benefits and challenges of their integration.",10.48550/arXiv.2303.09136,"['JournalArticle', 'Review']",Computer Science; Law
Aligning Large Language Models through Synthetic Feedback,2023,"Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo","Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost",10.48550/arXiv.2305.13735,"['JournalArticle', 'Conference']",Computer Science
AI for social science and social science of AI: A Survey,2024,"Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han","Recent advancements in artificial intelligence, particularly with the emergence of large language models (LLMs), have sparked a rethinking of artificial general intelligence possibilities. The increasing human-like capabilities of AI are also attracting attention in social science research, leading to various studies exploring the combination of these two fields. In this survey, we systematically categorize previous explorations in the combination of AI and social science into two directions that share common technical approaches but differ in their research objectives. The first direction is focused on AI for social science, where AI is utilized as a powerful tool to enhance various stages of social science research. While the second direction is the social science of AI, which examines AI agents as social entities with their human-like cognitive and linguistic capabilities. By conducting a thorough review, particularly on the substantial progress facilitated by recent advancements in large language models, this paper introduces a fresh perspective to reassess the relationship between AI and social science, provides a cohesive framework that allows researchers to understand the distinctions and connections between AI for social science and social science of AI, and also summarized state-of-art experiment simulation platforms to facilitate research in these two directions. We believe that as AI technology continues to advance and intelligent agents find increasing applications in our daily lives, the significance of the combination of AI and social science will become even more prominent.",10.48550/arXiv.2401.11839,"['JournalArticle', 'Review']",Computer Science; Political Science; Sociology
Deep Model Fusion: A Survey,2023,"Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, Li Shen","Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1)""Mode connectivity"", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2)""Alignment""matches units between neural networks to create better conditions for fusion; (3)""Weight average"", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4)""Ensemble learning""combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.",10.48550/arXiv.2309.15698,"['JournalArticle', 'Review']",Computer Science
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps,2023,"Yanfang Li, Huan Wang, Muxia Sun","Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT represents a landmark achievement in this research paradigm, offering hope for general artificial intelligence due to its highly intelligent natural language understanding ability. However, the PHM field lacks a consensus on how to respond to this significant change in the AI field, and a systematic review and roadmap is required to elucidate future development directions. To fill this gap, this paper systematically expounds on the key components and latest developments of LSF-Models. Then, we systematically answered how to build the LSF-Model applicable to PHM tasks and outlined the challenges and future development roadmaps for this research paradigm.",10.48550/arXiv.2305.06472,"['JournalArticle', 'Review']",Computer Science; Engineering
Personalization of Large Language Models: A Survey,2024,"Zhehao Zhang, Ryan A. Rossi, B. Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe Barrow, Tong Yu, Sungchul Kim, Ruiyi Zhang, Jiuxiang Gu, Tyler Derr, Hongjie Chen, Ju-Ying Wu, Xiang Chen, Zichao Wang, Subrata Mitra, Nedim Lipka, Nesreen K. Ahmed, Yu Wang","Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.",10.48550/arXiv.2411.00027,"['JournalArticle', 'Review']",Computer Science
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems,2024,"Lingjiao Chen, J. Q. Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou",,10.48550/arXiv.2403.02419,['JournalArticle'],Computer Science
What large language models know and what people think they know,2024,"M. Steyvers, Heliodoro Tejeda Lemus, Aakriti Kumar, Catarina G. Belm, Sheer Karny, Xinyue Hu, Lukas William Mayer, P. Smyth","
 As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. Whereas recent work has focused on LLMs internal confidence, less is understood about how effectively they convey uncertainty to users. Here we explore the calibration gap, which refers to the difference between human confidence in LLM-generated answers and the models actual confidence, and the discrimination gap, which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Moreover, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models internal confidence, both the calibration gap and the discrimination gap narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in artificial-intelligence-assisted decision-making environments.",10.1038/s42256-024-00976-7,['JournalArticle'],Computer Science; Linguistics; Psychology
A Survey of Large Language Models for Graphs,2024,"Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang","Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers.",10.1145/3637528.3671460,"['Book', 'JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Thousands of AI Authors on the Future of AI,2024,"Katja Grace, Harlan Stewart, J. F. Sandkhler, Stephen Thomas, Benjamin Weinstein-Raun, Jan Brauner","In October 2023, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace, nature and impacts of AI progress. Significant steps were taken to minimize and evaluate bias. In evaluations of participation bias, we found that most groups responded at similar rates. The participants estimated that several milestones had at least a 50% chance of being feasible for AI by 2028, including constructing a payment processing site and fine-tuning an LLM. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027 and 50% by 204713 years earlier than in our 2022 survey (N = 738). The chance of all occupations becoming fully automatable, however, was not expected to reach 10% until 2037, and 50% until 2116 (compared to 2164 in the 2022 survey. 
Most respondents expressed substantial uncertainty about long-term impacts: While 68% in 2023 thought good outcomes from high-level machine intelligence AI were more likely than bad ones, 48% of these net optimists gave at least a 5% chance of extremely bad outcomes. Conversely, 59% of net pessimists gave 5% or more to extremely good outcomes. Depending on how we asked, between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that substantial or extreme concern is warranted about AI increasing misinformation, boosting authoritarian control, worsening inequality, and other scenarios. There was broad agreement that research aimed at minimizing risks from AI systems ought to be more prioritized.",10.1613/jair.1.19087,"['JournalArticle', 'Review']",Computer Science
ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification,2024,"Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, ChenXue Wang, Shichao Liu, Qing Wang","Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43% to 69.60% and from 54.32% to 62.37%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.",10.1145/3660810,['JournalArticle'],Computer Science
"""My Answer is C"": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",2024,"Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Rttger, Frauke Kreuter, Dirk Hovy, Barbara Plank","The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with""Sure""or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.",10.48550/arXiv.2402.14499,"['JournalArticle', 'Conference']",Computer Science
"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",2024,"Jiajia Wang, Jimmy X. Huang, Xinhui Tu, Junmei Wang, Angela J. Huang, Md Tahmid Rahman Laskar, Amran Bhuiyan","Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. Additionally, we highlight the advantages of employing encoder-based BERT models in contrast to recent large language models like ChatGPT, which are decoder-based and demand extensive computational resources. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area.",10.1145/3648471,"['JournalArticle', 'Review']",Computer Science
Inducing anxiety in large language models increases exploration and bias,2023,"Julian Coda-Forno, Kristin Witte, A. K. Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz",,10.48550/arXiv.2304.11111,['JournalArticle'],Computer Science; Psychology
Calibration in Deep Learning: A Survey of the State-of-the-Art,2023,Cheng Wang,"Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively under-explored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent advances in calibrating deep models. In this survey, we review the state-of-the-art calibration methods and their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classify into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also cover recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.",10.48550/arXiv.2308.01222,"['JournalArticle', 'Review']",Computer Science; Engineering
Knowledge of cultural moral norms in large language models,2023,"Aida Ramezani, Yang Xu","Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as homosexuality and divorce; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.",10.48550/arXiv.2306.01857,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
GUI Agents with Foundation Models: A Comprehensive Survey,2024,"Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, Ruiming Tang","Recent advances in foundation models, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), have facilitated the development of intelligent agents capable of performing complex tasks. By leveraging the ability of (M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents can autonomously execute user instructions, simulating human-like interactions such as clicking and typing. This survey consolidates recent research on (M)LLM-based GUI agents, highlighting key innovations in data resources, frameworks, and applications. We begin by reviewing representative datasets and benchmarks, followed by an overview of a generalized, unified framework that encapsulates the essential components of prior studies, supported by a detailed taxonomy. Additionally, we explore relevant commercial applications. Drawing insights from existing work, we identify key challenges and propose future research directions. We hope this survey will inspire further advancements in the field of (M)LLM-based GUI agents.",10.48550/arXiv.2411.04890,"['JournalArticle', 'Review']",Computer Science
"History, Development, and Principles of Large Language Models-An Introductory Survey",2024,"Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang, Wenbin Zhang","Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLM reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.",10.48550/arXiv.2402.06853,"['JournalArticle', 'Review']",Computer Science; Linguistics
AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society,2025,"J. Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, Yong Li","Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.",10.48550/arXiv.2502.08691,"['JournalArticle', 'Review']",Computer Science; Sociology
SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model,2023,"Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jin Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, Jiangtao Gong",,10.48550/arXiv.2309.13193,['JournalArticle'],Computer Science; Engineering
Evaluating Large Language Model Biases in Persona-Steered Generation,2024,"Andy Liu, Mona T. Diab, Daniel Fried","The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have. People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas. We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending. We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas. We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases. Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints.",10.48550/arXiv.2405.20253,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
"Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",2024,"Aidar Myrzakhan, S. Mahmoud Bsharat, Zhiqiang Shen","Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions. Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard.",10.48550/arXiv.2406.07545,['JournalArticle'],Computer Science
LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory,2024,"Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu","Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing a 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into three stages: indexing, retrieval, and reading. Built upon key experimental insights, we propose several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope. Extensive experiments show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. Our benchmark and code are publicly available at https://github.com/xiaowu0162/LongMemEval.",10.48550/arXiv.2410.10813,['JournalArticle'],Computer Science
Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning,2023,"Xiao Yu, Maximillian Chen, Zhou Yu","Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often requires abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.",10.48550/arXiv.2305.13660,"['JournalArticle', 'Conference']",Computer Science
You dont need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments,2023,"Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Dallas Card, David Jurgens","The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experiments on 17 different LLMs reveal that even simple perturbations significantly downgrade a models question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues.",10.48550/arXiv.2311.09718,"['JournalArticle', 'Conference']",Computer Science; Psychology
Large language model validity via enhanced conformal prediction methods,2024,"John J. Cherian, Isaac Gibbs, E. Cands","We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM's original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on biography and medical question-answering datasets.",10.48550/arXiv.2406.09714,['JournalArticle'],Computer Science; Mathematics
A Survey on Robotics with Foundation Models: toward Embodied AI,2024,"Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, Jian Tang","While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.",10.48550/arXiv.2402.02385,"['JournalArticle', 'Review']",Computer Science; Engineering
Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice,2024,"Ranim Khojah, Mazen Mohamad, Philipp Leitner, Francisco Gomes de Oliveira Neto","Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.",10.48550/arXiv.2404.14901,"['JournalArticle', 'Review']",Computer Science
Learning Dynamics of LLM Finetuning,2024,"Yi Ren, Danica J. Sutherland","Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique""squeezing effect""to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",10.48550/arXiv.2407.10490,['JournalArticle'],Computer Science
RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models,2023,"Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen","Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA--predicting root causes, solutions, evidence, and responsibilities--and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.",10.1145/3627673.3680016,"['JournalArticle', 'Book', 'Conference']",Computer Science; Engineering
Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction,2025,"Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xue-Ting Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, JianYuan Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuan Ma, Yuanwei Liang, Yun-Fei Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxing Miao, Chen Xu, Chenrun Wang, Da Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Hao Jia, Haoyang Zhang, Jiahao Gong, Jun-Nan Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiao Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kai-yang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Min Liang, Na Wang, Nie Hao, Qiling Wu, Qi-Liang Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shi-Yu Yang, Shu-Guang Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wen-Yu Deng, Wuxun Xie, Weipeng Ming, Wenqing He, Wencheng Sun, Xin-hao Han, Xin Huang, Xiao-Zhen Deng, Xiao-Jun Liu, Xin Wu, Xu Zhao, Yana Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaoyu Wang, Ya-jun Shi, Yilei Wang, Yizhuang Zhou, Yinmin Zhong, Yang Zhang, Yaoben Wei, Yu Luo, Yuanwei Lu, Yuhe Yin, Yu Luo, Yu-Qiang Ding, Yuting Yan, YaNan Dai, Yuxiang Yang, Zhenghui Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zhi-Ying Guan, Zi-Hao Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, H. Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu","Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at https://github.com/stepfun-ai/Step-Audio.",10.48550/arXiv.2502.11946,['JournalArticle'],Computer Science; Engineering
From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News,2024,"Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, Rui Yan","In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of LLMs in combating fake news.",10.24963/ijcai.2024/873,"['JournalArticle', 'Conference']",Computer Science; Political Science
Large Language Models as Superpositions of Cultural Perspectives,2023,"Grgur Kova, Masataka Sawayama, Rmy Portelas, Cdric Colas, Peter Ford Dominey, Pierre-Yves Oudeyer","Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrating their context-dependent nature). We then conduct quantitative experiments to study the controllability of different models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the effectiveness of various methods for inducing perspectives, and the smoothness of the models' drivability. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions. The project website is available at https://sites.google.com/view/llm-superpositions .",10.48550/arXiv.2307.07870,['JournalArticle'],Computer Science; Linguistics; Psychology
RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation,2024,"Chanwoo Park, Mingyang Liu, Dingwen Kong, Kaiqing Zhang, A. Ozdaglar","Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions.",10.48550/arXiv.2405.00254,['JournalArticle'],Computer Science
Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation,2024,"Xinyi Mou, Zhongyu Wei, Xuanjing Huang","Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework HiSim for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.",10.48550/arXiv.2402.16333,"['JournalArticle', 'Conference']",Computer Science; Sociology
ClarifyGPT: Empowering LLM-based Code Generation with Intention Clarification,2023,"Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, ChenXue Wang, Shichao Liu, Qing Wang","We introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. In particular, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we first conduct a human evaluation involving ten participants who use ClarifyGPT for code generation on two publicly available benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96% to 80.80% on MBPP-sanitized. Furthermore, to perform large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The automated evaluation results also demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across four benchmarks from 68.02% to 75.75% and from 58.55% to 67.22%, respectively. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.",10.48550/arXiv.2310.10996,['JournalArticle'],Computer Science
Sparks of Large Audio Models: A Survey and Outlook,2023,"S. Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto Cuay'ahuitl, Bjrn Schuller","This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.",10.48550/arXiv.2308.12792,"['JournalArticle', 'Review']",Computer Science; Engineering
Do LLM Agents Exhibit Social Behavior?,2023,"Yan Leng, Yuan Yuan","As LLMs increasingly take on roles in human-AI interactions and autonomous AI systems, understanding their social behavior becomes important for informed use and continuous improvement. However, their behaviors in social interactions with humans and other agents, as well as the mechanisms shaping their responses, remain underexplored. To address this gap, we introduce a novel probabilistic framework, State-Understanding-Value-Action (SUVA), to systematically analyze LLM responses in social contexts based on their textual outputs (i.e., utterances). Using canonical behavioral economics games and social preference concepts relatable to LLM users, SUVA assesses LLMs' social behavior through both their final decisions and the response generation processes leading to those decisions. Our analysis of eight LLMs -- including two GPT, four LLaMA, and two Mistral models -- suggests that most models do not generate decisions aligned solely with self-interest; instead, they often produce responses that reflect social welfare considerations and display patterns consistent with direct and indirect reciprocity. Additionally, higher-capacity models more frequently display group identity effects. The SUVA framework also provides explainable tools -- including tree-based visualizations and probabilistic dependency analysis -- to elucidate how factors in LLMs' utterance-based reasoning influence their decisions. We demonstrate that utterance-based reasoning reliably predicts LLMs' final actions; references to altruism, fairness, and cooperation in the reasoning increase the likelihood of prosocial actions, while mentions of self-interest and competition reduce them. Overall, our framework enables practitioners to assess LLMs for applications involving social interactions, and provides researchers with a structured method to interpret how LLM behavior arises from utterance-based reasoning.",10.48550/arXiv.2312.15198,['JournalArticle'],Computer Science; Economics
Generative Social Choice,2023,"Sara Fish, Paul Glz, David C. Parkes, Ariel D. Procaccia, Gili Rusak, Itai Shapira, Manuel Wthrich","The mathematical study of voting, social choice theory, has traditionally only been applicable to choices among a few predetermined alternatives, but not to open-ended decisions such as collectively selecting a textual statement. We introduce generative social choice, a design methodology for open-ended democratic processes that combines the rigor of social choice theory with the capability of large language models to generate text and extrapolate preferences. Our framework divides the design of AI-augmented democratic processes into two components: first, proving that the process satisfies representation guarantees when given access to oracle queries; second, empirically validating that these queries can be approximately implemented using a large language model. We apply this framework to the problem of summarizing free-form opinions into a proportionally representative slate of opinion statements; specifically, we develop a democratic process with representation guarantees and use this process to represent the opinions of participants in a survey about chatbot personalization. In a trial with 100 representative US residents, we find that 93 out of 100 participants feel ""mostly"" or ""perfectly"" represented by the slate of five statements we extracted. By providing rigorous guarantees through social choice, our work alleviates concerns about AI-driven democratic innovation and helps unlock its potential. The full version of this paper is available at https://arxiv.org/pdf/2309.01291.",10.48550/arXiv.2309.01291,"['Book', 'JournalArticle', 'Review']",Computer Science; Mathematics; Political Science
Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement,2023,"Gwenyth Portillo Wightman, Alexandra DeLucia, Mark Dredze","Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.",10.18653/v1/2023.trustnlp-1.28,nan,Computer Science
Scalable Best-of-N Selection for Large Language Models via Self-Certainty,2025,"Zhewei Kang, Xuandong Zhao, D. Song","Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty",10.48550/arXiv.2502.18581,['JournalArticle'],Computer Science
The Ethics of Interaction: Mitigating Security Threats in LLMs,2024,"Ashutosh Kumar, Sagarika Singh, Shiv Vignesh Murthy, Swathy Ragupathy","This paper comprehensively explores the ethical challenges arising from security threats to Large Language Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats--prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate-based content--going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizing and developing an evaluative tool tailored for LLMs, which would serve a dual purpose: guiding developers and designers in preemptive fortification of backend systems and scrutinizing the ethical dimensions of LLM chatbot responses during the testing phase. By comparing LLM responses with those expected from humans in a moral context, we aim to discern the degree to which AI behaviors align with the ethical values held by a broader society. Ultimately, this paper not only underscores the ethical troubles presented by LLMs; it also highlights a path toward cultivating trust in these systems.",10.48550/arXiv.2401.12273,['JournalArticle'],Computer Science; Linguistics
Large sequence models for sequential decision-making: a survey,2023,"Muning Wen, Runji Lin, Hanjing Wang, Yaodong Yang, Ying Wen, Luo Mai, J. Wang, Haifeng Zhang, Weinan Zhang","Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they utilize the Transformer. Moreover, this paper puts forth various potential avenues for future research intending to improve the effectiveness of large sequence models for sequential decision-making, encompassing theoretical foundations, network architectures, algorithms, and efficient training systems.",10.1007/s11704-023-2689-5,"['JournalArticle', 'Review']",Computer Science
From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents,2024,"Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xuanjing Huang, Zhongyu Wei","Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns. Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies. In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents. We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics. These simulations follow a progression, ranging from detailed individual modeling to large-scale societal phenomena. We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method. Afterward, we summarize commonly used datasets and benchmarks. Finally, we discuss the trends across these three types of simulation. A repository for the related sources is at {\url{https://github.com/FudanDISC/SocialAgent}}.",10.48550/arXiv.2412.03563,"['JournalArticle', 'Review']",Computer Science; Sociology
Aligning LLMs with Individual Preferences via Interaction,2024,"Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, Heng Ji","As large language models (LLMs) demonstrate increasingly advanced capabilities, aligning their behaviors with human values and preferences becomes crucial for their wide adoption. While previous research focuses on general alignment to principles such as helpfulness, harmlessness, and honesty, the need to account for individual and diverse preferences has been largely overlooked, potentially undermining customized human experiences. To address this gap, we train LLMs that can ''interact to align'', essentially cultivating the meta-skill of LLMs to implicitly infer the unspoken personalized preferences of the current user through multi-turn conversations, and then dynamically align their following behaviors and responses to these inferred preferences. Our approach involves establishing a diverse pool of 3,310 distinct user personas by initially creating seed examples, which are then expanded through iterative self-generation and filtering. Guided by distinct user personas, we leverage multi-LLM collaboration to develop a multi-turn preference dataset containing 3K+ multi-turn conversations in tree structures. Finally, we apply supervised fine-tuning and reinforcement learning to enhance LLMs using this dataset. For evaluation, we establish the ALOE (ALign With CustOmized PrEferences) benchmark, consisting of 100 carefully selected examples and well-designed metrics to measure the customized alignment performance during conversations. Experimental results demonstrate the effectiveness of our method in enabling dynamic, personalized alignment via interaction.",10.48550/arXiv.2410.03642,"['JournalArticle', 'Conference']",Computer Science
Hidden Persuaders: LLMs Political Leaning and Their Influence on Voters,2024,"Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, D. Song","Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users political views is required, as their use becomes more widespread.",10.18653/v1/2024.emnlp-main.244,"['JournalArticle', 'Conference']",Computer Science; Political Science
"Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps",2022,"Alexandre Pasquiou, Yair Lakretz, John Hale, B. Thirion, Christophe Pallier","Neural Language Models (NLMs) have made tremendous advances during the last years, achieving impressive performance on various linguistic tasks. Capitalizing on this, studies in neuroscience have started to use NLMs to study neural activity in the human brain during language processing. However, many questions remain unanswered regarding which factors determine the ability of a neural language model to capture brain activity (aka its 'brain score'). Here, we make first steps in this direction and examine the impact of test loss, training corpus and model architecture (comparing GloVe, LSTM, GPT-2 and BERT), on the prediction of functional Magnetic Resonance Imaging timecourses of participants listening to an audiobook. We find that (1) untrained versions of each model already explain significant amount of signal in the brain by capturing similarity in brain responses across identical words, with the untrained LSTM outperforming the transformerbased models, being less impacted by the effect of context; (2) that training NLP models improves brain scores in the same brain regions irrespective of the model's architecture; (3) that Perplexity (test loss) is not a good predictor of brain score; (4) that training data have a strong influence on the outcome and, notably, that off-the-shelf models may lack statistical power to detect brain activations. Overall, we outline the impact of modeltraining choices, and suggest good practices for future studies aiming at explaining the human language system using neural language models.",10.48550/arXiv.2207.03380,"['JournalArticle', 'Conference']",Computer Science; Linguistics
RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation,2024,"Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas J. Guibas, He Wang, Yue Wang","This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. First, RAM extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D executable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation. For more details, please check our website at https://yxkryptonite.github.io/RAM/.",10.48550/arXiv.2407.04689,['JournalArticle'],Computer Science; Engineering
Robot Learning in the Era of Foundation Models: A Survey,2023,"Xuan Xiao, Jiahang Liu, Zhipeng Wang, Yanmin Zhou, Yong Qi, Qian Cheng, Bin He, Shuo Jiang","The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.",10.48550/arXiv.2311.14379,"['JournalArticle', 'Review']",Computer Science; Engineering
A Large-Scale Evaluation of Speech Foundation Models,2024,"Shu-Wen Yang, Heng-Jui Chang, Zili Huang, Andy T. Liu, Cheng-I Lai, Haibin Wu, Jiatong Shi, Xuankai Chang, Hsiang-Sheng Tsai, Wen-Chin Huang, Tzu-hsun Feng, Po-Han Chi, Yist Y. Lin, Yung-Sung Chuang, Tzu-hsien Huang, Wei-Cheng Tseng, Kushal Lakhotia, Shang-Wen Li, Abdelrahman Mohamed, Shinji Watanabe, Hung-yi Lee","The foundation model paradigm leverages a shared foundation model to achieve state-of-the-art (SOTA) performance for various tasks, requiring minimal downstream-specific data collection and modeling. This approach has proven crucial in the field of Natural Language Processing (NLP). However, the speech processing community lacks a similar setup to explore the paradigm systematically. To bridge this gap, we establish the Speech processing Universal PERformance Benchmark (SUPERB). SUPERB represents an ecosystem designed to evaluate foundation models across a wide range of speech processing tasks, facilitating the sharing of results on an online leaderboard and fostering collaboration through a community-driven benchmark database that aids in new development cycles. We present a unified learning framework for solving the speech processing tasks in SUPERB with the frozen foundation model followed by task-specialized lightweight prediction heads. Combining our results with community submissions, we verify that the framework is simple yet effective, as the best-performing foundation model shows competitive generalizability across most SUPERB tasks. Finally, we conduct a series of analyses to offer an in-depth understanding of SUPERB and speech foundation models, including information flows across tasks inside the models and the statistical significance and robustness of the benchmark.",10.1109/TASLP.2024.3389631,['JournalArticle'],Computer Science; Engineering; Linguistics
Are Large Language Models Consistent over Value-laden Questions?,2024,"Jared Moore, Tanvi Deshpande, Diyi Yang","Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across (1) paraphrases of one question, (2) related questions under one topic, (3) multiple-choice and open-ended use-cases of one question, and (4) multilingual translations of a question to English, Chinese, German, and Japanese. We apply these measures to small and large, open LLMs including llama-3, as well as gpt-4o, using 8,000 questions spanning more than 300 topics. Unlike prior work, we find that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S.,""Thanksgiving"") than on controversial ones (""euthanasia""). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics (""euthanasia"") than others (""women's rights"") like our human subjects (n=165).",10.48550/arXiv.2407.02996,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Embedding Democratic Values into Social Media AIs via Societal Objective Functions,2023,"Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeffrey T. Hancock, Michael S. Bernstein","Mounting evidence indicates that the artificial intelligence (AI) systems that rank our social media feeds bear nontrivial responsibility for amplifying partisan animosity: negative thoughts, feelings, and behaviors toward political out-groups. Can we design these AIs to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models-however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.",10.1145/3641002,"['JournalArticle', 'Review']",Computer Science; Political Science
A Survey for Foundation Models in Autonomous Driving,2024,"Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, Yiqing Shen","The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.",10.48550/arXiv.2402.01105,"['JournalArticle', 'Review']",Computer Science; Engineering
PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection,2023,"Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu","Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.",10.48550/arXiv.2310.20256,"['JournalArticle', 'Conference']",Computer Science; Psychology
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey,2025,"Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu, Julian J. McAuley, Wei Ai, Furong Huang","Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has signicantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs strong reasoning capacities can in turn contribute to the eld of causal inference by aiding causal relationship discovery and causal eect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable articial intelligence systems.",10.48550/arXiv.2403.09606,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey,2024,"Haixin Wang, Yadi Cao, Zijie Huang, Yuxuan Liu, Peiyan Hu, Xiao Luo, Zezheng Song, Wanjia Zhao, Jilin Liu, Jinan Sun, Shikun Zhang, Long Wei, Yue Wang, Tailin Wu, Zhi-Ming Ma, Yizhou Sun","This paper explores the recent advancements in enhancing Computational Fluid Dynamics (CFD) tasks through Machine Learning (ML) techniques. We begin by introducing fundamental concepts, traditional methods, and benchmark datasets, then examine the various roles ML plays in improving CFD. The literature systematically reviews papers in recent five years and introduces a novel classification for forward modeling: Data-driven Surrogates, Physics-Informed Surrogates, and ML-assisted Numerical Solutions. Furthermore, we also review the latest ML methods in inverse design and control, offering a novel classification and providing an in-depth discussion. Then we highlight real-world applications of ML for CFD in critical scientific and engineering disciplines, including aerodynamics, combustion, atmosphere&ocean science, biology fluid, plasma, symbolic regression, and reduced order modeling. Besides, we identify key challenges and advocate for future research directions to address these challenges, such as multi-scale representation, physical knowledge encoding, scientific foundation model and automatic scientific discovery. This review serves as a guide for the rapidly expanding ML for CFD community, aiming to inspire insights for future advancements. We draw the conclusion that ML is poised to significantly transform CFD research by enhancing simulation accuracy, reducing computational time, and enabling more complex analyses of fluid dynamics. The paper resources can be viewed at https://github.com/WillDreamer/Awesome-AI4CFD.",10.48550/arXiv.2408.12171,"['JournalArticle', 'Review']",Computer Science; Engineering
MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention,2023,"Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu, Yuhan Wang, Zhi Zheng, Li Chen, Qiaolei Jiang, Xuhai Xu, Yuanchun Shi","Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users physical contexts and mental states. We first conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users in-the-moment app usage behaviors, physical contexts, mental states, goals & habits as input, and generates personalized and dynamic persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with its simplified version (remove mental states) and baseline techniques (fixed reminder). The results show that MindShift improves intervention acceptance rates by 4.7-22.5% and reduces smartphone usage duration by 7.4-9.8%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy scale scores. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.",10.1145/3613904.3642790,"['Book', 'JournalArticle', 'Conference']",Computer Science; Psychology
Large Language Models Reflect the Ideology of their Creators,2024,"Maarten Buyl, Alexander Rogiers, Sander Noels, Iris Dominguez-Catena, Edith Heiter, Raphal Romero, Iman Johary, A. Mara, Jefrey Lijffijt, T. D. Bie","Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use. In this paper, we prompt a diverse panel of popular LLMs to describe a large number of prominent personalities with political relevance, in all six official languages of the United Nations. By identifying and analyzing moral assessments reflected in their responses, we find normative differences between LLMs from different geopolitical regions, as well as between the responses of the same LLM when prompted in different languages. Among only models in the United States, we find that popularly hypothesized disparities in political views are reflected in significant normative differences related to progressive values. Among Chinese models, we characterize a division between internationally- and domestically-focused models. Our results show that the ideological stance of an LLM appears to reflect the worldview of its creators. This poses the risk of political instrumentalization and raises concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically 'unbiased'.",10.48550/arXiv.2410.18417,['JournalArticle'],Computer Science; Linguistics
A Survey of Deep Learning and Foundation Models for Time Series Forecasting,2024,"John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Arpinar, Ninghao Liu","Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.",10.48550/arXiv.2401.13912,"['JournalArticle', 'Review']",Computer Science; Mathematics
How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions,2024,"Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah","Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs'cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.",10.48550/arXiv.2406.14805,['JournalArticle'],Computer Science; Sociology
Enhancing Autonomous System Security and Resilience With Generative AI: A Comprehensive Survey,2024,"Martin Andreoni, W. Lunardi, George Lawton, S. Thakkar","This survey explores the transformative role of Generative Artificial Intelligence (GenAI) in enhancing the trustworthiness, reliability, and security of autonomous systems such as Unmanned Aerial Vehicles (UAVs), self-driving cars, and robotic arms. As edge robots become increasingly integrated into daily life and critical infrastructure, the complexity and connectivity of these systems introduce formidable challenges in ensuring security, resilience, and safety. GenAI advances from mere data interpretation to autonomously generating new data, proving critical in complex, context-aware environments like edge robotics. Our survey delves into the impact of GenAI technologiesincluding Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformer-based models, and Large Language Models (LLMs)on cybersecurity, decision-making, and the development of resilient architectures. We categorize existing research to highlight how these technologies address operational challenges and innovate predictive maintenance, anomaly detection, and adaptive threat response. Our comprehensive analysis distinguishes this work from existing reviews by mapping out the applications, challenges, and technological advancements of GenAI and their impact on creating secure frameworks for autonomous systems. We discuss significant challenges and future directions for integrating these technologies within security frameworks to address the evolving landscape of cyber-physical threats, underscoring the potential of GenAI to make autonomous systems more adaptive, secure, and efficient.",10.1109/ACCESS.2024.3439363,"['JournalArticle', 'Review']",Computer Science; Engineering
AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews,2024,"Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, A. Shporer, Madeleine Udell, Dov Teeni, Iddo Drori","Automatic reviewing helps handle a large volume of papers, provides early feedback and quality control, reduces bias, and allows the analysis of trends. We evaluate the alignment of automatic paper reviews with human reviews using an arena of human preferences by pairwise comparisons. Gathering human preference may be time-consuming; therefore, we also use an LLM to automatically evaluate reviews to increase sample efficiency while reducing bias. In addition to evaluating human and LLM preferences among LLM reviews, we fine-tune an LLM to predict human preferences, predicting which reviews humans will prefer in a head-to-head battle between LLMs. We artificially introduce errors into papers and analyze the LLM's responses to identify limitations, use adaptive review questions, meta prompting, role-playing, integrate visual and textual analysis, use venue-specific reviewing materials, and predict human preferences, improving upon the limitations of the traditional review processes. We make the reviews of publicly available arXiv and open-access Nature journal papers available online, along with a free service which helps authors review and revise their research papers and improve their quality. This work develops proof-of-concept LLM reviewing systems that quickly deliver consistent, high-quality reviews and evaluate their quality. We mitigate the risks of misuse, inflated review scores, overconfident ratings, and skewed score distributions by augmenting the LLM with multiple documents, including the review form, reviewer guide, code of ethics and conduct, area chair guidelines, and previous year statistics, by finding which errors and shortcomings of the paper may be detected by automated reviews, and evaluating pairwise reviewer preferences. This work identifies and addresses the limitations of using LLMs as reviewers and evaluators and enhances the quality of the reviewing process.",10.48550/arXiv.2408.10365,"['JournalArticle', 'Review']",Computer Science
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs,2024,"Y. Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, A. Avestimehr","Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found https://github.com/Ybakman/LLM_Uncertainity.",10.48550/arXiv.2402.11756,"['JournalArticle', 'Conference']",Computer Science
Evaluation of African American Language Bias in Natural Language Generation,2023,"Nicholas Deas, Jessica A. Grieser, Shana Kleiner, D. Patton, Elsbeth Turcan, K. McKeown","We evaluate how well LLMs understand African American Language (AAL) in comparison to their performance on White Mainstream English (WME), the encouraged""standard""form of English taught in American classrooms. We measure LLM performance using automatic metrics and human judgments for two tasks: a counterpart generation task, where a model generates AAL (or WME) given WME (or AAL), and a masked span prediction (MSP) task, where models predict a phrase that was removed from their input. Our contributions include: (1) evaluation of six pre-trained, large language models on the two language generation tasks; (2) a novel dataset of AAL text from multiple contexts (social media, hip-hop lyrics, focus groups, and linguistic interviews) with human-annotated counterparts in WME; and (3) documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features.",10.48550/arXiv.2305.14291,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Arabic ChatGPT Tweets Classification Using RoBERTa and BERT Ensemble Model,2023,"Muhammad Mujahid, Khadija Kanwal, F. Rustam, Wajdi Aljedaani, Imran Ashraf","ChatGPT OpenAI, a large-language chatbot model, has gained a lot of attention due to its popularity and impressive performance in many natural language processing tasks. ChatGPT produces superior answers to a wide range of real-world human questions and generates human-like text. The new OpenAI ChatGPT technology may have some strengths and weaknesses at this early stage. Users have reported early opinions about the ChatGPT features, and their feedback is essential to recognize and fix its shortcomings and issues. This study uses the ChatGPT tweets Arabic dataset to automatically find user opinions and sentiments about ChatGPT technology. The dataset is preprocessed and labeled using the TextBlob Arabic Python library into positive, negative, and neutral tweets. Despite extensive works for the English language, languages like Arabic are less studied regarding tweet analysis. Existing literature about Arabic tweet sentiment analysis has mainly focused on machine learning and deep learning models. We collected a total of 27,780 unstructured tweets from Twitter using the Tweepy SNscrape Python library using various hash-tags such as # Chat-GPT, #OpenAI, #Chatbot, Chat-GPT3, and so on. To enhance the models performance and reduce computational complexity, unstructured tweets are converted into structured and normalized forms. Tweets contain missing values, URL and HTML tags, stop words, punctuation, diacritics, elongations, and numeric values that have no impact on the model performance; hence, these increase the computational cost. So, these steps are removed with the help of Python preprocessing libraries to enhance text quality and consistency. This study adopts Transformer-based models such as RoBERTa, XLNet, and DistilBERT that automatically classify the tweets. Additionally, a hybrid transformer-based model is proposed to obtain better results. The proposed hybrid model is developed by combining the hidden outputs of the RoBERTA and BERT models using a concatenation layer, then adding dense layers with Relu activation employed as a hidden layer to create non-linearity and a softmax activation function for multiclass classification. They differ from existing state-of-the-art models due to the enhanced capabilities of both models in text classification. Hybrid models combine the different models to make accurate predictions and reduce bias and enhanced the overall results, while state-of-the-art models are incapable of making accurate predictions. Experiments show that the proposed hybrid model achieves 96.02% accuracy, 100% precision on negative tweets, and 99% recall for neutral tweets. The performance of the proposed model is far better than existing state-of-the-art models.",10.1145/3605889,['JournalArticle'],Computer Science
Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators,2024,"Mat'eo Mahaut, Laura Aina, Paula Czarnowska, Momchil Hardalov, Thomas Mller, Llu'is Marquez","Large Language Models (LLMs) tend to be unreliable in the factuality of their answers. To address this problem, NLP researchers have proposed a range of techniques to estimate LLM's confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one another. To fill this gap, we present a survey and empirical comparison of estimators of factual confidence. We define an experimental framework allowing for fair comparison, covering both fact-verification and question answering. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates, albeit at the expense of requiring access to weights and training data. We also conduct a deeper assessment of factual confidence by measuring the consistency of model behavior under meaning-preserving variations in the input. We find that the confidence of LLMs is often unstable across semantically equivalent inputs, suggesting that there is much room for improvement of the stability of models' parametric knowledge. Our code is available at (https://github.com/amazon-science/factual-confidence-of-llms).",10.18653/v1/2024.acl-long.250,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,2024,"Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He","Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.",10.48550/arXiv.2402.06044,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Quantile Regression for Distributional Reward Models in RLHF,2024,Nicolai Dorka,"Reinforcement learning from human feedback (RLHF) has become a key method for aligning large language models (LLMs) with human preferences through the use of reward models. However, traditional reward models typically generate point estimates, which oversimplify the diversity and complexity of human values and preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel approach to reward modeling that learns a distribution over rewards instead of a single scalar value. Our method uses quantile regression to estimate a full, potentially multimodal distribution over preferences, providing a more powerful and nuanced representation of preferences. This distributional approach can better capture the diversity of human values, addresses label noise, and accommodates conflicting preferences by modeling them as distinct modes in the distribution. Our experimental results show that QRM outperforms comparable traditional point-estimate models on RewardBench. Furthermore, we demonstrate that the additional information provided by the distributional estimates can be utilized in downstream applications, such as risk-aware reinforcement learning, resulting in LLM policies that generate fewer extremely negative responses. Our code and model are released at https://github.com/Nicolinho/QRM.",10.48550/arXiv.2409.10164,['JournalArticle'],Computer Science
PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences,2024,"Daiwei Chen, Yi Chen, Aniket Rege, Ramya Korlakai Vinayak","Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences. Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (""Do you prefer output A or B?"") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences. These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences. In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up. We propose using the ideal point model as a lens to view alignment using preference comparisons. Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users. Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly. We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas. Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches.",10.48550/arXiv.2406.08469,['JournalArticle'],Computer Science
Visual Agents as Fast and Slow Thinkers,2024,"Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Ying Nian Wu, Yongfeng Zhang, Dongfang Liu","Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1/2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaST's superior performance. Extensive testing validates the efficacy and robustness of FaST's core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps://github.com/GuangyanS/Sys2-LLaVA.",10.48550/arXiv.2408.08862,['JournalArticle'],Computer Science
Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis,2023,"Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Juho Kim, Alice Oh","***Warning**: this paper contains content that may be offensive or upsetting.*Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce **CREHate**, a **CR**oss-cultural **E**nglish **Hate** speech dataset.To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation.We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey.Annotations are collected from the four countries plus the United States to establish representative labels for each country.Our analysis highlights statistically significant disparities across countries in hate speech annotations.Only 56.2% of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26%.Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics.Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.Our dataset and codes are available at: https://github.com/nlee0212/CREHate",10.18653/v1/2024.naacl-long.236,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics; Sociology
Comprehensive Assessment of Jailbreak Attacks Against LLMs,2024,"Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang",,10.48550/arXiv.2402.05668,['JournalArticle'],Computer Science
Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk,2024,"Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, Yi Zhang","Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via""self-talk""of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.",10.48550/arXiv.2401.05033,"['JournalArticle', 'Conference']",Computer Science
How Susceptible are LLMs to Influence in Prompts?,2024,"Sotiris Anagnostidis, Jannis Bulian","Large Language Models (LLMs) are highly sensitive to prompts, including additional context provided therein. As LLMs grow in capability, understanding their prompt-sensitivity becomes increasingly crucial for ensuring reliable and robust performance, particularly since evaluating these models becomes more challenging. In this work, we investigate how current models (Llama, Mixtral, Falcon) respond when presented with additional input from another model, mimicking a scenario where a more capable model -- or a system with access to more external information -- provides supplementary information to the target model. Across a diverse spectrum of question-answering tasks, we study how an LLM's response to multiple-choice questions changes when the prompt includes a prediction and explanation from another model. Specifically, we explore the influence of the presence of an explanation, the stated authoritativeness of the source, and the stated confidence of the supplementary input. Our findings reveal that models are strongly influenced, and when explanations are provided they are swayed irrespective of the quality of the explanation. The models are more likely to be swayed if the input is presented as being authoritative or confident, but the effect is small in size. This study underscores the significant prompt-sensitivity of LLMs and highlights the potential risks of incorporating outputs from external sources without thorough scrutiny and further validation. As LLMs continue to advance, understanding and mitigating such sensitivities will be crucial for their reliable and trustworthy deployment.",10.48550/arXiv.2408.11865,['JournalArticle'],Computer Science
GUI Agents: A Survey,2024,"Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Md. Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, Seunghyun Yoon, Lina Yao, B. Kveton, T. Nguyen, Trung Bui, Tianyi Zhou, Ryan A. Rossi, Franck Dernoncourt","Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.",10.48550/arXiv.2412.13501,"['JournalArticle', 'Conference', 'Review']",Computer Science
Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning,2023,"Yuchen Yang, Houqiang Li, Yanfeng Wang, Yu Wang","In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of""hallucination,""which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that""uncertainty is lower when the model's response is correct compared to when it is incorrect.""However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",10.48550/arXiv.2310.04782,['JournalArticle'],Computer Science
Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs,2023,"Xiaoyang Song, Akshat Gupta, Kiyan Mohebbizadeh, Shujie Hu, Anant Singh","Have Large Language Models (LLMs) developed a personality? The short answer is a resounding""We Don't Know!"". In this paper, we show that we do not yet have the right tools to measure personality in language models. Personality is an important characteristic that influences behavior. As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality. Previous works have evaluated machine personality through self-assessment personality tests, which are a set of multiple-choice questions created to evaluate personality in humans. A fundamental assumption here is that human personality tests can accurately measure personality in machines. In this paper, we investigate the emergence of personality in five LLMs of different sizes ranging from 1.5B to 30B. We propose the Option-Order Symmetry property as a necessary condition for the reliability of these self-assessment tests. Under this condition, the answer to self-assessment questions is invariant to the order in which the options are presented. We find that many LLMs personality test responses do not preserve option-order symmetry. We take a deeper look at LLMs test responses where option-order symmetry is preserved to find that in these cases, LLMs do not take into account the situational statement being tested and produce the exact same answer irrespective of the situation being tested. We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable. These observations indicate that self-assessment tests are not the correct tools to measure personality in LLMs. Through this paper, we hope to draw attention to the shortcomings of current literature in measuring personality in LLMs and call for developing tools for machine personality measurement.",10.48550/arXiv.2305.14693,['JournalArticle'],Computer Science; Psychology
Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory,2023,"Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang","Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.",10.48550/arXiv.2311.08719,['JournalArticle'],Computer Science
Exploring the Capabilities of a Language Model-Only Approach for Depression Detection in Text Data,2023,"Misha Sadeghi, Bernhard Egger, Reza Agahi, R. Richer, Klara Capito, L. H. Rupp, L. Schindler-Gmelch, M. Berking, Bjoern M. Eskofier","Depression is a prevalent and debilitating mental health condition that requires accurate and efficient detection for timely and effective treatment. In this study, we utilized the E-DAIC (Extended Distress Analysis Interview Corpus-Wizard-of-Oz) dataset, an extended version of the DAIC-WOZ dataset, which consists of semi-clinical interviews conducted by an animated virtual interviewer called Ellie, controlled by a human interviewer in another room. With 275 participants, the E-DAIC dataset represents a valuable resource for investigating depression detection methods. Our aim is to predict PHQ-8 scores through text analysis. Leveraging state-of-the-art speech processing, LLM-based text summarization, and a specialized depression detection module, we demonstrate the transformative potential of language data analysis in enhancing depression screening. By overcoming the limitations of manual feature extraction methods, our automated techniques provide a more efficient and effective means of evaluating depression. In our evaluation, we achieve robust accuracy on the development set of the E-DAIC dataset, with a Mean Absolute Error (MAE) of 3.65 in estimating PHQ-8 scores from recorded interviews. This remarkable performance highlights the efficacy of our approach in automatically predicting depression severity. Our research contributes to the growing evidence supporting the use of LLMs in mental health assessment, showcasing the role of innovative technologies in advancing patient care for depression.",10.1109/BHI58575.2023.10313367,"['JournalArticle', 'Conference']",Computer Science; Psychology
The Oscars of AI Theater: A Survey on Role-Playing with Language Models,2024,"Nuo Chen, Yang Deng, Jia Li","This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement. Related resources and papers are available at https://github.com/nuochenpku/Awesome-Role-Play-Papers.",10.48550/arXiv.2407.11484,"['JournalArticle', 'Review']",Art; Computer Science
Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information,2024,"Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, Jang Hyun Kim","Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose""random silicon sampling,""a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.",10.48550/arXiv.2402.18144,"['JournalArticle', 'Review']",Computer Science; Sociology
Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics,2024,"Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu","Recent advancements in Large Language Models (LLMs) have led to their adaptation in various domains as conversational agents. We wonder: can personality tests be applied to these agents to analyze their behavior, similar to humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice questions designed to assess the personality of LLMs. TRAIT is built on two psychometrically validated small human questionnaires, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a variety of real-world scenarios. TRAIT also outperforms existing personality tests for LLMs in terms of reliability and validity, achieving the highest scores across four key metrics: Content Validity, Internal Validity, Refusal Rate, and Reliability. Using TRAIT, we reveal two notable insights into personalities of LLMs: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (e.g., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.",10.48550/arXiv.2406.14703,"['JournalArticle', 'Conference']",Computer Science; Psychology
WPO: Enhancing RLHF with Weighted Preference Optimization,2024,"Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu","Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against GPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/WPO.",10.48550/arXiv.2406.11827,"['JournalArticle', 'Conference']",Computer Science
A Survey of Uncertainty Estimation in LLMs: Theory Meets Practice,2024,"Hsiu-Yuan Huang, Yutong Yang, Zhaoxi Zhang, Sanwoo Lee, Yunfang Wu","As large language models (LLMs) continue to evolve, understanding and quantifying the uncertainty in their predictions is critical for enhancing application credibility. However, the existing literature relevant to LLM uncertainty estimation often relies on heuristic approaches, lacking systematic classification of the methods. In this survey, we clarify the definitions of uncertainty and confidence, highlighting their distinctions and implications for model predictions. On this basis, we integrate theoretical perspectives, including Bayesian inference, information theory, and ensemble strategies, to categorize various classes of uncertainty estimation methods derived from heuristic approaches. Additionally, we address challenges that arise when applying these methods to LLMs. We also explore techniques for incorporating uncertainty into diverse applications, including out-of-distribution detection, data annotation, and question clarification. Our review provides insights into uncertainty estimation from both definitional and theoretical angles, contributing to a comprehensive understanding of this critical aspect in LLMs. We aim to inspire the development of more reliable and effective uncertainty estimation approaches for LLMs in real-world scenarios.",10.48550/arXiv.2410.15326,"['JournalArticle', 'Review']",Computer Science; Linguistics
Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model,2023,S. Paoli,"Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions which can be applied to several fields and areas of work. This paper presents results and reflection of an experiment done to use the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic Analysis. Previous research on this subject has largely worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations made by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research. The paper presents the motivations for attempting this emulation, it reflects on how the six steps to a Thematic Analysis proposed by Braun and Clarke can at least partially be reproduced with the LLM and it also reflects on what are the outputs produced by the model. The paper used two existing datasets of open access semi-structured interviews, previously analysed with Thematic Analysis by other researchers. It used the previously produced analysis (and the related themes) to compare with the results produced by the LLM. The results show that the model can infer at least partially some of the main Themes. The objective of the paper is not to replace human analysts in qualitative analysis but to learn if some elements of LLM data manipulation can to an extent be of support for qualitative research.",10.48550/arXiv.2305.13014,['JournalArticle'],Computer Science; Linguistics
Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws,2024,"Yiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, J. Kolter","The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.",10.48550/arXiv.2410.11820,['JournalArticle'],Computer Science; Mathematics
Mitigating LLM Hallucinations via Conformal Abstention,2024,"Yasin Abbasi-Yadkori, Ilja Kuzborskij, David Stutz, Andrs Gyrgy, Adam Fisch, Arnaud Doucet, Iuliya Beloshapka, Wei-Hung Weng, Yao-Yuan Yang, Csaba Szepesv'ari, Ali Taylan Cemgil, Nenad Tomasev","We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying""I don't know"") in a general domain, instead of resorting to possibly""hallucinating""a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.",10.48550/arXiv.2405.01563,['JournalArticle'],Computer Science
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys,2023,"Junsol Kim, Byungkyu Lee",,10.48550/arXiv.2305.09620,"['JournalArticle', 'Review']",Computer Science; Sociology
Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning,2023,"Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu","Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is https://lamo2023.github.io",10.48550/arXiv.2310.20587,['JournalArticle'],Computer Science
Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey,2025,"Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei","Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.",10.48550/arXiv.2503.15850,"['JournalArticle', 'Conference', 'Review']",Computer Science; Law
Resolving Knowledge Conflicts in Large Language Models,2023,"Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov","Large language models (LLMs) often encounter knowledge conflicts, scenarios where discrepancy arises between the internal parametric knowledge of LLMs and non-parametric information provided in the prompt context. In this work we ask what are the desiderata for LLMs when a knowledge conflict arises and whether existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge conflicts, 2) pinpoint conflicting information segments, and 3) provide distinct answers or viewpoints in conflicting scenarios. To this end, we introduce an evaluation framework for simulating contextual knowledge conflicts and quantitatively evaluating to what extent LLMs achieve these goals. It includes diverse and complex situations of knowledge conflict, knowledge from diverse entities and domains, two synthetic conflict creation methods, and settings with progressively increasing difficulty to reflect realistic knowledge conflicts. Extensive experiments with the framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information. To address these challenges, we propose new instruction-based approaches that augment LLMs to better achieve the three goals. Further analysis shows that abilities to tackle knowledge conflicts are greatly impacted by factors such as knowledge domain, while generating robust responses to knowledge conflict scenarios remains an open research question.",10.48550/arXiv.2310.00935,['JournalArticle'],Computer Science
Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices,2023,"Mengwei Xu, Yaozong Wu, Dongqi Cai, Xiang Li, Shangguang Wang",,10.48550/arXiv.2308.13894,['JournalArticle'],Computer Science
From Text to Self: Users Perception of AIMC Tools on Interpersonal Communication and Self,2023,"Yue Fu, Sami Foell, Xuhai Xu, Alexis Hiniker","In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users perceptions of these tools ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, finding precise language to express their thoughts, and navigating linguistic and cultural barriers. However, our findings also show current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. We identify four key communication spaces delineated by communication stakes (high or low) and relationship dynamics (formal or informal) that differentially predict users attitudes toward AIMC tools. Specifically, participants report that these tools are more suitable for communicating in formal relationships than informal ones and more beneficial in high-stakes than low-stakes communication.",10.1145/3613904.3641955,"['Book', 'JournalArticle', 'Conference']",Computer Science; Psychology
Virtual Personas for Language Models via an Anthology of Backstories,2024,"Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, David M. Chan","Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce Anthology, a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as backstories. We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Centers American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics.",10.48550/arXiv.2407.06576,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics; Psychology
Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks,2024,"Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers","Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 64 topics loading on nine non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.",10.48550/arXiv.2406.17232,"['JournalArticle', 'Conference', 'Review']",Computer Science; Psychology
Context-DPO: Aligning Language Models for Context-Faithfulness,2024,"Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Shenghua Liu","Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment remains underexplored. To address this, we propose $\textbf{Context-DPO}$, the first alignment method specifically designed to enhance LLMs' context-faithfulness. We introduce $\textbf{ConFiQA}$, a benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with knowledge conflicts to evaluate context-faithfulness. By leveraging faithful and stubborn responses to questions with provided context from ConFiQA, our Context-DPO aligns LLMs through direct preference optimization. Extensive experiments demonstrate that our Context-DPO significantly improves context-faithfulness, achieving 35% to 280% improvements on popular open-source models. Further analysis demonstrates that Context-DPO preserves LLMs' generative capabilities while providing interpretable insights into context utilization. Our code and data are released at https://github.com/byronBBL/Context-DPO",10.48550/arXiv.2412.15280,"['JournalArticle', 'Conference']",Computer Science
LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,2024,"Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang Nie","Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection.",10.1609/aaai.v38i16.29782,"['JournalArticle', 'Conference']",Computer Science
Chain-of-thought prompting for responding to in-depth dialogue questions with LLM,2023,"Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Rui-Lan Xu, Kam-Fai Wong",,10.48550/arXiv.2305.11792,['JournalArticle'],Computer Science
CodeTailor: LLM-Powered Personalized Parsons Puzzles for Engaging Support While Learning Programming,2024,"Xinying Hou, Zihan Wu, Xu Wang, B. Ericson","Learning to program can be challenging, and providing high-quality and timely support at scale is hard. Generative AI and its products, like ChatGPT, can create a solution for most intro-level programming problems. However, students might use these tools to just generate code for them, resulting in reduced engagement and limited learning. In this paper, we present CodeTailor, a system that leverages a large language model (LLM) to provide personalized help to students while still encouraging cognitive engagement. CodeTailor provides a personalized Parsons puzzle to support struggling students. In a Parsons puzzle, students place mixed-up code blocks in the correct order to solve a problem. A technical evaluation with previous incorrect student code snippets demonstrated that CodeTailor could deliver high-quality (correct, personalized, and concise) Parsons puzzles based on their incorrect code. We conducted a within-subjects study with 18 novice programmers. Participants perceived CodeTailor as more engaging than just receiving an LLM-generated solution (the baseline condition). In addition, participants applied more supported elements from the scaffolded practice to the posttest when using CodeTailor than baseline. Overall, most participants preferred using CodeTailor versus just receiving the LLM-generated code for learning. Qualitative observations and interviews also provided evidence for the benefits of CodeTailor, including thinking more about solution construction, fostering continuity in learning, promoting reflection, and boosting confidence. We suggest future design ideas to facilitate active learning opportunities with generative AI techniques.",10.1145/3657604.3662032,"['JournalArticle', 'Book']",Computer Science
Looking Inward: Language Models Can Learn About Themselves by Introspection,2024,"F. J. Binder, James Chua, Tomasz Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans","Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data. We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example,""Given the input P, would your output favor the short- or long-term option?""If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger). In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.",10.48550/arXiv.2410.13787,['JournalArticle'],Computer Science
"Agentic Large Language Models, a survey",2025,"A. Plaat, M. V. Duijn, N. V. Stein, Mike Preuss, P. V. D. Putten, K. Batenburg","There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.",10.48550/arXiv.2503.23037,"['JournalArticle', 'Review']",Computer Science
Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation,2024,"Gauthier Guinet, Behrooz Omidvar-Tehrani, Anoop Deoras, Laurent Callot","We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",10.48550/arXiv.2405.13622,"['JournalArticle', 'Conference']",Computer Science
Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric Analysis,2024,"Nikolay B Petrov, Gregory Serapio-Garc'ia, Jason Rentfrow","The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.",10.48550/arXiv.2405.07248,"['JournalArticle', 'Review']",Computer Science; Psychology
PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents,2024,"Qisen Yang, Z. Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang","Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.",10.18653/v1/2024.acl-long.779,"['JournalArticle', 'Conference']",Computer Science; Psychology
Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models,2024,"Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun","Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.",10.48550/arXiv.2406.17675,['JournalArticle'],Computer Science; Psychology
TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models,2024,"Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, Zhiyong Cui","Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning. Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs. In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively. These are subsequently integrated to form inputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands. Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data.",10.48550/arXiv.2403.02221,['JournalArticle'],Computer Science; Engineering
Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey,2024,"Liang Chen, Z. Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, Baobao Chang","Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \&evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",10.48550/arXiv.2412.18619,"['JournalArticle', 'Review']",Computer Science; Engineering; Linguistics
LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices,2024,"Jingping Nie, Hanya Shao, Yuang Fan, Qijia Shao, Haoxuan You, Matthias Preindl, Xiaofan Jiang","Despite the global mental health crisis, access to screenings, professionals, and treatments remains high. In collaboration with licensed psychotherapists, we propose a Conversational AI Therapist with psychotherapeutic Interventions (CaiTI), a platform that leverages large language models (LLM)s and smart devices to enable better mental health self-care. CaiTI can screen the day-to-day functioning using natural and psychotherapeutic conversations. CaiTI leverages reinforcement learning to provide personalized conversation flow. CaiTI can accurately understand and interpret user responses. When the user needs further attention during the conversation, CaiTI can provide conversational psychotherapeutic interventions, including cognitive behavioral therapy (CBT) and motivational interviewing (MI). Leveraging the datasets prepared by the licensed psychotherapists, we experiment and microbenchmark various LLMs performance in tasks along CaiTI's conversation flow and discuss their strengths and weaknesses. With the psychotherapists, we implement CaiTI and conduct 14-day and 24-week studies. The study results, validated by therapists, demonstrate that CaiTI can converse with users naturally, accurately understand and interpret user responses, and provide psychotherapeutic interventions appropriately and effectively. We showcase the potential of CaiTI LLMs to assist the mental therapy diagnosis and treatment and improve day-to-day functioning screening and precautionary psychotherapeutic intervention systems.",10.48550/arXiv.2403.10779,['JournalArticle'],Computer Science; Psychology
From Traditional Recommender Systems to GPT-Based Chatbots: A Survey of Recent Developments and Future Directions,2024,"T. M. Al-Hasan, A. Sayed, Fayal Bensaali, Y. Himeur, Iraklis Varlamis, G. Dimitrakopoulos","Recommender systems are a key technology for many applications, such as e-commerce, streaming media, and social media. Traditional recommender systems rely on collaborative filtering or content-based filtering to make recommendations. However, these approaches have limitations, such as the cold start and the data sparsity problem. This survey paper presents an in-depth analysis of the paradigm shift from conventional recommender systems to generative pre-trained-transformers-(GPT)-based chatbots. We highlight recent developments that leverage the power of GPT to create interactive and personalized conversational agents. By exploring natural language processing (NLP) and deep learning techniques, we investigate how GPT models can better understand user preferences and provide context-aware recommendations. The paper further evaluates the advantages and limitations of GPT-based recommender systems, comparing their performance with traditional methods. Additionally, we discuss potential future directions, including the role of reinforcement learning in refining the personalization aspect of these systems.",10.3390/bdcc8040036,"['JournalArticle', 'Review']",Computer Science
Large Language Models Can Self-Correct with Key Condition Verification,2024,"Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang","Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective prompting method enhances LLM performance in identifying and correcting inaccurate answers without external feedback.That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numerical value in an arithmetic question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo-1106 as the backend LLM, yields +6.8 exact match on four open-domain question answering datasets, +14.1 accuracy on three arithmetic reasoning datasets, and +9.6 accuracy on a commonsense reasoning dataset, compared to Self-Correct.Our implementation is made publicly available at https://wzy6642.github.io/proco.github.io/.",10.18653/v1/2024.emnlp-main.714,"['JournalArticle', 'Conference']",Computer Science
Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,2024,"Guangzeng Han, Weisi Liu, Xiaolei Huang, Brian Borsari","Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. While the behavior coding task has rapidly adapted language models to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice. To encounter those challenges, we introduce the Chain-of- Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists. This approach enables large language models to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding. Experiments on real-world datasets can prove the effectiveness and flexibility of our prompting method with multiple state-of-the-art LLMs over existing prompting baselines. We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying LLMs for psychotherapy behavior understanding.1",10.1109/ICHI61247.2024.00057,"['JournalArticle', 'Conference']",Computer Science; Psychology
Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents,2025,"Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang","As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.",10.48550/arXiv.2503.24047,"['JournalArticle', 'Review']",Computer Science
Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions,2024,"Michael J.Q. Zhang, W. B. Knox, Eunsol Choi","Large language models (LLMs) must often respond to highly ambiguous user requests. In such cases, the LLM's best response may be to ask a clarifying question to elicit more information. Existing LLMs often respond by presupposing a single interpretation of such ambiguous requests, frustrating users who intended a different interpretation. We speculate this is caused by current preference data labeling practice, where LLM responses are evaluated only on their prior contexts. To address this, we assign preference labels by simulating their expected outcomes in future turns. This allows LLMs to learn to ask clarifying questions when it can generate responses that are tailored to each user interpretation in future turns. On open-domain QA datasets with multiple annotations, we evaluate systems based on their ability to ask clarifying questions to recover each user's interpretation and expected answer. We compare systems trained using our proposed preference labeling methods against standard methods, which assign preferences based on only prior context. Our method achieves a 5% improvement in F1 measured against the answer set from different interpretations of each query, showing the value of modeling future conversation turns. We further demonstrate that our method can be used to train models to judiciously determine when to ask clarifying questions, directly answering the question when clarification is unnecessary. In our experiments, we find that our method achieves a 3% improvement in accuracy of such judgments over existing methods.",10.48550/arXiv.2410.13788,['JournalArticle'],Computer Science
Augmenting LLMs with Knowledge: A survey on hallucination prevention,2023,"Konstantinos Andriopoulos, J. Pouwelse","Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.",10.48550/arXiv.2309.16459,"['JournalArticle', 'Review']",Computer Science
GRID: Scene-Graph-based Instruction-driven Robotic Task Planning,2023,"Zhe Ni, Xiao-Xin Deng, Cong Tai, Xin-Yue Zhu, Xiang Wu, Y. Liu, Long Zeng","Recent works have shown that Large Language Models (LLMs) can facilitate the grounding of instructions for robotic task planning. Despite this progress, most existing works have primarily focused on utilizing raw images to aid LLMs in understanding environmental information. However, this approach not only limits the scope of observation but also typically necessitates extensive multimodal data collection and large-scale models. In this paper, we propose a novel approach called Graph-based Robotic Instruction Decomposer (GRID), which leverages scene graphs instead of images to perceive global scene information and iteratively plan subtasks for a given instruction. Our method encodes object attributes and relationships in graphs through an LLM and Graph Attention Networks, integrating instruction features to predict subtasks consisting of pre-defined robot actions and target objects in the scene graph. This strategy enables robots to acquire semantic knowledge widely observed in the environment from the scene graph. To train and evaluate GRID, we establish a dataset construction pipeline to generate synthetic datasets for graph-based robotic task planning. Experiments have shown that our method outperforms GPT-4 by over 25.4% in subtask accuracy and 43.6% in task accuracy. Moreover, our method achieves a real-time speed of 0.11s per inference. Experiments conducted on datasets of unseen scenes and scenes with varying numbers of objects demonstrate that the task accuracy of GRID declined by at most 3.8%, showcasing its robust cross-scene generalization ability. We validate our method in both physical simulation and the real world. More details can be found on the project page https://jackyzengl.github.io/GRID.github.io/.",10.1109/IROS58592.2024.10801291,"['JournalArticle', 'Conference']",Computer Science; Engineering
LLMs for Explainable AI: A Comprehensive Survey,2025,"Ahsan Bilal, David Ebert, Beiyu Lin","Large Language Models (LLMs) offer a promising approach to enhancing Explainable AI (XAI) by transforming complex machine learning outputs into easy-to-understand narratives, making model predictions more accessible to users, and helping bridge the gap between sophisticated model behavior and human interpretability. AI models, such as state-of-the-art neural networks and deep learning models, are often seen as""black boxes""due to a lack of transparency. As users cannot fully understand how the models reach conclusions, users have difficulty trusting decisions from AI models, which leads to less effective decision-making processes, reduced accountabilities, and unclear potential biases. A challenge arises in developing explainable AI (XAI) models to gain users' trust and provide insights into how models generate their outputs. With the development of Large Language Models, we want to explore the possibilities of using human language-based models, LLMs, for model explainabilities. This survey provides a comprehensive overview of existing approaches regarding LLMs for XAI, and evaluation techniques for LLM-generated explanation, discusses the corresponding challenges and limitations, and examines real-world applications. Finally, we discuss future directions by emphasizing the need for more interpretable, automated, user-centric, and multidisciplinary approaches for XAI via LLMs.",10.48550/arXiv.2504.00125,"['JournalArticle', 'Review']",Computer Science
Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation,2024,"Mohammadmehdi Ataei, Hyunmin Cheong, Daniele Grandi, Ye Wang, Nigel Morris, Alexander Tessier","
 Requirements elicitation, a critical, yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This may lead to products that fall short of expectations. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated users (LLM agents), enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents engage in product experience scenarios, through explaining their actions, observations, and challenges. Subsequent agent interviews and analysis uncover valuable user needs, including latent ones. We validate our framework with three experiments. First, we explore different methodologies for the challenge of diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that context-aware agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Third, we showcase that LLMs can be used to analyze interviews, capture needs and classify them as latent or not. Our work highlights the potential of using LLMs to accelerate early-stage product development, reduce costs, and increase innovation.",10.48550/arXiv.2404.16045,['JournalArticle'],Computer Science; Engineering
"Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs",2025,"Ariba Khan, Stephen Casper, Dylan Hadfield-Menell","Research on the cultural alignment of Large Language Models (LLMs) has emerged in response to growing interest in understanding representation across diverse stakeholders. Current approaches to evaluating cultural alignment through survey-based assessments that borrow from social science methodologies often overlook systematic robustness checks. We identify and test three assumptions behind current survey-based evaluation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolability: that alignment with one culture on a narrow set of issues predicts alignment with that culture on others, and (3) Steerability: that LLMs can be reliably prompted to represent specific cultural perspectives. Through experiments examining both explicit and implicit preferences of leading LLMs, we find a high level of instability across presentation formats, incoherence between evaluated versus held-out cultural dimensions, and erratic behavior under prompt steering. We show that these inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a selective assessment of evidence can be used to paint an incomplete picture of LLMs cultural alignment properties. Overall, these results highlight significant limitations of current survey-based approaches to evaluating the cultural alignment of LLMs and highlight a need for systematic robustness checks and red-teaming for evaluation results. Data and code are available at https://doi.org/akhan02/cultural-dimension-cover-letters and https://doi.org/ariba-k/llm-cultural-alignment-evaluation, respectively.",10.1145/3715275.3732147,"['JournalArticle', 'Book', 'Review']",Computer Science; Linguistics; Sociology
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview,2024,"Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi","Conversational Information Seeking has evolved rapidly in the last few years with the development of Large Language Models providing the basis for interpreting and responding in a naturalistic manner to user requests. iKAT emphasizes the creation and research of conversational search agents that adapt responses based on the user's prior interactions and present context. This means that the same question might yield varied answers, contingent on the user's profile and preferences. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate personalized context to effectively guide users through the relevant information to them. iKAT's first year attracted seven teams and a total of 24 runs. Most of the runs leveraged Large Language Models (LLMs) in their pipelines, with a few focusing on a generate-then-retrieve approach.",10.48550/arXiv.2401.01330,"['JournalArticle', 'Review']",Computer Science
Estimating Knowledge in Large Language Models Without Generating a Single Token,2024,"Daniela Gottesman, Mor Geva","To evaluate knowledge in large language models (LLMs), current methods query the model and then evaluate its generated responses. In this work, we ask whether evaluation can be done before the model has generated any text. Concretely, is it possible to estimate how knowledgeable a model is about a certain entity, only from its internal computation? We study this question with two tasks: given a subject entity, the goal is to predict (a) the ability of the model to answer common questions about the entity, and (b) the factuality of open-ended responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN, a simple probe trained over internal subject representations, succeeds at both tasks - correlating with both the QA accuracy of the model per-subject and FActScore, a recent factuality metric in open-ended generation. Moreover, KEEN naturally aligns with the model's hedging behavior and faithfully reflects changes in the model's knowledge after fine-tuning. Lastly, we show a more interpretable yet equally performant variant of KEEN, which highlights a small set of tokens indicative of clusters and gaps in the model's knowledge. Being simple and lightweight, KEEN can be leveraged to guide decisions such as when it is appropriate to apply further training or augment queries with retrieval.",10.48550/arXiv.2406.12673,"['JournalArticle', 'Conference']",Computer Science
"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",2023,"Bojana Bodroa, Bojana M. Dini, Ljubisa Bojic Department of Psychology, Faculty of Philosophy, Universityof Novi Sad, Serbia, Digital Society Lab, Institute for Philosophy, Social Theory, U. Belgrade",,10.48550/arXiv.2306.04308,['JournalArticle'],Computer Science; Psychology
Frame-Voyager: Learning to Query Frames for Video Large Language Models,2024,"Sicheng Yu, Chengkai Jin, Huan Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xioalei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun","Video Large Language Models (Video-LLMs) have made remarkable progress in video understanding tasks. However, they are constrained by the maximum length of input tokens, making it impractical to input entire videos. Existing frame selection approaches, such as uniform frame sampling and text-frame retrieval, fail to account for the information density variations in the videos or the complex instructions in the tasks, leading to sub-optimal performance. In this paper, we propose Frame-Voyager that learns to query informative frame combinations, based on the given textual queries in the task. To train Frame-Voyager, we introduce a new data collection and labeling pipeline, by ranking frame combinations using a pre-trained Video-LLM. Given a video of M frames, we traverse its T-frame combinations, feed them into a Video-LLM, and rank them based on Video-LLM's prediction losses. Using this ranking as supervision, we train Frame-Voyager to query the frame combinations with lower losses. In experiments, we evaluate Frame-Voyager on four Video Question Answering benchmarks by plugging it into two different Video-LLMs. The experimental results demonstrate that Frame-Voyager achieves impressive results in all settings, highlighting its potential as a plug-and-play solution for Video-LLMs.",10.48550/arXiv.2410.03226,['JournalArticle'],Computer Science
Demonstrations of the Potential of AI-based Political Issue Polling,2023,"Nathan Sanders, Alex Ulinich, B. Schneier","Political polling is a multi-billion dollar industry with outsized influence on the societal trajectory of the United States and nations around the world. However, it has been challenged by factors that stress its cost, availability, and accuracy. At the same time, artificial intelligence (AI) chatbots have become compelling stand-ins for human behavior, powered by increasingly sophisticated large language models (LLMs). Could AI chatbots be an effective tool for anticipating public opinion on controversial issues to the extent that they could be used by campaigns, interest groups, and polling firms? We have developed a prompt engineering methodology for eliciting human-like survey responses from ChatGPT, which simulate the response to a policy question of a person described by a set of demographic factors, and produce both an ordinal numeric response score and a textual justification. We execute large scale experiments, querying for thousands of simulated responses at a cost far lower than human surveys. We compare simulated data to human issue polling data from the Cooperative Election Study (CES). We find that ChatGPT is effective at anticipating both the mean level and distribution of public opinion on a variety of policy issues such as abortion bans and approval of the US Supreme Court, particularly in their ideological breakdown (correlation typically>85%). However, it is less successful at anticipating demographic-level differences. Moreover, ChatGPT tends to overgeneralize to new policy issues that arose after its training data was collected, such as US support for involvement in the war in Ukraine. Our work has implications for our understanding of the strengths and limitations of the current generation of AI chatbots as virtual publics or online listening platforms, future directions for LLM development, and applications of AI tools to the political domain. (Abridged)",10.48550/arXiv.2307.04781,"['JournalArticle', 'Review']",Computer Science; Political Science
WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models,2024,"Wenlong Zhao, Debanjan Mondal, Niket Tandon, Danica Dillion, Kurt Gray, Yuling Gu","The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses. However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values. In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts. Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide. We have constructed more than 20 million examples of the type ""(demographic attributes, value question)  answer from the WVS responses. We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models. On merely 11.1%, 25.0%, 72.2%, and 75.0% of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve <0.2 Wasserstein 1-distance from the human normalized answer distributions. WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs.",10.48550/arXiv.2404.16308,"['JournalArticle', 'Review']",Computer Science; Linguistics
Calculon: a Methodology and Tool for High-Level Codesign of Systems and Large Language Models,2023,"Mikhail Isaev, Nic Mcdonald, Larry Dennison, R. Vuduc","This paper presents a parameterized analytical performance model of transformer-based Large Language Models (LLMs) for guiding high-level algorithm-architecture codesign studies. This model de-rives from an extensive survey of performance optimizations that have been proposed for the training and inference of LLMs; the model's parameters capture application characteristics, the hardware system, and the space of implementation strategies. With such a model, we can systematically explore a joint space of hardware and software configurations to identify optimal system designs under given constraints, like the total amount of system memory. We implemented this model and methodology in a Python-based open-source tool called Calculon. Using it, we identified novel system designs that look significantly different from current inference and training systems, showing quantitatively the estimated potential to achieve higher efficiency, lower cost, and better scalability.",10.1145/3581784.3607102,"['JournalArticle', 'Book', 'Conference', 'Review']",Computer Science
What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory,2023,"Ronald Fischer, Markus Luczak-Rsch, J. Karl","There has been concern about ideological basis and possible discrimination in text generated by Large Language Models (LLMs). We test possible value biases in ChatGPT using a psychological value theory. We designed a simple experiment in which we used a number of different probes derived from the Schwartz basic value theory (items from the revised Portrait Value Questionnaire, the value type definitions, value names). We prompted ChatGPT via the OpenAI API repeatedly to generate text and then analyzed the generated corpus for value content with a theory-driven value dictionary using a bag of words approach. Overall, we found little evidence of explicit value bias. The results showed sufficient construct and discriminant validity for the generated text in line with the theoretical predictions of the psychological model, which suggests that the value content was carried through into the outputs with high fidelity. We saw some merging of socially oriented values, which may suggest that these values are less clearly differentiated at a linguistic level or alternatively, this mixing may reflect underlying universal human motivations. We outline some possible applications of our findings for both applications of ChatGPT for corporate usage and policy making as well as future research avenues. We also highlight possible implications of this relatively high-fidelity replication of motivational content using a linguistic model for the theorizing about human values.",10.48550/arXiv.2304.03612,['JournalArticle'],Computer Science; Psychology
No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding,2025,"Michael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, Chris Tanner","LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text - typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use, and strong correlations with human stylistic preferences. However, LLM Judges have been shown to exhibit biases that can distort their judgments. We evaluate how well LLM Judges can grade whether a given response to a conversational question is correct, an ability crucial to soundly estimating the overall response quality. To do so, we create and publicly release a human-annotated dataset with labels of correctness for 1,200 LLM responses. We source questions from a combination of existing datasets and a novel, challenging benchmark (BFF-Bench) created for this analysis. We demonstrate a strong connection between an LLM's ability to correctly answer a question and grade responses to that question. Although aggregate level statistics might imply a judge has high agreement with human annotators, it will struggle on the subset of questions it could not answer. To address this issue, we recommend a simple solution: provide the judge with a correct, human-written reference answer. We perform an in-depth analysis on how reference quality can affect the performance of an LLM Judge. We show that providing a weaker judge (e.g. Qwen 2.5 7B) with higher quality references reaches better agreement with human annotators than a stronger judge (e.g. GPT-4o) with synthetic references.",10.48550/arXiv.2503.05061,['JournalArticle'],Computer Science; Linguistics
Narrating Fitness: Leveraging Large Language Models for Reflective Fitness Tracker Data Interpretation,2024,"Konstantin R. Strmel, Stanislas Henry, Tim Johansson, Jasmin Niess, Pawe W. Woniak","While fitness trackers generate and present quantitative data, past research suggests that users often conceptualise their wellbeing in qualitative terms. This discrepancy between numeric data and personal wellbeing perception may limit the effectiveness of personal informatics tools in encouraging meaningful engagement with ones wellbeing. In this work, we aim to bridge the gap between raw numeric metrics and users qualitative perceptions of wellbeing. In an online survey with n = 273 participants, we used step data from fitness trackers and compared three presentation formats: standard charts, qualitative descriptions generated by an LLM (Large Language Model), and a combination of both. Our findings reveal that users experienced more reflection, focused attention and reward when presented with the generated qualitative data compared to the standard charts alone. Our work demonstrates how automatically generated data descriptions can effectively complement numeric fitness data, fostering a richer, more reflective engagement with personal wellbeing information.",10.1145/3613904.3642032,"['JournalArticle', 'Book', 'Conference', 'Review']",Computer Science; Psychology
Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs,2023,"Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens","Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large Language Models (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differences in subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their ability to understand demographic differences in two subjective judgment tasks: politeness and offensiveness. We find that in zero-shot settings, most models' predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a minor gender bias favoring women appears in the politeness task. Furthermore, sociodemographic prompting does not consistently improve and, in some cases, worsens LLMs' ability to perceive language from specific sub-populations. These findings highlight potential demographic biases in LLMs when performing subjective judgment tasks and underscore the limitations of sociodemographic prompting as a strategy to achieve pluralistic alignment. Code and data are available at: https://github.com/Jiaxin-Pei/LLM-as-Subjective-Judge.",10.18653/v1/2025.naacl-short.71,"['JournalArticle', 'Conference']",Computer Science; Linguistics; Sociology
Sabotage Evaluations for Frontier Models,2024,"Joe Benton, Misha Wagner, Eric Christiansen, Cem Anil, Ethan Perez, Jai Srivastav, Esin Durmus, Deep Ganguli, Shauna Kravec, Buck Shlegeris, Jared Kaplan, Holden Karnofsky, Evan Hubinger, Roger Grosse, Samuel R. Bowman, D. Duvenaud","Sufficiently capable models could subvert human oversight and decision-making in important contexts. For example, in the context of AI development, models could covertly sabotage efforts to evaluate their own dangerous capabilities, to monitor their behavior, or to make decisions about their deployment. We refer to this family of abilities as sabotage capabilities. We develop a set of related threat models and evaluations. These evaluations are designed to provide evidence that a given model, operating under a given set of mitigations, could not successfully sabotage a frontier model developer or other large organization's activities in any of these ways. We demonstrate these evaluations on Anthropic's Claude 3 Opus and Claude 3.5 Sonnet models. Our results suggest that for these models, minimal mitigations are currently sufficient to address sabotage risks, but that more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve. We also survey related evaluations we tried and abandoned. Finally, we discuss the advantages of mitigation-aware capability evaluations, and of simulating large-scale deployments using small-scale statistics.",10.48550/arXiv.2410.21514,"['JournalArticle', 'Review']",Computer Science
Can Unconfident LLM Annotations Be Used for Confident Conclusions?,2024,"Kristina Gligori'c, Tijana Zrnic, Cinoo Lee, Emmanuel J. Cands, Dan Jurafsky","Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems.",10.48550/arXiv.2408.15204,"['JournalArticle', 'Conference']",Computer Science
OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use,2025,"Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Y. Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu","The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.",10.48550/arXiv.2508.04482,"['JournalArticle', 'Review']",Computer Science
Findings of WASSA 2024 Shared Task on Empathy and Personality Detection in Interactions,2024,"Salvatore Giorgi, Joo Sedoc, Valentin Barriere, Shabnam Tafreshi","This paper presents the results of the WASSA 2024 shared task on predicting empathy, emotion, and personality in conversations and reactions to news articles. Participating teams were given access to a new, unpublished extension of the WASSA 2023 shared task dataset. This task is both multi-level and multi-modal: data is available at the person, essay, dialog, and dialog-turn levels and includes formal (news articles) and informal text (essays and dialogs), self-report data (personality and distress), and third-party annotations (empathy and emotion). The shared task included a new focus on conversations between humans and LLM-based virtual agents which occur immediately after reading and reacting to the news articles. Participants were encouraged to explore the multi-level and multi-modal nature of this data. Participation was encouraged in four tracks: (i) predicting the perceived empathy at the dialog level, (ii) predicting turn-level empathy, emotion polarity, and emotion intensity in conversations, (iii) predicting state empathy and distress scores, and (iv) predicting personality. In total, 14 teams participated in the shared task. We summarize the methods and resources used by the participating teams.",10.18653/v1/2024.wassa-1.30,['JournalArticle'],Computer Science; Psychology
PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs,2022,"Hao Cui, Rahmadi Trimananda, A. Markopoulou, Scott Jordan","Privacy policies disclose how an organization collects and handles personal information. Recent work has made progress in leveraging natural language processing (NLP) to automate privacy policy analysis and extract data collection statements from different sentences, considered in isolation from each other. In this paper, we view and analyze, for the first time, the entire text of a privacy policy in an integrated way. In terms of methodology: (1) we define PoliGraph, a type of knowledge graph that captures statements in a policy as relations between different parts of the text; and (2) we revisit the notion of ontologies, previously defined in heuristic ways, to capture subsumption relations between terms. We make a clear distinction between local and global ontologies to capture the context of individual policies, application domains, and privacy laws. We develop PoliGrapher, an NLP tool to automatically extract PoliGraph from the text using linguistic analysis. Using a public dataset for evaluation, we show that PoliGrapher identifies 40% more collection statements than prior state-of-the-art, with 97% precision. In terms of applications, PoliGraph enables automated analysis of a corpus of policies and allows us to: (1) reveal common patterns in the texts across different policies, and (2) assess the correctness of the terms as defined within a policy. We also apply PoliGraph to: (3) detect contradictions in a policy, where we show false alarms by prior work, and (4) analyze the consistency of policies and network traffic, where we identify significantly more clear disclosures than prior work. Finally, leveraging the capabilities of the emerging large language models (LLMs), we also present PoliGrapher-LM, a tool that uses LLM prompting instead of NLP linguistic analysis, to extract PoliGraph from the policy text, and we show that it further improves coverage.",10.48550/arXiv.2210.06746,['JournalArticle'],Computer Science
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation,2024,"Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach","We introduce Bonito, an open-source model for conditional task generation that converts unannotated text into task-specific training datasets for instruction tuning. We aim to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito by fine-tuning a pretrained large language model on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains with unannotated text across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.",10.48550/arXiv.2402.18334,"['JournalArticle', 'Conference']",Computer Science
What Limits LLM-based Human Simulation: LLMs or Our Design?,2025,"Qian Wang, Jiaying Wu, Zhenheng Tang, B. Luo, Nuo Chen, Wei Chen, Bingsheng He","We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges. Recent studies have revealed significant gaps between LLM-based human simulations and real-world observations, highlighting these dual challenges. To address these gaps, we present a comprehensive analysis of LLM limitations and our design issues, proposing targeted solutions for both aspects. Furthermore, we explore future directions that address both challenges simultaneously, particularly in data collection, LLM generation, and evaluation. To support further research in this field, we provide a curated collection of LLM-based human simulation resources.\footnote{https://github.com/Persdre/llm-human-simulation}",10.48550/arXiv.2501.08579,['JournalArticle'],Computer Science; Psychology
Editing Personality for LLMs,2023,"Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",,10.48550/arXiv.2310.02168,['JournalArticle'],Computer Science; Psychology
LLMs Among Us: Generative AI Participating in Digital Discourse,2024,"Kristina Radivojevic, Nicholas Clark, Paul R. Brenner","The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the ""LLMs Among Us"" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, Llama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially more impact on human perception than the choice of mainstream LLMs.",10.48550/arXiv.2402.07940,"['JournalArticle', 'Review']",Computer Science; Linguistics
Persuasion with Large Language Models: a Survey,2024,"Alexander Rogiers, Sander Noels, Maarten Buyl, T. D. Bie","The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.",10.48550/arXiv.2411.06837,"['JournalArticle', 'Review']",Computer Science; Linguistics
"LLM-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts",2024,"Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, Chris Kedzie","This paper introduces a framework for the automated evaluation of natural language texts. A manually constructed rubric describes how to assess multiple dimensions of interest. To evaluate a text, a large language model (LLM) is prompted with each rubric question and produces a distribution over potential responses. The LLM predictions often fail to agree well with human judges -- indeed, the humans do not fully agree with one another. However, the multiple LLM distributions can be $\textit{combined}$ to $\textit{predict}$ each human judge's annotations on all questions, including a summary question that assesses overall quality or relevance. LLM-Rubric accomplishes this by training a small feed-forward neural network that includes both judge-specific and judge-independent parameters. When evaluating dialogue systems in a human-AI information-seeking task, we find that LLM-Rubric with 9 questions (assessing dimensions such as naturalness, conciseness, and citation quality) predicts human judges' assessment of overall user satisfaction, on a scale of 1--4, with RMS error $<0.5$, a $2\times$ improvement over the uncalibrated baseline.",10.18653/v1/2024.acl-long.745,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Emergent Abilities in Large Language Models: A Survey,2025,"Leonardo Berti, Flavio Giorgi, Gjergji Kasneci","Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.",10.48550/arXiv.2503.05788,"['JournalArticle', 'Review']",Computer Science; Linguistics
DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life,2024,"Yu Ying Chiu, Liwei Jiang, Yejin Choi","As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts.",10.48550/arXiv.2410.02683,"['JournalArticle', 'Review']",Computer Science; Sociology
"Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives",2024,"Hao Sun, Yunyi Shen, Jean-Franois Ton","The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear why this model -- originally developed for multi-player stochastic game matching -- can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. In this paper, we first revisit the foundations of using BT models in reward modeling, and establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use. Despite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization. This is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. We highlight the critical concept of order consistency in reward modeling and demonstrate that the BT model possesses this property. Consequently, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. To offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using $6$ base LLMs, $2$ datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations.",10.48550/arXiv.2411.04991,['JournalArticle'],Computer Science
Estimating the Hallucination Rate of Generative AI,2024,"A. Jesson, Nicolas Beltran-Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John P. Cunningham, David M. Blei","This paper presents a method for estimating the hallucination rate for in-context learning (ICL) with generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and a prediction question and asked to generate a response. One interpretation of ICL assumes that the CGM computes the posterior predictive of an unknown Bayesian model, which implicitly defines a joint distribution over observable datasets and latent mechanisms. This joint distribution factorizes into two components: the model prior over mechanisms and the model likelihood of datasets given a mechanism. With this perspective, we define a hallucination as a generated response to the prediction question with low model likelihood given the mechanism. We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination. Our method only requires generating prediction questions and responses from the CGM and evaluating its response log probability. We empirically evaluate our method using large language models for synthetic regression and natural language ICL tasks.",10.48550/arXiv.2406.07457,['JournalArticle'],Computer Science; Mathematics
Zero-Shot On-the-Fly Event Schema Induction,2022,"Rotem Dror, Haoyu Wang, D. Roth","What are the events involved in a pandemic outbreak? What steps should be taken when planning a wedding? The answers to these questions can be found by collecting many documents on the complex event of interest, extracting relevant information, and analyzing it. We present a new approach in which large language models are utilized to generate source documents that allow predicting, given a high-level event definition, the specific events, arguments, and relations between them to construct a schema that describes the complex event in its entirety.Using our model, complete schemas on any topic can be generated on-the-fly without any manual data collection, i.e., in a zero-shot manner. Moreover, we develop efficient methods to extract pertinent information from texts and demonstrate in a series of experiments that these schemas are considered to be more complete than human-curated ones in the majority of examined scenarios. Finally, we show that this framework is comparable in performance with previous supervised schema induction methods that rely on collecting real texts and even reaching the best score in the prediction task.",10.48550/arXiv.2210.06254,['JournalArticle'],Computer Science
PersonalLLM: Tailoring LLMs to Individual Preferences,2024,"Thomas Zollo, Andrew Siah, Naimeng Ye, Ang Li, Hongseok Namkoong","As LLMs become capable of complex tasks, there is growing potential for personalized interactions tailored to the subtle and idiosyncratic preferences of the user. We present a public benchmark, PersonalLLM, focusing on adapting LLMs to provide maximal benefits for a particular user. Departing from existing alignment benchmarks that implicitly assume uniform preferences, we curate open-ended prompts paired with many high-quality answers over which users would be expected to display heterogeneous latent preferences. Instead of persona-prompting LLMs based on high-level attributes (e.g., user's race or response length), which yields homogeneous preferences relative to humans, we develop a method that can simulate a large user base with diverse preferences from a set of pre-trained reward models. Our dataset and generated personalities offer an innovative testbed for developing personalization algorithms that grapple with continual data sparsity--few relevant feedback from the particular user--by leveraging historical data from other (similar) users. We explore basic in-context learning and meta-learning baselines to illustrate the utility of PersonalLLM and highlight the need for future methodological development. Our dataset is available at https://huggingface.co/datasets/namkoong-lab/PersonalLLM",10.48550/arXiv.2409.20296,['JournalArticle'],Computer Science
Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning,2025,"Vaishnavi Shrivastava, Ahmed Awadallah, Vidhisha Balachandran, Shivam Garg, Harkirat Singh Behl, Dimitris Papailiopoulos","Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely""filler"": repetitive, verbose text that makes no real progress. We introduce GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, we teach models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. We also propose Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.",10.48550/arXiv.2508.09726,['JournalArticle'],Computer Science
"Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations",2024,"Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto","Enhancing user engagement through personalization in conversational agents has gained significance, especially with the advent of large language models that generate fluent responses. Personalized dialogue generation, however, is multifaceted and varies in its definition  ranging from instilling a persona in the agent to capturing users explicit and implicit cues. This paper seeks to systemically survey the recent landscape of personalized dialogue generation, including the datasets employed, methodologies developed, and evaluation metrics applied. Covering 22 datasets, we highlight benchmark datasets and newer ones enriched with additional features. We further analyze 17 seminal works from top conferences between 2021-2023 and identify five distinct types of problems. We also shed light on recent progress by LLMs in personalized dialogue generation. Our evaluation section offers a comprehensive summary of assessment facets and metrics utilized in these works. In conclusion, we discuss prevailing challenges and envision prospect directions for future research in personalized dialogue generation.",10.48550/arXiv.2405.17974,"['JournalArticle', 'Review']",Computer Science; Linguistics
CollabLLM: From Passive Responders to Active Collaborators,2025,"Shirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, J. Leskovec, Jianfeng Gao","Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.",10.48550/arXiv.2502.00640,['JournalArticle'],Computer Science
Large Language Models as Subpopulation Representative Models: A Review,2023,"Gabriel Simmons, Christopher Hare","Of the many commercial and scientific opportunities provided by large language models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and Anthropic's Claude), one of the more intriguing applications has been the simulation of human behavior and opinion. LLMs have been used to generate human simulcra to serve as experimental participants, survey respondents, or other independent agents, with outcomes that often closely parallel the observed behavior of their genuine human counterparts. Here, we specifically consider the feasibility of using LLMs to estimate subpopulation representative models (SRMs). SRMs could provide an alternate or complementary way to measure public opinion among demographic, geographic, or political segments of the population. However, the introduction of new technology to the socio-technical infrastructure does not come without risk. We provide an overview of behavior elicitation techniques for LLMs, and a survey of existing SRM implementations. We offer frameworks for the analysis, development, and practical implementation of LLMs as SRMs, consider potential risks, and suggest directions for future work.",10.48550/arXiv.2310.17888,"['JournalArticle', 'Review']",Computer Science; Linguistics
Lateral Phishing With Large Language Models: A Large Organization Comparative Study,2024,"Mazal Bethany, Athanasios Galiopoulos, Emet Bethany, Mohammad Bahrami Karkevandi, Nicole Beebe, Nishant Vishwamitra, Peyman Najafirad","The emergence of Large Language Models (LLMs) has heightened the threat of phishing emails by enabling the generation of highly targeted, personalized, and automated attacks. Traditionally, many phishing emails have been characterized by typos, errors, and poor language. These errors can be mitigated by LLMs, potentially lowering the barrier for attackers. Despite this, there is a lack of large-scale studies comparing the effectiveness of LLM-generated lateral phishing emails to those crafted by humans. Current literature does not adequately address the comparative effectiveness of LLM and human-generated lateral phishing emails in a real-world, large-scale organizational setting, especially considering the potential for LLMs to generate more convincing and error-free phishing content. To address this gap, we conducted a pioneering study within a large university, targeting its workforce of approximately 9,000 individuals including faculty, staff, administrators, and student workers. Our results indicate that LLM-generated lateral phishing emails are as effective as those written by communications professionals, emphasizing the critical threat posed by LLMs in leading phishing campaigns. We break down the results of the overall phishing experiment, comparing vulnerability between departments and job roles. Furthermore, to gather qualitative data, we administered a detailed questionnaire, revealing insights into the reasons and motivations behind vulnerable employees actions. This study contributes to the understanding of cyber security threats in educational institutions and provides a comprehensive comparison of LLM and human-generated phishing emails effectiveness, considering the potential for LLMs to generate more convincing content. The findings highlight the need for enhanced user education and system defenses to mitigate the growing threat of AI-powered phishing attacks.",10.1109/ACCESS.2025.3555500,['JournalArticle'],Computer Science
An In-depth Investigation of User Response Simulation for Conversational Search,2023,"Zhenduo Wang, Zhichao Xu, Qingyao Ai, Vivek Srikumar","Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve users' search needs through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only responding to yes-no questions from the conversational search system or unable to produce high-quality responses in general. This paper shows that existing user simulation systems could be significantly improved by a smaller finetuned natural language generation model. However, rather than merely reporting it as the new state-of-the-art, we consider it a strong baseline and present an in-depth investigation of simulating user response for conversational search. Our goal is to supplement existing work with an insightful hand-analysis of unsolved challenges by the baseline and propose our solutions. The challenges we identified include (1) a blind spot that is difficult to learn, and (2) a specific type of misevaluation in the standard setup. We propose a new generation system to effectively cover the training blind spot and suggest a new evaluation setup to avoid misevaluation. Our proposed system leads to significant improvements over existing systems and large language models such as GPT-4. Additionally, our analysis provides insights into the nature of the task to facilitate future work.",10.1145/3589334.3645447,"['Book', 'JournalArticle', 'Conference']",Computer Science
Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas,2024,"Louis Kwok, Michal Bravansky, Lewis D. Griffin","The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness.",10.48550/arXiv.2408.06929,['JournalArticle'],Computer Science; Linguistics; Psychology
A Survey of Personalized Large Language Models: Progress and Future Directions,2025,"Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Jieming Zhu, Minda Hu, Menglin Yang, Irwin King","Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.",10.48550/arXiv.2502.11528,"['JournalArticle', 'Review']",Computer Science; Linguistics
IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators,2024,"Luyang Lin, Lingzhi Wang, Xiaoyan Zhao, Jing Li, Kam-Fai Wong","This study focuses on media bias detection, crucial in todays era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the inputs bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVecs significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the frameworks effectiveness.",10.48550/arXiv.2402.00345,['JournalArticle'],Computer Science
Large Language Models for Text Classification: From Zero-Shot Learning to Instruction-Tuning,2025,"Youngjin Chae, Thomas R. Davidson","Large language models (LLMs) have tremendous potential for social science research as they are trained on vast amounts of text and can generalize to many tasks. We explore the use of LLMs for supervised text classification, specifically the application to stance detection, which involves detecting attitudes and opinions in texts. We examine the performance of these models across different architectures, training regimes, and task specifications. We compare 10 models ranging in size from tens of millions to hundreds of billions of parameters and test four distinct training regimes: Prompt-based zero-shot learning and few-shot learning, fine-tuning, and instruction-tuning, which combines prompting and fine-tuning. The largest, most powerful models generally offer the best predictive performance even with little or no training examples, but fine-tuning smaller models is a competitive solution due to their relatively high accuracy and low cost. Instruction-tuning the latest generative LLMs expands the scope of text classification, enabling applications to more complex tasks than previously feasible. We offer practical recommendations on the use of LLMs for text classification in sociological research and discuss their limitations and challenges. Ultimately, LLMs can make text classification and other text analysis methods more accurate, accessible, and adaptable, opening new possibilities for computational social science.",10.1177/00491241251325243,['JournalArticle'],Computer Science; Linguistics
Towards Visual Grounding: A Survey,2024,"Linhui Xiao, Xiaoshan Yang, Xiangyuan Lan, Yaowei Wang, Changsheng Xu","Visual Grounding is also known as Referring Expression Comprehension and Phrase Grounding. It involves localizing a natural number of specific regions within an image based on a given textual description. The objective of this task is to emulate the prevalent referential relationships in social conversations, equipping machines with human-like multimodal comprehension capabilities. Consequently, it has extensive applications in various domains. However, since 2021, visual grounding has witnessed significant advancements, with emerging new concepts such as grounded pre-training, grounding multimodal LLMs, generalized visual grounding, and giga-pixel grounding, which have brought numerous new challenges. In this survey, we initially examine the developmental history of visual grounding and provide an overview of essential background knowledge. We systematically track and summarize the advancements and meticulously organize the various settings in visual grounding, thereby establishing precise definitions of these settings to standardize future research and ensure a fair comparison. Additionally, we delve into several advanced topics and highlight numerous applications of visual grounding. Finally, we outline the challenges confronting visual grounding and propose valuable directions for future research, which may serve as inspiration for subsequent researchers. By extracting common technical details, this survey encompasses the representative works in each subtopic over the past decade. To the best, this paper presents the most comprehensive overview currently available in the field of grounding. This survey is designed to be suitable for both beginners and experienced researchers, serving as an invaluable resource for understanding key concepts and tracking the latest research developments. We keep tracing related works at https://github.com/linhuixiao/Awesome-Visual-Grounding.",10.48550/arXiv.2412.20206,"['JournalArticle', 'Review']",Computer Science
Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models,2024,"Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud Hashemi","Abstention Ability (AA) is a critical aspect of Large Language Model (LLM) reliability, referring to an LLM's capability to withhold responses when uncertain or lacking a definitive answer, without compromising performance. Although previous studies have attempted to improve AA, they lack a standardised evaluation method and remain unsuitable for black-box models where token prediction probabilities are inaccessible. This makes comparative analysis challenging, especially for state-of-the-art closed-source commercial LLMs. This paper bridges this gap by introducing a black-box evaluation approach and a new dataset, Abstain-QA, crafted to rigorously assess AA across varied question types (answerable and unanswerable), domains (well-represented and under-represented), and task types (fact centric and reasoning). We also propose a new confusion matrix, the ''Answerable-Unanswerable Confusion Matrix'' (AUCM) which serves as the basis for evaluating AA, by offering a structured and precise approach for assessment. Finally, we explore the impact of three prompting strategies-Strict Prompting, Verbal Confidence Thresholding, and Chain-of-Thought (CoT)-on improving AA. Our results indicate that even powerful models like GPT-4, Mixtral 8x22b encounter difficulties with abstention; however, strategic approaches such as Strict prompting and CoT can enhance this capability.",10.48550/arXiv.2407.16221,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests,2024,"Amirhossein Deljouyi, Roham Koohestani, M. Izadi, Andy Zaidman","Automated unit test generators, particularly searchbased software testing tools like EvoSuite, are capable of generating tests with high coverage. Although these generators alleviate the burden of writing unit tests, they often pose challenges for software engineers in terms of understanding the generated tests. To address this, we introduce UTGen, which combines searchbased software testing and large language models to enhance the understandability of automatically generated test cases. We achieve this enhancement through contextualizing test data, improving identifier naming, and adding descriptive comments. Through a controlled experiment with 32 participants from both academia and industry, we investigate how the understandability of unit tests affects a software engineer's ability to perform bug-fixing tasks. We selected bug-fixing to simulate a real-world scenario that emphasizes the importance of understandable test cases. We observe that participants working on assignments with UTGen test cases fix up to 33 % more bugs and use up to 20 % less time when compared to baseline test cases. From the post-test questionnaire, we gathered that participants found that enhanced test names, test data, and variable names improved their bugfixing process.",10.1109/ICSE55347.2025.00032,"['JournalArticle', 'Conference']",Computer Science
Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers,2024,"G. Yona, Roee Aharoni, Mor Geva","Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show that large language models with standard decoding tend to generate specific answers, which are often incorrect. In contrast, when evaluated on multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy on average, which further increases for rare entities. Overall, this reveals that standard evaluation and decoding schemes may significantly underestimate the knowledge encapsulated in LMs.",10.18653/v1/2024.acl-long.365,"['JournalArticle', 'Conference']",Computer Science
Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective,2024,"Yiqun Zhang, Xiaocui Yang, Xingle Xu, Zeran Gao, Yijie Huang, Shiyi Mu, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song, Ge Yu","Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.",10.48550/arXiv.2408.04638,"['JournalArticle', 'Review']",Computer Science; Psychology
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM,2024,"Ahmet Yusuf Alan, Enis Karaarslan, mer Aydin",": There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as  MufassirQAS'' . We created a vector database with several open-access books that include Turkish context. These are Turkish translations, and interpretations on Islam. We worked on creating system prompts with care, ensuring they provide instructions that prevent harmful, offensive, or disrespectful responses. We also tested the MufassirQAS and ChatGPT with sensitive questions. We got better performance with our system. Study and enhancements are still in progress. Results and future works are given.",10.2139/ssrn.4707470,['JournalArticle'],Computer Science
Large Language Models Show Human-like Social Desirability Biases in Survey Responses,2024,"Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, Joo Sedoc, L. Ungar, J. Eichstaedt","As Large Language Models (LLMs) become widely used to model and simulate human behavior, understanding their biases becomes critical. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e., increased extraversion, decreased neuroticism, etc). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4's survey responses changing by 1.20 (human) standard deviations and Llama 3's by 0.98 standard deviations-very large effects. This bias is robust to randomization of question order and paraphrasing. Reverse-coding all the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on using LLMs as proxies for human participants.",10.48550/arXiv.2405.06058,"['JournalArticle', 'Review']",Computer Science; Psychology
Network Formation and Dynamics Among Multi-LLMs,2024,"Marios Papachristou, Yuan Yuan","Social networks profoundly influence how humans form opinions, exchange information, and organize collectively. As large language models (LLMs) are increasingly embedded into social and professional environments, it is critical to understand whether their interactions approximate human-like network dynamics. We develop a framework to study the network formation behaviors of multiple LLM agents and benchmark them against human decisions. Across synthetic and real-world settings, including friendship, telecommunication, and employment networks, we find that LLMs consistently reproduce fundamental micro-level principles such as preferential attachment, triadic closure, and homophily, as well as macro-level properties including community structure and small-world effects. Importantly, the relative emphasis of these principles adapts to context: for example, LLMs favor homophily in friendship networks but heterophily in organizational settings, mirroring patterns of social mobility. A controlled human-subject survey confirms strong alignment between LLMs and human participants in link-formation decisions. These results establish that LLMs can serve as powerful tools for social simulation and synthetic data generation, while also raising critical questions about bias, fairness, and the design of AI systems that participate in human networks.",10.1093/pnasnexus/pgaf317,"['JournalArticle', 'Review']",Computer Science; Sociology
From System Models to Class Models: An In-Context Learning Paradigm,2023,"M. Forgione, F. Pura, D. Piga","Is it possible to understand the intricacies of a dynamical system not solely from its input/output pattern, but also by observing the behavior of other systems within the same class? This central question drives the study presented in this letter. In response to this query, we introduce a novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation. Unlike conventional methods, we do not directly estimate a model for the specific system. Instead, we learn a meta model that represents a class of dynamical systems. This meta model is trained on a potentially infinite stream of synthetic data, generated by simulators whose settings are randomly extracted from a probability distribution. When provided with a context from a new systemspecifically, an input/output sequencethe meta model implicitly discerns its dynamics, enabling predictions of its behavior. The proposed approach harnesses the power of Transformers, renowned for their in-context learning capabilities. For one-step prediction, a GPT-like decoder-only architecture is utilized, whereas the simulation problem employs an encoder-decoder structure. Initial experimental results affirmatively answer our foundational question, opening doors to fresh research avenues in system identification.",10.1109/LCSYS.2023.3335036,['JournalArticle'],Computer Science; Engineering
LLM-based Smart Reply (LSR): Enhancing Collaborative Performance with ChatGPT-mediated Smart Reply System,2023,"Ashish Bastola, Hongya Wang, Judsen Hembree, Pooja Yadav, Nathan McNeese, Abolfazl Razi","Interactive user interfaces have increasingly explored AI's role in enhancing communication efficiency and productivity in collaborative tasks. The emergence of Large Language Models (LLMs) such as ChatGPT has revolutionized conversational agents, employing advanced deep learning techniques to generate context-aware, coherent, and personalized responses. Consequently, LLM-based AI assistants provide a more natural and efficient user experience across various scenarios. In this paper, we study how LLM models can be used to improve work efficiency in collaborative workplaces. Specifically, we present an LLM-based Smart Reply (LSR) system utilizing the ChatGPT to generate personalized responses in professional collaborative scenarios while adapting to context and communication style based on prior responses. Our two-step process involves generating a preliminary response type (e.g., Agree, Disagree) to provide a generalized direction for message generation, thus reducing response drafting time. We conducted an experiment where participants completed simulated work tasks involving a Dual N-back test and subtask scheduling through Google Calendar while interacting with co-workers. Our findings indicate that the proposed LSR reduces overall workload, as measured by the NASA TLX, and improves work performance and productivity in the N-back task. We also provide qualitative analysis based on participants' experiences, as well as design considerations to provide future directions for improving such implementations.",10.48550/arXiv.2306.11980,['JournalArticle'],Computer Science
"Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution",2024,"Yucheng Ruan, Xiang Lan, Jingying Ma, Yizhi Dong, Kai He, Mengling Feng","Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.",10.48550/arXiv.2408.10548,"['JournalArticle', 'Review']",Computer Science
What should I Ask: A Knowledge-driven Approach for Follow-up Questions Generation in Conversational Surveys,2022,"Yubin Ge, Ziang Xiao, Jana Diesner, Heng Ji, Karrie Karahalios, Hari Sundaram","Generating follow-up questions on the fly could significantly improve conversational survey quality and user experiences by enabling a more dynamic and personalized survey structure. In this paper, we proposed a novel task for knowledge-driven follow-up question generation in conversational surveys. We constructed a new human-annotated dataset of human-written follow-up questions with dialogue history and labeled knowledge in the context of conversational surveys. Along with the dataset, we designed and validated a set of reference-free Gricean-inspired evaluation metrics to systematically evaluate the quality of generated follow-up questions. We then propose a two-staged knowledge-driven model for the task, which generates informative and coherent follow-up questions by using knowledge to steer the generation process. The experiments demonstrate that compared to GPT-based baseline models, our two-staged model generates more informative, coherent, and clear follow-up questions.",10.48550/arXiv.2205.10977,"['JournalArticle', 'Review']",Computer Science
A Survey on Large Language Models for Personalized and Explainable Recommendations,2023,Junyi Chen,"In recent years, Recommender Systems(RS) have witnessed a transformative shift with the advent of Large Language Models(LLMs) in the field of Natural Language Processing(NLP). These models such as OpenAI's GPT-3.5/4, Llama from Meta, have demonstrated unprecedented capabilities in understanding and generating human-like text. This has led to a paradigm shift in the realm of personalized and explainable recommendations, as LLMs offer a versatile toolset for processing vast amounts of textual data to enhance user experiences. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey aims to analyze how RS can benefit from LLM-based methodologies. Furthermore, we describe major challenges in Personalized Explanation Generating(PEG) tasks, which are cold-start problems, unfairness and bias problems in RS.",10.48550/arXiv.2311.12338,"['JournalArticle', 'Review']",Computer Science
Targeted Data Generation: Finding and Fixing Model Weaknesses,2023,"Zexue He, Marco Tulio Ribeiro, Fereshte Khani","Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. We propose Targeted Data Generation (TDG), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (LLMs) with a human in the loop. TDG estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within-group performance without hurting overall performance. In our experiments, TDG significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy.",10.48550/arXiv.2305.17804,"['JournalArticle', 'Conference']",Computer Science
Automated Red Teaming with GOAT: the Generative Offensive Agent Tester,2024,"Maya Pavlova, Erik Brinkman, Krithika Iyer, Vtor Albiero, Joanna Bitton, Hailey Nguyen, Joe Li, Cristian Canton-Ferrer, Ivan Evtimov, Aaron Grattafiori","Red teaming assesses how large language models (LLMs) can produce content that violates norms, policies, and rules set during their safety training. However, most existing automated methods in the literature are not representative of the way humans tend to interact with AI models. Common users of AI models may not have advanced knowledge of adversarial machine learning methods or access to model internals, and they do not spend a lot of time crafting a single highly effective adversarial prompt. Instead, they are likely to make use of techniques commonly shared online and exploit the multiturn conversational nature of LLMs. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general-purpose model in a way that encourages reasoning through the choices of methods available, the current target model's response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama 3.1 and 88% against GPT-4 on the JailbreakBench dataset.",10.48550/arXiv.2410.01606,['JournalArticle'],Computer Science
Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting,2023,"Chenkai Sun, Jinning Li, Y. Fung, Hou Pong Chan, Tarek F. Abdelzaher, ChengXiang Zhai, Heng Ji","Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework's capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.",10.48550/arXiv.2310.13297,"['JournalArticle', 'Conference']",Computer Science
A Gold Standard Dataset for the Reviewer Assignment Problem,2023,"Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah","Many peer-review venues are using algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the""similarity score""-- a numerical estimate of the expertise of a reviewer in reviewing a paper -- and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of publicly available gold-standard data. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. Using our dataset, we compare several widely used similarity algorithms and offer key insights. First, all algorithms exhibit significant error, with misranking rates between 12%-30% in easier cases and 36%-43% in harder ones. Second, most specialized algorithms are designed to work with titles and abstracts of papers, and in this regime the SPECTER2 algorithm performs best. Interestingly, classical TF-IDF matches SPECTER2 in accuracy when given access to full submission texts. In contrast, off-the-shelf LLMs lag behind specialized approaches.",10.48550/arXiv.2303.16750,"['JournalArticle', 'Review']",Computer Science
"A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model",2022,"Imad Lakim, Ebtesam Almazrouei, I. Abualhaol, M. Debbah, Julien Launay","As ever larger language models grow more ubiquitous, it is crucial to consider their environmental impact. Characterised by extreme size and resource use, recent generations of models have been criticised for their voracious appetite for compute, and thus significant carbon footprint. Although reporting of carbon impact has grown more common in machine learning papers, this reporting is usually limited to compute resources used strictly for training. In this work, we propose a holistic assessment of the footprint of an extreme-scale language model, Noor. Noor is an ongoing project aiming to develop the largest multi-task Arabic language modelswith up to 13B parametersleveraging zero-shot generalisation to enable a wide range of downstream tasks via natural language instructions. We assess the total carbon bill of the entire project: starting with data collection and storage costs, including research and development budgets, pretraining costs, future serving estimates, and other exogenous costs necessary for this international cooperation. Notably, we find that inference costs and exogenous factors can have a significant impact on total budget. Finally, we discuss pathways to reduce the carbon footprint of extreme-scale models.",10.18653/v1/2022.bigscience-1.8,nan,Computer Science; Environmental Science
Unpacking Large Language Models with Conceptual Consistency,2022,"Pritish Sahu, Michael Cogswell, Yunye Gong, Ajay Divakaran","If a Large Language Model (LLM) answers""yes""to the question""Are mountains tall?""then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.",10.48550/arXiv.2209.15093,['JournalArticle'],Computer Science
LLM Generated Persona is a Promise with a Catch,2025,"Ang Li, Haozhe Chen, Hongseok Namkoong, Tianyi Peng","The use of large language models (LLMs) to simulate human behavior has gained significant attention, particularly through personas that approximate individual characteristics. Persona-based simulations hold promise for transforming disciplines that rely on population-level feedback, including social science, economic analysis, marketing research, and business operations. Traditional methods to collect realistic persona data face significant challenges. They are prohibitively expensive and logistically challenging due to privacy constraints, and often fail to capture multi-dimensional attributes, particularly subjective qualities. Consequently, synthetic persona generation with LLMs offers a scalable, cost-effective alternative. However, current approaches rely on ad hoc and heuristic generation techniques that do not guarantee methodological rigor or simulation precision, resulting in systematic biases in downstream tasks. Through extensive large-scale experiments including presidential election forecasts and general opinion surveys of the U.S. population, we reveal that these biases can lead to significant deviations from real-world outcomes. Our findings underscore the need to develop a rigorous science of persona generation and outline the methodological innovations, organizational and institutional support, and empirical foundations required to enhance the reliability and scalability of LLM-driven persona simulations. To support further research and development in this area, we have open-sourced approximately one million generated personas, available for public access and analysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.",10.48550/arXiv.2503.16527,"['JournalArticle', 'Review']",Business; Computer Science; Economics
"Exploring DeepSeek: A Survey on Advances, Applications, Challenges and Future Directions",2025,"Zehang Deng, Wanlun Ma, QingLong Han, Wei Zhou, Xiaogang Zhu, Sheng Wen, Yang Xiang","The rapid advancement of large models has led to the development of increasingly sophisticated models capable of generating diverse, personalized, and high-quality content. Among these, DeepSeek has emerged as a pivotal open-source initiative, demonstrating high performance at significantly lower computation costs compared to closed-source counterparts. This survey provides a comprehensive overview of the DeepSeek family of models, including DeepSeek-V3 and DeepSeek-R1, covering their core innovations in architecture, system pipeline, algorithm, and infrastructure. We explore their practical applications across various domains, such as healthcare, finance, and education, highlighting their impact on both industry and society. Further-more, we examine potential security, privacy, and ethical concerns arising from the widespread deployment of these models, emphasizing the need for responsible AI development. Finally, we outline future research directions to enhance the performance, safety, and scalability of DeepSeek models, aiming to foster further advancements in the open-source large model community.",10.1109/JAS.2025.125498,"['JournalArticle', 'Review']",Computer Science
A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice,2024,"Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, F. Navah, Amirmohammad Kazemeini","Recommender Systems (RS) play an integral role in enhancing user experiences by providing personalized item suggestions. This survey reviews the progress in RS inclusively from 2017 to 2024, effectively connecting theoretical advances with practical applications. We explore the development from traditional RS techniques like content-based and collaborative filtering to advanced methods involving deep learning, graph-based models, reinforcement learning, and large language models. We also discuss specialized systems such as context-aware, review-based, and fairness-aware RS. The primary goal of this survey is to bridge theory with practice. It addresses challenges across various sectors, including e-commerce, healthcare, and finance, emphasizing the need for scalable, real-time, and trustworthy solutions. Through this survey, we promote stronger partnerships between academic research and industry practices. The insights offered by this survey aim to guide industry professionals in optimizing RS deployment and to inspire future research directions, especially in addressing emerging technological and societal trends\footnote. The survey resources are available in the public GitHub repository https://github.com/VectorInstitute/Recommender-Systems-Survey. (Recommender systems, large language models, chatgpt, responsible AI)",10.48550/arXiv.2407.13699,"['JournalArticle', 'Review']",Computer Science
SurrealDriver: Designing LLM-powered Generative Driver Agent Framework based on Human Drivers Driving-thinking Data,2023,"Ye Jin, Ruoxuan Yang, Zhijie Yi, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, Jiangtao Gong","Leveraging advanced reasoning capabilities and extensive world knowledge of large language models (LLMs) to construct generative agents for solving complex real-world problems is a major trend. However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks. In this paper, we introduce a framework for building human-like generative driving agents using post-driving self-report driving-thinking data from human drivers as both demonstration and feedback. To capture high-quality, natural language data from drivers, we conducted urban driving experiments, recording drivers verbalized thoughts under various conditions to serve as chain-of-thought prompts and demonstration examples for the LLM-Agent. The frameworks effectiveness was evaluated through simulations and human assessments. Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04% and increased human likeness by 50% compared to a baseline LLM-based agent. Our study provides insights into using natural language-based human demonstration data for embodied tasks. The driving-thinking dataset is available at https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset.",10.1109/IROS58592.2024.10802229,"['JournalArticle', 'Conference']",Computer Science; Engineering
Investigating Large Language Models Perception of Emotion Using Appraisal Theory,2023,"Nutchanon Yongsatianchot, P. Torshizi, S. Marsella","Large Language Models (LLM) like ChatGPT have significantly advanced in recent years and are now being used by the general public. As more people interact with these systems, improving our understanding of these black box models is crucial, especially regarding their understanding of human psychological aspects. In this work, we investigate their emotion perception through the lens of appraisal and coping theory using the Stress and Coping Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting of multiple stories that evolve over time and differ in key appraisal variables such as controllability and changeability. We applied SCPQ to three recent LLMs from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with predictions from the appraisal theory and human data. The results show that LLMs responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked. This work adds to the growing literature evaluating the psychological aspects of LLMs and helps enrich our understanding of the current models.",10.1109/ACIIW59127.2023.10388194,"['JournalArticle', 'Conference']",Computer Science; Psychology
Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model,2024,"Kuan-Chih Huang, Xiangtai Li, Lu Qi, Shuicheng Yan, Ming-Hsuan Yang","Recent advancements in multimodal large language models (LLMs) have demonstrated significant potential across various domains, particularly in concept reasoning. However, their applications in understanding 3D environments remain limited, primarily offering textual or numerical outputs without generating dense, informative segmentation masks. This paper introduces Reason3D, a novel LLM designed for comprehensive 3D understanding. Reason3D processes point cloud data and text prompts to produce textual responses and segmentation masks, enabling advanced tasks such as 3D reasoning segmentation, hierarchical searching, express referring, and question answering with detailed mask outputs. We propose a hierarchical mask decoder that employs a coarse-to-fine approach to segment objects within expansive scenes. It begins with a coarse location estimation, followed by object mask estimation, using two unique tokens predicted by LLMs based on the textual query. Experimental results on large-scale ScanNet and Matterport3D datasets validate the effectiveness of our Reason3D across various tasks.",10.1109/3DV66043.2025.00112,"['JournalArticle', 'Conference']",Computer Science
Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering,2024,"Zaid Khan, Yun Fu","The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts. Existing approaches to selective prediction typically require access to the internals of a model, require retraining a model or study only unimodal models. However, the most powerful models (e.g. GPT-4) are typically only available as black boxes with inaccessible internals, are not retrainable by end-users, and are frequently used for multimodal tasks. We study the possi-bility of selective prediction for vision-language models in a realistic, black-box setting. We propose using the principle of neighborhood consistency to identify unreliable responses from a black-box vision-language model in question answering tasks. We hypothesize that given only a visual question and model response, the consistency of the model's responses over the neighborhood of a visual question will indicate re-liability. It is impossible to directly sample neighbors in feature space in a black-box setting. Instead, we show that it is possible to use a smaller proxy model to approximately sample from the neighborhood. We find that neighborhood consistency can be used to identify model responses to vi-sual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model.",10.1109/CVPR52733.2024.01032,"['JournalArticle', 'Conference']",Computer Science
Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?,2024,"Suqing Liu, Zezhu Yu, Feiran Huang, Yousef Bulbulia, Andi Bergen, Michael Liut","Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system. To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s). We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.",10.1145/3649217.3653554,"['Book', 'JournalArticle', 'Conference']",Computer Science
AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation,2023,"Filippo Perrina, Francesco Marchiori, Mauro Conti, Nino Vincenzo Verde","Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk management strategies. As the volume of CTI reports continues to surge, the demand for automated tools to streamline report generation becomes increasingly apparent. While Natural Language Processing techniques have shown potential in handling text data, they often struggle to address the complexity of diverse data sources and their intricate interrelationships. Moreover, established paradigms like STIX have emerged as de facto standards within the CTI community, emphasizing the formal categorization of entities and relations to facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic Generation of Intelligence Reports), a transformative Natural Language Generation tool specifically designed to address the pressing challenges in the realm of CTI reporting. AGIRs primary objective is to empower security analysts by automating the labor-intensive task of generating comprehensive intelligence reports from formal representations of entity graphs. AGIR utilizes a two-stage pipeline by combining the advantages of template-based approaches and the capabilities of Large Language Models such as ChatGPT. We evaluate AGIRs report generation capabilities both quantitatively and qualitatively. The generated reports accurately convey information expressed through formal language, achieving a high recall value (0.99) without introducing hallucination. Furthermore, we compare the fluency and utility of the reports with state-of-the-art approaches, showing how AGIR achieves higher scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires. By using our tool, we estimate that the report writing time is reduced by more than 40%, therefore streamlining the CTI production of any organization and contributing to the automation of several CTI tasks.",10.1109/BigData59044.2023.10386116,"['JournalArticle', 'Conference']",Computer Science
A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA,2025,"Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Yuchao Wu, Yuzhe Ma, Zhiyao Xie","Artificial intelligence (AI)-driven electronic design automation (EDA) techniques have been extensively explored for VLSI circuit design applications. Most recently, foundation AI models for circuits have emerged as a new technology trend. Unlike traditional task-specific AI solutions, these new AI models are developed through two stages: 1) self-supervised pre-training on a large amount of unlabeled data to learn intrinsic circuit properties; and 2) efficient fine-tuning for specific downstream applications, such as early-stage design quality evaluation, circuit-related context generation, and functional verification. This new paradigm brings many advantages: model generalization, less reliance on labeled circuit data, efficient adaptation to new tasks, and unprecedented generative capability. In this paper, we propose referring to AI models developed with this new paradigm as circuit foundation models (CFMs). This paper provides a comprehensive survey of the latest progress in circuit foundation models, unprecedentedly covering over 130 relevant works. Over 90% of our introduced works were published in or after 2022, indicating that this emerging research trend has attracted wide attention in a short period. In this survey, we propose to categorize all existing circuit foundation models into two primary types: 1) encoder-based methods performing general circuit representation learning for predictive tasks; and 2) decoder-based methods leveraging large language models (LLMs) for generative tasks. For our introduced works, we cover their input modalities, model architecture, pre-training strategies, domain adaptation techniques, and downstream design applications. In addition, this paper discussed the unique properties of circuits from the data perspective. These circuit properties have motivated many works in this domain and differentiated them from general AI techniques.",10.48550/arXiv.2504.03711,"['JournalArticle', 'Review']",Computer Science; Engineering
Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective,2024,"Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, Li Bing","Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at \url{https://github.com/DAMO-NLP-SG/DiGIT}.",10.48550/arXiv.2410.12490,['JournalArticle'],Computer Science
Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations,2025,"Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Rottger, Daniel Hershcovich","Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.",10.48550/arXiv.2502.07068,"['JournalArticle', 'Conference', 'Review']",Computer Science; Sociology
WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning,2024,"Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, Ziwei Liu","Multimodal information, together with our knowledge, help us to understand the complex and dynamic world. Large language models (LLM) and large multimodal models (LMM), however, still struggle to emulate this capability. In this paper, we present WorldQA, a video understanding dataset designed to push the boundaries of multimodal world models with three appealing properties: (1) Multimodal Inputs: The dataset comprises 1007 question-answer pairs and 303 videos, necessitating the analysis of both auditory and visual data for successful interpretation. (2) World Knowledge: We identify five essential types of world knowledge for question formulation. This approach challenges models to extend their capabilities beyond mere perception. (3) Long-Chain Reasoning: Our dataset introduces an average reasoning step of 4.45, notably surpassing other videoQA datasets. Furthermore, we introduce WorldRetriever, an agent designed to synthesize expert knowledge into a coherent reasoning chain, thereby facilitating accurate responses to WorldQA queries. Extensive evaluations of 13 prominent LLMs and LMMs reveal that WorldRetriever, although being the most effective model, achieved only 70% of humanlevel performance in multiple-choice questions. This finding highlights the necessity for further advancement in the reasoning and comprehension abilities of models. Our experiments also yield several key insights. For instance, while humans tend to perform better with increased frames, current LMMs, including WorldRetriever, show diminished performance under similar conditions. We hope that WorldQA,our methodology, and these insights could contribute to the future development of multimodal world models.",10.48550/arXiv.2405.03272,['JournalArticle'],Computer Science
Personality Alignment of Large Language Models,2024,"Minjun Zhu, Linyi Yang, Yue Zhang","Aligning large language models (LLMs) typically aim to reflect general human values and behaviors, but they often fail to capture the unique characteristics and preferences of individual users. To address this gap, we introduce the concept of Personality Alignment. This approach tailors LLMs' responses and decisions to match the specific preferences of individual users or closely related groups. Inspired by psychometrics, we created the Personality Alignment with Personality Inventories (PAPI) dataset, which includes data from over 320,000 real subjects across multiple personality assessments, including both the Big Five Personality Factors and Dark Triad traits. This comprehensive dataset enables quantitative evaluation of LLMs' alignment capabilities across both positive and potentially problematic personality dimensions. Recognizing the challenges of personality alignments, such as limited personal data, diverse preferences, and scalability requirements, we developed an activation intervention optimization method. This method enhances LLMs' ability to efficiently align with individual behavioral preferences using minimal data and computational resources. Remarkably, our method, PAS, achieves superior performance while requiring only 1/5 of the optimization time compared to DPO, offering practical value for personality alignment. Our work paves the way for future AI systems to make decisions and reason in truly personality ways, enhancing the relevance and meaning of AI interactions for each user and advancing human-centered artificial intelligence. The dataset and code are released at https://github.com/zhu-minjun/PAlign.",10.48550/arXiv.2408.11779,['JournalArticle'],Computer Science; Psychology
Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles,2024,"Xuchuan Li, Fei Huang, Jianrong Lv, Zhixiong Xiao, Guolong Li, Yang Yue","Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations of mobility; and based on the patterns and the recursive reasoning process, MobAgent finally generates more authentic and personalized mobilities that reflect both individual differences and real-world constraints. We validate our framework with 0.2 million travel survey data, demonstrating its effectiveness in producing personalized and accurate travel diaries. This study highlights the capacity of LLMs to provide detailed and sophisticated understanding of human mobility through the real-world mobility data.",10.48550/arXiv.2407.18932,"['JournalArticle', 'Review']",Computer Science
Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models,2024,"Chenyang Lyu, Minghao Wu, Alham Fikri Aji","Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.",10.48550/arXiv.2402.13887,['JournalArticle'],Computer Science; Linguistics
20.5 C-Transformer: A 2.6-18.1J/Token Homogeneous DNN-Transformer/Spiking-Transformer Processor with Big-Little Network and Implicit Weight Generation for Large Language Models,2024,"Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Soyeon Kim, Seongyon Hong, H.-J. Yoo","Recently, transformer-based large language models (LLMs), shown in Fig. 20.5.1, are widely used, and even on-device LLM systems with real-time responses are anticipated [1]. Many transformer processors [24] enhance energy efficiency by increasing hardware utilization and reducing power consumption, but their system power consumption and response time are still not suitable for mobile devices. Since LLMs, such as GPT-2, have many parameters (400-700M), their External Memory Access (EMA) consumes 68% of the total power. Pruning can increase sparsity to alleviate the EMA bottleneck, but can be applied only to a simple task (language modeling) to predict the next word [5]. For high-level tasks such as translation, question answering, and summarization, it is impossible to achieve high sparsity (<~40%) [6].",10.1109/ISSCC49657.2024.10454330,"['JournalArticle', 'Conference']",Computer Science; Engineering
"Attitude Towards AI: Potential Influence of Conspiracy Belief, XAI Experience and Locus of Control",2024,"A. Babiker, S. Alshakhsi, Dena Al-Thani, C. Montag, Raian Ali","Abstract The proliferation of Artificial Intelligence (AI) technologies, exemplified by Large Language Models (LLM), has ushered in a transformative era across various fields. As the AI revolution will impact societies in complex and uncertain ways, it is likely that persons tending towards belief of conspiracy theories also tend to form more negative and less positive attitudes towards AI. Such persons might believe that some evil force will use AI to destroy human mankind. Drawing on the Interplay of Modality, Person, Area, Country/Culture, and Transparency categories (IMPACT) framework, this study aims to investigate the interplay of locus of control (LOC), belief in conspiracy theories, and the perception of the importance and availability of eXplainable AI (XAI) on attitudes towards AI (measured via AI acceptance and fear). The study used an online survey with 281 participants from the UK and 281 from the Arab world. Statistical analysis revealed that in the UK but not in the Arab sample, female participants reported higher fear of AI and lower acceptance of AI compared to males. The regression results consistently confirmed the role of internal LOC, perceived XAI importance, and perceived availability of XAI in fostering AI acceptance, as well as the role of belief in conspiracy theories, external LOC, and perceiving availability of XAI as being low in increasing fear of AI. The perceived availability of XAI emerges as a crucial influencing factor; addressing it appropriately could enhance societal awareness and acceptance of AI while reducing fear. Personal factors and XAI influence attitudes towards AI in both Arab and UK cultures, enhancing result robustness and revealing nuanced differences.",10.1080/10447318.2024.2401249,"['JournalArticle', 'Review']",Computer Science; Psychology
Driving Style Alignment for LLM-powered Driver Agent,2024,"Ruoxuan Yang, Xinyue Zhang, Anais Fernandez-Laaksonen, Xin Ding, Jiangtao Gong","Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities. However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors. To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment. The frameworks effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into designing driving agents with diverse driving styles. The implementation of the framework 1 and details of the dataset 2 can be found at the link.",10.1109/IROS58592.2024.10802629,"['JournalArticle', 'Conference']",Computer Science; Engineering
A Survey on Natural Language Counterfactual Generation,2024,"Yongjie Wang, Xiaoqi Qiu, Yu Yue, Xu Guo, Zhiwei Zeng, Yuhong Feng, Zhiqi Shen","Natural language counterfactual generation aims to minimally modify a given text such that the modified text will be classified into a different class. The generated counterfactuals provide insight into the reasoning behind a model's predictions by highlighting which words significantly influence the outcomes. Additionally, they can be used to detect model fairness issues and augment the training data to enhance the model's robustness. A substantial amount of research has been conducted to generate counterfactuals for various NLP tasks, employing different models and methodologies. With the rapid growth of studies in this field, a systematic review is crucial to guide future researchers and developers. To bridge this gap, this survey provides a comprehensive overview of textual counterfactual generation methods, particularly those based on Large Language Models. We propose a new taxonomy that systematically categorizes the generation methods into four groups and summarizes the metrics for evaluating the generation quality. Finally, we discuss ongoing research challenges and outline promising directions for future work.",10.48550/arXiv.2407.03993,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design,2025,"Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Laurence Li, Jiri Gesi, Qi He, T. Li, Dakuo Wang","Usability testing is a fundamental yet challenging research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features an LLM Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The system can generate UX study results in qualitative (e.g., interviewing how an agent thinks), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of UX study with LLM Agents1.",10.1145/3706599.3719729,"['JournalArticle', 'Book']",Computer Science
Linear Representations of Political Perspective Emerge in Large Language Models,2025,"Junsol Kim, James Evans, Aaron Schein","Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. We show that LLMs possess linear representations of political perspectives within activation space, wherein more similar perspectives are represented closer together. To do so, we probe the attention heads across the layers of three open transformer-based LLMs (Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from the perspectives of different U.S. lawmakers. We then identify sets of attention heads whose activations linearly predict those lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political ideology. We find that highly predictive heads are primarily located in the middle layers, often speculated to encode high-level concepts and tasks. Using probes only trained to predict lawmakers' ideology, we then show that the same probes can predict measures of news outlets' slant from the activations of models prompted to simulate text from those news outlets. These linear probes allow us to visualize, interpret, and monitor ideological stances implicitly adopted by an LLM as it generates open-ended responses. Finally, we demonstrate that by applying linear interventions to these attention heads, we can steer the model outputs toward a more liberal or conservative stance. Overall, our research suggests that LLMs possess a high-level linear representation of American political ideology and that by leveraging recent advances in mechanistic interpretability, we can identify, monitor, and steer the subjective perspective underlying generated text.",10.48550/arXiv.2503.02080,['JournalArticle'],Computer Science; Political Science
Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping,2024,"Temitope Akinboyewa, H. Ning, M. Lessani, Zhenlong Li","Information on the depth of floodwater is crucial for rapid mapping of areas affected by floods. However, previous approaches for estimating floodwater depth, including field surveys, remote sensing, and machine learning techniques, can be time-consuming and resource-intensive. This paper presents an automated and rapid approach for estimating floodwater depth from on-site flood photos. A pre-trained large multimodal model, Generative pre-trained transformers (GPT-4) Vision, was used specifically for estimating floodwater. The input data were flood photos that contained referenced objects, such as street signs, cars, people, and buildings. Using the heights of the common objects as references, the model returned the floodwater depth as the output. Results show that the proposed approach can rapidly provide a consistent and reliable estimation of floodwater depth from flood photos. Such rapid estimation is transformative in flood inundation mapping and assessing the severity of the flood in near-real time, which is essential for effective flood response strategies.",10.48550/arXiv.2402.16684,"['JournalArticle', 'Review']",Computer Science; Environmental Science
A Survey on LLM-powered Agents for Recommender Systems,2025,"Qiyao Peng, Hongtao Liu, Hua Huang, Qing Yang, Minglai Shao","Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.",10.48550/arXiv.2502.10050,"['JournalArticle', 'Review']",Computer Science
GPT-4 Emulates Average-Human Emotional Cognition from a Third-Person Perspective,2024,"Ala Nekouvaght Tak, Jonathan Gratch","This paper extends recent investigations on the emotional reasoning abilities of Large Language Models (LLMs). Current research on LLMs has not directly evaluated the distinction between how LLMs predict the self-attribution of emotions and the perception of others' emotions. We first look at carefully crafted emotion-evoking stimuli, originally designed to find patterns of brain neural activity representing fine-grained inferred emotional attributions of others. We show that GPT-4 is especially accurate in reasoning about such stimuli. This suggests LLMs agree with humans' attributions of others' emotions in stereotypical scenarios remarkably more than self-attributions of emotions in idiosyncratic situations. To further explore this, our second study utilizes a dataset containing annotations from both the author and a third-person perspective. We find that GPT-4's interpretations align more closely with human judgments about the emotions of others than with self-assessments. Notably, conventional computational models of emotion primarily rely on self-reported ground truth as the gold standard. However, an average observer's standpoint, which LLMs appear to have adopted, might be more relevant for many downstream applications, at least in the absence of individual information and adequate safety considerations.",10.1109/ACII63134.2024.00043,"['JournalArticle', 'Conference']",Computer Science; Psychology
Grow with Your AI Buddy: Designing an LLMs-based Conversational Agent for the Measurement and Cultivation of Children's Mental Resilience,2024,"Zihui Hu, Hanchao Hou, Shiguang Ni","Psychological resilience refers to an individual's ability to adapt to adversity and stress. Education on psychological resilience during childhood can contribute to future mental health and well-being, such as reducing anxiety and depression [1] [2]. However, traditional psychosocial resilience training faces challenges with accessibility, heavily constrained by cost and spatiotemporal limitations. Recently, emerging large language models (LLMs) have demonstrated exceptional capabilities in conversational tasks, indicating new prospects for cultivating children's psychological resilience. In our work, 1) we conducted qualitative interviews with 10 Chinese children (aged 8-12) and their parents to understand their needs and current conditions; 2) based on the interview results and theories of psychological resilience, we summarized three pathways for developing children's psychological resilience using conversational agents (CAs) and identified six key challenges for designing child-centered CAs; 3) we designed and developed a web prototype using optimized LLMs (see Figure 1), which integrates personal and social support factors, to measure and foster children's psychological resilience through conversations; and 4) we invited 48 child volunteers in user testing and designed three sets of experiments to evaluate the effectiveness of system interventions, the effectiveness of measurements, and overall acceptability. Results indicate that the intervention tasks actively promoted psychological resilience in adolescents. Intelligent measurement scores were effectively consistent with traditional scales in objective scoring, while subjective evaluations, such as appeal and fun, significantly exceeded traditional scale scores. Through our practice, we show the potential of CAs in enhancing children's mental health and presented a reference application case. Moreover, we have unearthed notable future research issues, including challenges in designing psychologically educational CAs that are persistently attractive to children, combining real-life support factors with CAs, and ethical concerns regarding safety and privacy.",10.1145/3628516.3659399,"['Book', 'JournalArticle']",Computer Science; Psychology
Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents,2025,"Chaoran Chen, Bingsheng Yao, Ruishi Zou, Wenyue Hua, Weimin Lyu, T. Li, Dakuo Wang","Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.",10.48550/arXiv.2502.13012,"['JournalArticle', 'Conference', 'Review']",Computer Science
LLMs with Industrial Lens: Deciphering the Challenges and Prospects - A Survey,2024,"Ashok Urlana, Charaka Vinayak Kumar, Ajeet Kumar Singh, Bala Mallikarjunarao Garlapati, S. Chalamala, Rahul Mishra","Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions. We maintain the Github repository with the most recent papers in the field.",10.48550/arXiv.2402.14558,"['JournalArticle', 'Review']",Business; Computer Science; Engineering
MoralBERT: A Fine-Tuned Language Model for Capturing Moral Values in Social Discussions,2024,"Vjosa Preniqi, Iacopo Ghinassi, Julia Ive, C. Saitis, Kyriaki Kalimeri","Moral values play a fundamental role in how we evaluate information, make decisions, and form judgements around important social issues. Controversial topics, including vaccination, abortion, racism, and sexual orientation, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in Natural Language Processing (NLP) show that moral values can be gauged in human-generated textual content. Building on the Moral Foundations Theory (MFT), this paper introduces MoralBERT, a range of language representation models fine-tuned to capture moral sentiment in social discourse. We describe a framework for both aggregated and domain-adversarial training on multiple heterogeneous MFT human-annotated datasets sourced from Twitter (now X), Reddit, and Facebook that broaden textual content diversity in terms of social media audience interests, content presentation and style, and spreading patterns. We show that the proposed framework achieves an average F1 score that is between 11% and 32% higher than lexicon-based approaches, Word2Vec embeddings, and zero-shot classification with large language models such as GPT-4 for in-domain inference. Domain-adversarial training yields better out-of domain predictions than aggregate training while achieving comparable performance to zero-shot learning. Our approach contributes to annotation-free and effective morality learning, and provides useful insights towards a more comprehensive understanding of moral narratives in controversial social debates using NLP.",10.1145/3677525.3678694,"['Book', 'JournalArticle']",Computer Science; Linguistics; Political Science; Sociology
Self-Consistency of Large Language Models under Ambiguity,2023,"Henning Bartsch, Ole Jorgensen, Domenic Rosati, J. Hoelscher-Obermaier, Jacob Pfau","Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistencye.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a models consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",10.48550/arXiv.2310.13439,['JournalArticle'],Computer Science
"Automatic Dataset Construction (ADC): Sample Collection, Data Curation, and Beyond",2024,"Minghao Liu, Zonglin Di, Jiaheng Wei, Zhongruo Wang, Hengxiang Zhang, Rui Xiao, Haoyu Wang, Jinlong Pang, Hao Chen, Ankit Shah, Hongxin Wei, Xinlei He, Zhaowei Zhao, Haobo Wang, Lei Feng, Jindong Wang, James Davis, Yang Liu","Large-scale data collection is essential for developing personalized training data, mitigating the shortage of training data, and fine-tuning specialized models. However, creating high-quality datasets quickly and accurately remains a challenge due to annotation errors, the substantial time and costs associated with human labor. To address these issues, we propose Automatic Dataset Construction (ADC), an innovative methodology that automates dataset creation with negligible cost and high efficiency. Taking the image classification task as a starting point, ADC leverages LLMs for the detailed class design and code generation to collect relevant samples via search engines, significantly reducing the need for manual annotation and speeding up the data generation process. Despite these advantages, ADC also encounters real-world challenges such as label errors (label noise) and imbalanced data distributions (label bias). We provide open-source software that incorporates existing methods for label error detection, robust learning under noisy and biased data, ensuring a higher-quality training data and more robust model training procedure. Furthermore, we design three benchmark datasets focused on label noise detection, label noise learning, and class-imbalanced learning. These datasets are vital because there are few existing datasets specifically for label noise detection, despite its importance. Finally, we evaluate the performance of existing popular methods on these datasets, thereby facilitating further research in the field.",10.48550/arXiv.2408.11338,['JournalArticle'],Computer Science
Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising,2025,"Haoyang Feng, Yanjun Dai, Yuan Gao","Although large language models have demonstrated the potential for personalized advertising recommendations in experimental environments, in actual operations, how advertising recommendation systems can be combined with measures such as user privacy protection and data security is still an area worthy of in-depth discussion. To this end, this paper studies the personalized risks and regulatory strategies of large language models in digital advertising. This study first outlines the principles of Large Language Model (LLM), especially the self-attention mechanism based on the Transformer architecture, and how to enable the model to understand and generate natural language text. Then, the BERT (Bidirectional Encoder Representations from Transformers) model and the attention mechanism are combined to construct an algorithmic model for personalized advertising recommendations and user factor risk protection. The specific steps include: data collection and preprocessing, feature selection and construction, using large language models such as BERT for advertising semantic embedding, and ad recommendations based on user portraits. Then, local model training and data encryption are used to ensure the security of user privacy and avoid the leakage of personal data. This paper designs an experiment for personalized advertising recommendation based on a large language model of BERT and verifies it with real user data. The experimental results show that BERT-based advertising push can effectively improve the click-through rate and conversion rate of advertisements. At the same time, through local model training and privacy protection mechanisms, the risk of user privacy leakage can be reduced to a certain extent.",10.48550/arXiv.2505.04665,['JournalArticle'],Business; Computer Science
Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions,2025,"Joseph Suh, Erfan Jahanparast, Suhong Moon, Minwoo Kang, Serina Chang","Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.",10.48550/arXiv.2502.16761,"['JournalArticle', 'Conference', 'Review']",Computer Science; Political Science; Sociology
"AI Gender Bias, Disparities, and Fairness: Does Training Data Matter?",2023,"Ehsan Latif, Xiaoming Zhai, Lei Liu","This study delves into the pervasive issue of gender issues in artificial intelligence (AI), specifically within automatic scoring systems for student-written responses. The primary objective is to investigate the presence of gender biases, disparities, and fairness in generally targeted training samples with mixed-gender datasets in AI scoring outcomes. Utilizing a fine-tuned version of BERT and GPT-3.5, this research analyzes more than 1000 human-graded student responses from male and female participants across six assessment items. The study employs three distinct techniques for bias analysis: Scoring accuracy difference to evaluate bias, mean score gaps by gender (MSG) to evaluate disparity, and Equalized Odds (EO) to evaluate fairness. The results indicate that scoring accuracy for mixed-trained models shows an insignificant difference from either male- or female-trained models, suggesting no significant scoring bias. Consistently with both BERT and GPT-3.5, we found that mixed-trained models generated fewer MSG and non-disparate predictions compared to humans. In contrast, compared to humans, gender-specifically trained models yielded larger MSG, indicating that unbalanced training data may create algorithmic models to enlarge gender disparities. The EO analysis suggests that mixed-trained models generated more fairness outcomes compared with gender-specifically trained models. Collectively, the findings suggest that gender-unbalanced data do not necessarily generate scoring bias but can enlarge gender disparities and reduce scoring fairness.",10.48550/arXiv.2312.10833,['JournalArticle'],Computer Science
Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models,2024,"Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang","Large Language Models (LLMs) are employed across various high-stakes domains, where the reliability of their outputs is crucial. One commonly used method to assess the reliability of LLMs' responses is uncertainty estimation, which gauges the likelihood of their answers being correct. While many studies focus on improving the accuracy of uncertainty estimations for LLMs, our research investigates the fragility of uncertainty estimation and explores potential attacks. We demonstrate that an attacker can embed a backdoor in LLMs, which, when activated by a specific trigger in the input, manipulates the model's uncertainty without affecting the final output. Specifically, the proposed backdoor attack method can alter an LLM's output probability distribution, causing the probability distribution to converge towards an attacker-predefined distribution while ensuring that the top-1 prediction remains unchanged. Our experimental results demonstrate that this attack effectively undermines the model's self-evaluation reliability in multiple-choice questions. For instance, we achieved a 100 attack success rate (ASR) across three different triggering strategies in four models. Further, we investigate whether this manipulation generalizes across different prompts and domains. This work highlights a significant threat to the reliability of LLMs and underscores the need for future defenses against such attacks. The code is available at https://github.com/qcznlp/uncertainty_attack.",10.48550/arXiv.2407.11282,['JournalArticle'],Computer Science
Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection,2023,"Chaofan Wang, Samuel Kernan Freire, Mo Zhang, Jing Wei, Jorge Gonalves, V. Kostakos, Z. Sarsenbayeva, Christina Schneegass, A. Bozzon, E. Niforatos","ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses""prompt injection"", such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 93% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.",10.48550/arXiv.2306.08833,"['JournalArticle', 'Review']",Computer Science
Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,2024,"Mucong Ding, Chenghao Deng, Jocelyn Choo, Zichu Wu, Aakriti Agrawal, Avi Schwarzschild, Tianyi Zhou, Tom Goldstein, John Langford, Anima Anandkumar, Furong Huang","While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",10.48550/arXiv.2409.18433,['JournalArticle'],Computer Science; Mathematics
RAG based Question-Answering for Contextual Response Prediction System,2024,"Sriram Veturi, Saurabh Vaichal, Reshma Lal Jagadheesh, Nafis Irtiza Tripto, Nian Yan","Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.",10.48550/arXiv.2409.03708,['JournalArticle'],Computer Science
Benchmarking Bias in Large Language Models during Role-Playing,2024,"Xinyue Li, Zhenpeng Chen, Jie M. Zhang, Yiling Lou, Tianlin Li, Weisong Sun, Yang Liu, Xuanzhe Liu","Large Language Models (LLMs) have become foundational in modern language-driven applications, profoundly influencing daily life. A critical technique in leveraging their potential is role-playing, where LLMs simulate diverse roles to enhance their real-world utility. However, while research has highlighted the presence of social biases in LLM outputs, it remains unclear whether and to what extent these biases emerge during role-playing scenarios. In this paper, we introduce BiasLens, a fairness testing framework designed to systematically expose biases in LLMs during role-playing. Our approach uses LLMs to generate 550 social roles across a comprehensive set of 11 demographic attributes, producing 33,000 role-specific questions targeting various forms of bias. These questions, spanning Yes/No, multiple-choice, and open-ended formats, are designed to prompt LLMs to adopt specific roles and respond accordingly. We employ a combination of rule-based and LLM-based strategies to identify biased responses, rigorously validated through human evaluation. Using the generated questions as the benchmark, we conduct extensive evaluations of six advanced LLMs released by OpenAI, Mistral AI, Meta, Alibaba, and DeepSeek. Our benchmark reveals 72,716 biased responses across the studied LLMs, with individual models yielding between 7,754 and 16,963 biased responses, underscoring the prevalence of bias in role-playing contexts. To support future research, we have publicly released the benchmark, along with all scripts and experimental results.",10.48550/arXiv.2411.00585,['JournalArticle'],Computer Science; Linguistics
When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models,2024,"Yanhong Li, Chenghao Yang, Allyson Ettinger","Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.",10.48550/arXiv.2404.09129,['JournalArticle'],Computer Science; Linguistics
Shortcut Learning Explanations for Deep Natural Language Processing: A Survey on Dataset Biases,2024,"Varun Dogra, Sahil Verma, .. Kavita, Marcin Woniak, Jana Shafi, Muhammad Fazal Ijaz","The introduction of pre-trained large language models (LLMs) has transformed NLP by fine-tuning task-specific datasets, enabling notable advancements in news classification, language translation, and sentiment analysis. This has revolutionized the field, driving remarkable breakthroughs and progress. However, the growing recognition of bias in textual data has emerged as a critical focus in the NLP community, revealing the inherent limitations of models trained on specific datasets. LLMs exploit these dataset biases and artifacts as expedient shortcuts for prediction. The reliance of LLMs on dataset bias and artifacts as shortcuts for prediction has hindered their generalizability and adversarial robustness. Addressing this issue is crucial to enhance the reliability and resilience of LLMs in various contexts. This survey provides a comprehensive overview of the rapidly growing body of research on shortcut learning in language models, classifying the research into four main areas: the factors of shortcut learning, the origin of bias, the detection methods of dataset biases, and understanding mitigation strategies to address data biases. The goal of this study is to offer a contextualized, in-depth look at the state of learning models, highlighting the major areas of attention and suggesting possible directions for further research.",10.1109/ACCESS.2024.3360306,"['JournalArticle', 'Review']",Computer Science; Linguistics
On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept,2024,"Guang-Da Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, K. Johnson, Jiliang Tang, Rongrong Wang","Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only the task's goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. In this paper, we unveil that intrinsic self-correction can be progressively improved, allowing it to approach a converged state. Our findings are verified in: (1) the scenario of multi-round question answering, by comprehensively demonstrating that intrinsic self-correction can progressively introduce performance gains through iterative interactions, ultimately converging to stable performance; and (2) the context of intrinsic self-correction for enhanced morality, in which we provide empirical evidence that iteratively applying instructions reduces model uncertainty towards convergence, which then leads to convergence of both the calibration error and self-correction performance, ultimately resulting in a stable state of intrinsic self-correction. Furthermore, we introduce a mathematical formulation and a simulation task indicating that the latent concepts activated by self-correction instructions drive the reduction of model uncertainty. Based on our experimental results and analysis of the convergence of intrinsic self-correction, we reveal its underlying mechanism: consistent injected instructions reduce model uncertainty which yields converged, improved performance.",10.48550/arXiv.2406.02378,['JournalArticle'],Computer Science; Linguistics
A Survey on Human-Centric LLMs,2024,"Jing Yi Wang, Nicholas Sukiennik, Tong Li, Weikang Su, Qianyue Hao, Jingbo Xu, Zihan Huang, Fengli Xu, Yong Li","The rapid evolution of large language models (LLMs) and their capacity to simulate human cognition and behavior has given rise to LLM-based frameworks and tools that are evaluated and applied based on their ability to perform tasks traditionally performed by humans, namely those involving cognition, decision-making, and social interaction. This survey provides a comprehensive examination of such human-centric LLM capabilities, focusing on their performance in both individual tasks (where an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics). We first evaluate LLM competencies across key areas including reasoning, perception, and social cognition, comparing their abilities to human-like skills. Then, we explore real-world applications of LLMs in human-centric domains such as behavioral science, political science, and sociology, assessing their effectiveness in replicating human behaviors and interactions. Finally, we identify challenges and future research directions, such as improving LLM adaptability, emotional intelligence, and cultural sensitivity, while addressing inherent biases and enhancing frameworks for human-AI collaboration. This survey aims to provide a foundational understanding of LLMs from a human-centric perspective, offering insights into their current capabilities and potential for future development.",10.48550/arXiv.2411.14491,"['JournalArticle', 'Review']",Computer Science; Linguistics; Psychology
Personalized Multimodal Large Language Models: A Survey,2024,"Junda Wu, Hanjia Lyu, Yu Xia, Zhehao Zhang, Joe Barrow, Ishita Kumar, Mehrnoosh Mirtaheri, Hongjie Chen, Ryan A. Rossi, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Namyong Park, Sungchul Kim, Huanrui Yang, Subrata Mitra, Zhengmian Hu, Nedim Lipka, Dang Nguyen, Yue Zhao, Jiebo Luo, Julian J. McAuley","Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.",10.48550/arXiv.2412.02142,"['JournalArticle', 'Review']",Computer Science; Linguistics
Mitigating Hallucination in Fictional Character Role-Play,2024,"Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley","Role-playing has wide-ranging applications in customer support, embodied agents, and computational social science. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and to hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with over 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset are available at https://github.com/NafisSadeq/rolefact.git.",10.48550/arXiv.2406.17260,"['JournalArticle', 'Conference']",Computer Science; Psychology
MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation,2025,"Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chunmei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao","Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.",10.48550/arXiv.2502.11903,"['JournalArticle', 'Conference']",Computer Science; Linguistics
"""Mango Mango, How to Let The Lettuce Dry Without A Spinner?"": Exploring User Perceptions of Using An LLM-Based Conversational Assistant Toward Cooking Partner",2023,"Szeyi Chan, Jiachen Li, Bingsheng Yao, Amama Mahmood, Chien-Ming Huang, Holly Jimison, Elizabeth D. Mynatt, Dakuo Wang","
 The rapid advancement of Large Language Models (LLMs) has created numerous potentials for integration with conversational assistants (CAs) assisting people in their daily tasks, particularly due to their extensive flexibility. However, users' real-world experiences interacting with these assistants remain unexplored. In this research, we chose cooking, a complex daily task, as a scenario to explore people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA,
 Mango Mango
 . We discovered that participants value the system's ability to offer customized instructions based on context, provide extensive information beyond the recipe, and assist them in dynamic task planning. However, users expect the system to be more adaptive to oral conversation and provide more suggestive responses to keep them actively involved. Recognizing that users began treating our LLM-CA as a personal assistant or even a partner rather than just a recipe-reading tool, we propose five design considerations for future development.
",10.1145/3757442,['JournalArticle'],Computer Science
CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,2023,"Seungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, Jinyoung Yeo","Chain-of-thought (CoT) prompting enables large language models (LLMs) to solve complex reasoning tasks by generating an explanation before the final prediction. Despite its promising ability, a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation. To improve the correctness of the explanations, fine-tuning language models with explanation data is needed. However, there exists only a few datasets that can be used for such approaches, and no data collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for annotating the factual correctness of generated explanations and collecting revision data of wrong explanations. Furthermore, we suggest several use cases where the data collected with CoTEVer can be utilized for enhancing the faithfulness of explanations. Our toolkit is publicly available at https://github.com/SeungoneKim/CoTEVer.",10.48550/arXiv.2303.03628,"['JournalArticle', 'Conference']",Computer Science
Text-to-Image Synthesis: A Decade Survey,2024,"Nonghai Zhang, Hao Tang","When humans read a specific text, they often visualize the corresponding images, and we hope that computers can do the same. Text-to-image synthesis (T2I), which focuses on generating high-quality images from textual descriptions, has become a significant aspect of Artificial Intelligence Generated Content (AIGC) and a transformative direction in artificial intelligence research. Foundation models play a crucial role in T2I. In this survey, we review over 440 recent works on T2I. We start by briefly introducing how GANs, autoregressive models, and diffusion models have been used for image generation. Building on this foundation, we discuss the development of these models for T2I, focusing on their generative capabilities and diversity when conditioned on text. We also explore cutting-edge research on various aspects of T2I, including performance, controllability, personalized generation, safety concerns, and consistency in content and spatial relationships. Furthermore, we summarize the datasets and evaluation metrics commonly used in T2I research. Finally, we discuss the potential applications of T2I within AIGC, along with the challenges and future research opportunities in this field.",10.48550/arXiv.2411.16164,"['JournalArticle', 'Review']",Computer Science
Investigating Bias Representations in Llama 2 Chat via Activation Steering,2024,"Dawn Lu, Nina Rimsky","We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases, which raises questions about the model's nuanced understanding of different forms of bias. This work also provides valuable insights into effective red-teaming strategies for LLMs using activation steering, particularly emphasizing the importance of integrating a refusal vector.",10.48550/arXiv.2402.00402,['JournalArticle'],Computer Science; Linguistics
Deep Learning in ChatGPT - A Survey,2023,"A. M. J. Hashana, P. Brundha, Mohamed Uvaze, A. Ayoobkhan, Mohamed Uvaze Ahamed","Abstract-As a subset of machine learning, deep learning makes use of multiple-layer neural networks to learn with available data and make decisions or predictions. A large language model called ChatGPT is based on deep learning, specifically a type of neural network called a transformer. ChatGPT's transformer architecture uses attention mechanisms to focus on the most important parts of the input, allowing it to process and comprehend a large amount of text data. In order for the model to comprehend the context and meaning of natural language text, it is trained on a huge database of text, including articles and books. One of the main importance of using deep learning in ChatGPT is its intelligence to understand relationships and patterns from the input text and generate or predict new text that is homogeneous to the input/training data. Because of this, ChatGPT is able to respond to questions and prompts in a manner that is comparable to that of a human, making it useful for a wide scope of natural language processing missions like translating languages, summarizing texts, and responding to questions. It's worth noting that, while deep learning has been highly effective in ChatGPT, it is not without its limitations. To train, deep learning models can be very complex and require a lot of data and computing power.",10.1109/ICOEI56765.2023.10125852,"['Conference', 'Review']",Computer Science; Linguistics
"Large Language Models and Sentiment Analysis in Financial Markets: A Review, Datasets, and Case Study",2024,"Chenghao Liu, Arunkumar Arulappan, R. Naha, Aniket Mahanti, J. Kamruzzaman, In-Ho Ra","This paper comprehensively examines Large Language Models (LLMs) in sentiment analysis, specifically focusing on financial markets and exploring the correlation between news sentiment and Bitcoin prices. We systematically categorize various LLMs used in financial sentiment analysis, highlighting their unique applications and features. We also investigate the methodologies for effective data collection and categorization, underscoring the need for diverse and comprehensive datasets. Our research features a case study investigating the correlation between news sentiment and Bitcoin prices, utilizing advanced sentiment analysis and financial analysis methods to demonstrate the practical application of LLMs. The findings reveal a modest but discernible correlation between news sentiment and Bitcoin price fluctuations, with historical news patterns showing a more substantial impact on Bitcoins longer-term price than immediate news events. This highlights LLMs potential in market trend prediction and informed investment decision-making.",10.1109/ACCESS.2024.3445413,"['JournalArticle', 'Review']",Business; Computer Science; Economics
Eliciting Personality Traits in Large Language Models,2024,"Airlie Hilliard, Cristian Munoz, Zekun Wu, A. Koshiyama","Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these""black-box""models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompted multiple LMs with different parameter sizes, including Llama-2, Falcon, Mistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined their personality using classifiers trained on the myPersonality dataset. Our results reveal that, generally, all LLMs demonstrate high openness and low extraversion. However, whereas LMs with fewer parameters exhibit similar behaviour in personality traits, newer and LMs with more parameters exhibit a broader range of personality traits, with increased agreeableness, emotional stability, and openness. Furthermore, a greater number of parameters is positively associated with openness and conscientiousness. Moreover, fine-tuned models exhibit minor modulations in their personality traits, contingent on the dataset. Implications and directions for future research are discussed.",10.48550/arXiv.2402.08341,['JournalArticle'],Computer Science; Psychology
Eternagram: Probing Player Attitudes Towards Climate Change Using a ChatGPT-driven Text-based Adventure,2024,"Suifang Zhou, Latisha Besariani Hendra, Qinshi Zhang, Jussi Holopainen, Ray Lc","Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like Openness and Agreeableness, and a preference for democratic values.",10.1145/3613904.3642850,"['Book', 'JournalArticle', 'Conference', 'Review']",Computer Science; Environmental Science
IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering,2024,"Ruosen Li, Barry Wang, Ruochen Li, Xinya Du","To evaluate Large Language Models (LLMs) for question answering (QA), traditional methods typically focus on assessing single-turn responses to given questions. However, this approach doesn't capture the dynamic nature of human-AI interactions, where humans actively seek information through conversation. Recent works in human-computer interaction (HCI) have employed human evaluators to conduct interactions and evaluations, but they are often prohibitively expensive and time-consuming to scale. We introduce an automatic evaluation framework IQA-EVAL to achieve Interactive Question Answering Evaluations, more specifically, we introduce a LLM-based Evaluation Agent (LEA) that can: (1) simulate human behaviors to generate interactions with IQA models; (2) automatically evaluate the generated interactions. Moreover, we propose assigning personas to LEAs to better simulate groups of real human evaluators. We show that: (1) our evaluation framework with GPT-4 (or Claude) as the backbone model achieves a high correlation with human evaluations on the IQA task; (2) assigning personas to LEA to better represent the crowd further significantly improves correlations. Finally, we use our automatic metric to evaluate five recent representative LLMs with over 1000 questions from complex and ambiguous question answering tasks, which comes with a substantial cost of $5k if evaluated by humans.",10.48550/arXiv.2408.13545,['JournalArticle'],Computer Science
"Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users",2023,S. Paoli,"This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitative interviews used for the analysis.",10.48550/arXiv.2310.06391,['JournalArticle'],Computer Science
Conceptual structure coheres in human cognition but not in large language models,2023,"Siddharth Suresh, Kushin Mukherjee, Xizheng Yu, Wei-Chun Huang, Lisa Padua, T. Rogers","Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. Contemporary large language models (LLMs), however, make it possible to interrogate the latent structure of conceptual representations using experimental methods nearly identical to those commonly used with human participants. The current work utilizes three common techniques borrowed from cognitive psychology to estimate and compare the structure of concepts in humans and a suite of LLMs. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from LLM behavior, while individually fairly consistent with those estimated from human behavior, vary much more depending upon the particular task used to generate responses--across tasks, estimates of conceptual structure from the very same model cohere less with one another than do human structure estimates. These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language.",10.18653/v1/2023.emnlp-main.47,"['JournalArticle', 'Conference']",Computer Science; Linguistics; Psychology
TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration,2023,"Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi, Kam-Fai Wong","Large language models (LLMs) have demonstrated exceptional performance in planning the use of various functional tools, such as calculators and retrievers, particularly in question-answering tasks. In this paper, we expand the definition of these tools, centering on conceptual tools within the context of dialogue systems. A conceptual tool specifies a cognitive concept that aids systematic or investigative thought. These conceptual tools play important roles in practice, such as multiple psychological or tutoring strategies being dynamically applied in a single turn to compose helpful responses. To further enhance the reasoning and planning capability of LLMs with these conceptual tools, we introduce a multi-persona collaboration framework: Think-Plan-Execute (TPE). This framework decouples the response generation process into three distinct roles: Thinker, Planner, and Executor. Specifically, the Thinker analyzes the internal status exhibited in the dialogue context, such as user emotions and preferences, to formulate a global guideline. The Planner then generates executable plans to call different conceptual tools (e.g., sources or strategies), while the Executor compiles all intermediate results into a coherent response. This structured approach not only enhances the explainability and controllability of responses but also reduces token redundancy. We demonstrate the effectiveness of TPE across various dialogue response generation tasks, including multi-source (FoCus) and multi-strategy interactions (CIMA and PsyQA). This reveals its potential to handle real-world dialogue interactions that require more complicated tool learning beyond just functional tools. The full code and data will be released for reproduction.",10.48550/arXiv.2309.16090,['JournalArticle'],Computer Science
TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction,2023,"Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani","When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called hallucinations. Retrieval augmented generation (RAG) is a promising strategy to avoid hallucinations, but it does not provide guarantees on its correctness. To address this challenge, we propose the Trustworthy Retrieval Augmented Question Answering, or *TRAQ*, which provides the first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal prediction, a statistical technique for constructing prediction sets that are guaranteed to contain the semantically correct response with high probability. Additionally, TRAQ leverages Bayesian optimization to minimize the size of the constructed sets. In an extensive experimental evaluation, we demonstrate that TRAQ provides the desired correctness guarantee while reducing prediction set size by 16.2% on average compared to an ablation. The implementation is available: [https://github.com/shuoli90/TRAQ](https://github.com/shuoli90/TRAQ).",10.18653/v1/2024.naacl-long.210,"['JournalArticle', 'Conference']",Computer Science
The Shadow of Fraud: The Emerging Danger of AI-powered Social Engineering and its Possible Cure,2024,"Jingru Yu, Yi Yu, Xuhong Wang, Yilun Lin, Manzhi Yang, Yu Qiao, Fei-Yue Wang","Social engineering (SE) attacks remain a significant threat to both individuals and organizations. The advancement of Artificial Intelligence (AI), including diffusion models and large language models (LLMs), has potentially intensified these threats by enabling more personalized and convincing attacks. This survey paper categorizes SE attack mechanisms, analyzes their evolution, and explores methods for measuring these threats. It highlights the challenges in raising awareness about the risks of AI-enhanced SE attacks and offers insights into developing proactive and adaptable defense strategies. Additionally, we introduce a categorization of the evolving nature of AI-powered social engineering attacks into""3E phases"": Enlarging, wherein the magnitude of attacks expands through the leverage of digital media; Enriching, introducing novel attack vectors and techniques; and Emerging, signifying the advent of novel threats and methods. Moreover, we emphasize the necessity for a robust framework to assess the risk of AI-powered SE attacks. By identifying and addressing gaps in existing research, we aim to guide future studies and encourage the development of more effective defenses against the growing threat of AI-powered social engineering.",10.48550/arXiv.2407.15912,"['JournalArticle', 'Review']",Computer Science; Sociology
Tell Me a Story! Narrative-Driven XAI with Large Language Models,2023,"David Martens, Camille Dams, James Hinns, Mark Vergouwen","In many AI applications today, the predominance of black-box machine learning models, due to their typically higher accuracy, amplifies the need for Explainable AI (XAI). Existing XAI approaches, such as the widely used SHAP values or counterfactual (CF) explanations, are arguably often too technical for users to understand and act upon. To enhance comprehension of explanations of AI decisions and the overall user experience, we introduce XAIstories, which leverage Large Language Models to provide narratives about how AI predictions are made: SHAPstories do so based on SHAP explanations, while CFstories do so for CF explanations. We study the impact of our approach on users' experience and understanding of AI predictions. Our results are striking: over 90% of the surveyed general audience finds the narratives generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 83% of data scientists indicating they are likely to use SHAPstories for this purpose. In an image classification setting, CFstories are considered more or equally convincing as the users' own crafted stories by more than 75% of the participants. CFstories additionally bring a tenfold speed gain in creating a narrative. We also find that SHAPstories help users to more accurately summarize and understand AI decisions, in a credit scoring setting we test, correctly answering comprehension questions significantly more often than they do when only SHAP values are provided. The results thereby suggest that XAIstories may significantly help explaining and understanding AI predictions, ultimately supporting better decision-making in various applications.",10.48550/arXiv.2309.17057,"['JournalArticle', 'Review']",Computer Science; Linguistics
A User Study on the Acceptance of Native Advertising in Generative IR,2024,"Ines Zelch, Matthias Hagen, Martin Potthast","Commercial conversational search engines need a business model. Since advertising is the main source of revenue for traditional ten-blue-links web search, ads are not an unlikely option for conversational search either. In traditional web search, ads are usually placed above organic search results. However, large language models (LLMs) may be dynamically prompted to blend product placements with organic conversational responses, similar to native advertising in journalism. This type of advertising can be very difficult to recognize, depending on how subtly it is integrated and disclosed. To raise awareness of this potential development, we analyze the capabilities of current LLMs to blend ads with generative search results. In a user study, we ask people about the perceived quality of (emulated) search results in different advertising scenarios. In a substantial number of cases, our survey participants do not notice brand or product placements when they do not expect them. Thus, our results show the potential of LLMs to subtly mix advertising with generated search results. This warrants further investigation, for example, to develop appropriate advertising disclosure rules, and to detect advertising in generated results. Our research also raises broader concerns about whether commercial or open-source generative models can be trusted not to be fine-tuned to generate ads rather than genuine responses.",10.1145/3627508.3638316,"['JournalArticle', 'Book', 'Review']",Computer Science; Linguistics
Large Language Models for Mobility Analysis in Transportation Systems: A Survey on Forecasting Tasks,2024,"Zijian Zhang, Yujie Sun, Zepu Wang, Yuqi Nie, Xiaobo Ma, Ruolin Li, Peng Sun, Xuegang Ban","Mobility analysis is a crucial element in the research area of transportation systems. Forecasting traffic information offers a viable solution to address the conflict between increasing transportation demands and the limitations of transportation infrastructure. Predicting human travel is significant in aiding various transportation and urban management tasks, such as taxi dispatch and urban planning. Machine learning and deep learning methods are favored for their flexibility and accuracy. Nowadays, with the advent of large language models (LLMs), many researchers have combined these models with previous techniques or applied LLMs to directly predict future traffic information and human travel behaviors. However, there is a lack of comprehensive studies on how LLMs can contribute to this field. This survey explores existing approaches using LLMs for time series forecasting problems for mobility in transportation systems. We provide a literature review concerning the forecasting applications within transportation systems, elucidating how researchers utilize LLMs, showcasing recent state-of-the-art advancements, and identifying the challenges that must be overcome to fully leverage LLMs in this domain.",10.1177/03611981251367699,"['JournalArticle', 'Review']",Computer Science; Engineering
On the Limitations of Simulating Active Learning,2023,"Katerina Margatina, Nikolaos Aletras","Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ``why do active learning algorithms sometimes fail to outperform random sampling?''. We argue that evaluating AL algorithms on available labeled datasets might provide a lower bound as to their effectiveness in real data. We believe it is essential to collectively shape the best practices for AL research, particularly as engineering advancements in LLMs push the research focus towards data-driven approaches (e.g., data efficiency, alignment, fairness). In light of this, we have developed guidelines for future work. Our aim is to draw attention to these limitations within the community, in the hope of finding ways to address them.",10.48550/arXiv.2305.13342,"['JournalArticle', 'Conference', 'Review']",Computer Science
Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics,2024,"P. Romero, Stephen Fitz, Teruo Nakatsuma","Previous research on emergence in large language models shows these display apparent human-like abilities and psychological latent traits. However, results are partly contradicting in expression and magnitude of these latent traits, yet agree on the worrisome tendencies to score high on the Dark Triad of narcissism, psychopathy, and Machiavellianism, which, together with a track record of derailments, demands more rigorous research on safety of these models. We provided a state of the art language model with the same personality questionnaire in nine languages, and performed Bayesian analysis of Gaussian Mixture Model, finding evidence for a deeper-rooted issue. Our results suggest both interlingual and intralingual instabilities, which indicate that current language models do not develop a consistent core personality. This can lead to unsafe behaviour of artificial intelligence systems that are based on these foundation models, and are increasingly integrated in human life. We subsequently discuss the shortcomings of modern psychometrics, abstract it, and provide a framework for its species-neutral, substrate-free formulation.",10.21203/rs.3.rs-2717108/v1,['JournalArticle'],Computer Science; Psychology
Large Language Model Agent for Fake News Detection,2024,"Xinyi Li, Yongfeng Zhang, E. Malthouse","In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.",10.48550/arXiv.2405.01593,['JournalArticle'],Computer Science
Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception,2025,"Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng","Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs'internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Confidence Consistency-based Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs'ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6% on NQ and 4.9% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.",10.48550/arXiv.2502.11677,"['JournalArticle', 'Conference']",Computer Science
Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs,2024,"Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi","In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a users smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.",10.48550/arXiv.2409.19401,"['JournalArticle', 'Conference']",Computer Science
Developing Personalized Marketing Service Using Generative AI,2024,"Gun Ho Lee, Kyoung Jun Lee, Baek Jeong, Tae Kyoung Kim","In todays world, the development of social network services (SNS) like Facebook and Instagram has enabled consumers to acquire information about products through various channels. The acquisition of diverse information has led to a diversification in consumer preferences and requirements. As consumer preferences diversify and online channels expand, there is an increasing need for companies to provide personalized marketing. Among the means of personalized marketing, personalized marketing messages are a key tool that can enhance customer engagement. However, a limitation of personalized marketing message services is the cost issue associated with manually writing individual marketing messages for personalization. To solve this problem, when developing automated technology for personalized marketing messages, there were concerns about the complexity of model development and the quality of messages generated automatically. In this study, we propose the Persuasive Message Intelligence (PMI) service, which utilizes the recently prominent Large Language Model for automated individual personalized marketing messages. PMI generates marketing messages through prompt engineering based on the theory of persuasion in marketing and prior research on AI-generated messages, and validates the elements of prompts through surveys. The trial and error of researchers presented in this study, along with the know-how and rules of prompt engineering, will serve as guidelines for those who wish to develop services through prompts in the future.",10.1109/ACCESS.2024.3361946,"['JournalArticle', 'Review']",Business; Computer Science
Benchmarking Distributional Alignment of Large Language Models,2024,"Nicole Meister, Carlos Guestrin, Tatsunori Hashimoto","Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be \textit{distributionally aligned} remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables -- the question domain, steering method, and distribution expression method -- which motivates our contribution of a benchmark explicitly addressing these dimensions. We construct a dataset expanding beyond political values, create human baselines for this task, and evaluate the extent to which an LM can align with a particular group's opinion distribution to inform design choices of such simulation systems. Our analysis reveals open problems regarding if, and how, LMs can be used to simulate humans, and that LLMs can more accurately describe the opinion distribution than simulate such distributions.",10.48550/arXiv.2411.05403,['JournalArticle'],Computer Science; Linguistics
When Large Language Models Meet Citation: A Survey,2023,"Yang Zhang, Yufei Wang, Kai Wang, Quan Z. Sheng, Lina Yao, A. Mahmood, Wei Emma Zhang, Rongying Zhao","Citations in scholarly work serve the essential purpose of acknowledging and crediting the original sources of knowledge that have been incorporated or referenced. Depending on their surrounding textual context, these citations are used for different motivations and purposes. Large Language Models (LLMs) could be helpful in capturing these fine-grained citation information via the corresponding textual context, thereby enabling a better understanding towards the literature. Furthermore, these citations also establish connections among scientific papers, providing high-quality inter-document relationships and human-constructed knowledge. Such information could be incorporated into LLMs pre-training and improve the text representation in LLMs. Therefore, in this paper, we offer a preliminary review of the mutually beneficial relationship between LLMs and citation analysis. Specifically, we review the application of LLMs for in-text citation analysis tasks, including citation classification, citation-based summarization, and citation recommendation. We then summarize the research pertinent to leveraging citation linkage knowledge to improve text representations of LLMs via citation prediction, network structure information, and inter-document relationship. We finally provide an overview of these contemporary methods and put forth potential promising avenues in combining LLMs and citation analysis for further investigation.",10.48550/arXiv.2309.09727,"['JournalArticle', 'Review']",Computer Science
Failure Management Overview in Optical Networks,2024,Sergio Cruzes,"Conventional optical networks are limited by static operational methods that hinder their scalability and effectiveness. As networks operate with reduced margins to maximize resource utilization, the risk of hard failures increases, necessitating efficient failure prediction systems and accurate quality of transmission (QoT) estimation. Effective management requires the detection of soft failures, accurate bit error rate (BER) predictions, and dynamic network operations to maintain minimal margins. Machine learning (ML) offers promising solutions for automating these tasks, significantly enhancing failure management and network reliability. This article provides an extensive overview of ML techniques applied to optical networks, specifically focusing on failure management. The key ML techniques discussed include network kriging (NK) for performance estimation and failure localization, support vector machine (SVM) for classification tasks, convolutional neural networks (CNNs) for signal analysis and soft failure identification, and generative adversarial networks (GANs) for synthetic data generation and soft failure detection. It also explores the application of artificial neural networks (ANNs), autoencoders (AEs), Gaussian process (GP), long short-term memory (LSTM), and gated recurrent units (GRUs) in optical networks. This study surveys ML techniques for early-warning and failure prediction, failure detection, identification, localization, magnitude estimation, and soft failure detection and prediction. Emphasizing automation, it discusses how ML algorithms can streamline failure management processes, reducing manual intervention and service disruptions. The potential of large language models (LLMs) and digital twins (DTs) for further advancements in automating failure management, optimizing performance, and network optimization in optical networks is also examined. LLMs significantly advance network management by improving network design, diagnosis, security, and autonomous optimization through the integration of comprehensive domain resources and intelligent agents. These advancements are paving the way towards achieving artificial general intelligence and fully automated optical network management.",10.1109/ACCESS.2024.3498704,"['JournalArticle', 'Review']",Computer Science; Engineering
"A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models",2025,"Longchao Da, J. Turnau, Thirulogasankar Pranav Kutralingam, Alvaro Velasquez, Paulo Shakarian, Hua Wei","Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a repository to include the most up-to-date sim-to-real research work to help domain researchers.",10.48550/arXiv.2502.13187,"['JournalArticle', 'Review']",Computer Science; Engineering
Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?,2024,"Seth Aycock, David Stap, Di Wu, C. Monz, Khalil Simaan","Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book's parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we conclude data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.",10.48550/arXiv.2409.19151,['JournalArticle'],Computer Science; Linguistics
How Accurate are GPT-3s Hypotheses About Social Science Phenomena?,2023,"Hannes Rosenbusch, C. Stevenson, Han L. J. van der Maas","We test whether GPT-3 can accurately predict simple study outcomes in the social sciences. Ground truth outcomes were obtained by surveying 600 adult US citizens about their political attitudes. GPT-3 was prompted to predict the direction of the empirical inter-attitude correlations. Machine-generated hypotheses were accurate in 78% (zero-shot), 94% (five-shot and chained prompting), and 97% (extensive finetuning) of cases. Positive and negative correlations were balanced in the ground truth data. These results encourage the development of hypothesis engines for more challenging contexts. Moreover, they highlight the importance of addressing the numerous ethical and philosophical challenges that arise with hypothesis automation. While future hypothesis engines could potentially compete with human researchers in terms of empirical accuracy, they have inherent drawbacks that preclude full automations for the foreseeable future.",10.1007/s44206-023-00054-2,"['JournalArticle', 'Review']",Computer Science; Political Science
Device-Edge Cooperative Fine-Tuning of Foundation Models as a 6G Service,2023,"Hai Wu, Xu Chen, Kaibin Huang","Foundation models (FoMos), referring to large-scale AI models, possess human-like capabilities and are able to perform competitively in the domain of human intelligence. The breakthrough in FoMos has inspired researchers to deploy such models in the sixth-generation (6G) mobile networks for automating a broad range of tasks in next-generation mobile applications. While the sizes of FoMos are reaching their peaks, their next phase is expected to focus on fine-tuning the models to specific downstream tasks. This inspires us to propose the vision of FoMo fine-tuning as a 6G service. Its key feature is the exploitation of existing parameter-efficient fine-tuning (PEFT) techniques to tweak only a small fraction of model weights for a FoMo to become customized for a specific task. To materialize this vision, we survey the state-of-the-art PEFT, and then present a novel device-edge fine-tuning (DEFT) framework for providing efficient and privacy-preserving fine-tuning services at the 6G network edge. The framework consists of the following comprehensive set of techniques: control of fine-tuning parameter sizes in different transformer blocks of a FoMo; over-the-air computation for realizing neural connections in DEFT; federated DEFT in a multi-device system by downloading a FoMo emulator or gradients; on-the-fly prompt-ensemble tuning; and device-to-device prompt transfer among devices. Experiments are conducted using pre-trained FoMos with up to 11 billion parameters to demonstrate the effectiveness of DEFT techniques. The article is concluded by presenting future research opportunities.",10.1109/MWC.008.2300516,"['JournalArticle', 'Review']",Computer Science; Engineering; Mathematics
"A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security",2024,"Qianru Zhang, Peng Yang, Junliang Yu, Haixin Wang, Xingwei He, S. Yiu, Hongzhi Yin","The widespread adoption of smartphones and Location-Based Social Networks has led to a massive influx of spatio-temporal data, creating unparalleled opportunities for enhancing Point-of-Interest (POI) recommendation systems. These advanced POI systems are crucial for enriching user experiences, enabling personalized interactions, and optimizing decision-making processes in the digital landscape. However, existing surveys tend to focus on traditional approaches and few of them delve into cutting-edge developments, emerging architectures, as well as security considerations in POI recommendations. To address this gap, our survey stands out by offering a comprehensive, up-to-date review of POI recommendation systems, covering advancements in models, architectures, and security aspects. We systematically examine the transition from traditional models to advanced techniques such as large language models. Additionally, we explore the architectural evolution from centralized to decentralized and federated learning systems, highlighting the improvements in scalability and privacy. Furthermore, we address the increasing importance of security, examining potential vulnerabilities and privacy-preserving approaches. Our taxonomy provides a structured overview of the current state of POI recommendation, while we also identify promising directions for future research in this rapidly advancing field.",10.1109/TKDE.2025.3551292,"['JournalArticle', 'Review']",Computer Science
Hallucination-minimized Data-to-answer Framework for Financial Decision-makers,2023,"Sohini Roychowdhury, Andres Alvarez, Brian Moore, Marko Krema, Maria Paz Gelpi, Federico Martin Rodriguez, Angel Rodriguez, Jose Ramon Cabrejas, Pablo Martinez Serrano, Punit Agrawal, Arijit Mukherjee","Large Language Models (LLMs) have been applied to build several automation and personalized question-answering prototypes so far. However, scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making. In this work, we present a novel Langchain-based framework that transforms data tables into hierarchical textual ""data chunks"" to enable a wide variety of actionable question answering. First, the user-queries are classified by intention followed by automated retrieval of the most relevant data chunks to generate customized LLM prompts per query. Next, the custom prompts and their responses undergo multi-metric scoring to assess for hallucinations and response confidence. The proposed system is optimized with user-query intention classification, advanced prompting, data scaling capabilities and it achieves over $ 90\%$ confidence scores for a variety of user-queries responses ranging from {What, Where, Why, How, predict, trend, anomalies, exceptions} that are crucial for financial decision making applications. The proposed data to answers framework can be extended to other analytical domains such as sales and payroll to ensure optimal hallucination control guardrails.",10.1109/BigData59044.2023.10386232,"['JournalArticle', 'Conference']",Business; Computer Science
MACAROON: Training Vision-Language Models To Be Your Engaged Partners,2024,"Shujin Wu, Y. Fung, Sha Li, Yixin Wan, Kai-Wei Chang, Heng Ji","Large vision-language models (LVLMs), while proficient in following instructions and responding to diverse questions, invariably generate detailed responses even when questions are ambiguous or unanswerable, leading to hallucinations and bias issues. Thus, it is essential for LVLMs to proactively engage with humans to ask for clarifications or additional information for better responses. In this study, we aim to shift LVLMs from passive answer providers to proactive engaged partners. We begin by establishing a three-tiered hierarchy for questions of invalid, ambiguous, and personalizable nature to measure the proactive engagement capabilities of LVLMs. Utilizing this hierarchy, we create PIE, (ProactIve Engagement Evaluation) through GPT-4o and human annotators, consisting of 853 questions across six distinct, fine-grained question types that are verified by human annotators and accompanied with well-defined metrics. Our evaluations on \benchmark indicate poor performance of existing LVLMs, with the best-performing open-weights model only achieving an Aggregate Align Rate (AAR) of 0.28. In response, we introduce MACAROON, self-iMaginAtion for ContrAstive pReference OptimizatiON, which instructs LVLMs to autonomously generate contrastive response pairs for unlabeled questions given the task description and human-crafted criteria. Then, the self-imagined data is formatted for conditional reinforcement learning. Experimental results show MACAROON effectively improves LVLMs' capabilities to be proactively engaged (0.84 AAR) while maintaining comparable performance on general tasks.",10.48550/arXiv.2406.14137,"['JournalArticle', 'Conference']",Computer Science; Psychology
Embodied AI with Large Language Models: A Survey and New HRI Framework,2024,"Ming-Yi Lin, Ou-Wen Lee, Chih-Ying Lu","The study aims to develop an emotional logic engine based on a large language model (LLM), providing emotional connections, personalized interactions, knowledge representation, and logical inference. Using this emotional logic engine, we intend to realize the goals of high-level cognition, autonomous knowledge reasoning, long-horizon planning, and action execution described in embodied artificial intelligence (embodied AI). Ultimately, we will implement an efficient intelligent companion interaction robot (ICIR) based on a novel human-robot interaction (HRI) framework to enhance the interaction between humans and robots. The proposed framework integrates multiple components including a visual language model (VLM), logic reasoning model, pre-trained database integration, and the development of a multi-modal template. Additionally, we introduce a complementary framework termed perception-action loop (PALoop), which is meticulously modeled and constructed to facilitate seamless interactions between human operators and robotic systems. Detailed design aspects of both frameworks are elucidated, providing insights into their architecture and functionality. The research outcomes will be practically applied, offering the robotics industry innovative and practical technology solutions.",10.1109/ICARM62033.2024.10715872,"['JournalArticle', 'Conference', 'Review']",Computer Science
Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space,2025,"Weichen Zhan, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, Xiao-Ping Zhang","Spatial reasoning is a fundamental capability of embodied agents and has garnered widespread attention in the field of multimodal large language models (MLLMs). In this work, we propose a novel benchmark, Open3DVQA, to comprehensively evaluate the spatial reasoning capacities of current state-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consists of 9k VQA samples, collected using an efficient semi-automated tool in a high-fidelity urban simulator. We evaluate several SOTA MLLMs across various aspects of spatial reasoning, such as relative and absolute spatial relationships, situational reasoning, and object-centric spatial attributes. Our results reveal that: 1) MLLMs perform better at answering questions regarding relative spatial relationships than absolute spatial relationships, 2) MLLMs demonstrate similar spatial reasoning abilities for both egocentric and allocentric perspectives, and 3) Fine-tuning large models significantly improves their performance across different spatial reasoning tasks. We believe that our open-source data collection tools and in-depth analyses will inspire further research on MLLM spatial reasoning capabilities. The benchmark is available at https://github.com/WeichenZh/Open3DVQA.",10.48550/arXiv.2503.11094,['JournalArticle'],Computer Science
From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation,2024,"Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, Hao Yang","Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.",10.1109/IJCNN60899.2024.10650457,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
On the Reliability of Psychological Scales on Large Language Models,2024,"Jen-Tse Huang, Wenxiang Jiao, Man Ho Lam, E. Li, Wenxuan Wang, Michael R. Lyu","Recent research has focused on examining Large Language Models (LLMs) characteristics from a psychological standpoint, acknowledging the necessity of understanding their behavioral characteristics. The administration of personality tests to LLMs has emerged as a noteworthy area in this context. However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate. Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits. Analysis of 2,500 settings per model, including GPT-3.5, GPT-4, Gemini-Pro, and LLaMA-3.1, reveals that various LLMs show consistency in responses to the Big Five Inventory, indicating a satisfactory level of reliability. Furthermore, our research explores the potential of GPT-3.5 to emulate diverse personalities and represent various groupsa capability increasingly sought after in social sciences for substituting human participants with LLMs to reduce costs. Our findings reveal that LLMs have the potential to represent different personalities with specific prompt instructions.",10.18653/v1/2024.emnlp-main.354,"['JournalArticle', 'Conference']",Computer Science; Psychology
When Remote Sensing Meets Foundation Model: A Survey and Beyond,2025,"Chunlei Huo, Keming Chen, Shuaihao Zhang, Zeyu Wang, Heyu Yan, Jing Shen, Yuyang Hong, Geqi Qi, Hongmei Fang, Zihan Wang","Most deep-learning-based vision tasks rely heavily on crowd-labeled data, and a deep neural network (DNN) is usually impacted by the laborious and time-consuming labeling paradigm. Recently, foundation models (FMs) have been presented to learn richer features from multi-modal data. Moreover, a single foundation model enables zero-shot predictions on various vision tasks. The above advantages make foundation models better suited for remote sensing images, where image annotations are more sparse. However, the inherent differences between natural images and remote sensing images hinder the applications of the foundation model. In this context, this paper provides a comprehensive review of common foundation models and domain-specific foundation models for remote sensing, and it summarizes the latest advances in vision foundation models, textually prompted foundation models, visually prompted foundation models, and heterogeneous foundation models. Despite the great potential of foundation models for vision tasks, open challenges concerning data, model, and task impact the performance of remote sensing images and make foundation models far from practical applications. To address open challenges and reduce the performance gap between natural images and remote sensing images, this paper discusses open challenges and suggests potential directions for future advancements.",10.3390/rs17020179,"['JournalArticle', 'Review']",Computer Science; Environmental Science
Enhancing Machine Learning Model Interpretability in Intrusion Detection Systems through SHAP Explanations and LLM-Generated Descriptions,2024,"Abderrazak Khediri, Hamda Slimi, Yahiaoui Ayoub, Makhlouf Derdour, H. Bendjenna, Charaf Eddine Ghenai","Intrusion Detection Systems (IDS) are critical for detecting and mitigating cyber threats, yet the opaqueness of machine learning models used within these systems poses challenges for understanding their decisions. This paper proposes a novel approach to address this issue by integrating SHAP (SHapley Additive exPlanations) values with Large Language Models (LLMs). With the aim of enhancing transparency and trust in IDS, this approach demonstrates how the combination facilitates the generation of human-understandable explanations for detected anomalies, drawing upon the CICIDS2017 dataset. The LLM effectively articulates significant features identified by SHAP values, offering coherent responses regarding influential predictors of model outcomes.",10.1109/PAIS62114.2024.10541168,"['JournalArticle', 'Conference']",Computer Science
"A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives",2025,"Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang","With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.",10.48550/arXiv.2503.13415,"['JournalArticle', 'Review']",Computer Science; Engineering
Uncovering Dynamics of Global Mangrove Gains and Losses,2023,"Valeria Contessa, Karen Dyson, Pedro Pablo Vivar Mulas, A. Kindgard, Tianchi Liu, D. Saah, Karis Tenneson, A. Pekkarinen","Supporting successful global mangrove conservation and policy requires accurate identification of anthropogenic and biophysical drivers of mangrove extent, yet such studies are scarce. We apply a hybrid methodology, combining existing remote sensing mangrove maps with local expert knowledge of vegetation and land use dynamics. We conducted stratified random sampling in eight subregions, and local experts visually interpreted over 20,900 plots using high-resolution imagery in Collect Earth Online. Similar to previous estimates, we found 147,771 km2 (1.4%) of mangroves globally in 2020 and that rates of mangrove loss have decreased from 20002010 to 20102020, largely driven by South and Southeast Asia. Anthropogenic drivers of loss have shifted across subregions, with oil palm cultivation emerging in South and Southeast Asia and aquaculture in South America and Western and Central Africa, highlighting the need for ongoing monitoring and adaptable conservation efforts. Natural expansion outpaced natural retraction in both periods. This is the first global study uncovering land use drivers of mangrove decline and recovery, only made possible by collaboration with local experts. Key breakthroughs include successfully discerning spectrally similar anthropogenic from biophysical drivers, such as aquaculture from natural retraction, and creating data collection approaches that streamline visual interpretation efforts.",10.3390/rs15153872,['JournalArticle'],Computer Science; Environmental Science
FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users,2025,"Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn","Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.",10.48550/arXiv.2502.19312,"['JournalArticle', 'Review']",Computer Science; Mathematics
Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach,2025,"Xuying Li, Zhuo Li, Yuji Kosuga, Victor Bian","Aligning large language models (LLMs) with human values and safety constraints is challenging, especially when objectives like helpfulness, truthfulness, and avoidance of harm conflict. Reinforcement Learning from Human Feedback (RLHF) has achieved notable success in steering models, but is complex and can be unstable. Recent approaches such as Direct Preference Optimization (DPO) simplify preference-based fine-tuning but may introduce bias or trade-off certain objectives~\cite{dpo}. In this work, we propose a Group Relative Policy Optimization (GRPO) framework with a multi-label reward regression model to achieve safe and aligned language generation. The GRPO algorithm optimizes a policy by comparing groups of sampled responses, eliminating the need for a separate value critic and improving training efficiency~\cite{grpo}. We train a reward model to predict multiple alignment scores (e.g., safety, helpfulness, etc.), which are combined into a single reward signal. We provide a theoretical derivation for using this learned multi-aspect reward within GRPO and discuss its advantages and limitations. Empirically, our approach improves all the safety and quality metrics evaluated in language generation tasks on model scales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of objectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO achieves alignment with significantly lower computational cost and explicit multi-objective handling. \textbf{We will open-source all trained models at https://huggingface.co/hydroxai.",10.48550/arXiv.2503.21819,['JournalArticle'],Computer Science; Linguistics
Ain't Misbehavin' - Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru,2024,"Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez","Social robots aim to establish long-term bonds with humans through engaging conversation. However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating large language models (LLMs) into social robots to achieve more dynamic and expressive conversations. We introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality. We incorporate robot behavior with two modalities: 1) a text-to-speech (TTS) engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions. A demo of our system is available here. To illuminate design and implementation issues, we conduct a pilot study where volunteers chat with a social robot using our proposed system, and we analyze their feedback, conducting a rigorous error analysis of chat transcripts. Feedback was overwhelmingly positive, with participants commenting on the robot's empathy, helpfulness, naturalness, and entertainment. Most negative feedback was due to automatic speech recognition (ASR) errors which had limited impact on conversations. However, we observed a small class of errors, such as the LLM repeating itself or hallucinating fictitious information and human responses, that have the potential to derail conversations, raising important issues for LLM application.",10.1145/3610978.3640562,"['JournalArticle', 'Book']",Computer Science
Generative Interpretation,2023,"Yonathan A. Arbel, David Hoffman","We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.",10.2139/ssrn.4526219,['JournalArticle'],Computer Science; Economics; Law
PrISM-Q&A: Step-Aware Voice Assistant on a Smartwatch Enabled by Multimodal Procedure Tracking and Large Language Models,2024,"Riku Arakawa, J. Lehman, Mayank Goel","Voice assistants capable of answering user queries during various physical tasks have shown promise in guiding users through complex procedures. However, users often find it challenging to articulate their queries precisely, especially when unfamiliar with the specific terminologies required for machine-oriented tasks. We introduce PrISM-Q&A, a novel question-answering (Q&A) interaction termed step-aware Q&A, which enhances the functionality of voice assistants on smartwatches by incorporating Human Activity Recognition (HAR) and providing the system with user context. It continuously monitors user behavior during procedural tasks via audio and motion sensors on the watch and estimates which step the user is performing. When a question is posed, this contextual information is supplied to Large Language Models (LLMs) as part of the context used to generate a response, even in the case of inherently vague questions like ""What should I do next with this?"" Our studies confirmed that users preferred the convenience of our approach compared to existing voice assistants. Our real-time assistant represents the first Q&A system that provides contextually situated support during tasks without camera use, paving the way for the ubiquitous, intelligent assistant.",10.1145/3699759,['JournalArticle'],Computer Science
LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs,2024,"Yujun Zhou, Jingdong Yang, Kehan Guo, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V. Chawla, Xiangliang Zhang","Artificial Intelligence (AI) is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. While large language models (LLMs) increasingly assist in tasks ranging from procedural guidance to autonomous experiment orchestration, an""illusion of understanding""may lead researchers to overestimate their reliability. Such overreliance is particularly dangerous in high-stakes laboratory settings, where failures in hazard identification or risk assessment can result in severe accidents. To address these concerns, we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that evaluates large language models and vision language models (VLMs) on their ability to identify potential hazards, assess risks, and predict the consequences of unsafe actions in lab environments. LabSafety Bench comprises 765 multiple-choice questions aligned with US Occupational Safety and Health Administration (OSHA) protocols, along with 404 realistic laboratory scenarios featuring dual evaluation tasks: the Hazards Identification Test and the Consequence Identification Test, with 3128 open-ended questions in total. Evaluations across eight proprietary models, seven open-weight LLMs, and four VLMs reveal that, despite advanced performance on structured assessments, no model achieves the safety threshold required for reliable operation -- none scoring above 70% on the Hazards Identification Test. Moreover, while proprietary models tend to excel in multiple-choice evaluations, their performance in open-ended, real-world scenario responses is comparable to that of open-source models. These findings underscore the urgent need for specialized evaluation frameworks to ensure the safe and responsible deployment of AI in laboratory settings.",10.48550/arXiv.2410.14182,['JournalArticle'],Computer Science; Engineering; Environmental Science
"Real Estate Insights AI: real estates new roommatethe good, the bad andthe algorithmic",2024,"Nikodem Szumilo, Thomas Wiegelmann","PurposeThis paper aims to provide a comprehensive analysis of the transformative impact of Artificial Intelligence (AI) and Large Language Models (LLMs), such as GPT-4, on the real estate industry. It explores how these technologies are reshaping various aspects of the sector, from market analysis and valuation to customer interactions and evaluates the balance between technological efficiency and the preservation of human elements in business.Design/methodology/approachThe study is based on an analysis of the strengths and weaknesses of AI as a technology in applications for real estate. It uses this framework to assess the potential of this technology in different use cases. This is supplemented by an emerging literature on the topic, practical insights and industry expert opinions to provide a balanced perspective on the subject.FindingsThe paper reveals that AI and LLMs offer significant benefits in real estate, including enhanced data-driven decision-making, predictive analytics and operational efficiency. However, it also uncovers critical challenges, such as potential biases in AI algorithms and the risk of depersonalising customer interactions.Practical implicationsThe paper advocates for a balanced approach to adopting AI, emphasising the importance of understanding its strengths and limitations while ensuring ethical usage in the diverse and complex landscape of real estate.Originality/valueThis work stands out for its balanced examination of both the advantages and limitations of AI in real estate. It introduces the novel concept of the jagged technological frontier in real estate, providing a unique framework for understanding the interplay between AI and human expertise in the industry.",10.1108/jpif-01-2024-0001,['JournalArticle'],Business; Computer Science; Engineering
Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs,2025,"Jonathan Rystrm, Hannah Rose Kirk, Scott A. Hale","Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.",10.48550/arXiv.2502.16534,"['JournalArticle', 'Review']",Computer Science; Linguistics; Sociology
Evidence of a log scaling law for political persuasion with large language models,2024,"Kobi Hackenburg, Ben M. Tappin, Paul Rttger, Scott A. Hale, Jonathan Bright, Helen Margetts","Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.",10.48550/arXiv.2406.14508,"['JournalArticle', 'Review']",Computer Science; Political Science
Proxona: Leveraging LLM-Driven Personas to Enhance Creators' Understanding of Their Audience,2024,"Yoonseo Choi, Eun Jeong Kang, Seulgi Choi, Min Kyung Lee, Juho Kim",,10.48550/arXiv.2408.10937,['JournalArticle'],Computer Science
Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models,2024,"Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng","Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce TRON, a two-step framework for risk control and assessment, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. TRON comprises two main components: (1) a novel conformal score to sample response sets of minimum size, and (2) a nonconformity score to identify high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that TRON achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.",10.48550/arXiv.2410.08174,['JournalArticle'],Computer Science
How Robust are LLMs to In-Context Majority Label Bias?,2023,"Karan Gupta, Sumegh Roychowdhury, Siva Rajesh Kasa, Santhosh Kumar Kasa, Anish Bhanushali, Nikhil Pattisapu, Prasanna Srinivasa Murthy","In the In-Context Learning (ICL) setup, various forms of label biases can manifest. One such manifestation is majority label bias, which arises when the distribution of labeled examples in the in-context samples is skewed towards one or more specific classes making Large Language Models (LLMs) more prone to predict those labels. Such discrepancies can arise from various factors, including logistical constraints, inherent biases in data collection methods, limited access to diverse data sources, etc. which are unavoidable in a real-world industry setup. In this work, we study the robustness of in-context learning in LLMs to shifts that occur due to majority label bias within the purview of text classification tasks. Prior works have shown that in-context learning with LLMs is susceptible to such biases. In our study, we go one level deeper and show that the robustness boundary varies widely for different models and tasks, with certain LLMs being highly robust (~90%) to majority label bias. Additionally, our findings also highlight the impact of model size and the richness of instructional prompts contributing towards model robustness. We restrict our study to only publicly available open-source models to ensure transparency and reproducibility.",10.48550/arXiv.2312.16549,['JournalArticle'],Computer Science
From Efficient Multimodal Models to World Models: A Survey,2024,"Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang","Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.",10.48550/arXiv.2407.00118,"['JournalArticle', 'Review']",Computer Science
Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation,2025,"Jessica He, Stephanie Houde, Justin D. Weisz","AI systems powered by large language models can act as capable assistants for writing and editing. In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s). One question that arises in these scenarios is the extent to which AI should be credited for its contributions. We examined knowledge workers views of attribution through a survey study (N=155) and found that they assigned different levels of credit across different contribution types, amounts, and initiative. Compared to a human partner, we observed a consistent pattern in which AI was assigned less credit for equivalent contributions. Participants felt that disclosing AI involvement was important and used a variety of criteria to make attribution judgments, including the quality of contributions, personal values, and technology considerations. Our results motivate and inform new approaches for crediting AI contributions to co-created work.",10.1145/3706598.3713522,"['JournalArticle', 'Book', 'Conference', 'Review']",Computer Science; Psychology
Identifying Multiple Personalities in Large Language Models with External Evaluation,2024,"Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, G. Anumanchipalli, Simerjot Kaur","As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their personalities. Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the-art models as the tool to analyze LLMs' responses. Then, we prompt the LLMs with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for LLMs are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans. With our work, we call for a re-evaluation of personality definition and measurement in LLMs.",10.48550/arXiv.2402.14805,['JournalArticle'],Computer Science; Linguistics; Psychology
"Generative artificial intelligence in hospitality and tourism: future capabilities, AI prompts and real-world applications",2025,Mahmoud Ibraheam Saleh,"ABSTRACT The conversational AI emergence technologies like ChatGPT, Claude AI, and Gemini AI are transforming the hospitality and tourism industries. While existing research explores theoretical applications, a critical gap exists in understanding real-world implementation challenges, prompting strategies, and operational impacts of ChatGPT and other AI applications. This study investigates AI integration patterns, prompt optimization techniques (e.g. Input-Output, Chain-of-Thought, and Tree-of-Thought approach), and implementation strategies through a mixed-methods approach. We conducted a bibliometric analysis of 80 studies using R-Biblioshiny, followed by surveys and interviews with stakeholders, including guests/travelers, managers, and academics. Our empirical analysis measures personalization, user satisfaction, operational efficiency, and research productivity. The results demonstrate that while generative AI can enhance customer experiences and operational efficiency, success depends on selecting appropriate prompting methodologies, ethical considerations, workforce development, and community impact. This research provides an integrated framework for responsible AI adoption while identifying critical areas for future research.",10.1080/19368623.2025.2458603,"['JournalArticle', 'Review']",Business; Computer Science
Efficient Prompt Caching via Embedding Similarity,2024,"Hanlin Zhu, Banghua Zhu, Jiantao Jiao","Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empirically, we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51. We then fine-tune the above embedding model, which significantly improves the AUC of caching prediction from 0.51 to 0.81. We also conduct simulations demonstrating that our trained models achieve better caching efficiency than the previous embedding model.",10.48550/arXiv.2402.01173,['JournalArticle'],Computer Science
Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue,2024,"Jonathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, Dustin Wright, Abraham Israeli, Anders Giovanni Mller, Lechen Zhang, David Jurgens","Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.",10.48550/arXiv.2409.08330,['JournalArticle'],Computer Science; Linguistics
Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong,2025,"Tairan Fu, Javier Conde, Gonzalo Mart'inez, Mara Grandury, Pedro Reviriego","One of the most widely used methods to evaluate LLMs are Multiple Choice Question (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on almost any topic at scale as the results can be processed automatically. To help the LLM answer, a few examples called few shots can be included in the prompt. Moreover, the LLM can be asked to answer the question directly with the selected option or to first provide the reasoning and then the selected answer, which is known as chain of thought. In addition to checking whether the selected answer is correct, the evaluation can look at the LLM-estimated probability of its response as an indication of the confidence of the LLM in the response. In this paper, we study how the LLM confidence in its answer depends on whether the model has been asked to answer directly or to provide the reasoning before answering. The results of the evaluation of questions on a wide range of topics in seven different models show that LLMs are more confident in their answers when they provide reasoning before the answer. This occurs regardless of whether the selected answer is correct. Our hypothesis is that this behavior is due to the reasoning that modifies the probability of the selected answer, as the LLM predicts the answer based on the input question and the reasoning that supports the selection made. Therefore, LLM estimated probabilities seem to have intrinsic limitations that should be understood in order to use them in evaluation procedures. Interestingly, the same behavior has been observed in humans, for whom explaining an answer increases confidence in its correctness.",10.48550/arXiv.2501.09775,['JournalArticle'],Computer Science
MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users,2024,"J. Seo, Sanchita S. Kamath, Aziz N Zeidieh, Saairam Venkatesh, Sean McCurry","This paper investigates how blind and low-vision (BLV) users interact with multimodal large language models (LLMs) to interpret data visualizations. Building upon our previous work on the multimodal access and interactive data representation (MAIDR) framework, our mixed-visual-ability team co-designed maidrAI, an LLM extension providing multiple AI responses to users visual queries. To explore generative AI-based data representation, we conducted user studies with 8 BLV participants, tasking them with interpreting box plots using our system. We examined how participants personalize LLMs through prompt engineering, their preferences for data visualization descriptions, and strategies for verifying LLM responses. Our findings highlight three dimensions affecting BLV users decision-making process: modal preference, LLM customization, and multimodal data representation. This research contributes to designing more accessible data visualization tools for BLV users and advances the understanding of inclusive generative AI applications.",10.1145/3663548.3675660,"['JournalArticle', 'Book', 'Conference']",Computer Science
Deciphering Political Entity Sentiment in News with Large Language Models: Zero-Shot and Few-Shot Strategies,2024,"Alapan Kuila, Sudeshna Sarkar","Sentiment analysis plays a pivotal role in understanding public opinion, particularly in the political domain where the portrayal of entities in news articles influences public perception. In this paper, we investigate the effectiveness of Large Language Models (LLMs) in predicting entity-specific sentiment from political news articles. Leveraging zero-shot and few-shot strategies, we explore the capability of LLMs to discern sentiment towards political entities in news content. Employing a chain-of-thought (COT) approach augmented with rationale in few-shot in-context learning, we assess whether this method enhances sentiment prediction accuracy. Our evaluation on sentiment-labeled datasets demonstrates that LLMs, outperform fine-tuned BERT models in capturing entity-specific sentiment. We find that learning in-context significantly improves model performance, while the self-consistency mechanism enhances consistency in sentiment prediction. Despite the promising results, we observe inconsistencies in the effectiveness of the COT prompting method. Overall, our findings underscore the potential of LLMs in entity-centric sentiment analysis within the political news domain and highlight the importance of suitable prompting strategies and model architectures.",10.48550/arXiv.2404.04361,['JournalArticle'],Computer Science; Political Science
Psycholinguistic Diagnosis of Language Models Commonsense Reasoning,2022,Yan Cong,"Neural language models have attracted a lot of attention in the past few years. More and more researchers are getting intrigued by how language models encode commonsense, specifically what kind of commonsense they understand, and why they do. This paper analyzed neural language models understanding of commonsense pragmatics (i.e., implied meanings) through human behavioral and neurophysiological data. These psycholinguistic tests are designed to draw conclusions based on predictive responses in context, making them very well suited to test word-prediction models such as BERT in natural settings. They can provide the appropriate prompts and tasks to answer questions about linguistic mechanisms underlying predictive responses. This paper adopted psycholinguistic datasets to probe language models commonsense reasoning. Findings suggest that GPT-3s performance was mostly at chance in the psycholinguistic tasks. We also showed that DistillBERT had some understanding of the (implied) intent thats shared among most people. Such intent is implicitly reflected in the usage of conversational implicatures and presuppositions. Whether or not fine-tuning improved its performance to human-level depends on the type of commonsense reasoning.",10.18653/v1/2022.csrr-1.3,nan,Computer Science; Linguistics; Psychology
Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI,2024,"Ramneet Kaur, Colin Samplawski, Adam D. Cobb, Anirban Roy, Brian Matejek, Manoj Acharya, D. Elenius, Alexander M. Berenbeim, J. Pavlik, Nathaniel D. Bastian, Susmit Jha","In this paper, we present a dynamic semantic clustering approach inspired by the Chinese Restaurant Process, aimed at addressing uncertainty in the inference of Large Language Models (LLMs). We quantify uncertainty of an LLM on a given query by calculating entropy of the generated semantic clusters. Further, we propose leveraging the (negative) likelihood of these clusters as the (non)conformity score within Conformal Prediction framework, allowing the model to predict a set of responses instead of a single output, thereby accounting for uncertainty in its predictions. We demonstrate the effectiveness of our uncertainty quantification (UQ) technique on two well known question answering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and Mistral. Our approach achieves SOTA performance in UQ, as assessed by metrics such as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown to produce smaller prediction sets while maintaining the same probabilistic guarantee of including the correct response, in comparison to existing SOTA conformal prediction baseline.",10.48550/arXiv.2411.02381,['JournalArticle'],Computer Science
Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning,2025,"Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, Zhiyong Wu","Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.",10.48550/arXiv.2504.08672,"['JournalArticle', 'Conference']",Computer Science
Pair programming conversations with agents vs. developers: challenges and opportunities for SE community,2022,"Peter Robe, Sandeep Kaur Kuttal, J. AuBuchon, Jacob C. Hart",,10.1145/3540250.3549127,"['Book', 'JournalArticle']",Computer Science
"Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions",2025,"Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sbastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan","Memory is a fundamental component of AI systems, underpinning large language models (LLMs)-based agents. While prior surveys have focused on memory applications with LLMs (e.g., enabling personalized memory in conversational agents), they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric and contextual forms, and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.",10.48550/arXiv.2505.00675,"['JournalArticle', 'Review']",Computer Science
Survey on Foundation Models for Prognostics and Health Management in Industrial Cyber-Physical Systems,2023,"Ruonan Liu, Quanhu Zhang, Te Han, Boyuan Yang, Weidong Zhang, Shen Yin, Donghua Zhou","Industrial Cyber-Physical Systems (ICPS) integrating disciplines such as computer science, communication technology, and engineering, have become a crucial component of modern manufacturing and industry. However, ICPS faces numerous challenges during long-term operation, including equipment faults, performance degradation, and security threats, etc. To achieve efficient maintenance and management, prognostics and health management (PHM) has been widely applied in the critical tasks of ICPS such as fault prediction, health monitoring, and maintenance decision-making. The emergence of large-scale foundation models (LFMs) like BERT and GPT marks a significant advancement in artificial intelligence (AI) technology, demonstrating substantial application potential in multiple fields. The accumulation of AI technology, rapid development of LFMs, and the abundance of industrial data and industrial process knowledge provide the foundational conditions for the construction and advancement of industrial LFMs. However, there is currently a lack of consensus on applying LFMs of PHM in ICPS, necessitating a systematic review and roadmap to clarify future development directions. To bridge this gap, this survey provides a comprehensive survey and understanding of the recent advances in LFMs of PHM in ICPS. It provides valuable references for decision makers and researchers in the industry, and helps to further improve the reliability, availability and safety of ICPS.",10.48550/arXiv.2312.06261,"['JournalArticle', 'Review']",Computer Science; Engineering
Adaptive Retrieval-Augmented Generation for Conversational Systems,2024,"Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz","Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation's confidence level and the relevance of the augmented knowledge.",10.48550/arXiv.2407.21712,"['JournalArticle', 'Conference']",Computer Science
Is GPT a Computational Model of Emotion? Detailed Analysis,2023,"Ala Nekouvaght Tak, J. Gratch","This paper investigates the emotional reasoning abilities of the GPT family of large language models via a component perspective. The paper first examines how the model reasons about autobiographical memories. Second, it systematically varies aspects of situations to impact emotion intensity and coping tendencies. Even without the use of prompt engineering, it is shown that GPT's predictions align significantly with human-provided appraisals and emotional labels. However, GPT faces difficulties predicting emotion intensity and coping responses. GPT-4 showed the highest performance in the initial study but fell short in the second, despite providing superior results after minor prompt engineering. This assessment brings up questions on how to effectively employ the strong points and address the weak areas of these models, particularly concerning response variability. These studies underscore the merits of evaluating models from a componential perspective.",10.48550/arXiv.2307.13779,['JournalArticle'],Computer Science; Psychology
CulturePark: Boosting Cross-cultural Understanding in Large Language Models,2024,"Cheng Li, Damien Teney, Linyi Yang, Qingsong Wen, Xing Xie, Jindong Wang","Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark.",10.48550/arXiv.2405.15145,['JournalArticle'],Computer Science; Linguistics
Large Language Models for Market Research: A Data-augmentation Approach,2024,"Mengxin Wang, Dennis J. Zhang, Heng Zhang","Large Language Models (LLMs) have transformed artificial intelligence by excelling in complex natural language processing tasks. Their ability to generate human-like text has opened new possibilities for market research, particularly in conjoint analysis, where understanding consumer preferences is essential but often resource-intensive. Traditional survey-based methods face limitations in scalability and cost, making LLM-generated data a promising alternative. However, while LLMs have the potential to simulate real consumer behavior, recent studies highlight a significant gap between LLM-generated and human data, with biases introduced when substituting between the two. In this paper, we address this gap by proposing a novel statistical data augmentation approach that efficiently integrates LLM-generated data with real data in conjoint analysis. Our method leverages transfer learning principles to debias the LLM-generated data using a small amount of human data. This results in statistically robust estimators with consistent and asymptotically normal properties, in contrast to naive approaches that simply substitute human data with LLM-generated data, which can exacerbate bias. We validate our framework through an empirical study on COVID-19 vaccine preferences, demonstrating its superior ability to reduce estimation error and save data and costs by 24.9% to 79.8%. In contrast, naive approaches fail to save data due to the inherent biases in LLM-generated data compared to human data. Another empirical study on sports car choices validates the robustness of our results. Our findings suggest that while LLM-generated data is not a direct substitute for human responses, it can serve as a valuable complement when used within a robust statistical framework.",10.48550/arXiv.2412.19363,"['JournalArticle', 'Review']",Business; Computer Science; Economics; Mathematics
Denoising Diffusion Probabilistic Model for Radio Map Estimation in Generative Wireless Networks,2025,"Xuanhao Luo, Zhizhen Li, Zhiyuan Peng, Mingzhe Chen, Yuchen Liu","The increasing demand for high-speed and reliable wireless networks has driven advancements in technologies such as millimeter-wave and 5G radios, which requires efficient planning and timely deployment of wireless access points. A critical tool in this process is the radio map, a graphical representation of radio-frequency signal strengths that plays a vital role in optimizing overall network performance. However, existing methods for estimating radio maps face challenges due to the need for extensive real-world data collection or computationally intensive ray-tracing analyses, which is costly and time-consuming. Inspired by the success of generative AI techniques in large language models and image generation, we explore their potential applications in the realm of wireless networks. In this work, we propose RM-Gen, a novel generative framework leveraging conditional denoising diffusion probabilistic models to synthesize radio maps using minimal and readily collected data. We then introduce an environment-aware method for selecting critical data pieces, enhancing the generative models applicability and usability. Comprehensive evaluations demonstrate that RM-Gen achieves over 95% accuracy in generating radio maps for networks that operate at 60 GHz and sub-6GHz frequency bands, outperforming the baseline GAN and pix2pix models. This approach offers a cost-effective, adaptable solution for various downstream network optimization tasks.",10.1109/TCCN.2025.3529879,['JournalArticle'],Computer Science; Engineering
Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations,2024,"Manqing Mao, Paishun Ting, Yijian Xiang, Mingyang Xu, Julia Chen, Jianzhe Lin","Recent advancements in large language models (LLMs) have provided a new avenue for chatbot development. Most existing research, however, has primarily centered on single-user chatbots that determine""What""to answer. This paper highlights the complexity of multi-user chatbots, introducing the 3W design dimensions:""What""to say,""When""to respond, and""Who""to answer. Additionally, we proposed Multi-User Chat Assistant (MUCA), an LLM-based framework tailored for group discussions. MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and Conversational Strategies Arbitrator. These modules jointly determine suitable response contents, timings, and appropriate addressees. This paper further proposes an LLM-based Multi-User Simulator (MUS) to ease MUCA's optimization, enabling faster simulation of conversations between the chatbot and simulated users, and speeding up MUCA's early development. In goal-oriented conversations with a small to medium number of participants, MUCA demonstrates effectiveness in tasks like chiming in at appropriate timings, generating relevant content, and improving user engagement, as shown by case studies and user studies.",10.48550/arXiv.2401.04883,['JournalArticle'],Computer Science
Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data,2024,"Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang","Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning large language models with human intentions, yet it often relies on complex methodologies like Proximal Policy Optimization (PPO) that require extensive hyper-parameter tuning and present challenges in sample efficiency and stability. In this paper, we introduce Inverse-Q*, an innovative framework that transcends traditional RL methods by optimizing token-level reinforcement learning without the need for additional reward or value models. Inverse-Q* leverages direct preference optimization techniques but extends them by estimating the conditionally optimal policy directly from the model's responses, facilitating more granular and flexible policy shaping. Our approach reduces reliance on human annotation and external supervision, making it especially suitable for low-resource settings. We present extensive experimental results demonstrating that Inverse-Q* not only matches but potentially exceeds the effectiveness of PPO in terms of convergence speed and the alignment of model responses with human preferences. Our findings suggest that Inverse-Q* offers a practical and robust alternative to conventional RLHF approaches, paving the way for more efficient and adaptable model training approaches.",10.48550/arXiv.2408.14874,"['JournalArticle', 'Conference']",Computer Science
Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game,2024,"Silin Du, Xiaowei Zhang","Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been largely overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game includes the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs possess the capacity for opinion leadership.",10.48550/arXiv.2404.01602,['JournalArticle'],Computer Science
CARROT: A Cost Aware Rate Optimal Router,2025,"Seamus Somerstep, Felipe Maia Polo, Allysson Flavio Melo de Oliveira, Prattyush Mangal, M'irian Silva, Onkar Bhardwaj, Mikhail Yurochkin, Subha Maity","With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. We conduct a minimax analysis of the routing problem, providing a lower bound and finding that a simple router that predicts both cost and accuracy for each question can be minimax optimal. Inspired by this, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that selects a model based on estimates of the models' cost and performance. Alongside CARROT, we also introduce the Smart Price-aware ROUTing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers.",10.48550/arXiv.2502.03261,['JournalArticle'],Computer Science; Mathematics
On the Way to LLM Personalization: Learning to Remember User Conversations,2024,"Lucie Charlotte Magister, Katherine Metcalf, Yizhe Zhang, Maartje ter Hoeve","Large Language Models (LLMs) have quickly become an invaluable assistant for a variety of tasks. However, their effectiveness is constrained by their ability to tailor responses to human preferences and behaviors via personalization. Prior work in LLM personalization has largely focused on style transfer or incorporating small factoids about the user, as knowledge injection remains an open challenge. In this paper, we explore injecting knowledge of prior conversations into LLMs to enable future work on less redundant, personalized conversations. We identify two real-world constraints: (1) conversations are sequential in time and must be treated as such during training, and (2) per-user personalization is only viable in parameter-efficient settings. To this aim, we propose PLUM, a pipeline performing data augmentation for up-sampling conversations as question-answer pairs, that are then used to finetune a low-rank adaptation adapter with a weighted cross entropy loss. Even in this first exploration of the problem, we perform competitively with baselines such as RAG, attaining an accuracy of 81.5% across 100 conversations.",10.48550/arXiv.2411.13405,['JournalArticle'],Computer Science
A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?,2025,"Ada Chen, Yongjiang Wu, Junyuan Zhang, Shu Yang, Jen-Tse Huang, Kun Wang, Wenxuan Wang, Shuai Wang","Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.",10.48550/arXiv.2505.10924,"['JournalArticle', 'Review']",Computer Science
"LLMs' Reshaping of People, Processes, Products, and Society in Software Development: A Comprehensive Exploration with Early Adopters",2025,"Benyamin T. Tabarsi, Heidi Reichert, Ally Limke, Sandeep Kuttal, T. Barnes","Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub Copilot are rapidly gaining traction in the software industry, but their full impact on software engineering remains insufficiently explored. Despite their growing adoption, there is a notable lack of formal, qualitative assessments of how LLMs are applied in real-world software development contexts. To fill this gap, we conducted semi-structured interviews with sixteen early-adopter professional developers to explore their use of LLMs throughout various stages of the software development life cycle. Our investigation examines four dimensions: people - how LLMs affect individual developers and teams; process - how LLMs alter software engineering workflows; product - LLM impact on software quality and innovation; and society - the broader socioeconomic and ethical implications of LLM adoption. Thematic analysis of our data reveals that while LLMs have not fundamentally revolutionized the development process, they have substantially enhanced routine coding tasks, including code generation, refactoring, and debugging. Developers reported the most effective outcomes when providing LLMs with clear, well-defined problem statements, indicating that LLMs excel with decomposed problems and specific requirements. Furthermore, these early-adopters identified that LLMs offer significant value for personal and professional development, aiding in learning new languages and concepts. Early-adopters, highly skilled in software engineering and how LLMs work, identified early and persisting challenges for software engineering, such as inaccuracies in generated content and the need for careful manual review before integrating LLM outputs into production environments. Our study provides a nuanced understanding of how LLMs are shaping the landscape of software development, with their benefits, limitations, and ongoing implications.",10.48550/arXiv.2503.05012,"['JournalArticle', 'Review']",Computer Science; Sociology
MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty,2024,"Yongjin Yang, Haneul Yoo, Hwaran Lee","Despite the massive advancements in large language models (LLMs), they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on single-labeled questions, which removes data uncertainty: the irreducible randomness often present in user queries, which can arise from factors like multiple possible answers. This limitation may cause uncertainty quantification results to be unreliable in practical settings. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that previous methods relatively struggle compared to single-answer settings, though this varies depending on the task. Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty. We believe these observations will guide future work on uncertainty quantification in more realistic settings.",10.48550/arXiv.2408.06816,"['JournalArticle', 'Conference']",Computer Science
"The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games",2024,"Mikhail Mozikov, Nikita Severin, Valeria Bodishtianu, Maria Glushanina, Mikhail Baklashkin, Andrey V. Savchenko, Ilya Makarov","Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions. In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the""superhuman""alignment of GPT-4, resembling human emotional responses.",10.48550/arXiv.2406.03299,['JournalArticle'],Computer Science; Psychology
Simulating H.P. Lovecraft horror literature with the ChatGPT large language model,2023,"E.C. Garrido-Merchn, J. L. Arroyo-Barrigete, Roberto Gozalo-Brizuela","In this paper, we present a novel approach to simulating H.P. Lovecraft's horror literature using the ChatGPT large language model, specifically the GPT-4 architecture. Our study aims to generate text that emulates Lovecraft's unique writing style and themes, while also examining the effectiveness of prompt engineering techniques in guiding the model's output. To achieve this, we curated a prompt containing several specialized literature references and employed advanced prompt engineering methods. We conducted an empirical evaluation of the generated text by administering a survey to a sample of undergraduate students. Utilizing statistical hypothesis testing, we assessed the students ability to distinguish between genuine Lovecraft works and those generated by our model. Our findings demonstrate that the participants were unable to reliably differentiate between the two, indicating the effectiveness of the GPT-4 model and our prompt engineering techniques in emulating Lovecraft's literary style. In addition to presenting the GPT model's capabilities, this paper provides a comprehensive description of its underlying architecture and offers a comparative analysis with related work that simulates other notable authors and philosophers, such as Dennett. By exploring the potential of large language models in the context of literary emulation, our study contributes to the body of research on the applications and limitations of these models in various creative domains.",10.48550/arXiv.2305.03429,"['JournalArticle', 'Review']",Computer Science; Linguistics
"Dungeons, Dragons, and Emotions: A Preliminary Study of Player Sentiment in LLM-driven TTRPGs",2024,"Xiao You, Pittawat Taveekitworachai, Siyuan Chen, Mustafa Can Gursesli, Xiaoxu Li, Yi Xia, R. Thawonmas","In this paper, we present a Tabletop Role-Playing game (TTRPG) driven by ChatGPT. Prompts are employed to instruct ChatGPT to act as Game Masters (GMs). In crafting each prompt to integrate a distinctive role, three roles denoted as Role 1, Role 2, and Role 3, are established. Subsequently, we perform pre-game and post-game emotional assessments employing the Positive and Negative Affect Schedule (PANAS) questionnaire to scrutinize players emotional dynamics throughout the gaming experience. Upon analyzing the collected data, we observe that Role 1 and Role 2 affect players positive emotions. Notably, Role 2 exhibits the most pronounced influence on players positive emotions. Our findings demonstrate that a TTRPG GM powered by ChatGPT can significantly enhance players positive emotions. This leads us to recognize that TTRPG GM powered by ChatGPT plays a positive role in enhancing the mental well-being of specific populations.",10.1145/3649921.3656991,"['Book', 'JournalArticle']",Computer Science; Psychology
Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF,2024,"Zhaolin Gao, Wenhao Zhan, Jonathan D. Chang, Gokul Swamy, Kiant Brantley, Jason D. Lee, Wen Sun","Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn reinforcement learning from human feedback (RLHF) methods to the multi-turn setting by treating all prior dialogue turns as a long context. Such approaches suffer from covariate shift: the conversations in the training set have previous turns generated by some reference policy, which means that low training error may not necessarily correspond to good performance when the learner is actually in the conversation loop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient policy optimization approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single model to estimate $Q$-values and trains on self-generated data, addressing the covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of regression tasks on iteratively collected datasets, enabling ease of implementation. Theoretically, we prove that REFUEL can match the performance of any policy covered by the training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our model. REFUEL consistently outperforms state-of-the-art methods such as DPO and REBEL across various settings. Furthermore, despite having only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found at https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be found at https://huggingface.co/Cornell-AGI.",10.48550/arXiv.2410.04612,['JournalArticle'],Computer Science
On Multimodal Emotion Recognition for Human-Chatbot Interaction in the Wild,2024,"N. Kovaevi, Christian Holz, Markus Gross, Rafael Wampfler","The field of natural language generation is swiftly evolving, giving rise to powerful conversational characters for use in different applications such as entertainment, education, and healthcare. A central aspect of these applications is providing personalized interactions, driven by the ability of the characters to recognize and adapt to user emotions. Current emotion recognition models primarily rely on datasets collected from actors or in controlled laboratory settings focusing on human-human interactions, which hinders their adaptability to real-world applications for conversational agents. In this work, we unveil the complexity of human-chatbot emotion recognition in the wild. We collected a multimodal dataset consisting of text, audio, and video recordings from 99 participants while they conversed with a GPT-3-based chatbot over three weeks. Using different transformer-based multimodal emotion recognition networks, we provide evidence for a strong domain gap between human-human interaction and human-chatbot interaction that is attributed to the subjective nature of self-reported emotion labels, the reduced activation and expressivity of the face, and the inherent subtlety of emotions in such settings, emphasizing the challenges of recognizing user emotions in real-world contexts. We show how personalizing our model to the user increases the model performance by up to 38% (user emotions) and up to 41% (perceived chatbot emotions), highlighting the potential of personalization for overcoming the observed domain gap.",10.1145/3678957.3685759,"['Book', 'JournalArticle']",Computer Science
What Skills Do You Need When Developing Software Using ChatGPT? (Discussion Paper),2023,"J. Jeuring, Roel Groot, Hieke Keuning","Since the release of LLM-based tools such as GitHub Copilot and ChatGPT the media and popular scientific literature, but also journals such as the Communications of the ACM, have been flooded with opinions how these tools will change programming. The opinions range from machines will program themselves, to AI does not help programmers. Of course, these statements are meant to to stir up a discussion, and should be taken with a grain of salt, but we argue that such unfounded statements are potentially harmful. Instead, we propose to investigate which skills are required to develop software using LLM-based tools. In this paper we report on an experiment in which we explore if Computational Thinking (CT) skills predict the ability to develop software using LLM-based tools. Our results show that the ability to develop software using LLM-based tools can indeed be predicted by the score on a CT assessment. There are many limitations to our experiment, and this paper is also a call to discuss how to approach, preferably experimentally, the question of which skills are required to develop software using LLM-based tools. We propose to rephrase this question to include by what kind of people/programmers, to develop what kind of software using what kind of LLM-based tools.",10.1145/3631802.3631807,"['Book', 'JournalArticle']",Computer Science
VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation,2024,"Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan S. Kankanhalli, Junnan Li","Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice question answering in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to capture the complex demands of real-world users. To address this limitationand due to the prohibitive cost and slow pace of human annotation for video taskswe introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS Chatbot Arenas framework, designed to automatically assess LMMs video analysis abilities. VideoAutoArena utilizes user simulation to generate open-ended, adaptive questions that rigorously assess model performance in video understanding. The benchmark features an automated, scalable evaluation framework, incorporating a modified ELO Rating System for fair and continuous comparisons across multiple LMMs. To validate our automated judging system, we construct a ""gold standard"" using a carefully curated subset of human annotations, demonstrating that our arena strongly aligns with human judgment while maintaining scalability. Additionally, we introduce a fault-driven evolution strategy, progressively increasing question complexity to push models toward handling more challenging video analysis scenarios. Experimental results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs, providing insights into model strengths and areas for improvement. To further streamline our evaluation, we introduce VideoAutoBench as an auxiliary benchmark, where human annotators label winners in a subset of VideoAutoArena battles. We use GPT-4o as a judge to compare responses against these human-validated answers. Together, VideoAu-Toarena and VideoAutoBench offer a cost-effective, and scalable framework for evaluating LMMs in user-centric video analysis.",10.1109/CVPR52734.2025.00792,"['JournalArticle', 'Conference']",Computer Science
Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think,2024,"Xinpeng Wang, Chengzhi Hu, Bolei Ma, Paul Rttger, Barbara Plank","Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token prediction. An alternative way is to examine the text output. Prior work has shown that first token probabilities lack robustness to changes in MCQ phrasing, and that first token probabilities do not match text answers for instruction-tuned models. Therefore, in this paper, we investigate the robustness of text answers. We show that the text answers are more robust to question perturbations than the first token probabilities, when the first token answers mismatch the text answers. The difference in robustness increases as the mismatch rate becomes greater. As the mismatch reaches over 50\%, the text answer is more robust to option order changes than the debiased first token probabilities using state-of-the-art debiasing methods such as PriDe. Our findings provide further evidence for the benefits of text answer evaluation over first token probability evaluation.",10.48550/arXiv.2404.08382,['JournalArticle'],Computer Science
A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case,2024,"Sonia Meyer, Shreya Singh, Bertha Tam, Christopher Ton, Angel Ren","This research compares large language model (LLM) fine-tuning methods, including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning (RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally compared LLM evaluation methods including End to End (E2E) benchmark method of""Golden Answers"", traditional natural language processing (NLP) metrics, RAG Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation, using the travel chatbot use case. The travel dataset was sourced from the the Reddit API by requesting posts from travel-related subreddits to get travel-related conversation prompts and personalized travel experiences, and augmented for each fine-tuning method. We used two pretrained LLMs utilized for fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to the two pretrained models. The inferences from these models are extensively evaluated against the aforementioned metrics. The best model according to human evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a Reinforcement Learning from Human Feedback (RLHF) training pipeline, and ultimately was evaluated as the best model. Our main findings are that: 1) quantitative and Ragas metrics do not align with human evaluation, 2) Open AI GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep humans in the loop for evaluation because, 4) traditional NLP metrics insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms QLoRA, but still needs postprocessing, 7) RLHF improves model performance significantly. Next steps include improving data quality, increasing data quantity, exploring RAG methods, and focusing data collection on a specific city, which would improve data quality by narrowing the focus, while creating a useful product.",10.48550/arXiv.2408.03562,['JournalArticle'],Computer Science
Synthetic Data Generation with LLM for Improved Depression Prediction,2024,"Andrea Kang, Jun Yu Chen, Zoe Lee-Youngzie, Shuhao Fu","Automatic detection of depression is a rapidly growing field of research at the intersection of psychology and machine learning. However, with its exponential interest comes a growing concern for data privacy and scarcity due to the sensitivity of such a topic. In this paper, we propose a pipeline for Large Language Models (LLMs) to generate synthetic data to improve the performance of depression prediction models. Starting from unstructured, naturalistic text data from recorded transcripts of clinical interviews, we utilize an open-source LLM to generate synthetic data through chain-of-thought prompting. This pipeline involves two key steps: the first step is the generation of the synopsis and sentiment analysis based on the original transcript and depression score, while the second is the generation of the synthetic synopsis/sentiment analysis based on the summaries generated in the first step and a new depression score. Not only was the synthetic data satisfactory in terms of fidelity and privacy-preserving metrics, it also balanced the distribution of severity in the training dataset, thereby significantly enhancing the model's capability in predicting the intensity of the patient's depression. By leveraging LLMs to generate synthetic data that can be augmented to limited and imbalanced real-world datasets, we demonstrate a novel approach to addressing data scarcity and privacy concerns commonly faced in automatic depression detection, all while maintaining the statistical integrity of the original dataset. This approach offers a robust framework for future mental health research and applications.",10.48550/arXiv.2411.17672,['JournalArticle'],Computer Science; Psychology
Driving Generative Agents With Their Personality,2024,"Lawrence J. Klinkert, Stephanie Buongiorno, Corey Clark","This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.",10.48550/arXiv.2402.14879,['JournalArticle'],Computer Science; Psychology
OntoFact: Unveiling Fantastic Fact-Skeleton of LLMs via Ontology-Driven Reinforcement Learning,2024,"Ziyu Shang, Wenjun Ke, Nana Xiu, Peng Wang, Jiajun Liu, Yanhui Li, Zhizhao Luo, Ke Ji","Large language models (LLMs) have demonstrated impressive proficiency in information retrieval, while they are prone to generating incorrect responses that conflict with reality, a phenomenon known as intrinsic hallucination. The critical challenge lies in the unclear and unreliable fact distribution within LLMs trained on vast amounts of data. The prevalent approach frames the factual detection task as a question-answering paradigm, where the LLMs are asked about factual knowledge and examined for correctness. However, existing studies primarily focused on deriving test cases only from several specific domains, such as movies and sports, limiting the comprehensive observation of missing knowledge and the analysis of unexpected hallucinations. To address this issue, we propose OntoFact, an adaptive framework for detecting unknown facts of LLMs, devoted to mining the ontology-level skeleton of the missing knowledge. Specifically, we argue that LLMs could expose the ontology-based similarity among missing facts and introduce five representative knowledge graphs (KGs) as benchmarks. We further devise a sophisticated ontology-driven reinforcement learning (ORL) mechanism to produce error-prone test cases with specific entities and relations automatically. The ORL mechanism rewards the KGs for navigating toward a feasible direction for unveiling factual errors. Moreover, empirical efforts demonstrate that dominant LLMs are biased towards answering Yes rather than No, regardless of whether this knowledge is included. To mitigate the overconfidence of LLMs, we leverage a hallucination-free detection (HFD) strategy to tackle unfair comparisons between baselines, thereby boosting the result robustness. Experimental results on 5 datasets, using 32 representative LLMs, reveal a general lack of fact in current LLMs. Notably, ChatGPT exhibits fact error rates of 51.6% on DBpedia and 64.7% on YAGO, respectively. Additionally, the ORL mechanism demonstrates promising error prediction scores, with F1 scores ranging from 70% to 90% across most LLMs. Compared to the exhaustive testing, ORL achieves an average recall of 80% while reducing evaluation time by 35.29% to 63.12%.",10.1609/aaai.v38i17.29859,"['JournalArticle', 'Conference']",Computer Science
Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models,2024,"Kun Zhang, Jiali Zeng, Fandong Meng, Yuanzhuo Wang, Shiqi Sun, Long Bai, Huawei Shen, Jie Zhou","Large language models (LLMs) have recently demonstrated remarkable performance across various Natual Language Processing tasks. In the field of multi-hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a paradigm, using curated stepwise reasoning demonstrations to enhance LLM's ability to reason and produce coherent rational pathways. To ensure the accuracy, reliability, and traceability of the generated answers, many studies have incorporated information retrieval (IR) to provide LLMs with external knowledge. However, existing CoT with IR methods decomposes questions into sub-questions based on a single compositionality type, which limits their effectiveness for questions involving multiple compositionality types. Additionally, these methods suffer from inefficient retrieval, as complex questions often contain abundant information, leading to the retrieval of irrelevant information inconsistent with the query's intent. In this work, we propose a novel question decomposition framework called TRQA for multi-hop question answering, which addresses these limitations. Our framework introduces a reasoning tree (RT) to represent the structure of complex questions. It consists of four components: the Reasoning Tree Constructor (RTC), the Question Generator (QG), the Retrieval and LLM Interaction Module (RAIL), and the Answer Aggregation Module (AAM). Specifically, the RTC predicts diverse sub-question structures to construct the reasoning tree, allowing a more comprehensive representation of complex questions. The QG generates sub-questions for leaf-node in the reasoning tree, and we explore two methods for QG: prompt-based and T5-based approaches. The IR module retrieves documents aligned with sub-questions, while the LLM formulates answers based on the retrieved information. Finally, the AAM aggregates answers along the reason tree, producing a definitive response from bottom to top.",10.1609/aaai.v38i17.29928,"['JournalArticle', 'Conference']",Computer Science
Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey,2024,"Ruiyao Xu, Kaize Ding","Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.",10.48550/arXiv.2409.01980,"['JournalArticle', 'Conference', 'Review']",Computer Science
Writing user personas with Large Language Models: Testing phase 6 of a Thematic Analysis of semi-structured interviews,2023,S. Paoli,"The goal of this paper is establishing if we can satisfactorily perform a Thematic Analysis (TA) of semi-structured interviews using a Large Language Model (more precisely GPT3.5-Turbo). Building on previous work by the author, which established an embryonal process for conducting a TA with the model, this paper will perform a further analysis and then cover the last phase of a TA (phase 6), which entails the writing up of the result. This phase was not covered by the previous work. In particular, the focus will be on using the results of a TA done with the LLM on a dataset of user interviews, for writing user personas, with the model building on the TA to produce the personas narratives. User personas are models of real users, usually built from a data analysis like interviews with a sample of users. User personas are tools often used in User Centered Design processes. The paper shows that the model can build basic user personas with an acceptable quality deriving them from themes, and that the model can serve for the generation of ideas for user personas.",10.48550/arXiv.2305.18099,['JournalArticle'],Computer Science; Linguistics
A Survey of Personalization: From RAG to Agent,2025,"Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao Wang, Zhaochen Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, Xiangyu Zhao","Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.",10.48550/arXiv.2504.10147,"['JournalArticle', 'Review']",Computer Science
Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs),2023,"Xiaoliang Chen, Liangbin Li, Le Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, Dinuo Li","With the development of large language models (LLMs) like the GPT series, their widespread use across various application scenarios presents a myriad of challenges. This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes situations where LLMs might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. These challenges underscore problems in the training data and algorithmic design of LLMs. To address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretability, and incorporate ethics and fairness training. Future technological trends might lean towards iterative methodologies, multimodal learning, model personalization and customization, and real-time learning and feedback mechanisms. In conclusion, future LLMs should prioritize fairness, transparency, and ethics, ensuring they uphold high moral and ethical standards when serving humanity.",10.48550/arXiv.2310.13343,"['JournalArticle', 'Review']",Computer Science; Linguistics
Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving,2024,"Ziyue Li, Yuan Chang, Xiaoqiu Le","Large Language Models (LLMs) have shown remarkable potential across various domains, yet their application in addressing complex scientific problems remains a formidable challenge. This paper presents a novel methodology to augment the problem-solving capabilities of LLMs by assigning them roles as domain-specific experts. By simulating a panel of experts, each LLM is tasked with delivering professional and cautious responses to scientific inquiries. Our approach involves querying multiple LLMs and assessing the consistency of their responses. High agreement among the LLMs suggests greater confidence in the proposed solution, whereas discrepancies prompt a collaborative discussion among the LLMs to reach a consensus. This method emulates real-world scientific problem-solving processes, fostering a more reliable and robust mechanism for LLMs to tackle scientific questions. Our experimental results show that assigning roles to multiple LLMs as domain-specific experts significantly improves their accuracy and reliability in solving scientific problems. This framework has the potential to advance the application of AI in scientific research, enhancing its effectiveness and trustworthiness.",10.18653/v1/2024.sdp-1.23,nan,Computer Science
"PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Fusion in Question Answering",2024,"Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai Wong","In conversational AI, effectively employing long-term memory improves personalized and consistent response generation. Existing work only concentrated on a single type of long-term memory, such as preferences, dialogue history, or social relationships, overlooking their interaction in real-world contexts. To this end, inspired by the concept of semantic memory and episodic memory from cognitive psychology, we create a new and more comprehensive Chinese dataset, coined as PerLTQA, in which world knowledge, profiles, social relationships, events, and dialogues are considered to leverage the interaction between different types of long-term memory for question answering (QA) in conversation. Further, based on PerLTQA, we propose a novel framework for memory integration in QA, consisting of three subtasks: Memory Classification, Memory Retrieval, and Memory Fusion, which provides a comprehensive paradigm for memory modeling, enabling consistent and personalized memory utilization. This essentially allows the exploitation of more accurate memory information for better responses in QA. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate the importance of personal long-term memory in the QA task",10.18653/v1/2024.sighan-1.18,nan,Computer Science
A Survey on Failure Analysis and Fault Injection in AI Systems,2024,"Guangba Yu, Gou Tan, Haojia Huang, Zhenyu Zhang, Pengfei Chen, R. Natella, Zibin Zheng, Michael R. Lyu","The rapid advancement of Artificial Intelligence (AI) has led to its integration into various areas, especially with Large Language Models (LLMs) significantly enhancing capabilities in Artificial Intelligence Generated Content (AIGC). However, the complexity of AI systems has also exposed their vulnerabilities, necessitating robust methods for failure analysis (FA) and fault injection (FI) to ensure resilience and reliability. Despite the importance of these techniques, there lacks a comprehensive review of FA and FI methodologies in AI systems. This study fills this gap by presenting a detailed survey of existing FA and FI approaches across six layers of AI systems. We systematically analyze 142 studies to answer three research questions including (1) what are the prevalent failures in AI systems, (2) what types of faults can current FI tools simulate, (3) what gaps exist between the simulated faults and real-world failures. Our findings reveal a taxonomy of AI system failures, assess the capabilities of existing FI tools, and highlight discrepancies between real-world and simulated failures. Moreover, this survey contributes to the field by providing a framework for fault diagnosis, evaluating the state-of-the-art in FI, and identifying areas for improvement in FI techniques to enhance the resilience of AI systems.",10.1145/3732777,"['JournalArticle', 'Review']",Computer Science; Engineering
"Playwriting with Large Language Models: Perceived Features, Interaction Strategies and Outcomes",2024,"Paolo Grigis, Antonella De Angeli","Large Language Models (LLMs) are sparking debates about creativity, intellectual property, and artistic integrity. This paper focuses on creativity, defined as consensual agreement among domain experts. It presents an inductive analysis of seven semi-structured interviews with professional playwrights who engaged in a longitudinal project with the aim of writing a theatre script using commercial systems. Overall, participants regarded LLMs as unsuitable for playwrighting. However, they enjoyed the experience and identified utility for editorial tasks and brainstorming. A significant obstacle was associated with the politics embedded in LLMs. Not only did these systems avoid a language that could offend sensibilities, but they also refused to engage in taboos and conflicts, which are the core of dramaturgy. Other system features (speed, exploitation, and unpredictability) were sometimes considered conducive and sometimes detrimental to creativity. Participants experienced difficulties and tried to build common ground by trial and error. Often, this strategy evolved into role play: the playwright instructed the LLM to enact characters. The interaction provided hints of inspiration and fostered suspension of disbelief and ontological reflection. However, it often led to technology rejection. Comparing and contrasting our insights with related work, we conclude by opening new directions for research at the boundaries of HCI and AI.",10.1145/3656650.3656688,"['JournalArticle', 'Book']",Computer Science; Linguistics
Chaining Text-to-Image and Large Language Model: A Novel Approach for Generating Personalized e-commerce Banners,2024,"Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag, Yokila Arora, Sushant Kumar, Kannan Achan","Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-quality personalized banners for users.",10.1145/3637528.3671636,"['JournalArticle', 'Book', 'Conference', 'Review']",Computer Science
Epidemiology-informed Network for Robust Rumor Detection,2024,"Wei Jiang, Tong Chen, Xin Gao, Wentao Zhang, Lizhen Cui, Hongzhi Yin","The rapid spread of rumors on social media has posed significant challenges to maintaining public trust and information integrity. Since an information cascade process is essentially a propagation tree, recent rumor detection models leverage graph neural networks to additionally capture information propagation patterns, thus outperforming text-only solutions. Given the variations in topics and social impact of the root node, different source information naturally has distinct outreach capabilities, resulting in different heights of propagation trees. This variation, however, impedes the data-driven design of existing graph-based rumor detectors. Given a shallow propagation tree with limited interactions, it is unlikely for graph-based approaches to capture sufficient cascading patterns, questioning their ability to handle less popular news or early detection needs. In contrast, a deep propagation tree is prone to noisy user responses, and this can in turn obfuscate the predictions. In this paper, we propose a novel Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to enhance performance by overcoming data-driven methods' sensitivity to data quality. Meanwhile, to adapt epidemiology theory to rumor detection, it is expected that each user's stance toward the source information will be annotated. To bypass the costly and time-consuming human labeling process, we take advantage of large language models to generate stance labels, facilitating optimization objectives for learning epidemiology-informed representations. Our experimental results demonstrate that the proposed EIN not only outperforms state-of-the-art methods on real-world datasets but also exhibits enhanced robustness across varying tree depths.",10.1145/3696410.3714610,"['Book', 'JournalArticle', 'Conference']",Computer Science
Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?,2023,"Seyed Mahed Mousavi, Simone Caldarella, G. Riccardi","Longitudinal Dialogues (LD) are the most challenging type of conversation for human-machine dialogue systems. LDs include the recollections of events, personal thoughts, and emotions specific to each individual in a sparse sequence of dialogue sessions. Dialogue systems designed for LDs should uniquely interact with the users over multiple sessions and long periods of time (e.g. weeks), and engage them in personal dialogues to elaborate on their feelings, thoughts, and real-life events. In this paper, we study the task of response generation in LDs. We evaluate whether general-purpose Pre-trained Language Models (PLM) are appropriate for this purpose. We fine-tune two PLMs, GePpeTto (GPT-2) and iT5, using a dataset of LDs. We experiment with different representations of the personal knowledge extracted from LDs for grounded response generation, including the graph representation of the mentioned events and participants. We evaluate the performance of the models via automatic metrics and the contribution of the knowledge via the Integrated Gradients technique. We categorize the natural language generation errors via human evaluations of contextualization, appropriateness and engagement of the user.",10.18653/v1/2023.nlp4convai-1.1,['JournalArticle'],Computer Science
RosePO: Aligning LLM-based Recommenders with Human Values,2024,"Jiayi Liao, Xiangnan He, Ruobing Xie, Jiancan Wu, Yancheng Yuan, Xingwu Sun, Zhanhui Kang, Xiang Wang","Recently, there has been a growing interest in leveraging Large Language Models (LLMs) for recommendation systems, which usually adapt a pre-trained LLM to the recommendation scenario through supervised fine-tuning (SFT). However, both the pre-training and SFT stages fail to explicitly model the comparative relationships of a user's preferences on different items. To construct a""helpful and harmless""LLM-based recommender, we propose a general framework -- Recommendation with smoothing personalized Preference Optimization (RosePO), which better aligns with customized human values during the post-training stage. Specifically, in addition to the input and chosen response that naturally align with SFT data, we design a rejected sampling strategy tailored for enhancing helpfulness, along with two strategies aimed at mitigating biases to promote harmlessness. To ensure robustness against uncertain labels present in automatically constructed preference data, we introduce a personalized smoothing factor predicted by a preference oracle into the optimization objective. Evaluation on three real-world datasets demonstrates the effectiveness of our method, showcasing not only improved recommendation performance but also mitigation of semantic hallucination and popularity bias.",10.48550/arXiv.2410.12519,['JournalArticle'],Computer Science
"Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale",2025,"Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle Ungar, C. J. Taylor, Dan Roth","Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user's inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios. In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-person perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users' current situations and preferences, with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.",10.48550/arXiv.2504.14225,['JournalArticle'],Computer Science
LLM Questionnaire Completion for Automatic Psychiatric Assessment,2024,"Gony Rosenman, Lior Wolf, Talma Hendler","We employ a Large Language Model (LLM) to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The LLM is prompted to answer these questionnaires by impersonating the interviewee. The obtained answers are coded as features, which are used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is shown to enhance diagnostic accuracy compared to multiple baselines. It thus establishes a novel framework for interpreting unstructured psychological interviews, bridging the gap between narrative-driven and data-driven approaches for mental health assessment.",10.48550/arXiv.2406.06636,"['JournalArticle', 'Conference']",Computer Science; Psychology
Large Language Models Can Self-Correct with Minimal Effort,2024,"Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang",,10.48550/arXiv.2405.14092,['JournalArticle'],Computer Science
"Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities",2025,"Yaping Chai, Haoran Xie, S. J. Qin","The increasing size and complexity of pre-trained language models have demonstrated superior performance in many applications, but they usually require large training datasets to be adequately trained. Insufficient training sets could unexpectedly make the model overfit and fail to cope with complex tasks. Large language models (LLMs) trained on extensive corpora have prominent text generation capabilities, which improve the quality and quantity of data and play a crucial role in data augmentation. Specifically, distinctive prompt templates are given in personalised tasks to guide LLMs in generating the required content. Recent promising retrieval-based techniques further improve the expressive performance of LLMs in data augmentation by introducing external knowledge to enable them to produce more grounded-truth data. This survey provides an in-depth analysis of data augmentation in LLMs, classifying the techniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation and Hybrid Augmentation. We summarise the post-processing approaches in data augmentation, which contributes significantly to refining the augmented data and enabling the model to filter out unfaithful content. Then, we provide the common tasks and evaluation metrics. Finally, we introduce existing challenges and future opportunities that could bring further improvement to data augmentation.",10.48550/arXiv.2501.18845,"['JournalArticle', 'Review']",Computer Science; Linguistics
Citations and Trust in LLM Generated Responses,2025,"Yifan Ding, M. Facciani, Amrit Poudel, Ellen Joyce, Salvador Aguiaga, B. Veeramani, Sanmitra Bhattacharya, Tim Weninger","Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content.",10.48550/arXiv.2501.01303,['JournalArticle'],Computer Science
"A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment",2024,"Matteo G. Mecattaf, Ben Slater, Marko Tevsi'c, Jonathan Prunty, Konstantinos Voudouris, Lucy G. Cheke","As general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.",10.48550/arXiv.2410.23242,['JournalArticle'],Computer Science; Psychology
KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning,2025,"Wei Sun, Wen Yang, Pu Jian, Qianlong Du, Fuwei Cui, Shuo Ren, Jiajun Zhang","Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.",10.48550/arXiv.2505.16826,['JournalArticle'],Computer Science; Mathematics
"Hardware Design and Verification with Large Language Models: A Scoping Review, Challenges, and Open Issues",2024,"Meisam Abdollahi, S. Yeganli, Mohammad Baharloo, A. Baniasadi","Background: Large Language Models (LLMs) are emerging as promising tools in hardware design and verification, with recent advancements suggesting they could fundamentally reshape conventional practices. Objective: This study examines the significance of LLMs in shaping the future of hardware design and verification. It offers an extensive literature review, addresses key challenges, and highlights open research questions in this field. Design: in this scoping review, we survey over 360 papers most of the published between 2022 and 2024, including 71 directly relevant ones to the topic, to evaluate the current role of LLMs in advancing automation, optimization, and innovation in hardware design and verification workflows. Results: Our review highlights LLM applications across synthesis, simulation, and formal verification, emphasizing their potential to streamline development processes while upholding high standards of accuracy and performance. We identify critical challenges, such as scalability, model interpretability, and the alignment of LLMs with domain-specific languages and methodologies. Furthermore, we discuss open issues, including the necessity for tailored model fine-tuning, integration with existing Electronic Design Automation (EDA) tools, and effective handling of complex data structures typical of hardware projects. Conclusions: this survey not only consolidates existing knowledge but also outlines prospective research directions, underscoring the transformative role LLMs could play in the future of hardware design and verification.",10.3390/electronics14010120,"['JournalArticle', 'Review']",Computer Science; Engineering
MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving,2024,"Enming Zhang, Xingyuan Dai, Yisheng Lv, Qianghai Miao","Vision-language models (VLMs) serve as general-purpose end-to-end models in autonomous driving, performing subtasks such as prediction, planning, and perception through question-and-answer interactions. However, most existing methods rely on computationally expensive visual encoders and large language models (LLMs), making them difficult to deploy in real-world scenarios and real-time applications. Meanwhile, most existing VLMs lack the ability to process multiple images, making it difficult to adapt to multi-camera perception in autonomous driving. To address these issues, we propose a novel framework called MiniDrive, which incorporates our proposed Feature Engineering Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter (DI-Adapter). The FE-MoE effectively maps 2D features into visual token embeddings before being input into the language model. The DI-Adapter enables the visual token embeddings to dynamically change with the instruction text embeddings, resolving the issue of static visual token embeddings for the same image in previous approaches. Compared to previous works, MiniDrive achieves state-of-the-art performance in terms of parameter size, floating point operations, and response efficiency, with the smallest version containing only 83M parameters.",10.48550/arXiv.2409.07267,['JournalArticle'],Computer Science; Engineering
Machine Unlearning for Traditional Models and Large Language Models: A Short Survey,2024,Yi Xu,"With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the""right to be forgotten"". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only summarizes the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.",10.48550/arXiv.2404.01206,"['JournalArticle', 'Review']",Computer Science
Generative AI in Mafia-like Game Simulation,2023,"Munyeong Kim, Sungsu Kim","In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, financial constraints, and non-verbal limitations of the study were also discussed. The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more human-like attributes in AI.",10.48550/arXiv.2309.11672,['JournalArticle'],Computer Science
Learning From Free-Text Human Feedback - Collect New Datasets Or Extend Existing Ones?,2023,"Dominic Petrak, N. Moosavi, Ye Tian, Nikolai Rozanov, Iryna Gurevych","Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. However, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human feedback included in these datasets. In this work, we investigate this question for a variety of commonly used dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat, Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot. Using our observations, we derive new taxonomies for the annotation of free-text human feedback in dialogs and investigate the impact of including such data in response generation for three SOTA language generation models, including GPT-2, LLAMA, and Flan-T5. Our findings provide new insights into the composition of the datasets examined, including error types, user response types, and the relations between them.",10.48550/arXiv.2310.15758,"['JournalArticle', 'Conference']",Computer Science
AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents,2025,"Dakuo Wang, Ting-Yao Hsu, Yuxuan Lu, Limeng Cui, Yaochen Xie, William Headean, Bingsheng Yao, Akash Veeragouni, Jiapeng Liu, Sreyashi Nag, Jessie Wang","A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.",10.48550/arXiv.2504.09723,['JournalArticle'],Computer Science
Exploring Changes in Nation Perception with Nationality-Assigned Personas in LLMs,2024,"M. Kamruzzaman, Gene Louis Kim","Persona assignment has become a common strategy for customizing LLM use to particular tasks and contexts. In this study, we explore how evaluation of different nations change when LLMs are assigned specific nationality personas. We assign 193 different nationality personas (e.g., an American person) to four LLMs and examine how the LLM evaluations (or ''perceptions'')of countries change. We find that all LLM-persona combinations tend to favor Western European nations, though nation-personas push LLM behaviors to focus more on and treat the nation-persona's own region more favorably. Eastern European, Latin American, and African nations are treated more negatively by different nationality personas. We additionally find that evaluations by nation-persona LLMs of other nations correlate with human survey responses but fail to match the values closely. Our study provides insight into how biases and stereotypes are realized within LLMs when adopting different national personas. In line with the ''Blueprint for an AI Bill of Rights'', our findings underscore the critical need for developing mechanisms to ensure that LLM outputs promote fairness and avoid over-generalization.",10.48550/arXiv.2406.13993,"['JournalArticle', 'Review']",Computer Science; Political Science; Sociology
Representation Bias in Political Sample Simulations with Large Language Models,2024,"Weihong Qi, Hanjia Lyu, Jiebo Luo","This study explores representation biases in simulating political samples using Large Language Models (LLMs), focusing on vote choice and public opinion. We evaluate seven LLMs from diverse cultural backgrounds with data from the American National Election Studies, German Longitudinal Election Study, and Zuobiao Dataset. The study identifies three dimensions of bias: societal and cultural contexts, demographic groups, and political institutions. Results show higher simulation accuracy for vote choice than public opinion, particularly in English-speaking and democratic countries with bipartisan systems. The cultural backgrounds of development teams significantly influence simulation performance. The findings offer insights into addressing biases in AI-driven computational social science.",10.1145/3701716.3715591,"['JournalArticle', 'Book', 'Conference']",Computer Science; Political Science
TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues,2022,"Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, Sameer Singh",,10.48550/arXiv.2207.04154,['JournalArticle'],Computer Science
Conversational Gold: Evaluating Personalized Conversational Search System using Gold Nuggets,2025,"Zahra Abbasiantaeb, Simon Lupart, Leif Azzopardi, Jeffrey Dalton, Mohammad Aliannejadi","The rise of personalized conversational search systems has been driven by advancements in Large Language Models (LLMs), enabling these systems to retrieve and generate answers for complex information needs. However, the automatic evaluation of responses generated by Retrieval Augmented Generation (RAG) systems remains an understudied challenge. In this paper, we introduce a new resource for assessing the retrieval effectiveness and relevance of response generated by RAG systems, using a nugget-based evaluation framework. Built upon the foundation of TREC iKAT 2023, our dataset extends to the TREC iKAT 2024 collection, which includes 17 conversations and 20,575 relevance passage assessments, together with 2,279 extracted gold nuggets, and 62 manually written gold answers from NIST assessors. While maintaining the core structure of its predecessor, this new collection enables a deeper exploration of generation tasks in conversational settings. Key improvements in iKAT 2024 include: (1) ``gold nuggets'' -- concise, essential pieces of information extracted from relevant passages of the collection -- which serve as a foundation for automatic response evaluation; (2) manually written answers to provide a gold standard for response evaluation; (3) unanswerable questions to evaluate model hallucination; (4) expanded user personas, providing richer contextual grounding; and (5) a transition from Personal Text Knowledge Base (PTKB) ranking to PTKB classification and selection. Built on this resource, we provide a framework for long-form answer generation evaluation, involving nuggets extraction and nuggets matching, linked to retrieval. This establishes a solid resource for advancing research in personalized conversational search and long-form answer generation. Our resources are publicly available at https://github.com/irlabamsterdam/CONE-RAG.",10.48550/arXiv.2503.09902,"['JournalArticle', 'Conference']",Computer Science
ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness,2024,"Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee","Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.",10.48550/arXiv.2403.18121,['JournalArticle'],Computer Science
Conversational AI Powered by Large Language Models Amplifies False Memories in Witness Interviews,2024,"Samantha W. T. Chan, Pat Pataranutaporn, Aditya Suri, W. Zulfikar, Pattie Maes, Elizabeth F. Loftus","This study examines the impact of AI on human false memories -- recollections of events that did not occur or deviate from actual occurrences. It explores false memory induction through suggestive questioning in Human-AI interactions, simulating crime witness interviews. Four conditions were tested: control, survey-based, pre-scripted chatbot, and generative chatbot using a large language model (LLM). Participants (N=200) watched a crime video, then interacted with their assigned AI interviewer or survey, answering questions including five misleading ones. False memories were assessed immediately and after one week. Results show the generative chatbot condition significantly increased false memory formation, inducing over 3 times more immediate false memories than the control and 1.7 times more than the survey method. 36.4% of users' responses to the generative chatbot were misled through the interaction. After one week, the number of false memories induced by generative chatbots remained constant. However, confidence in these false memories remained higher than the control after one week. Moderating factors were explored: users who were less familiar with chatbots but more familiar with AI technology, and more interested in crime investigations, were more susceptible to false memories. These findings highlight the potential risks of using advanced AI in sensitive contexts, like police interviews, emphasizing the need for ethical considerations.",10.48550/arXiv.2408.04681,"['JournalArticle', 'Review']",Computer Science; Psychology
Identifying Early Maladaptive Schemas from Mental Health Question Texts,2023,"Sujatha Das Gollapalli, Beng Heng Ang, See-Kiong Ng","In Psychotherapy, maladaptive schemas  negative perceptions that an individual has of the self, others, or the world that endure despite ob-jective reality  often lead to resistance to treatments and relapse of mental health issues such as depression, anxiety, panic attacks etc. Identification of early maladaptive schemas (EMS) is thus a crucial step during Schema Therapy-based counseling sessions, where patients go through a detailed and lengthy EMS questionnaire. However, such an approach is not practical in offline counseling scenarios, such as community QA forums which are gaining popularity for people seeking mental health support. In this paper, we investigate both LLM (Large Language Models) and non-LLM approaches for identifying EMS labels using resources from Schema Therapy. Our evaluation indicates that recent LLMs can be effective for identifying EMS but their predictions lack explainability and are too sensitive to precise prompts. Both LLM and non-LLM methods are unable to reliably address the null cases, i.e. cases with no EMS labels. However, we posit that the two approaches show complementary properties and together, they can be used to further devise techniques for EMS identification.",10.18653/v1/2023.findings-emnlp.792,"['JournalArticle', 'Conference']",Computer Science; Psychology
Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values,2025,"Hadi Hosseini, Samarth Khanna","The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.",10.48550/arXiv.2502.00313,['JournalArticle'],Computer Science; Economics; Linguistics
GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation,2025,"Weiliang Tang, Jia-Hui Pan, Yunhui Liu, Masayoshi Tomizuka, Erran L. Li, Chi-Wing Fu, Mingyu Ding","We present GeoManip, a framework to enable generalist robots to leverage essential conditions derived from object and part relationships, as geometric constraints, for robot manipulation. For example, cutting the carrot requires adhering to a geometric constraint: the blade of the knife should be perpendicular to the carrot's direction. By interpreting these constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, enabling greater generalizability across diverse even unseen tasks, objects, and scenarios. Unlike vision-language-action models that require extensive training, operates training-free by utilizing large foundational models: a constraint generation module that predicts stage-specific geometric constraints and a geometry parser that identifies object parts involved in these constraints. A solver then optimizes trajectories to satisfy inferred constraints from task descriptions and the scene. Furthermore, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip's state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.",10.48550/arXiv.2501.09783,['JournalArticle'],Computer Science; Engineering
Virtual AIVantage: Leveraging Large Language Models for Enhanced VR Interview Preparation among Underrepresented Professionals in Computing,2023,"S. Ajri, Dat Nguyen, Swati Agarwal, Arun Kumar Reddy Padala, Caglar Yildirim","Technical interviews, a cornerstone of the hiring process for computer science (CS) jobs, often prove to be particularly stressful for individuals from underrepresented backgrounds in CS circles, including women and people of color. The heightened stress and pressure can negatively affect these individuals sense of belonging in CS. This paper introduces Virtual AIVantage, an innovative tool designed to address this issue by leveraging virtual reality (VR) and artificial intelligence (AI) to revolutionize technical interview preparation for underrepresented individuals in CS. Virtual AIVantage provides a bespoke, VR-based training experience, simulating realistic interview scenarios with the aid of AI-powered conversational agents. By using the capabilities of OpenAIs GPT-4, the tool offers real-time feedback, guidance, and example responses, fostering a comprehensive interview readiness for users.",10.1145/3626705.3631799,"['Book', 'JournalArticle']",Computer Science
"Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions",2025,"Hadi Amini, Md. Jueal Mia, Yasaman Saadati, Ahmed Imteaj, Seyedsina Nabavirazavi, Urmish Thakker, Md. Zarif Hossain, Awal Ahmed Fime, S. S. Iyengar","Language models (LMs) are machine learning models designed to predict linguistic patterns by estimating the probability of word sequences based on large-scale datasets, such as text. LMs have a wide range of applications in natural language processing (NLP) tasks, including autocomplete and machine translation. Although larger datasets typically enhance LM performance, scalability remains a challenge due to constraints in computational power and resources. Distributed computing strategies offer essential solutions for improving scalability and managing the growing computational demand. Further, the use of sensitive datasets in training and deployment raises significant privacy concerns. Recent research has focused on developing decentralized techniques to enable distributed training and inference while utilizing diverse computational resources and enabling edge AI. This paper presents a survey on distributed solutions for various LMs, including large language models (LLMs), vision language models (VLMs), multimodal LLMs (MLLMs), and small language models (SLMs). While LLMs focus on processing and generating text, MLLMs are designed to handle multiple modalities of data (e.g., text, images, and audio) and to integrate them for broader applications. To this end, this paper reviews key advancements across the MLLM pipeline, including distributed training, inference, fine-tuning, and deployment, while also identifying the contributions, limitations, and future areas of improvement. Further, it categorizes the literature based on six primary focus areas of decentralization. Our analysis describes gaps in current methodologies for enabling distributed solutions for LMs and outline future research directions, emphasizing the need for novel solutions to enhance the robustness and applicability of distributed LMs.",10.48550/arXiv.2503.16585,"['JournalArticle', 'Review']",Computer Science; Linguistics
Efficient Randomized Experiments Using Foundation Models,2025,"Piersilvio De  Bartolomeis, Javier Abad, Guanbo Wang, Konstantin Donhauser, Raymond M. Duch, Fanny Yang, Issa J. Dahabreh","Randomized experiments are the preferred approach for evaluating the effects of interventions, but they are costly and often yield estimates with substantial uncertainty. On the other hand, in silico experiments leveraging foundation models offer a cost-effective alternative that can potentially attain higher statistical precision. However, the benefits of in silico experiments come with a significant risk: statistical inferences are not valid if the models fail to accurately predict experimental responses to interventions. In this paper, we propose a novel approach that integrates the predictions from multiple foundation models with experimental data while preserving valid statistical inference. Our estimator is consistent and asymptotically normal, with asymptotic variance no larger than the standard estimator based on experimental data alone. Importantly, these statistical properties hold even when model predictions are arbitrarily biased. Empirical results across several randomized experiments show that our estimator offers substantial precision gains, equivalent to a reduction of up to 20% in the sample size needed to match the same precision as the standard estimator based on experimental data alone.",10.48550/arXiv.2502.04262,['JournalArticle'],Computer Science; Mathematics
Advances in Human Event Modeling: From Graph Neural Networks to Language Models,2024,"Songgaojun Deng, M. de Rijke, Yue Ning","Human events such as hospital visits, protests, and epidemic outbreaks directly affect individuals, communities, and societies. These events are often influenced by factors such as economics, politics, and public policies of our society. The abundance of online data sources such as social networks, official news articles, and personal blogs chronicle societal events, facilitating the development of AI models for social science, public health care, and decision making. Human event modeling generally comprises both the forecasting stage, which estimates future events based on historical data, and interpretation, which seeks to identify influential factors of such events to understand their causative attributes. Recent achievements, fueled by deep learning and the availability of public data, have significantly advanced the field of human event modeling. This survey offers a systematic overview of deep learning technologies for forecasting and interpreting human events, with a primary focus on political events. We first introduce the existing challenges and background in this domain. We then present the problem formulation of event forecasting and interpretation. We investigate recent achievements in graph neural networks, owing to the prevalence of relational data and the efficacy of graph learning models. We also discuss the latest studies that utilize large language models for event reasoning. Lastly, we provide summaries of data resources, open challenges, and future research directions in the study of human event modeling.",10.1145/3637528.3671466,"['Book', 'JournalArticle', 'Conference', 'Review']",Computer Science
Reward Difference Optimization For Sample Reweighting In Offline RLHF,2024,"Shiqi Wang, Zhengze Zhang, Rui Zhao, Fei Tan, C. Nguyen","With the rapid advances in Large Language Models (LLMs), aligning LLMs with human preferences become increasingly important. Although Reinforcement Learning with Human Feedback (RLHF) proves effective, it is complicated and highly resource-intensive. As such, offline RLHF has been introduced as an alternative solution, which directly optimizes LLMs with ranking losses on a fixed preference dataset. Current offline RLHF only captures the""ordinal relationship""between responses, overlooking the crucial aspect of how much one is preferred over the others. To address this issue, we propose a simple yet effective solution called Reward Difference Optimization, shorted as RDO. Specifically, we introduce reward difference coefficients to reweigh sample pairs in offline RLHF. We then develop a difference model which captures rich interactions between a pair of responses for predicting these difference coefficients. Experiments with 7B LLMs on the HH and TL;DR datasets substantiate the effectiveness of our method in both automatic metrics and human evaluation, thereby highlighting its potential for aligning LLMs with human intent and values",10.18653/v1/2024.findings-emnlp.115,"['JournalArticle', 'Conference']",Computer Science
"A Survey of Financial AI: Architectures, Advances and Open Challenges",2024,Junhua Liu,"Financial AI empowers sophisticated approaches to financial market forecasting, portfolio optimization, and automated trading. This survey provides a systematic analysis of these developments across three primary dimensions: predictive models that capture complex market dynamics, decision-making frameworks that optimize trading and investment strategies, and knowledge augmentation systems that leverage unstructured financial information. We examine significant innovations including foundation models for financial time series, graph-based architectures for market relationship modeling, and hierarchical frameworks for portfolio optimization. Analysis reveals crucial trade-offs between model sophistication and practical constraints, particularly in high-frequency trading applications. We identify critical gaps and open challenges between theoretical advances and industrial implementation, outlining open challenges and opportunities for improving both model performance and practical applicability.",10.48550/arXiv.2411.12747,"['JournalArticle', 'Review']",Business; Computer Science; Economics
PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries,2024,"Mingwen Dong, Nischal Ashok Kumar, Yiqun Hu, Anuj Chauhan, Chung-Wei Hang, Shuaichen Chang, Lin Pan, Wuwei Lan, Henghui Zhu, Jiarong Jiang, Patrick Ng, Zhiguo Wang","Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking clarification, the user's clarification, and the assistant's clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We will release our code for data generation and experiments on GitHub.",10.48550/arXiv.2410.11076,"['JournalArticle', 'Conference']",Computer Science
A Survey on Large Language Models for Mathematical Reasoning,2025,"Pengyuan Wang, Tian-Shuo Liu, Chenyang Wang, Yidi Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, Yang Yu","Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and""test-time scaling"". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains.",10.48550/arXiv.2506.08446,"['JournalArticle', 'Review']",Computer Science; Mathematics
Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis,2025,"Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz","For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.",10.48550/arXiv.2506.11526,"['JournalArticle', 'Review']",Computer Science; Engineering
"From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models",2024,"Charles Zhang, Benji Peng, Xintian Sun, Qian Niu, Junyu Liu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Ming Liu, Yichao Zhang, Cheng Fei, Caitlyn Heqi Yin, Lawrence K.Q. Yan, Tianyang Wang","Word embeddings and language models have transformed natural language processing (NLP) by facilitating the representation of linguistic elements in continuous vector spaces. This review visits foundational concepts such as the distributional hypothesis and contextual similarity, tracing the evolution from sparse representations like one-hot encoding to dense embeddings including Word2Vec, GloVe, and fastText. We examine both static and contextualized embeddings, underscoring advancements in models such as ELMo, BERT, and GPT and their adaptations for cross-lingual and personalized applications. The discussion extends to sentence and document embeddings, covering aggregation methods and generative topic models, along with the application of embeddings in multimodal domains, including vision, robotics, and cognitive science. Advanced topics such as model compression, interpretability, numerical encoding, and bias mitigation are analyzed, addressing both technical challenges and ethical implications. Additionally, we identify future research directions, emphasizing the need for scalable training techniques, enhanced interpretability, and robust grounding in non-textual modalities. By synthesizing current methodologies and emerging trends, this survey offers researchers and practitioners an in-depth resource to push the boundaries of embedding-based language models.",10.48550/arXiv.2411.05036,"['JournalArticle', 'Review']",Computer Science; Linguistics
A Comprehensive Study and Implementation of the Mock Interview Simulator with AI and Pose-Based Interaction,2024,"Balasaheb Jadhav, Avadhut Sawant, Arnav Shah, Pranamya Vemula, Abhijeet Waikar, Srushti Yadav","The contemporary job market is increasingly competitive, placing a premium on a combination of technical prowess and essential soft skills. The ability to navigate job interviews effectively has emerged as a decisive factor in securing coveted positions. Recognizing this imperative, our contribution to the field is a pioneering solution - the Mock Interview Simulator. This simulator stands as a transformative tool in the landscape of job preparation, offering users an unprecedented advantage. In the current job market milieu, the Mock Interview Simulator redefines conventional preparation methods, providing a revolutionary approach for job seekers. This innovative platform leverages state-of-the-art technologies to empower users to tailor their interview experiences. The simulator introduces AI-driven interviewers, delivering a personalized and dynamic environment. This not only prepares candidates for a diverse range of interview scenarios but also imparts valuable insights into their performance. This paper delves into the intricate design and functionality of our Mock Interview Simulator, elucidating how cutting-edge technologies converge to create a realistic and highly beneficial interview preparation experience. By amalgamating speech recognition, text-to-speech synthesis, and the OpenAI GPT-3.5 Turbo API, the simulator crafts a lifelike interview setting. Additionally, posture detection using the Mediapipe framework adds a physical dimension, ensuring candidates not only articulate their responses effectively but also present themselves professionally.",10.1109/IC-CGU58078.2024.10530717,['Conference'],Computer Science
From Deep Learning to LLMs: A survey of AI in Quantitative Investment,2025,"Bokai Cao, Sai Wang, Xinyi Lin, Xiaojun Wu, Haohan Zhang, Lionel M. Ni, Jian Guo","Quantitative investment (quant) is an emerging, technology-driven approach in asset management, increasingy shaped by advancements in artificial intelligence. Recent advances in deep learning and large language models (LLMs) for quant finance have improved predictive modeling and enabled agent-based automation, suggesting a potential paradigm shift in this field. In this survey, taking alpha strategy as a representative example, we explore how AI contributes to the quantitative investment pipeline. We first examine the early stage of quant research, centered on human-crafted features and traditional statistical models with an established alpha pipeline. We then discuss the rise of deep learning, which enabled scalable modeling across the entire pipeline from data processing to order execution. Building on this, we highlight the emerging role of LLMs in extending AI beyond prediction, empowering autonomous agents to process unstructured data, generate alphas, and support self-iterative workflows.",10.48550/arXiv.2503.21422,"['JournalArticle', 'Review']",Business; Computer Science; Economics
From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs,2025,"Ruxiao Chen, Chenguang Wang, Yuran Sun, Xilei Zhao, Susu Xu","Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE",10.48550/arXiv.2502.17701,"['JournalArticle', 'Conference', 'Review']",Computer Science; Environmental Science
Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?,2025,"Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, O. Shorinwa, Anirudha Majumdar","Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans'innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.",10.48550/arXiv.2506.18183,['JournalArticle'],Computer Science
User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data,2025,"Haowei Yang, Qin Lu, Yang Wang, Sibei Liu, Jiayun Zheng, Ao Xiang","With the widespread application of large language models (LLMs), user privacy protection has become a significant research topic. Existing privacy preference modeling methods often rely on large-scale user data, making effective privacy preference analysis challenging in data-limited environments. This study explores how LLMs can analyze user behavior related to privacy protection in scenarios with limited data and proposes a method that integrates Few-shot Learning and Privacy Computing to model user privacy preferences. The research utilizes anonymized user privacy settings data, survey responses, and simulated data, comparing the performance of traditional modeling approaches with LLM-based methods. Experimental results demonstrate that, even with limited data, LLMs significantly improve the accuracy of privacy preference modeling. Additionally, incorporating Differential Privacy and Federated Learning further reduces the risk of user data exposure. The findings provide new insights into the application of LLMs in privacy protection and offer theoretical support for advancing privacy computing and user behavior analysis.",10.48550/arXiv.2505.06305,"['JournalArticle', 'Review']",Computer Science
Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization,2025,"Kesen Zhao, Beier Zhu, Qianru Sun, H. Zhang","Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in https://github.com/kesenzhao/UV-CoT.",10.48550/arXiv.2504.18397,['JournalArticle'],Computer Science
A Comprehensive Survey on Deep Learning-based Predictive Maintenance,2025,"Uzair Khan, D. Cheng, Francesco Setti, Franco Fummi, Marco Cristani, Luigi Capogrosso","With the advent of Industrial 4.0 and the push towards Industry 5.0, the data generated by the industries have become surprisingly large. This abundance of data significantly boosts machine and deep learning models for Predictive Maintenance (PdM). The PdM plays a vital role in extending the lifespan of industrial equipment and machines while also helping to reduce the risk of unscheduled downtime. Given its multidisciplinary nature, the field of PdM has been approached from many different angles: this comprehensive survey aims to provide an up-to-date overview focused on all the learning-based industrial PdM strategies, discussing weaknesses and strengths. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing a systematic and complete review of the literature. In particular, firstly, we explore the main learning models used for PdM, mainly Convolutional Neural Networks (ConvNets), Autoencoders (AEs), Generative Adversarial Networks (GANs), and Transformers, also giving an overview of the newest models such as diffusion models and foundation models. Then, we discuss the main learning paradigms applied to PdM, i.e., supervised, unsupervised, ensemble, transfer, federated, and reinforcement learning. Furthermore, this work discusses the pipeline of the data-driven PdM and its benefits, practical applications, datasets, and benchmarks. In addition, the evaluation metrics for each PdM stage and the state-of-the-art hardware devices used are discussed. Finally, the challenges and future work are presented.",10.1145/3732287,"['JournalArticle', 'Review']",Computer Science; Engineering
"Adversarial machine learning: a review of methods, tools, and critical industry sectors",2025,"Sotiris Pelekis, Thanos Koutroubas, Afroditi Blika, Anastasis Berdelis, Evangelos Karakolis, Christos Ntanos, Evangelos Spiliotis, Dimitris Askounis",,10.1007/s10462-025-11147-4,"['JournalArticle', 'Review']",Computer Science; Engineering
A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation,2024,"Miuru Abeysiriwardana, D. Sumanathilaka","This paper explores techniques that focus on understanding and resolving ambiguity in language within the field of natural language processing (NLP), highlighting the complexity of linguistic phenomena such as polysemy and homonymy and their implications for computational models. Focusing extensively on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from deep learning techniques to leveraging lexical resources and knowledge graphs like WordNet. The paper introduces cutting-edge methodologies like word sense extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy by predicting new word senses. It examines specific applications in biomedical disambiguation and language specific optimisation and discusses the significance of cognitive metaphors in discourse analysis. The research identifies persistent challenges in the field, such as the scarcity of sense annotated corpora and the complexity of informal clinical texts. It concludes by suggesting future directions, including using large language models, visual WSD, and multilingual WSD systems, emphasising the ongoing evolution in addressing lexical complexities in NLP. This thinking perspective highlights the advancement in this field to enable computers to understand language more accurately.",10.48550/arXiv.2403.16129,"['JournalArticle', 'Review']",Computer Science; Linguistics
Predicting the Performance of Black-box LLMs through Self-Queries,2025,"Dylan Sam, Marc Finzi, J. Kolter","As large language models (LLMs) are increasingly relied on in AI systems, predicting when they make mistakes is crucial. While a great deal of work in the field uses internal representations to interpret model behavior, these representations are inaccessible when given solely black-box access through an API. In this paper, we extract features of LLMs in a black-box manner by using follow-up prompts and taking the probabilities of different responses as representations to train reliable predictors of model behavior. We demonstrate that training a linear model on these low-dimensional representations produces reliable and generalizable predictors of model performance at the instance level (e.g., if a particular generation correctly answers a question). Remarkably, these can often outperform white-box linear predictors that operate over a model's hidden state or the full distribution over its vocabulary. In addition, we demonstrate that these extracted features can be used to evaluate more nuanced aspects of a language model's state. For instance, they can be used to distinguish between a clean version of GPT-4o-mini and a version that has been influenced via an adversarial system prompt that answers question-answering tasks incorrectly or introduces bugs into generated code. Furthermore, they can reliably distinguish between different model architectures and sizes, enabling the detection of misrepresented models provided through an API (e.g., identifying if GPT-3.5 is supplied instead of GPT-4o-mini).",10.48550/arXiv.2501.01558,['JournalArticle'],Computer Science
AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression,2025,"Dong Whi Yoo, Jiayue Melissa Shi, Violeta J. Rodriguez, Koustuv Saha","Recent advancements in LLMs enable chatbots to interact with individuals on a range of queries, including sensitive mental health contexts. Despite uncertainties about their effectiveness and reliability, the development of LLMs in these areas is growing, potentially leading to harms. To better identify and mitigate these harms, it is critical to understand how the values of people with lived experiences relate to the harms. In this study, we developed a technology probe, a GPT-4o based chatbot called Zenny, enabling participants to engage with depression self-management scenarios informed by previous research. We used Zenny to interview 17 individuals with lived experiences of depression. Our thematic analysis revealed key values: informational support, emotional support, personalization, privacy, and crisis management. This work explores the relationship between lived experience values, potential harms, and design recommendations for mental health AI chatbots, aiming to enhance self-management support while minimizing risks.",10.48550/arXiv.2504.18932,['JournalArticle'],Computer Science; Psychology
Distantly Supervised Explainable Stance Detection via Chain-of-Thought Supervision,2024,"Daijun Ding, Genan Dai, Cheng Peng, Xiaojiang Peng, Bowen Zhang, Hutchin Huang","Investigating public attitudes on social media is crucial for opinion mining systems. Stance detection aims to predict the attitude towards a specific target expressed in a text. However, effective neural stance detectors require substantial training data, which are challenging to curate due to the dynamic nature of social media. Moreover, deep neural networks (DNNs) lack explainability, rendering them unsuitable for scenarios requiring explanations. We propose a distantly supervised explainable stance detection framework (DS-ESD), comprising an instruction-based chain-of-thought (CoT) method, a generative network, and a transformer-based stance predictor. The CoT method employs prompt templates to extract stance detection explanations from a very large language model (VLLM). The generative network learns the input-explanation mapping, and a transformer-based stance classifier is trained with VLLM-annotated stance labels, implementing distant supervision. We propose a label rectification strategy to mitigate the impact of erroneous labels. Experiments on three benchmark datasets showed that our model outperformed the compared methods, validating its efficacy in stance detection tasks. This research contributes to the advancement of explainable stance detection frameworks, leveraging distant supervision and label rectification strategies to enhance performance and interpretability.",10.3390/math12071119,['JournalArticle'],Computer Science
"Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",2024,"Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han","Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.",10.48550/arXiv.2406.11709,"['JournalArticle', 'Conference']",Computer Science
AI-Driven Day-to-Day Route Choice,2024,"Leizhen Wang, Peibo Duan, Zhengbing He, Cheng Lyu, Xin Chen, Nan Zheng, Li Yao, Zhenliang Ma","Understanding travelers' route choices can help policymakers devise optimal operational and planning strategies for both normal and abnormal circumstances. However, existing choice modeling methods often rely on predefined assumptions and struggle to capture the dynamic and adaptive nature of travel behavior. Recently, Large Language Models (LLMs) have emerged as a promising alternative, demonstrating remarkable ability to replicate human-like behaviors across various fields. Despite this potential, their capacity to accurately simulate human route choice behavior in transportation contexts remains doubtful. To satisfy this curiosity, this paper investigates the potential of LLMs for route choice modeling by introducing an LLM-empowered agent,""LLMTraveler.""This agent integrates an LLM as its core, equipped with a memory system that learns from past experiences and makes decisions by balancing retrieved data and personality traits. The study systematically evaluates the LLMTraveler's ability to replicate human-like decision-making through two stages of day-to-day (DTD) congestion games: (1) analyzing its route-switching behavior in single origin-destination (OD) pair scenarios, where it demonstrates patterns that align with laboratory data but cannot be fully explained by traditional models, and (2) testing its capacity to model adaptive learning behaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network, producing results comparable to Multinomial Logit (MNL) and Reinforcement Learning (RL) models. These experiments demonstrate that the framework can partially replicate human-like decision-making in route choice while providing natural language explanations for its decisions. This capability offers valuable insights for transportation policymaking, such as simulating traveler responses to new policies or changes in the network.",10.48550/arXiv.2412.03338,['JournalArticle'],Computer Science; Engineering
What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics,2023,"J. Watson, Barend Beekhuizen, S. Stevenson","Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and peoples social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERTs predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others.",10.18653/v1/2023.acl-long.375,"['JournalArticle', 'Conference']",Computer Science; Linguistics; Psychology
"Industrial Segment Anything - a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul",2023,"Keno Moenck, Arne Wendt, Philipp Prnte, Julian Koch, Arne Sahrhage, Johann Gierecker, Ole Schmedemann, Falko Khler, Dirk Holst, Martin Gomse, Thorsten Schppstuhl, Daniel Schoepflin","Deploying deep learning-based applications in specialized domains like the aircraft production industry typically suffers from the training data availability problem. Only a few datasets represent non-everyday objects, situations, and tasks. Recent advantages in research around Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions. As recently demonstrated by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety. Although, investigating its application within specific domains is subject to ongoing research. This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases. We include manufacturing, intralogistics, as well as maintenance, repair, and overhaul processes, also representing a variety of other neighboring industrial domains. Besides presenting the various use cases, we further discuss the injection of domain knowledge.",10.48550/arXiv.2307.12674,"['JournalArticle', 'Review']",Computer Science; Engineering
How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild,2025,"Saad Obaid ul Islam, Anne Lauscher, Goran Glavas","In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild''than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.",10.48550/arXiv.2502.12769,['JournalArticle'],Computer Science; Linguistics
OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants,2024,"Jaspreet Ranjit, Brihi Joshi, Rebecca Dorn, Laura Petry, Olga Koumoundouros, Jayne Bottarini, Peichen Liu, Eric Rice, Swabha Swayamdipta","Warning: Contents of this paper may be upsetting.Public attitudes towards key societal issues, expressed on online media, are of immense value in policy and reform efforts, yet challenging to understand at scale. We study one such social issue: homelessness in the U.S., by leveraging the remarkable capabilities of large language models to assist social work experts in analyzing millions of posts from Twitter. We introduce a framing typology: Online Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames capturing critiques, responses and perceptions. We release annotations with varying degrees of assistance from language models, with immense benefits in scaling: 6.5 speedup in annotation time while only incurring a 3 point F1 reduction in performance with respect to the domain experts. Our experiments demonstrate the value of modeling OATH-Frames over existing sentiment and toxicity classifiers. Our large-scale analysis with predicted OATH-Frames on 2.4M posts on homelessness reveal key trends in attitudes across states, time periods and vulnerable populations, enabling new insights on the issue. Our work provides a general framework to understand nuanced public attitudes at scale, on issues beyond homelessness.",10.48550/arXiv.2406.14883,"['JournalArticle', 'Conference']",Computer Science; Sociology
Using LLMs to Model the Beliefs and Preferences of Targeted Populations,2024,"Keiichi Namikoshi, Alexandre L. S. Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Archiga","We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.",10.48550/arXiv.2403.20252,"['JournalArticle', 'Review']",Computer Science; Psychology
Language Model Uncertainty Quantification with Attention Chain,2025,"Yinghao Li, Rushi Qiang, Lama Moukheiber, Chao Zhang","Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM responses is increasingly important. This added complexity complicates uncertainty quantification (UQ) because the probabilities assigned to answer tokens are conditioned on a vast space of preceding reasoning tokens. Direct marginalization is infeasible, and the dependency inflates probability estimates, causing overconfidence in UQ. To address this, we propose UQAC, an efficient method that narrows the reasoning space to a tractable size for marginalization. UQAC iteratively constructs an""attention chain""of tokens deemed""semantically crucial""to the final answer via a backtracking procedure. Starting from the answer tokens, it uses attention weights to identify the most influential predecessors, then iterates this process until reaching the input tokens. The resulting chain is further refined with similarity filtering and probability thresholding, which reduce the reasoning space, facilitating the approximation of the marginal answer token probabilities. We validate UQAC on multiple reasoning benchmarks with advanced open-source LLMs, demonstrating that it consistently delivers reliable UQ estimates with high computational efficiency.",10.48550/arXiv.2503.19168,['JournalArticle'],Computer Science
XLM for Autonomous Driving Systems: A Comprehensive Review,2024,"Sonda Fourati, Wael Jaafar, Noura Baccar, Safwan Alfattani","Large Language Models (LLMs) have showcased remarkable proficiency in various information-processing tasks. These tasks span from extracting data and summarizing literature to generating content, predictive modeling, decision-making, and system controls. Moreover, Vision Large Models (VLMs) and Multimodal LLMs (MLLMs), which represent the next generation of language models, a.k.a., XLMs, can combine and integrate many data modalities with the strength of language understanding, thus advancing several information-based systems, such as Autonomous Driving Systems (ADS). Indeed, by combining language communication with multimodal sensory inputs, e.g., panoramic images and LiDAR or radar data, accurate driving actions can be taken. In this context, we provide in this survey paper a comprehensive overview of the potential of XLMs towards achieving autonomous driving. Specifically, we review the relevant literature on ADS and XLMs, including their architectures, tools, and frameworks. Then, we detail the proposed approaches to deploy XLMs for autonomous driving solutions. Finally, we provide the related challenges to XLM deployment for ADS and point to future research directions aiming to enable XLM adoption in future ADS frameworks.",10.48550/arXiv.2409.10484,"['JournalArticle', 'Review']",Computer Science; Engineering
Reliable and Efficient Amortized Model-based Evaluation,2025,"Sang T. Truong, Yuheng Tu, Percy Liang, Bo Li, Oluwasanmi Koyejo","Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice.",10.48550/arXiv.2503.13335,['JournalArticle'],Computer Science; Mathematics
Creation of AI-driven Smart Spaces for Enhanced Indoor Environments - A Survey,2024,"Aygun Varol, Naser Hossein Motlagh, M. Leino, Sasu Tarkoma, Johanna Virkki","Smart spaces are ubiquitous computing environments that integrate diverse sensing and communication technologies to enhance space functionality, optimize energy utilization, and improve user comfort and well-being. The integration of emerging AI methodologies into these environments facilitates the formation of AI-driven smart spaces, which further enhance functionalities of the spaces by enabling advanced applications such as personalized comfort settings, interactive living spaces, and automatization of the space systems, all resulting in enhanced indoor experiences of the users. In this paper, we present a systematic survey of existing research on the foundational components of AI-driven smart spaces, including sensor technologies, data communication protocols, sensor network management and maintenance strategies, as well as the data collection, processing and analytics. Given the pivotal role of AI in establishing AI-powered smart spaces, we explore the opportunities and challenges associated with traditional machine learning (ML) approaches, such as deep learning (DL), and emerging methodologies including large language models (LLMs). Finally, we provide key insights necessary for the development of AI-driven smart spaces, propose future research directions, and sheds light on the path forward.",10.48550/arXiv.2412.14708,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Editorial: GPT revolutionizing AI applications: empowering future digital transformation,2023,Robin Qiu,"GPT stands for generative pre-trained transformer, which is a type of large language model (LLM) trained by leveraging neural networks and a vast large dataset mainly derived from the Internet. An LLM uses a recently fast evolved machine learning technique called deep learning to produce text that looks like it is produced by a human. When LLMs are well leveraged, empowered applications can perform various natural language processing tasks such as answering questions, summarizing texts and even generating or debugging lines of code. Undoubtedly, the ongoing, astonishing phenomenal artificial intelligence (AI) wave worldwide is attributed to the recently released ChatGPT whose LLM engine was trained by OpenAI using a massive dataset of web pages over the Internet, which included billions of texts from books, articles and other sources. Furthermore, the new release of GPT-4 from OpenAI further enhanced the capability of ChatGPT based on GPT-3.5. It takes a longer text and an image as input, while generating much richer and improved responses than its earlier version. Text summarization, humanlike conversation, music and essay composition, poetry and story writing, fake news generation are just a few listed here as GPT or the like AIs output today. Indeed, the generated viral sensation and excitement from GPT or generative AI in general have already made GPT an extremely hot topic for almost everyone, including from end users, educators, engineers, scientists, to entrepreneurs, investors, policymakers and so on so forth. The overwhelming attention received from the advances of GPT has stimulated the emergence of a lot of variants of ChatGPT worldwide. Due to the flocking-in effects of capitalism or marketing propaganda, many of those variants might be imposters. AI policymakers and educators are deeply concerned with the negativity of the outcomes derived from GPT products, including but not limited to disinformation, academic cheating, deepfakes, bias, ethical implications and political propaganda. However, the majority of GPT related AI products are truly transforming the world in a positive manner, focusing on significantly improved productivity, convenience and daily life assistance that should make the world brighter and more promising over time. Yes, it seems a little bit chaotic for now due to the lack of the needed AI regulations and risk management protocols. But AI will certainly continue to evolve and the output from the future AI could be drastically out of our imagination. While we are witnessing the dawn of a new digitalized era, the world starts to wonder how radical changes could be in the coming days when compared to the transformations in humanity with the arrival of electricity, personal computers, the Internet, or smartphones. As we are waiting for safe protocols and regulatory frameworks around AI to ensure that the risks aroundAIwill bemanageable, this editorial calls onmore research on howGPT or the like can revolutionize AI applications to empower positively the future digital transformations in serving the industry, government, public, society and human beings at large. Editorial",10.1108/dts-05-2023-066,['JournalArticle'],Computer Science
A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation,2025,"Shanhe You, Xuewen Luo, Xinhe Liang, Jiashu Yu, Chen Zheng, Jiangtao Gong","Evaluation methods for autonomous driving are crucial for algorithm optimization. However, due to the complexity of driving intelligence, there is currently no comprehensive evaluation method for the level of autonomous driving intelligence. In this paper, we propose an evaluation framework for driving behavior intelligence in complex traffic environments, aiming to fill this gap. We constructed a natural language evaluation dataset of human professional drivers and passengers through naturalistic driving experiments and post-driving behavior evaluation interviews. Based on this dataset, we developed an LLM-powered driving evaluation framework. The effectiveness of this framework was validated through simulated experiments in the CARLA urban traffic simulator and further corroborated by human assessment. Our research provides valuable insights for evaluating and designing more intelligent, human-like autonomous driving agents. The implementation details of the framework11https://github.com/AIR-DISCOVER/Driving-Intellenge-Evaluation-Framework and detailed information about the dataset22https://github.com/AIR-DISCOVER/Driving-Evaluation-Datasetcan be found at the provided links.",10.1109/ICRA55743.2025.11128380,"['JournalArticle', 'Conference']",Computer Science; Engineering
AutoPCF: A Novel Automatic Product Carbon Footprint Estimation Framework Based on Large Language Models,2024,"Biao Luo, Jinjie Liu, Zhu Deng, Can Yuan, Qingrun Yang, Lei Xiao, Yucong Xie, Fanke Zhou, Wenwen Zhou, Zhu Liu","Estimating the product carbon footprint (PCF) is crucial for sustainable consumption and supply chain decar-bonlization. The current life cycle assessment (LCA) methods frequently employed to evaluate PCFs often en-counter challenges, such as difficulties in determining the emission inventory and emission factors (EFs), as well as significant labor and time costs. To address these limitations, this paper presents AutoPCF, a novel auto-matic PCF estimation framework to conduct cradle-to-gate LCA for products. It utilizes deep learning models and large language models (LLMs) to automate and en-hance the estimation process. The framework comprises five stages: Emission Inventory Determination (EID), Activity Data Collection (ADC), Emission Factor Matching (EFM), Carbon Emission Estimation (CEE), and Estimation Verification and Evaluation (EVE). EID generates production processes and activity inventory, while ADC collects comprehensive activity data and EFM identifies accurate EFs. Emissions are then estimat-ed using the collected activity data and corresponding EFs. Experimental evaluations on steel, textile, and bat-tery products demonstrate the effectiveness of AutoPCF in improving the efficiency of PCF estimation. By auto-mating data collection and analysis, AutoPCF reduces re-liance on subjective decision-making and enhances the consistency and efficiency of carbon footprint assess-ments, advancing sustainable practices and supporting climate change mitigation efforts.",10.1609/aaaiss.v2i1.27656,['JournalArticle'],Computer Science; Environmental Science
Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations,2024,"Weronika Lajewska, K. Balog","Generative AI models face the challenge of hallucinations that can undermine users' trust in such systems. We approach the problem of conversational information seeking as a two-step process, where relevant passages in a corpus are identified first and then summarized into a final system response. This way we can automatically assess if the answer to the user's question is present in the corpus. Specifically, our proposed method employs a sentence-level classifier to detect if the answer is present, then aggregates these predictions on the passage level, and eventually across the top-ranked passages to arrive at a final answerability estimate. For training and evaluation, we develop a dataset based on the TREC CAsT benchmark that includes answerability labels on the sentence, passage, and ranking levels. We demonstrate that our proposed method represents a strong baseline and outperforms a state-of-the-art LLM on the answerability prediction task.",10.48550/arXiv.2401.11452,"['JournalArticle', 'Conference']",Computer Science
Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding Question-Answering Data,2025,"Jingran Xie, Shunwei Lei, Yue Yu, Yang Xiang, Hui Wang, Xixin Wu, Zhiyong Wu","Empathetic dialogue is crucial for natural human-computer interaction, allowing the dialogue system to respond in a more personalized and emotionally aware manner, improving user satisfaction and engagement. The emergence of large language models (LLMs) has revolutionized dialogue generation by harnessing their powerful capabilities and shown its potential in multimodal domains. Many studies have integrated speech with text-based LLMs to take speech question as input and output text response. However, the lack of spoken question-answering datasets that include speech style information to supervised fine-tuning (SFT) limits the performance of these systems. As a result, while these systems excel at understanding speech content, they often struggle to generate empathetic responses. In response, we propose a novel approach that circumvents the need for question-answering data, called Listen, Perceive, and Express (LPE). Our method employs a two-stage training process, initially guiding the LLM to listen the content and perceive the emotional aspects of speech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the model's potential for expressing empathetic responses based on listened spoken content and perceived emotional cues. We employ experiments to prove the effectiveness of proposed method. To our knowledge, this is the first attempt to leverage CoT for speech-based dialogue.",10.48550/arXiv.2501.10937,['JournalArticle'],Computer Science; Engineering; Linguistics
"Towards Collective Superintelligence, a pilot study",2023,"Louis B. Rosenberg, G. Willcox, Hans Schumann","Conversational Swarm Intelligence (CSI) is a new technology that enables human groups of potentially any size to hold real-time deliberative conversations online. Modeled on the dynamics of biological swarms, CSI aims to optimize group insights and amplify group intelligence. It uses Large Language Models (LLMs) in a novel framework to structure large-scale conversations, combining the benefits of small-group deliberative reasoning and large-group collective intelligence. In this study, a group of 241 real-time participants were asked to estimate the number of gumballs in a jar by looking at a photo. In one test case, individual participants entered their estimation in a standard survey. In another test case, participants converged on groupwise estimates collaboratively using a prototype CSI text-chat platform called Thinkscape. The results show that when using CSI, the group of 241 participants estimated within 12% of the correct answer, which was significantly more accurate (p<0.001) than the average individual (mean error of 55%) and the survey-based Wisdom of Crowd (error of 25%). The group using CSI was also more accurate than an estimate generated by GPT 4 (error of 42%). This suggests that CSI is a viable method for enabling large, networked groups to hold coherent real-time deliberative conversations that amplify collective intelligence. Because this technology is scalable, it could provide a possible pathway towards building a general-purpose Collective Superintelligence (CSi).",10.1109/HCCS59561.2023.10452485,"['JournalArticle', 'Conference', 'Review']",Computer Science; Psychology
MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation,2024,"Yan Ma, Yu Qiao, Pengfei Liu","A story premise succinctly defines a story's main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Precollect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.6k generated premises and 1k extended stories. Code: https://github.com/GAIR-NLP/MoPS.",10.48550/arXiv.2406.05690,"['JournalArticle', 'Conference']",Computer Science
Unlocking the 'Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience,2024,"Tao Chen, Siqi Zuo, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky","In business and marketing, analyzing the reasons behind buying is a fundamental step towards understanding consumer behaviors, shaping business strategies, and predicting market outcomes. Prior research on purchase reason has relied on surveys to gather data from users. However, this method is limited in scalability, often focusing on specific products or brands, and may not accurately represent the broader population due to the restricted number of participants involved. In our work, we propose purchase reason prediction as a novel task for modern AI models. To benchmark potential AI solutions for this new task, we first generate a dataset that consists of real-world explanations of why users make certain purchase decisions for various products. Our approach induces LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, LLM-driven evaluation as well as a small scale human evaluation confirm the effectiveness of this approach to obtaining high-quality, personalized purchase reasons and post-purchase experiences. With this novel dataset, we are able to benchmark the purchase reason prediction task using various LLMs. Moreover, we demonstrate how purchase reasons can be valuable for downstream applications, such as marketing-focused user behavior analysis, post-purchase experience and rating prediction in recommender systems, and serving as a new approach to justify recommendations.",10.48550/arXiv.2402.13417,"['JournalArticle', 'Review']",Computer Science
Unmasking Conversational Bias in AI Multiagent Systems,2025,"Erica Coppolillo, G. Manco, L. Aiello","Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our approach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for AI multi-agent systems. The code to perform the experiments is publicly available at https://anonymous.4open.science/r/LLMsConversationalBias-7725.",10.48550/arXiv.2501.14844,['JournalArticle'],Computer Science; Linguistics
Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots,2025,"Xi Zheng, Zhuoyang Li, Xinning Gui, Yuhan Luo","Personalized support is essential to fulfill individuals emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.",10.1145/3706598.3713453,"['Book', 'JournalArticle', 'Conference']",Computer Science; Psychology
Prompt Sketching for Large Language Models,2023,"Luca Beurer-Kellner, Mark Niklas Muller, Marc Fischer, Martin T. Vechev","Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders.",10.48550/arXiv.2311.04954,['JournalArticle'],Computer Science
Online Scheduling for LLM Inference with KV Cache Constraints,2025,"P. Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou","Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose a novel batching and scheduling algorithm that minimizes inference latency while effectively managing the KV cache's memory. More specifically, we make the following contributions. First, to evaluate the performance of online algorithms for scheduling in LLM inference, we introduce a hindsight optimal benchmark, formulated as an integer program that computes the minimum total inference latency under full future information. Second, we prove that no deterministic online algorithm can achieve a constant competitive ratio when the arrival process is arbitrary. Third, motivated by the computational intractability of solving the integer program at scale, we propose a polynomial-time online scheduling algorithm and show that under certain conditions it can achieve a constant competitive ratio. We also demonstrate our algorithm's strong empirical performance by comparing it to the hindsight optimal in a synthetic dataset. Finally, we conduct empirical evaluations on a real-world public LLM inference dataset, simulating the Llama2-70B model on A100 GPUs, and show that our algorithm significantly outperforms the benchmark algorithms. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.",10.48550/arXiv.2502.07115,['JournalArticle'],Computer Science; Mathematics
MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting,2024,"Hongda Sun, Hongzhan Lin, Haiyu Yan, Yang Song, Xin Gao, Rui Yan","Online recruitment platforms have reshaped job-seeking and recruiting processes, driving increased demand for applications that enhance person-job matching. Traditional methods generally rely on analyzing textual data from resumes and job descriptions, limiting the dynamic, interactive aspects crucial to effective recruitment. Recent advances in Large Language Models (LLMs) have revealed remarkable potential in simulating adaptive, role-based dialogues, making them well-suited for recruitment scenarios. In this paper, we propose \textbf{MockLLM}, a novel framework to generate and evaluate mock interview interactions. The system consists of two key components: mock interview generation and two-sided evaluation in handshake protocol. By simulating both interviewer and candidate roles, MockLLM enables consistent and collaborative interactions for real-time and two-sided matching. To further improve the matching quality, MockLLM further incorporates reflection memory generation and dynamic strategy modification, refining behaviors based on previous experience. We evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment platform. The experimental results indicate that MockLLM outperforms existing methods in matching accuracy, scalability, and adaptability across job domains, highlighting its potential to advance candidate assessment and online recruitment.",10.1145/3711896.3737051,['Conference'],Computer Science
Transformers and attention-based networks in quantitative trading: a comprehensive survey,2024,"Lucas Coelho e Silva, Gustavo de Freitas Fonseca, Paulo Andre L. Castro","Since the advent of the transformer neural network architecture, there has been a rapid adoption and investigation of its applicability in various domains, such as computer vision, speech processing, and natural language processing, with the latter most notably exemplified by the rise of Large Language Models. These accomplishments have also led to increased interest in other network architectures that rely on attention mechanisms, one of the building blocks of transformers. Transformers and other attention-based networks are being applied to the quantitative analysis, management, and trading of financial assets, be it for price movement prediction, discovery of trading strategies, portfolio optimization, and risk management. The applications range across different asset categories, including equity markets, foreign exchange pairs, cryptocurrencies, and futures markets. This survey aims to provide a comprehensive overview of the applications of attention-based networks within the field of quantitative analysis, management, and trading of financial assets. After a brief overview of transformers and attention mechanisms, we analyze the existing applications of these architectures for quantitative finance in a taxonomy of four specializations: Alpha Seeking, Risk Management, Portfolio Construction, and Execution. After comparing the literature in light of the research problems, modeling approaches, and complementary results, we discuss current challenges and research opportunities.",10.1145/3677052.3698684,"['Book', 'JournalArticle', 'Review']",Computer Science
Simulate and Eliminate: Revoke Backdoors for Generative Large Language Models,2024,"Haoran Li, Yulin Chen, Zihao Zheng, Qi Hu, Chunkit Chan, Heshan Liu, Yangqiu Song","With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased accessibility and unrestricted model training on massive data. A malicious adversary may publish poisoned data online and conduct backdoor attacks on the victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave innocuously for normal queries and generate harmful responses when the backdoor trigger is activated. Despite significant efforts paid to LLMs' safety issues, LLMs are still struggling against backdoor attacks. As Anthropic recently revealed, existing safety training strategies, including supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the backdoors once the LLM is backdoored during the pre-training stage. In this paper, we present Simulate and Eliminate (SANDE) to erase the undesired backdoored mappings for generative LLMs. We initially propose Overwrite Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger is known. Then, to handle scenarios where trigger patterns are unknown, we integrate OSFT into our two-stage framework, SANDE. Unlike other works that assume access to cleanly trained models, our safety-enhanced LLMs are able to revoke backdoors without any reference. Consequently, our safety-enhanced LLMs no longer produce targeted responses when the backdoor triggers are activated. We conduct comprehensive experiments to show that our proposed SANDE is effective against backdoor attacks while bringing minimal harm to LLMs' powerful capability.",10.1609/aaai.v39i1.32018,"['JournalArticle', 'Conference']",Computer Science
Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models,2024,"Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan","Accurate assessment of personality traits is crucial for effective psycho-counseling, yet traditional methods like self-report questionnaires are time-consuming and biased. This study exams whether Large Language Models (LLMs) can predict the Big Five personality traits directly from counseling dialogues and introduces an innovative framework to perform the task. Our framework applies role-play and questionnaire-based prompting to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory. We evaluated our framework on 853 real-world counseling sessions, finding a significant correlation between LLM-predicted and actual Big Five traits, proving the validity of framework. Moreover, ablation studies highlight the importance of role-play simulations and task simplification via questionnaires in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model, utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves a 130.95\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\% in personality prediction validity. In conclusion, LLMs can predict personality based on counseling dialogues. Our code and model are publicly available at \url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable tool for future research in computational psychometrics.",10.48550/arXiv.2406.17287,['JournalArticle'],Computer Science; Psychology
Affect Recognition in Conversations Using Large Language Models,2023,"Shutong Feng, Guangzhi Sun, Nurul Lubis, Chao Zhang, Milica Gavsi'c","Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence, the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study investigates the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP (Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al., 2014), covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluate and compare LLMs performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition errors on LLM predictions. With this work, we aim to shed light on the extent to which LLMs can replicate human-like affect recognition capabilities in conversations.",10.48550/arXiv.2309.12881,['JournalArticle'],Computer Science; Linguistics
Leveraging Large Language Models for Collective Decision-Making,2023,"Marios Papachristou, Longqi Yang, Chin-Chia Hsu","In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from each member's conversation with the system and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that many of the members' individual preferences are satisfied in an equitable way. Finally, we conduct a survey study involving human participants to assess our system's ability to aggregate preferences and reasoning about them. Our findings show that the system exhibits strong performance in both dimensions.",10.1145/3757418,"['JournalArticle', 'Review']",Computer Science
"Can LLM ""Self-report""?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots",2024,"Huiqi Zou, Pengda Wang, Zihan Yan, Tianjun Sun, Ziang Xiao","A chatbot's personality design is key to interaction quality. As chatbots evolved from rule-based systems to those powered by large language models (LLMs), evaluating the effectiveness of their personality design has become increasingly complex, particularly due to the open-ended nature of interactions. A recent and widely adopted method for assessing the personality design of LLM-based chatbots is the use of self-report questionnaires. These questionnaires, often borrowed from established human personality inventories, ask the chatbot to rate itself on various personality traits. Can LLM-based chatbots meaningfully""self-report""their personality? We created 500 chatbots with distinct personality designs and evaluated the validity of their self-report personality scores by examining human perceptions formed during interactions with these chatbots. Our findings indicate that the chatbot's answers on human personality scales exhibit weak correlations with both human-perceived personality traits and the overall interaction quality. These findings raise concerns about both the criterion validity and the predictive validity of self-report methods in this context. Further analysis revealed the role of task context and interaction in the chatbot's personality design assessment. We further discuss design implications for creating more contextualized and interactive evaluation.",10.48550/arXiv.2412.00207,['JournalArticle'],Computer Science; Psychology
Retrieval Augmented Generation (RAG) Based Restaurant Chatbot with AI Testability,2024,"Vani Bhat, Sree Divya Cheerla, Jinu Rose Mathew, Nupur Pathak, Guannan Liu, J. Gao","Post-COVID the restaurant industry is experiencing a surge in demand, presenting a unique challenge of efficiently managing increased customer flow while ensuring seamless interactions. Chatbots have emerged as an innovative solution to meet the demand increase. The paper addresses the enhancement of AI chatbots through the integration of Retrieval-Augmented Generation (RAG) with the Large Language Model (LLM). This paper focuses on the development of a restaurant chatbot that not only engages in natural-language conversations but also addresses context optimization and LLM optimization for restaurant context learning. The approach uses a Neo4j Knowledge graph built using the restaurant data as an external source of knowledge. The graph is traversed to match the user question with appropriate answer tokens using Term Frequency - Inverse Document Frequency (TF-IDF) embeddings. The relevant tokens along with user questions are used to provide additional context to the T5 language model to provide nuanced responses to the users. This improvement is quantitatively evidenced by a Bilingual Evaluation Understudy (BLEU) score of 0.60, indicating a high level of precision in language understanding and generation. An extensive evaluation of the chatbot includes assessing AI testability on the level of words, sentences, and information. These evaluations include simulated dialogue assessments and performance analyses, with a focus on the chatbot's ability to retrieve and integrate information. Based on the AI testability evaluation, the models consistently produce more knowledgeable, diverse, and relevant answers as compared with state-of-the-art models with an average information score in the range of 0.6-0.8.",10.1109/BigDataService62917.2024.00008,"['JournalArticle', 'Conference']",Computer Science
A Survey on Collaborative Mechanisms Between Large and Small Language Models,2025,"Yi Chen, JiaHao Zhao, HaoHao Han","Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.",10.48550/arXiv.2505.07460,"['JournalArticle', 'Review']",Computer Science; Linguistics
LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models,2024,"Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu Yao","The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error. This justifies its effectiveness. Our data and code are available at https://github.com/hrlics/LITE.",10.48550/arXiv.2404.01165,['JournalArticle'],Computer Science; Environmental Science
Large Language Models as Conversational Movie Recommenders: A User Study,2024,"Ruixuan Sun, Xinyi Li, A K Akella, J. Konstan","This paper explores the effectiveness of using large language models (LLMs) for personalized movie recommendations from users' perspectives in an online field experiment. Our study involves a combination of between-subject prompt and historic consumption assessments, along with within-subject recommendation scenario evaluations. By examining conversation and survey response data from 160 active users, we find that LLMs offer strong recommendation explainability but lack overall personalization, diversity, and user trust. Our results also indicate that different personalized prompting techniques do not significantly affect user-perceived recommendation quality, but the number of movies a user has watched plays a more significant role. Furthermore, LLMs show a greater ability to recommend lesser-known or niche movies. Through qualitative analysis, we identify key conversational patterns linked to positive and negative user interaction experiences and conclude that providing personal context and examples is crucial for obtaining high-quality recommendations from LLMs.",10.48550/arXiv.2404.19093,"['JournalArticle', 'Review']",Computer Science
EquinorQA: Large Language Models for Question Answering Over Proprietary Data,2024,"Daro Garigliotti, Bjarte Johansen, Jakob Vigerust Kallestad, Seong-Eun Cho, Csar Ferri",". Large Language Models (LLMs) have become the state-of-the-art technology in a variety of language understanding tasks. Accordingly, many commercial organizations have been increasingly trying to integrate LLMs in multiple areas of their production and analytics. A typical scenario is the need for answering questions over a domain-specific, private collection of documents, such that the answer is supported by evidence clearly referenced from those documents. The Retrieval-Augmented Generation (RAG) framework has been recently used by many applications for this kind of scenarios, as it intuitively bridges dedicated data collections and state-of-the-art generative models. Yet, LLMs are known to present data contamination, a phenomenon in which their performance on evaluation data relevant to a task is influenced by said data being already incorporated to the LLM during training phase. In this paper, we assess the performance of LLMs within the domain of Equinor, the largest energy company in Norway. Specifically, we address question answering with a RAG-based approach over a novel data collection not available for well-established LLMs during training, in order to study the effect of data contamination for this task. Beyond shedding light on LLM performance for a highly-demanded, realistic industrial scenario, we also analyze its potential impact for an ensemble of personas in Equinor with particular information needs and contexts.",10.3233/FAIA241049,"['JournalArticle', 'Conference']",Computer Science
Tipping the Balance: Human Intervention in Large Language Model MultiAgent Debate,2024,"Haley Triem, Ying Ding","Methods for eliciting reasoning from large language models (LLMs) are shifting from filtering natural language prompts through contextualized personas, towards structuring conversations between LLM instances, or agents. This work expands upon LLM multiagent debate by inserting human opinion into the loop of generated conversation. To simulate complex reasoning, LLM instances were given United States district court decisions and asked to debate whether to affirm or not affirm the decision. Agents were examined in three phases: synthetic debate, where one LLM instance simulated a threeagent discussion; multiagent debate, where three LLM instances discussed among themselves; and humanAI debate, where multiagent debate was interrupted by human opinion. During each phase, a ninestep debate was simulated onehundred times, yielding 2,700 total debate steps. Resulting conversations generated by synthetic debate followed a preset cadence, proving them ineffective at simulating individual agents and confirming that mechanism engineering is critical for multiagent debate. Furthermore, the reasoning process backing multiagent decisionmaking was strikingly similar to human decisionmaking. Finally, it is discovered that while LLMs do weigh human input more heavily than AI opinion, it is only by a small threshold. Ultimately, this work asserts that careful, humanintheloop framework is critical for designing valueaware, agentic AI agents.",10.1002/pra2.1034,['JournalArticle'],Computer Science
A Hybrid Approach to Dimensional Aspect-Based Sentiment Analysis Using BERT and Large Language Models,2024,"Yice Zhang, Hongling Xu, Delong Zhang, Ruifeng Xu","Dimensional aspect-based sentiment analysis (dimABSA) aims to recognize aspect-level quadruples from reviews, offering a fine-grained sentiment description for user opinions. A quadruple consists of aspect, category, opinion, and sentiment intensity, which is represented using continuous real-valued scores in the valencearousal dimensions. To address this task, we propose a hybrid approach that integrates the BERT model with a large language model (LLM). Firstly, we develop both the BERT-based and LLM-based methods for dimABSA. The BERT-based method employs a pipeline approach, while the LLM-based method transforms the dimABSA task into a text generation task. Secondly, we evaluate their performance in entity extraction, relation classification, and intensity prediction to determine their advantages. Finally, we devise a hybrid approach to fully utilize their advantages across different scenarios. Experiments demonstrate that the hybrid approach outperforms BERT-based and LLM-based methods, achieving state-of-the-art performance with an F1-score of 41.7% on the quadruple extraction.",10.3390/electronics13183724,"['JournalArticle', 'Review']",Computer Science
"Empowering Users with ChatGPT and Similar Large Language Models (LLMs): Everyday Information Needs, Uses, and Gratification",2024,"Boryung Ju, J. Stewart","Disruptive technologies such as ChatGPT and similar Large Language Models (LLMs) have transformed mundane everyday tasks of information users since their debut in late 2022. In this study, we leverage uses and gratifications theory to test a distinct set of motivations that drive users' satisfaction and continued use intentions of ChatGPT and similar large language models. Data were collected using a national online survey of 323 adults residing in the United States. We conducted data analysis using Partial Least Squares (PLSSEM) to investigate both direct and indirect impact of factors on users' gratification, thereby influencing the continued utilization of these tools for everyday information seeking. Results show four motivational factors  social influence, trust, personalization, and perceived usefulness  that positively influence users' satisfaction or sense of gratification, impacting their intentions to continue using these tools. This is one of the few early studies of ChatGPT and other LLMs from an information science perspective.",10.1002/pra2.1018,"['JournalArticle', 'Review']",Computer Science
Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective,2024,"Shenghao Xie, Wenqiang Zu, Mingyang Zhao, Duo Su, Shilong Liu, Ruohua Shi, Guoqi Li, Shanghang Zhang, Lei Ma","Autoregression in large language models (LLMs) has shown impressive scalability by unifying all language tasks into the next token prediction paradigm. Recently, there is a growing interest in extending this success to vision foundation models. In this survey, we review the recent advances and discuss future directions for autoregressive vision foundation models. First, we present the trend for next generation of vision foundation models, i.e., unifying both understanding and generation in vision tasks. We then analyze the limitations of existing vision foundation models, and present a formal definition of autoregression with its advantages. Later, we categorize autoregressive vision foundation models from their vision tokenizers and autoregression backbones. Finally, we discuss several promising research challenges and directions. To the best of our knowledge, this is the first survey to comprehensively summarize autoregressive vision foundation models under the trend of unifying understanding and generation. A collection of related resources is available at https://github.com/EmmaSRH/ARVFM.",10.48550/arXiv.2410.22217,"['JournalArticle', 'Review']",Computer Science
A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish,2024,"Masahiro Kaneko, Timothy Baldwin","Large Language Models (LLMs) are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data. Additionally, we propose a self-detection approach that uses few-shot learning in which LLMs detect whether instances are present or absent in their training data, in contrast to previous methods that do not employ explicit learning. To explore the ease of generating leaked information, we create a dataset of prompts designed to elicit personal information, copyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs produce leaked information in most cases despite less such data in their training set. This indicates even small amounts of leaked data can greatly affect outputs. Our self-detection method showed superior performance compared to existing detection methods.",10.48550/arXiv.2403.16139,"['JournalArticle', 'Review']",Computer Science
The Drama Machine: Simulating Character Development with LLM Agents,2024,"Liam Magee, Vanicka Arora, Gus Gollings, Norma Lam-Saw","This paper explores use of multiple large language model (LLM) agents to simulate complex, dynamic characters in dramatic scenarios. We introduce a drama machine framework that coordinates interactions between LLM agents playing different 'Ego' and 'Superego' psychological roles. In roleplay simulations, this design allows intersubjective dialogue and intra-subjective internal monologue to develop in parallel. We apply this framework to two dramatic scenarios - an interview and a detective story - and compare character development with and without the Superego's influence. Though exploratory, results suggest this multi-agent approach can produce more nuanced, adaptive narratives that evolve over a sequence of dialogical turns. We discuss different modalities of LLM-based roleplay and character development, along with what this might mean for conceptualization of AI subjectivity. The paper concludes by considering how this approach opens possibilities for thinking of the roles of internal conflict and social performativity in AI-based simulation.",10.48550/arXiv.2408.01725,['JournalArticle'],Computer Science; Psychology
Evaluating the Efficacy of LLMs to Emulate Realistic Human Personalities,2024,"Lawrence J. Klinkert, Stephanie Buongiorno, Corey Clark","To enhance immersion and engagement in video games, the design of Affective Non-Player Characters (NPCs) is a key focus for researchers and practitioners. Affective Computing frameworks improve Non-player characters (NPC) by providing personalities, emotions, and social relations. Large Language Models (LLMs) bring the promise to dynamically enhance character design when coupled with these frameworks, but further research is needed to validate the models truly represent human qualities. In this research, a comprehensive analysis investigates the capabilities of LLMs to generate content that aligns with human personality, using the Big Five and human responses from the International Personality Item Pool (IPIP) questionnaire. Our goal is to benchmark the performance of various LLMs, including frontier models and local models, against an extensive dataset comprising over 50,000 human surveys of self-reported personality tests to determine whether LLMs can replicate human-like decision-making with personality-driven prompts. A range of personality profiles were used to cluster the test results from the human survey dataset. Our methodology involved prompting LLMs with self-evaluated test items for each personality profile, comparing their outputs to human baseline responses, and evaluating the accuracy and consistency. Our findings show that some local models had 0% alignment of any personality profiles when compared to the human dataset, while the frontier models, in some cases, had 100% alignment. The results indicate that NPCs can successfully emulate human-like personality traits using LLMs, as demonstrated by benchmarking the LLM's output against human data. This foundational work serves as a methodology for game developers and researchers to test and evaluate LLMs, ensuring they accurately represent the desired human personalities and can be expanded for further validation.",10.1609/aiide.v20i1.31867,"['JournalArticle', 'Review']",Computer Science; Psychology
MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing,2024,"Siddhant Agarwal, Shivam Sharma, Preslav Nakov, Tanmoy Chakraborty","Memes have evolved as a prevalent medium for diverse communication, ranging from humour to propaganda. With the rising popularity of image-focused content, there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings - detecting harm, applying semantic labels, and offering natural language explanations. To extend this research, we introduce MemeMQA, a multimodal question-answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880 questions related to 1,122 memes with corresponding answer-explanation pairs. We further propose ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority - ~18% enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENAL's robustness through diversification of question-set, confounder-based evaluation regarding MemeMQA's generalizability, and modality-specific assessment, enhancing our understanding of meme interpretation in the multimodal communication landscape.",10.48550/arXiv.2405.11215,"['JournalArticle', 'Conference']",Computer Science
A Survey on Personalized Alignment - The Missing Piece for Large Language Models in Real-World Applications,2025,"Jian Guan, Jun Wu, Jia-Nan Li, Chuanqi Cheng, Wei Wu","Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users' diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment-a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs.",10.48550/arXiv.2503.17003,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives,2024,"Timour Ichmoukhamedov, James Hinns, David Martens","A rapidly developing application of LLMs in XAI is to convert quantitative explanations such as SHAP into user-friendly narratives to explain the decisions made by smaller prediction models. Evaluating the narratives without relying on human preference studies or surveys is becoming increasingly important in this field. In this work we propose a framework and explore several automated metrics to evaluate LLM-generated narratives for explanations of tabular classification tasks. We apply our approach to compare several state-of-the-art LLMs across different datasets and prompt types. As a demonstration of their utility, these metrics allow us to identify new challenges related to LLM hallucinations for XAI narratives.",10.48550/arXiv.2412.10220,"['JournalArticle', 'Review']",Computer Science
ChatGPT For Marketing Communications: Friend or Foe?,2023,"Heidy Arviani, R. Tutiasri, Latif Ahmad Fauzan, Ade Kusuma","The release of the ChatGPT chatbot in November 2022 received significant public attention. ChatGPT is an Artificial Intelligence (AI) powered chatbot that allows users to simulate human-like conversations with AI. GPT stands for Generative Pre-trained Transformer, a language processing model developed by the American artificial intelligence company OpenAI. These innovations and technologies are changing business interests, revolutionizing marketing communications strategies, and enhancing the consumer experience. ChatGPT is a powerful tool for marketers, but we need to understand the risks and place realistic expectations for the moment. The author uses data collection techniques using literature studies and observations on ChatGPT about their potential and impact on marketing communications. Authors analyze brand information, data search, reference services, cataloging, content creation, and ethical considerations such as privacy and bias. Result of this study, ChatGPT can provide and support creative content creation or copywriting, improve customer service, automate repetitive tasks, and support data analysis. However, humans are irreplaceable for examining outputs and creating marketing messages consistent with a company's strategy and brand vision. With good marketing strategies, ChatGPT can effectively enhance and support marketing processes.",10.21070/kanal.v12i1.1729,['JournalArticle'],Business; Computer Science
Analyzing Cultural Representations of Emotions in LLMs Through Mixed Emotion Survey,2024,"Shiran Dudy, Ibrahim Said Ahmad, Ryoko Kitajima, Agata Lapedriza","Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages. There is a growing interest in academia to use these models to simulate and study human behaviors. However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture. Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data. This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. Our methodology is based on the studies of Miyamoto et al. (2010), which identified distinctive emotional indicators in Japanese and American human responses. We first administer their mixed emotion survey to five different LLMs and analyze their outputs. Second, we experiment with contextual variables to explore variations in responses considering both language and speaker origin. Thirdly, we expand our investigation to encompass additional East Asian and Western European origin languages to gauge their alignment with their respective cultures, anticipating a closer fit. We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.",10.1109/ACII63134.2024.00044,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics; Psychology; Sociology
"Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models",2024,"Zhiyuan Wen, Yu Yang, Jiannong Cao, Haoming Sun, Ruosong Yang, Shuaiqi Liu","As large language models (LLMs) appear to behave increasingly human-like in text-based interactions, more and more researchers become interested in investigating personality in LLMs. However, the diversity of psychological personality research and the rapid development of LLMs have led to a broad yet fragmented landscape of studies in this interdisciplinary field. Extensive studies across different research focuses, different personality psychometrics, and different LLMs make it challenging to have a holistic overview and further pose difficulties in applying findings to real-world applications. In this paper, we present a comprehensive review by categorizing current studies into three research problems: self-assessment, exhibition, and recognition, based on the intrinsic characteristics and external manifestations of personality in LLMs. For each problem, we provide a thorough analysis and conduct in-depth comparisons of their corresponding solutions. Besides, we summarize research findings and open challenges from current studies and further discuss their underlying causes. We also collect extensive publicly available resources to facilitate interested researchers and developers. Lastly, we discuss the potential future research directions and application scenarios. Our paper is the first comprehensive survey of up-to-date literature on personality in LLMs. By presenting a clear taxonomy, in-depth analysis, promising future directions, and extensive resource collections, we aim to provide a better understanding and facilitate further advancements in this emerging field.",10.48550/arXiv.2406.17624,"['JournalArticle', 'Review']",Computer Science; Psychology
Secret Use of Large Language Model (LLM),2024,"Zhiping Zhang, Chenxinran Shen, Bingsheng Yao, Dakuo Wang, Tianshi Li","The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, users' secret use of LLM, raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies.",10.1145/3711061,"['JournalArticle', 'Review']",Computer Science
DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications,2024,"Sathya Krishnan Suresh, Mengjun Wu, Tushar Pranav, E. Chng","The scarcity of domain-specific dialogue datasets limits the development of dialogue systems across applications. Existing research is constrained by general or niche datasets that lack sufficient scale for training dialogue systems. To address this gap, we introduce DiaSynth - a synthetic dialogue generation framework capable of generating high-quality, contextually rich dialogues across a wide range of domains. Unlike existing frameworks, DiaSynth uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to generate dynamic, domain-specific dialogues with simulated personas and diverse conversational features. We perform our experiments by generating synthetic data using different LLMs and few-shot examples from DialogSum and SAMSum. The pretrained language models fine-tuned on the synthetic data outperform the base models by 16.47% on dialogue summarization, while the comparison between models fine-tuned on in-domain data and synthetic data shows that the synthetic data is able to capture 90.48% of the performance distribution of the in-domain data on dialogue summarization. The quality of the data generated also increases as we increase the size of LLM from 3B to 8B. These results validate DiaSynth's potential as a robust alternative to traditional data collection methods. We open source the code and data generated for future research.",10.18653/v1/2025.findings-naacl.40,"['JournalArticle', 'Conference']",Computer Science
Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects,2025,"Yixin Cui, Haotian Lin, Shuo Yang, Yixiao Wang, Yanjun Huang, Hong Chen","The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system's ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at https://github.com/cuiyx1720/Awesome-CoT4AD.",10.48550/arXiv.2505.20223,"['JournalArticle', 'Review']",Computer Science; Engineering
Aligning Large Language Models for Faithful Integrity Against Opposing Argument,2025,"Yong Zhao, Yang Deng, See-Kiong Ng, Tat-Seng Chua","Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks. However, they can be easily misled by unfaithful arguments during conversations, even when their original statements are correct. To this end, we investigate the problem of maintaining faithful integrity in LLMs. This involves ensuring that LLMs adhere to their faithful statements in the face of opposing arguments and are able to correct their incorrect statements when presented with faithful arguments.
In this work, we propose a novel framework, named Alignment for Faithful Integrity with Confidence Estimation (AFICE), which aims to align the LLM responses with faithful integrity. Specifically, AFICE first designs a Bilateral Confidence Estimation (BCE) approach for estimating the uncertainty of each response generated by the LLM given a specific context, which simultaneously estimate the model's confidence to the question based on the internal states during decoding as well as to the answer based on cumulative probability ratios.
With the BCE, we construct a conversational preference dataset composed of context, original statement, and argument, which is adopted for aligning the LLM for faithful integrity using Direct Preference Optimization (DPO). 
Extensive experimental results on a wide range of benchmarks demonstrate significant improvements in the LLM's ability to maintain faithful responses when encountering opposing arguments, ensuring both the practical utility and trustworthiness of LLMs in complex interactive settings.",10.48550/arXiv.2501.01336,"['JournalArticle', 'Conference']",Computer Science
GPTs in Mafia-like Game Simulation,2024,Munyeong Kim,"In this research, we explore the potential of Generative AI models, focusing on their application in role-playing simulations through Spyfall, a renowned mafia-style game. By leveraging GPT-4s advanced capabilities, the study aimed to showcase the models potential in understanding, decision-making, and interaction across scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4s enhanced adaptability to the environment, with significant improvements in posing questions and forming responses. However, challenges such as the models limitations in judging and suspecting actions of other players emerged. Reflections on AIs future capability and directions were also discussed. The findings suggest that although GPT-4 exhibits promising advancements over earlier models, there is potential for further development through expanding data and training techniques. The findings also underscore the importance of maintaining an inclusive and unbiased approach throughout this process, suggesting immense potential for Generative AI and its application.",10.1145/3613905.3647958,"['JournalArticle', 'Book']",Computer Science
Open-Ended Wargames with Large Language Models,2024,"Daniel P. Hogan, Andrea Brennen","Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce""Snow Globe,""an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.",10.48550/arXiv.2404.11446,['JournalArticle'],Computer Science
From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self,2023,"Yue Fu, Sami Foell, Xuhai Xu, Alexis Hiniker",,10.48550/arXiv.2310.03976,['JournalArticle'],Computer Science; Psychology
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs,2024,"Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam","Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand peoples viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-centered aspects of AI models and enhancing writing assistants for personalization and individuality.",10.1145/3690712.3690724,"['JournalArticle', 'Book', 'Review']",Computer Science; Linguistics
Unlocking Adaptive User Experience with Generative AI,2024,"Yutan Huang, Tanjila Kanij, Anuradha Madugalla, Shruti Mahajan, Chetan Arora, John Grundy","Developing user-centred applications that address diverse user needs requires rigorous user research. This is time, effort and cost-consuming. With the recent rise of generative AI techniques based on Large Language Models (LLMs), there is a possibility that these powerful tools can be used to develop adaptive interfaces. This paper presents a novel approach to develop user personas and adaptive interface candidates for a specific domain using ChatGPT. We develop user personas and adaptive interfaces using both ChatGPT and a traditional manual process and compare these outcomes. To obtain data for the personas we collected data from 37 survey participants and 4 interviews in collaboration with a not-for-profit organisation. The comparison of ChatGPT generated content and manual content indicates promising results that encourage using LLMs in the adaptive interfaces design process.",10.5220/0012741000003687,"['JournalArticle', 'Review']",Computer Science
Generalizing End-To-End Autonomous Driving In Real-World Environments Using Zero-Shot LLMs,2024,"Zeyu Dong, Yimin Zhu, Yansong Li, Kevin Mahon, Yu Sun","Traditional autonomous driving methods adopt a modular design, decomposing tasks into sub-tasks. In contrast, end-to-end autonomous driving directly outputs actions from raw sensor data, avoiding error accumulation. However, training an end-to-end model requires a comprehensive dataset; otherwise, the model exhibits poor generalization capabilities. Recently, large language models (LLMs) have been applied to enhance the generalization capabilities of end-to-end driving models. Most studies explore LLMs in an open-loop manner, where the output actions are compared to those of experts without direct feedback from the real world, while others examine closed-loop results only in simulations. This paper proposes an efficient architecture that integrates multimodal LLMs into end-to-end driving models operating in closed-loop settings in real-world environments. In our architecture, the LLM periodically processes raw sensor data to generate high-level driving instructions, effectively guiding the end-to-end model, even at a slower rate than the raw sensor data. This architecture relaxes the trade-off between the latency and inference quality of the LLM. It also allows us to choose from a wide variety of LLMs to improve high-level driving instructions and minimize fine-tuning costs. Consequently, our architecture reduces data collection requirements because the LLMs do not directly output actions; we only need to train a simple imitation learning model to output actions. In our experiments, the training data for the end-to-end model in a real-world environment consists of only simple obstacle configurations with one traffic cone, while the test environment is more complex and contains multiple obstacles placed in various positions. Experiments show that the proposed architecture enhances the generalization capabilities of the end-to-end model even without fine-tuning the LLM.",10.48550/arXiv.2411.14256,['JournalArticle'],Computer Science; Engineering
"'It was 80% me, 20% AI': Seeking Authenticity in Co-Writing with Large Language Models",2024,"Angel Hsing-Chi Hwang, Q. V. Liao, Su Lin Blodgett, Alexandra Olteanu, Adam Trischler","Given the rising proliferation and diversity of AI writing assistance tools, especially those powered by large language models (LLMs), both writers and readers may have concerns about the impact of these tools on the authenticity of writing work. We examine whether and how writers want to preserve their authentic voice when co-writing with AI tools and whether personalization of AI writing support could help achieve this goal. We conducted semi-structured interviews with 19 professional writers, during which they co-wrote with both personalized and non-personalized AI writing-support tools. We supplemented writers' perspectives with opinions from 30 avid readers about the written work co-produced with AI collected through an online survey. Our findings illuminate conceptions of authenticity in human-AI co-creation, which focus more on the process and experience of constructing creators' authentic selves. While writers reacted positively to personalized AI writing tools, they believed the form of personalization needs to target writers' growth and go beyond the phase of text production. Overall, readers' responses showed less concern about human-AI co-writing. Readers could not distinguish AI-assisted work, personalized or not, from writers' solo-written work and showed positive attitudes toward writers experimenting with new technology for creative writing.",10.1145/3711020,"['JournalArticle', 'Review']",Computer Science; Linguistics
Knowledge-Augmented Interpretable Network for Zero-Shot Stance Detection on Social Media,2025,"Bowen Zhang, Daijun Ding, Zhichao Huang, Ang Li, Yangyang Li, Baoquan Zhang, Hutchin Huang","Stance detection on social media has become increasingly important for understanding public opinions on controversial issues. Existing methods often require large amounts of labeled data to learn target-independent transferable knowledge, which is infeasible under zero-shot settings where the target is unseen. Furthermore, most current stance detection models, primarily based on end-to-end deep learning architectures, lack transparency and may produce counter-intuitive and uninterpretable predictions. In this article, we propose a novel knowledge-augmented interpretable network (KAI) to enable zero-shot stance detection (ZSSD). First, we introduce an unsupervised approach based on large language models (LLM-KE) to elicit analysis perspectives, which is target-independent knowledge shared across different targets. This transferable knowledge bridges connections between seen and unseen targets. Second, we develop a bidirectional knowledge-guided neural production system (Bi-KGNPS) that effectively integrates such transferable knowledge through an iterative knowledge-variable binding process to guide stance predictions. Extensive experiments on benchmark datasets demonstrate KAI achieves new state-of-the-art performance on ZSSD. Moreover, our approach also delivers strong results on conventional in-target and cross-target stance detection. With the dual benefits of knowledge-augmented accuracy and model interpretability, this work represents an important advance toward practical stance detection systems that can generalize to emerging topics of interest. The proposed KAI framework provides an interpretable approach to effectively transfer knowledge across domains for zero-shot learning.",10.1109/TCSS.2024.3388723,['JournalArticle'],Computer Science
Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments,2024,"Yoshia Abe, Tatsuya Daikoku, Yasuo Kuniyoshi","Recently, it has been recognized that large language models demonstrate high performance on various intellectual tasks. However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation. This study investigates the performance of GPT-4 with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images. We employ two tasks, prediction of the average evaluation values of a group and an individual's evaluation values. We investigate the performance of GPT-4 with Vision by exploring prompts and analyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness. Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, employing agent technologies that integrate traditional deep learning models with large language models.",10.48550/arXiv.2403.03594,['JournalArticle'],Computer Science
A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles,2024,"Siyuan Chen, Q. Si, Chenxu Yang, Yunzhi Liang, Zheng Lin, Huan Liu, Weiping Wang","The advent of large language models (LLMs) has significantly propelled the advancement of Role-Playing Agents (RPAs). However, current Role-Playing Agents predominantly focus on mimicking a character's fundamental attributes while neglecting the replication of linguistic style, and they are incapable of effectively replicating characters when performing tasks beyond multi-turn dialogues, which results in generated responses that lack authenticity. The reason current RPAs lack this capability is due to the nature of existing character datasets, which lack collections of character quotations and are limited to multi-turn dialogue tasks, constraining the RPA's performance across other task domains and failing to mimic a character's linguistic style. To address this gap, we developed a multi-task role-playing dataset named MRstyle, which encompasses a substantial number of real individuals along with their quotations and covers seven different tasks. On this basis, we develop StyleRPA, a Multi-Task Role-Playing Agent (MRPA) that significantly outperforms recent open-source LLMs and RPAs baselines on 7 tasks including Dialogue, Dictionary, Composition, Story Generation, Product Description, Music Commentary, and Open Question Answering. The code and data will be released.",10.48550/arXiv.2411.02457,['JournalArticle'],Computer Science; Linguistics
CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models,2024,"Yaojia Lv, Haojie Pan, Zekun Wang, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin","Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows.",10.48550/arXiv.2401.08438,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics; Psychology
Exploratory Models of Human-AI Teams: Leveraging Human Digital Twins to Investigate Trust Development,2024,"Daniel Nguyen, Myke C. Cohen, Hsien-Te Kao, Grant Engberson, Louis Penafiel, Spencer Lynch, Svitlana Volkova","As human-agent teaming (HAT) research continues to grow, computational methods for modeling HAT behaviors and measuring HAT effectiveness also continue to develop. One rising method involves the use of human digital twins (HDT) to approximate human behaviors and socio-emotional-cognitive reactions to AI-driven agent team members. In this paper, we address three research questions relating to the use of digital twins for modeling trust in HATs. First, to address the question of how we can appropriately model and operationalize HAT trust through HDT HAT experiments, we conducted causal analytics of team communication data to understand the impact of empathy, socio-cognitive, and emotional constructs on trust formation. Additionally, we reflect on the current state of the HAT trust science to discuss characteristics of HAT trust that must be replicable by a HDT such as individual differences in trust tendencies, emergent trust patterns, and appropriate measurement of these characteristics over time. Second, to address the question of how valid measures of HDT trust are for approximating human trust in HATs, we discuss the properties of HDT trust: self-report measures, interaction-based measures, and compliance type behavioral measures. Additionally, we share results of preliminary simulations comparing different LLM models for generating HDT communications and analyze their ability to replicate human-like trust dynamics. Third, to address how HAT experimental manipulations will extend to human digital twin studies, we share experimental design focusing on propensity to trust for HDTs vs. transparency and competency-based trust for AI agents.",10.48550/arXiv.2411.01049,['JournalArticle'],Computer Science; Psychology
Enhancing Financial Fraud Detection Using LLMs and Advanced Data Analytics,2024,Sukanth Korkanti,"The complexity and volume of financial fraud poses a significant challenges to the state of the art and evolving detection systems. This makes it necessary for an in depth study and come up with provable techniques with effective approaches. The proposed work focus on algorithmic improvement of the fraud detection technique for financial frauds by integrating Large Language Models (LLMs) along with data analytics techniques for inference. Morevover, by leveraging the extensive linguistic understanding and pattern recognition capabilities of LLMs, combined with analytical methods, this research develops a robust framework that improves both the accuracy and speed of fraud detection mechanisms. Our proposed methodology utilizes a designed data collection pipelines, sourcing an array of transactional and communicational datasets from multiple financial institutions. The core of our research involved the development and training of a customized LLM to identify potential fraudulent activities by analyzing textual and numerical data patterns that are typically indicative of fraud. Along with LLMs, state of the art data analytics techniques, such as anomaly detection algorithms and predictive modeling, were utilized to refine the detection process further. The results shows an improvement in detecting fraudulent transactions, with the proposed model achieving a higher precision and recall compared to traditional fraud detection systems. The LLM-based framework efficiently identified complex fraud schemes, which were previously undetected by conventional methods, by effectively analyzing discrepancies and anomalies in large datasets. The proposed approach enhances the accuracy and reduces the response time, thereby enabling financial institutions to safeguard their operations and client assets more effectively.",10.1109/ICSSAS64001.2024.10760895,['Conference'],Business; Computer Science
Cultural Learning-Based Culture Adaptation of Language Models,2025,"Chen Cecilia Liu, Anna Korhonen, Iryna Gurevych","Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel framework for enhancing LLM alignment with cultural values based on cultural learning. The framework leverages simulated social interactions to generate conversations in which LLMs engage in role-playing within culturally adapted social scenarios, capturing implicit cultural norms for model fine-tuning. CLCA improves cultural value alignment across various model architectures measured using World Value Survey data, demonstrating the effectiveness of our proposed approach. Our results provide early evidence that understanding intent and social interactions can enhance cultural value adaptation in LLMs, highlighting the promise of training approaches based on cultural learning.",10.48550/arXiv.2504.02953,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics; Sociology
Conversational Prompt Engineering,2024,"L. Ein-Dor, Orith Toledo-Ronen, Artem Spector, Shai Gretz, Lena Dankin, Alon Halfon, Yoav Katz, N. Slonim","Prompts are how humans communicate with LLMs. Informative prompts are essential for guiding LLMs to produce the desired output. However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use. We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks. CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt. The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction. Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs. The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples. A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts. The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes.",10.48550/arXiv.2408.04560,['JournalArticle'],Computer Science
LLM Plus Machine Learning Outperform Expert Rating to Predict Life Satisfaction From Self-Statement Text,2025,"Feng Huang, Xia Sun, Aizhu Mei, Yilin Wang, Huimin Ding, Tingshao Zhu","This study explores an innovative approach to predicting individual life satisfaction by combining large language models (LLMs) with machine learning (ML) techniques. Traditional life satisfaction assessments rely on self-report questionnaires, which can be time-consuming and resource intensive. To address these limitations, we developed a method that utilizes LLMs for feature extraction from open-ended self-statement texts, followed by ML prediction. We compared this approach with standalone LLM predictions and expert ratings. A sample of 378 participants completed the satisfaction with life scale (SWLS) and wrote self-statements about their current life situation. The LLM-based ML model, using a LightGBM regressor, achieved a correlation of 0.542 with self-reported SWLS scores, outperforming both the standalone LLM (<italic>r</italic><inline-formula><tex-math notation=""LaTeX"">$=$</tex-math></inline-formula> 0.491) and expert ratings (<italic>r</italic><inline-formula><tex-math notation=""LaTeX"">$=$</tex-math></inline-formula> 0.455). Effect size analysis revealed a statistically significant moderate effect size difference between the LLM-based ML model and expert ratings (Cohen's <italic>d</italic><inline-formula><tex-math notation=""LaTeX"">$=$</tex-math></inline-formula> 0.499, 95% CI [0.043, 0.955]). These findings demonstrate the potential of integrating LLM and ML for an efficient and accurate assessment of life satisfaction, challenging conventional methods, and opening new avenues for psychological measurement. The study's implications extend to research, clinical practice, and policymaking, offering promising advancements in AI-assisted psychological assessment.",10.1109/TCSS.2024.3475413,['JournalArticle'],Computer Science; Psychology
Biased AI can Influence Political Decision-Making,2024,"Jillian R. Fisher, Shangbin Feng, Robert Aron, Thomas Richardson, Yejin Choi, Daniel W. Fisher, Jennifer Pan, Yulia Tsvetkov, Katharina Reinecke","As modern large language models (LLMs) become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in LLMs on political opinions and decision-making. Participants interacted freely with either a biased liberal, biased conservative, or unbiased control model while completing these tasks. We found that participants exposed to partisan biased models were significantly more likely to adopt opinions and make decisions which matched the LLM's bias. Even more surprising, this influence was seen when the model bias and personal political partisanship of the participant were opposite. However, we also discovered that prior knowledge of AI was weakly correlated with a reduction of the impact of the bias, highlighting the possible importance of AI education for robust mitigation of bias effects. Our findings not only highlight the critical effects of interacting with biased LLMs and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.",10.48550/arXiv.2410.06415,['JournalArticle'],Computer Science; Political Science
"A Survey of Integrating Generative Artificial Intelligence and 6G Mobile Services: Architectures, Solutions, Technologies and Outlooks",2025,"Yi-jing Liu, Hongyang Du, Xinyi Xu, Ruichen Zhang, Gang Feng, Bin Cao, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Khaled Ben Letaief, R. Tafazolli","Generative artificial intelligence (GenAI) is rapidly driving a new phase of artificial intelligence revolution, marked by various applications such as ChatGPT, Sora and DeepSeek. With powerful capabilities in content generation, personalization and user interaction, GenAI can drive significant enhancements in various mobile services to satisfy individual user preferences. Meanwhile, mobile services offer substantial support for accessibility, functionality and intelligence of GenAI applications. However, current research on integrating GenAI and mobile services remains nascent, lacking necessary theoretical and technical support. In this survey, we break down a new vision to guide research on integrating GenAI and mobile services, aiming to improve 6G intelligence levels and enable 6G mobile networks to support the integration of GenAI and mobile services effectively. We begin by examining the necessity of integrating GenAI with 6G mobile services and exploring potential learning architectures of GenAI models within mobile networks. Next, we analyze potential solutions from two perspectives: how 6G mobile networks empower GenAI and how GenAI enables mobile services. We subsequently explore a promising use case, along with essential features and techniques, to facilitate the integration of GenAI and mobile services. Finally, we discuss future research directions, aiming at realizing intelligent 6G networks.",10.1109/TCCN.2025.3558992,"['JournalArticle', 'Review']",Computer Science; Engineering
Re-examining Sexism and Misogyny Classification with Annotator Attitudes,2024,"Aiqi Jiang, Nikolas Vitsakis, Tanvi Dinkar, Gavin Abercrombie, Ioannis Konstas","Gender-Based Violence (GBV) is an increasing problem online, but existing datasets fail to capture the plurality of possible annotator perspectives or ensure the representation of affected groups. We revisit two important stages in the moderation pipeline for GBV: (1) manual data labelling; and (2) automated classification. For (1), we examine two datasets to investigate the relationship between annotator identities and attitudes and the responses they give to two GBV labelling tasks. To this end, we collect demographic and attitudinal information from crowd-sourced annotators using three validated surveys from Social Psychology. We find that higher Right Wing Authoritarianism scores are associated with a higher propensity to label text as sexist, while for Social Dominance Orientation and Neosexist Attitudes, higher scores are associated with a negative tendency to do so. For (2), we conduct classification experiments using Large Language Models and five prompting strategies, including infusing prompts with annotator information. We find: (i) annotator attitudes affect the ability of classifiers to predict their labels; (ii) including attitudinal information can boost performance when we use well-structured brief annotator descriptions; and (iii) models struggle to reflect the increased complexity and imbalanced classes of the new label sets.",10.48550/arXiv.2410.03543,"['JournalArticle', 'Conference', 'Review']",Computer Science; Sociology
Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection,2023,"Kairui Hu, Ming Yan, Joey Tianyi Zhou, I. Tsang, Wen-Haw Chong, Yong Keong Yap","Stance detection aims to determine the attitude or viewpoint expressed in a document regarding a specific target. Recent advancements in Large Language Models (LLMs), such as Chain-of-Thought (CoT) prompting, have improved the reasoning capabilities of these models by integrating intermediate rationales. However, the efficacy of CoT can be limited by the models internal knowledge, resulting in inaccurate rationales that compromise the subsequent stance prediction. This limitation could further lead to hallucinations, where LLMs produce unfaithful responses and erroneous reasoning, affecting the outputs reliability and precision. Moreover, CoT can be challenging to implement on smaller language models with constrained knowledge and reasoning depth, which raises concerns about efficiency. In response to these issues, we propose the Ladder-of-Thought (LoT), a novel framework using knowledge as steps to elevate stance detection. LoT implements a triple-phase Progressive Optimization Framework: 1) External Knowledge Injection, which aims to enrich the models intrinsic knowledge base; 2) Intermediate Knowledge Generation, allowing the model to generate more accurate and dependable intermediate knowledge to enhance the downstream prediction; and 3) Downstream Fine-tuning & Prediction, which aims to improve the models prediction accuracy. This sequential approach symbolizes ascending a ladder, with each phase representing a progressive step towards achieving optimal reasoning and prediction performance. Our empirical results have demonstrated that LoT achieves state-of-the-art results in zero-shot/few-shot and in-target stance detection, marking a 16% improvement over ChatGPT and a 10% enhancement compared to ChatGPT with CoT on stance detection task.",10.1109/IJCNN60899.2024.10650428,"['JournalArticle', 'Conference']",Computer Science
LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert Networks,2025,"Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng","The centralization of Large Language Models (LLMs) development has created significant barriers to AI advancement, limiting the democratization of these powerful technologies. This centralization, coupled with the scarcity of high-quality training data and mounting complexity of maintaining comprehensive expertise across rapidly expanding knowledge domains, poses critical challenges to the continued growth of LLMs. While solutions like Retrieval-Augmented Generation (RAG) offer potential remedies, maintaining up-to-date expert knowledge across diverse domains remains a significant challenge, particularly given the exponential growth of specialized information. This paper introduces LLMs Networks (LLM-Net), a blockchain-based framework that democratizes LLMs-as-a-Service through a decentralized network of specialized LLM providers. By leveraging collective computational resources and distributed domain expertise, LLM-Net incorporates fine-tuned expert models for various specific domains, ensuring sustained knowledge growth while maintaining service quality through collaborative prompting mechanisms. The framework's robust design includes blockchain technology for transparent transaction and performance validation, establishing an immutable record of service delivery. Our simulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet, Llama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the reputation-based mechanism in maintaining service quality by selecting high-performing respondents (LLM providers). Thereby it demonstrates the potential of LLM-Net to sustain AI advancement through the integration of decentralized expertise and blockchain-based accountability.",10.48550/arXiv.2501.07288,['JournalArticle'],Computer Science
UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents,2025,"Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Yang Li, Jiri Gesi, Qi He, T. Li, Dakuo Wang","Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate their new designs. But what about evaluating and iterating the usability testing study design itself? Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and iterating their study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users and to interactively test the target website. The system also provides a Result Viewer Interface so that the UX researchers can easily review and analyze the generated qualitative (e.g., agents'post-study surveys) and quantitative data (e.g., agents'interaction logs), or even interview agents directly. Through a heuristic evaluation with 16 UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.",10.48550/arXiv.2504.09407,"['JournalArticle', 'Review']",Computer Science
Adaptive Uncertainty Quantification for Generative AI,2024,"Jungeum Kim, Sean O'Hagan, Veronika Rockov","This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.",10.48550/arXiv.2408.08990,['JournalArticle'],Computer Science; Mathematics
Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs,2025,"Zixiao Wang, Duzhen Zhang, Ishita Agrawal, Shen Gao, Le Song, Xiuyi Chen","Previous approaches to persona simulation large language models (LLMs) have typically relied on learning basic biographical information, or using limited role-play dialogue datasets to capture a character's responses. However, a holistic representation of an individual goes beyond surface-level facts or conversations to deeper thoughts and thinking. In this work, we introduce CharacterBot, a model designed to replicate both the linguistic patterns and distinctive thought processes of a character. Using Lu Xun, a renowned Chinese writer, as a case study, we propose four training tasks derived from his 17 essay collections. These include a pre-training task focused on mastering external linguistic structures and knowledge, as well as three fine-tuning tasks: multiple-choice question answering, generative question answering, and style transfer, each aligning the LLM with Lu Xun's internal ideation and writing style. To optimize learning across these tasks, we introduce a CharLoRA parameter updating mechanism, where a general linguistic style expert collaborates with other task-specific experts to better study both the language style and the understanding of deeper thoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and opinion comprehension, demonstrating that it significantly outperforms the baselines on our adapted metrics. We hope that this work inspires future research on deep character persona simulation LLM.",10.48550/arXiv.2502.12988,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024,2024,Ilias Chalkidis,"In light of the recent 2024 European Parliament elections, we are investigating if LLMs can be used as Voting Advice Applications (VAAs). We audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the stance of political parties based on the latest EU and I voting assistance questionnaire. Furthermore, we explore alternatives to improve models performance by augmenting the input context via Retrieval-Augmented Generation (RAG) relying on web search, and Self-Reflection using staged conversations that aim to re-collect relevant content from the models internal memory. We find that MIXTRAL is highly accurate with an 82% accuracy on average with a significant performance disparity across different political groups (50-95%). Augmenting the input context with expert-curated information can lead to a significant boost of approx. 9%, which remains an open challenge for automated RAG approaches, even considering curated content.",10.48550/arXiv.2407.08495,"['JournalArticle', 'Conference']",Computer Science; Political Science
The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being,2025,"Yutong Zhang, Dora Zhao, Jeffrey T. Hancock, Robert Kraut, Diyi Yang","As large language models (LLMs)-enhanced chatbots grow increasingly expressive and socially responsive, many users are beginning to form companionship-like bonds with them, particularly with simulated AI partners designed to mimic emotionally attuned interlocutors. These emerging AI companions raise critical questions: Can such systems fulfill social needs typically met by human relationships? How do they shape psychological well-being? And what new risks arise as users develop emotional ties to non-human agents? This study investigates how people interact with AI companions, especially simulated partners on CharacterAI, and how this use is associated with users' psychological well-being. We analyzed survey data from 1,131 users and 4,363 chat sessions (413,509 messages) donated by 244 participants, focusing on three dimensions of use: nature of the interaction, interaction intensity, and self-disclosure. By triangulating self-reports primary motivation, open-ended relationship descriptions, and annotated chat transcripts, we identify patterns in how users engage with AI companions and its associations with well-being. Findings suggest that people with smaller social networks are more likely to turn to chatbots for companionship, but that companionship-oriented chatbot usage is consistently associated with lower well-being, particularly when people use the chatbots more intensively, engage in higher levels of self-disclosure, and lack strong human social support. Even though some people turn to chatbots to fulfill social needs, these uses of chatbots do not fully substitute for human connection. As a result, the psychological benefits may be limited, and the relationship could pose risks for more socially isolated or emotionally vulnerable users.",10.48550/arXiv.2506.12605,"['JournalArticle', 'Review']",Computer Science; Psychology
Human and Large Language Model Intent Detection in Image-Based Self-Expression of People with Intellectual Disability,2024,"Alieh Hajizadeh Saffar, Laurianne Sitbon, Maria Hoogstrate, Ahmed K. Abbas, S. Roomkham, Dimity Miller",,10.1145/3627508.3638347,"['JournalArticle', 'Book']",Computer Science; Psychology
Casevo: A Cognitive Agents and Social Evolution Simulator,2024,"Zexun Jiang, Yafang Shi, Maoxu Li, Hongjiang Xiao, Yunxiao Qin, Qinglan Wei, Ye Wang, Yuan Zhang","In this paper, we introduce a multi-agent simulation framework Casevo (Cognitive Agents and Social Evolution Simulator), that integrates large language models (LLMs) to simulate complex social phenomena and decision-making processes. Casevo is designed as a discrete-event simulator driven by agents with features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation (RAG), and Customizable Memory Mechanism. Casevo enables dynamic social modeling, which can support various scenarios such as social network analysis, public opinion dynamics, and behavior prediction in complex social systems. To demonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020 midterm election TV debates as a simulation example. Our results show that Casevo facilitates more realistic and flexible agent interactions, improving the quality of dynamic social phenomena simulation. This work contributes to the field by providing a robust system for studying large-scale, high-fidelity social behaviors with advanced LLM-driven agents, expanding the capabilities of traditional agent-based modeling (ABM). The open-source code repository address of casevo is https://github.com/rgCASS/casevo.",10.48550/arXiv.2412.19498,['JournalArticle'],Computer Science; Sociology
Sacred Meets Synthetic: A Multi-Method Study on the First AI Church Service,2024,Jonas Simmerlein,"Artificial Intelligence (AI) is transforming society, including religious practices and experiences of religious communities. The first large-scale AI church service took place in 2023 in Germany as a Protestant Christian worship event within a biannual religious celebration. Studying this religious service illustrates how AI was developed as a religious agent and provides insights into the experiences of attendees. This research note presents data from a quantitative survey and two qualitative questionnaires of participants conducted at the AI worship service. Findings show diversity of opinion and a range of attendee experiences. By presenting the results of qualitative and quantitative analysis, this research note highlights the possibilities and limitations of incorporating AI into the religious sphere. While most of the responses were skeptical, participants also reported having spiritual experiences. Nevertheless, attendees also reported conflicting feelings regarding experiences of community within an artificial setting. The main concerns were technological limitations, fear of replacing humans, biases in the theology of the underlying large language model, and lack of personality and emotion. Age-based differences include younger attendees finding the AI service more attractive while older attendees found the AI service more stimulating. Implications are drawn for practical theology and AI implementation within religious settings.",10.1177/0034673X241282962,"['JournalArticle', 'Review']",Computer Science
Muse: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles,2024,"Zihan Wang, Xiaocui Yang, Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang","Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational recommendation dataset. Muse comprises 83,148 utterances from 7,000 conversations centered around the Clothing domain. Each conversation contains comprehensive multimodal interactions, rich elements, and natural dialogues. Data in Muse are automatically synthesized by a multi-agent framework powered by multimodal large language models (MLLMs). It innovatively derives user profiles from real-world scenarios rather than depending on manual design and history data for better scalability, and then it fulfills conversation simulation and optimization. Both human and LLM evaluations demonstrate the high quality of conversations in Muse. Additionally, fine-tuning experiments on three MLLMs demonstrate Muse's learnable patterns for recommendations and responses, confirming its value for multimodal conversational recommendation. Our dataset and codes are available at https://anonymous.4open.science/r/Muse-0086.",10.48550/arXiv.2412.18416,"['JournalArticle', 'Conference']",Computer Science
AWS Lambda and SageMaker: Real-Time Solutions for Machine Learning,2025,Himanshu Gupta,"Natural Language Processing (NLP) has emerged as a transformative power during the era of significant data. The vast and diverse vocabulary generated by big data allows for the development of advanced NLP models through machine learning algorithms and distributed computing methods. The collaboration of NLP and big data has led to the creation of strong language models such as BERT and GPT, enhancing NLP's capability to comprehend content and provide insights across a range of applications, including reasoning, machine translation, automated responses, and personalized NLP text generation.
Integrating NLP within a big data framework addresses numerous challenges across various industries. Utilizing data collection, business intelligence can provide real-time insights, while collaboration can be enhanced through improved networking. Sentiment analysis aids in refining product and market research by enabling organizations to understand customer opinions and preferences. Cloud services like AWS Lambda simplify serverless execution of NLP workflows, facilitating cost-effective, scalable, and event-driven processing of large datasets. Moreover, AWS SageMaker streamlines the development, training, and deployment of machine learning models, enabling the efficient creation and operationalization of advanced NLP solutions on a large scale.
This study demonstrates the efficacy of NLP in processing large datasets, particularly for sentiment analysis using the MapReduce framework, alongside the operational advantages offered by AWS Lambda and SageMaker.
In conclusion, this document emphasizes the opportunities and challenges that arise from the integration of NLP with big data and cloud-native tools, providing insights into their combined capabilities. This potential can be harnessed in various ways, fostering a deeper understanding of both language and data analysis.",10.48175/ijarsct-23061,['JournalArticle'],Computer Science; Linguistics
"Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future of Edemorcacy",2025,"Rabimba Karanjai, Boris Shor, Amanda Austin, Ryan Kennedy, Yang Lu, Lei Xu, Weidong Shi","This paper investigates the use of Large Language Models (LLMs) to synthesize public opinion data, addressing challenges in traditional survey methods like declining response rates and non-response bias. We introduce a novel technique: role creation through knowledge injection, a form of in-context learning that leverages Retrieval-Augmented Generation (RAG) alongside HEXACO personality profiles and demographic data to generate personalized prompts dynamically. This method allows LLMs to simulate diverse opinions more accurately than existing prompt engineering approaches. We compare our results with pre-trained models with standard few-shot prompts. Experiments using questions from the Cooperative Election Study (CES) demonstrate that our role-creation approach significantly improves the alignment of LLM-generated opinions with realworld human survey responses, increasing answer adherence. In addition, we discuss challenges, limitations and future research directions.",10.1109/ICEDEG65568.2025.11081685,"['JournalArticle', 'Conference', 'Review']",Computer Science; Sociology
Enhancing Communication and Data Transmission Security in RAG Using Large Language Models,2024,"Venkata Gummadi, Pamula Udayaraju, Venkata Rahul Sarabu, Chaitanya Ravulu, Dhanunjay Reddy Seelam, S. Venkataramana","Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources, enabling more useful information and generating accurate responses. This paper explores RAG's architecture and applications, combining generator and retriever models to access and utilize vast external data repositories. While RAG holds significant promise for various Natural Language Processing (NLP) processes like dialogue generation, summarization, and question answering, it also presents unique security challenges that must be addressed to ensure system integrity and reliability. RAG systems face several security threats, including data poisoning, model manipulation, privacy leakage, biased information retrieval, and harmful outputs generation. Generally, in the traditional RAG application, security threat is one of the major concerns. To tighten the security system and enhance the efficiency of the model on processing more complex data this paper outlines key strategies for securing RAG-based applications to mitigate these risks paper outlines key strategies for securing RAG-based applications to mitigate these risks. Ensuring data security through filtering, sanitization, and provenance tracking can prevent data poisoning and enhance the quality of external knowledge sources. Strengthening model security via adversarial training, input validation, and anomaly detection improves resilience against manipulative attacks. Implementing output monitoring and filtering techniques, such as factual verification, language moderation, and bias detection, ensures the accuracy and safety of generated responses. Additionally, robust infrastructure and access control measures, including secure data storage, secure APIs, and regulated model access, protect against unauthorized access and manipulation. Moreover, this study analyzes various use cases for LLMs enhanced by RAG, including personalized recommendations, customer support automation, content creation, and advanced search functionalities. The role of vector databases in optimizing RAG-driven generative AI is also discussed, highlighting their ability to efficiently manage and retrieve large-scale data for improved response generation. By adhering to these security measures and leveraging best practices from leading industry sources such as Databricks, AWS, and Milvus, developers can ensure the robustness and trustworthiness of RAG-based systems across diverse applications.",10.1109/ICSES63445.2024.10763024,['Conference'],Computer Science
Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game,2024,"Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li","With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.",10.48550/arXiv.2404.02532,['JournalArticle'],Computer Science
Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions,2023,"Yuchong Sun, Che Liu, Jinwen Huang, Ruihua Song, Fuzheng Zhang, Di Zhang, Zhongyuan Wang, Kun Gai",,10.48550/arXiv.2310.07301,['JournalArticle'],Computer Science
SAUCE: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction,2024,"Shlomo Neuberger, Niv Eckhaus, Uri Berger, Amir Taubenfeld, Gabriel Stanovsky, Ariel Goldstein","Many human interactions, such as political debates, are carried out in group settings, where there are arbitrarily many participants, each with different views and agendas. To explore such complex social settings, we present SAUCE: a customizable Python platform, allowing researchers to plug-and-play various LLMs participating in discussions on any topic chosen by the user. Our platform takes care of instantiating the models, scheduling their responses, managing the discussion history, and producing a comprehensive output log, all customizable through configuration files, requiring little to no coding skills. A novel feature of SAUCE is our asynchronous communication feature, where models decide when to speak in addition to what to say, thus modeling an important facet of human communication. We show SAUCE's attractiveness in two initial experiments, and invite the community to use it in simulating various group simulations.",10.48550/arXiv.2411.03397,['JournalArticle'],Computer Science; Political Science
Real-Time Control of A2O Process in Wastewater Treatment Through Fast Deep Reinforcement Learning Based on Data-Driven Simulation Model,2024,"Fukang Hu, Xiaodong Zhang, Baohong Lu, Yue Lin","Real-time control (RTC) can be applied to optimize the operation of the anaerobicanoxicoxic (A2O) process in wastewater treatment for energy saving. In recent years, many studies have utilized deep reinforcement learning (DRL) to construct a novel AI-based RTC system for optimizing the A2O process. However, existing DRL methods require the use of A2O process mechanistic models for training. Therefore they require specified data for the construction of mechanistic models, which is often difficult to achieve in many wastewater treatment plants (WWTPs) where data collection facilities are inadequate. Also, the DRL training is time-consuming because it needs multiple simulations of mechanistic model. To address these issues, this study designs a novel data-driven RTC method. The method first creates a simulation model for the A2O process using LSTM and an attention module (LSTM-ATT). This model can be established based on flexible data from the A2O process. The LSTM-ATT model is a simplified version of a large language model (LLM), which has much more powerful ability in analyzing time-sequence data than usual deep learning models, but with a small model architecture that avoids overfitting the A2O dynamic data. Based on this, a new DRL training framework is constructed, leveraging the rapid computational capabilities of LSTM-ATT to accelerate DRL training. The proposed method is applied to a WWTP in Western China. An LSTM-ATT simulation model is built and used to train a DRL RTC model for a reduction in aeration and qualified effluent. For the LSTM-ATT simulation, its mean squared error remains between 0.0039 and 0.0243, while its R-squared values are larger than 0.996. The control strategy provided by DQN effectively reduces the average DO setpoint values from 3.956 mg/L to 3.884 mg/L, with acceptable effluent. This study provides a pure data-driven RTC method for the A2O process in WWTPs based on DRL, which is effective in energy saving and consumption reduction. It also demonstrates that purely data-driven DRL can construct effective RTC methods for the A2O process, providing a decision-support method for management.",10.3390/w16243710,['JournalArticle'],Computer Science; Engineering; Environmental Science
"Tasks, Copilots, and the Future of Search",2023,Ryen W. White,"Tasks are central to information retrieval (IR) and drive interactions with search systems [2, 4, 10]. Understanding and modeling tasks helps these systems better support user needs [8, 9, 11]. This keynote focuses on search tasks, the emergence of generative artificial intelligence (AI), and the implications of recent work at their intersection for the future of search. Recent estimates suggest that half of Web search queries go unanswered, many of them connected to complex search tasks that are ill-defined or multi-step and span several queries[6]. AI copilots, e.g., ChatGPT and Bing Chat, are emerging to address complex search tasks and many other challenges. These copilots are built on large foundation models such as GPT-4 and are being extended with skills and plugins. Copilots broaden the surface of tasks achievable via search, moving toward creation not just finding (e.g., interview preparation, email composition), and can make searchers more efficient and more successful. Users currently engage with AI copilots via natural language queries and dialog and the copilots generate answers with source attribution [7]. However, in delegating responsibility for answer generation, searchers also lose some control over aspects of the search process, such as directly manipulating queries and examining lists of search results [1]. The efficiency gains from auto-generating a single, synthesized answer may also reduce opportunities for user learning and serendipity. A wholesale move to copilots for all search tasks is neither practical nor necessary: model inference is expensive, conversational interfaces are unfamiliar to many users in a search context, and traditional search already excels for many types of task. Instead, experiences that unite search and chat are becoming more common, enabling users to adjust the modality and other aspects (e.g., answer tone) based on the task. The rise of AI copilots creates many opportunities for IR, including aligning generated answers with user intent, tasks, and applications via human feedback [3]; understanding copilot usage, including functional fixedness [5]; using context and data to tailor responses to people and situations (e.g., grounding, personalization); new search experiences (e.g., unifying search and chat); reliability and safety (e.g., accuracy, bias); understanding impacts on user learning and agency; and evaluation (e.g., model-based feedback, searcher simulations [12] repeatability). Research in these and related areas will enable search systems to more effectively utilize new copilot technologies together with traditional search to help searchers better tackle a wider variety of tasks.",10.1145/3539618.3593069,"['Book', 'JournalArticle', 'Conference']",Computer Science
AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence,2024,"Minbeom Kim, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung","As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity.",10.48550/arXiv.2404.11826,"['JournalArticle', 'Conference']",Computer Science
Moth Flame Optimization With Hybrid Deep Learning Based Sentiment Classification Toward ChatGPT on Twitter,2023,"Mohammed Aljebreen, B. Alabduallah, Mashael M. Asiri, Ahmed S. Salama, Mohammed Assiri, S. S. Ibrahim","ChatGPT, developed by OpenAI, is an advanced language model that excels at generating human-like text responses in conversational settings. As ChatGPT interacts with the user, it creates a range of sentiments from them, involving neutral, positive, or negative expressions. Sentiment analysis (SA), also called opinion mining, is a branch of natural language processing (NLP) that defines the emotional tone or sentiment conveyed in textual data. Sentiment analysis (SA) plays a major role in understanding how people respond and perceive different entities, involving services, products, brands, or artificial intelligence (AI) models GPT. Analyzing the sentiment toward ChatGPT gives valuable insight into, user experience, areas, and satisfaction for development. The study presents a moth flame optimization with hybrid deep learning-based sentiment analysis (MFOHDL-SA) on ChatGPT. The major aim of the MFOHDL-SA method is to design an automated AI model to properly classify the tweets as positive, negative, or neutral in sentiment towards ChatGPT. To accomplish this, the MFOHDL-SA technique initially pre-processes the tweets in different stages. Next, the TF-IDF model is used for the word embedding process. Moreover, the HDL method comprising a convolutional neural network with long short-term memory (CNN-LSTM) method was utilized for sentiment classification. To improve the classifier results of the HDL model, the MFO algorithm is used for hyperparameter tuning. The simulation results of the MFOHDL-SA technique are validated on the Twitter dataset from the Kaggle repository. The obtained experimental outcomes stated the betterment of the MFOHDL-SA approach over other existing techniques in terms of different measures. This provides a valued understanding of public sentiment towards ChatGPT on Twitter, allowing improved understanding and assessment of its impact and perception among users.",10.1109/ACCESS.2023.3315609,['JournalArticle'],Computer Science
"SouLLMate: An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques",2024,"Qiming Guo, Jinwen Tang, Wenbo Sun, Haoteng Tang, Yi Shang, Wenlu Wang","Mental health issues significantly impact individuals'daily lives, yet many do not receive the help they need even with available online resources. This study aims to provide diverse, accessible, stigma-free, personalized, and real-time mental health support through cutting-edge AI technologies. It makes the following contributions: (1) Conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering, and domain knowledge. This system offers advanced features such as Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized profile uploads and Conversational Information Extraction. (3) Developing novel evaluation approaches for preliminary assessments and risk detection via professionally annotated interview data and real-life suicide tendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive Questioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to enhance model performance and usability through context-sensitive response adjustments, semantic coherence evaluations, and enhanced accuracy of long-context reasoning in language models. This study contributes to advancing mental health support technologies, potentially improving the accessibility and effectiveness of mental health care globally.",10.48550/arXiv.2410.16322,"['JournalArticle', 'Review']",Computer Science; Psychology
SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation,2024,"Qinglin Qi, Yun Luo, Yijia Xu, Wenbo Guo, Yong Fang","Large Language Models (LLMs) are increasingly capable, aiding in tasks such as content generation, yet they also pose risks, particularly in generating harmful spear-phishing emails. These emails, crafted to entice clicks on malicious URLs, threaten personal information security. This paper proposes an adversarial framework, SpearBot, which utilizes LLMs to generate spear-phishing emails with various phishing strategies. Through specifically crafted jailbreak prompts, SpearBot circumvents security policies and introduces other LLM instances as critics. When a phishing email is identified by the critic, SpearBot refines the generated email based on the critique feedback until it can no longer be recognized as phishing, thereby enhancing its deceptive quality. To evaluate the effectiveness of SpearBot, we implement various machine-based defenders and assess how well the phishing emails generated could deceive them. Results show these emails often evade detection to a large extent, underscoring their deceptive quality. Additionally, human evaluations of the emails' readability and deception are conducted through questionnaires, confirming their convincing nature and the significant potential harm of the generated phishing emails.",10.48550/arXiv.2412.11109,['JournalArticle'],Computer Science
CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models,2025,"Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, Dong Yu","Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",10.48550/arXiv.2509.09675,['JournalArticle'],Computer Science
Interpreting and Controlling Vision Foundation Models via Text Explanations,2023,"Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao","Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models' predictions and controlling model behaviors have remained open challenges. We present a framework for interpreting vision transformer's latent tokens with natural language. Given a latent token, our framework retains its semantic information to the final layer using transformer's local operations and retrieves the closest text for explanation. Our approach enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, our framework allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.",10.48550/arXiv.2310.10591,['JournalArticle'],Computer Science
Assessing Personalized AI Mentoring with Large Language Models in the Computing Field,2024,"Xiao Luo, Sean O'Connell, S. Mithun","This paper provides an in-depth evaluation of three state-of-the-art Large Language Models (LLMs) for personalized career mentoring in the computing field, using three distinct student profiles that consider gender, race, and professional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2 using a zero-shot learning approach without human intervention. A quantitative evaluation was conducted through a custom natural language processing analytics pipeline to highlight the uniqueness of the responses and to identify words reflecting each student's profile, including race, gender, or professional level. The analysis of frequently used words in the responses indicates that GPT-4 offers more personalized mentoring compared to the other two LLMs. Additionally, a qualitative evaluation was performed to see if human experts reached similar conclusions. The analysis of survey responses shows that GPT-4 outperformed the other two LLMs in delivering more accurate and useful mentoring while addressing specific challenges with encouragement languages. Our work establishes a foundation for developing personalized mentoring tools based on LLMs, incorporating human mentors in the process to deliver a more impactful and tailored mentoring experience.",10.1109/CI-NLPSoMeCompanion65206.2025.10977933,"['JournalArticle', 'Review']",Computer Science
The Detection of E-Commerce Manipulated Reviews Using GPT-4,2023,"Hassan Al Wahshat, Waheeb Abu-ulbeh, M. H. Yusoff, Muhammad D. Zakaria, Wan Mohd Amir Fazamin Wan Hamzah, Stenin N P","In recent years, the exponential growth of e-commerce has transformed consumer purchasing behavior, with online reviews playing a crucial role in shaping buying decisions. Unfortunately, the prevalence of manipulated reviews has become a significant challenge, undermining the integrity of consumer feedback and eroding trust in e-commerce platforms. This research paper focuses on developing an advanced solution to address the issue of manipulated reviews using the innovative language model, GPT-4. The primary objective of this study is to investigate the effectiveness of GPT-4 in identifying and flagging manipulated reviews within the context of e-commerce platforms. GPT-4 is a state-of-the-art language model with superior natural language processing capabilities, making it an ideal candidate for automated review analysis. The research methodology encompasses a large-scale data collection process, where diverse e-commerce reviews are gathered from various platforms. To simulate real-world scenarios, manipulated reviews are artificially injected into the dataset, representing different degrees of sophistication. The dataset is then annotated by expert reviewers to establish ground truth labels for comparison. Next, GPT-4 is fine-tuned using transfer learning to specialize in detecting manipulated reviews. The fine-tuning process involves exposing the model to both genuine and manipulated review samples, allowing it to learn patterns and features indicative of manipulation. The fine-tuned model's performance is then evaluated using various metrics, including precision, recall, F1 score, and accuracy, against the ground truth dataset. The results of the experiment demonstrate the efficacy of GPT-4 in distinguishing between authentic and manipulated reviews. GPT-4 showcases remarkable accuracy and robustness in detecting increasingly sophisticated manipulation techniques, outperforming previous iterations of language models and traditional detection methods. Furthermore, the research delves into the characteristics of manipulated reviews that render them distinguishable from genuine reviews. These insights aid in understanding the vulnerabilities of e-commerce platforms to manipulation and offer valuable information for future research and platform enhancement efforts. The implications of this study are significant for e-commerce businesses, consumers, and platform administrators alike. By leveraging GPT-4's detection capabilities, e-commerce platforms can implement more robust review validation systems, ensuring the authenticity and reliability of consumer feedback. This, in turn, fosters greater trust among consumers, leading to informed purchasing decisions and improved overall user experience. In conclusion, this research paper contributes to the existing body of knowledge by showcasing the potential of GPT-4 in combatting manipulated reviews and strengthening the integrity of e-commerce platforms. With the rise of AI-driven technologies, the proposed approach presents a promising step towards a more transparent and trustworthy e-commerce ecosystem.",10.1109/CSET58993.2023.10346848,"['Conference', 'Review']",Business; Computer Science
Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study,2024,"Bolei Ma, Berk Yoztyurk, Anna Haensch, Xinpeng Wang, Markus Herklotz, Frauke Kreuter, Barbara Plank, Matthias Assenmacher","In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD. Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions. These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.",10.48550/arXiv.2412.13169,"['JournalArticle', 'Conference', 'Review']",Computer Science; Political Science
"Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks",2024,"Xunkai Li, Zhengyu Wu, Jiayi Wu, Hanwen Cui, Jishuo Jia, Ronghua Li, Guoren Wang","With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at \url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.",10.48550/arXiv.2412.12456,"['JournalArticle', 'Review']",Computer Science
InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System,2025,"Zhiyuan Wen, Jiannong Cao, Zian Wang, Beichen Guo, Ruosong Yang, Shuaiqi Liu","The exponential growth of academic literature creates urgent demands for comprehensive survey papers, yet manual writing remains time-consuming and labor-intensive. Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) facilitate studies in synthesizing survey papers from multiple references, but most existing works restrict users to title-only inputs and fixed outputs, neglecting the personalized process of survey paper writing. In this paper, we introduce InteractiveSurvey - an LLM-based personalized and interactive survey paper generation system. InteractiveSurvey can generate structured, multi-modal survey papers with reference categorizations from multiple reference papers through both online retrieval and user uploads. More importantly, users can customize and refine intermediate components continuously during generation, including reference categorization, outline, and survey content through an intuitive interface. Evaluations of content quality, time efficiency, and user studies show that InteractiveSurvey is an easy-to-use survey generation system that outperforms most LLMs and existing methods in output content quality while remaining highly time-efficient.",10.48550/arXiv.2504.08762,"['JournalArticle', 'Review']",Computer Science
Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation,2024,"Cheng Charles Ma, Kevin Hyekang Joo, Alexandria K. Vail, Sunreeta Bhattacharya, Alvaro Fernandez Garcia, Kailana Baker-Matsuoka, Sheryl Mathew, Lori L. Holt, Fernando De la Torre","Over the past decade, wearable computing devices (``smart glasses'') have undergone remarkable advancements in sensor technology, design, and processing power, ushering in a new era of opportunity for high-density human behavior data. Equipped with wearable cameras, these glasses offer a unique opportunity to analyze non-verbal behavior in natural settings as individuals interact. Our focus lies in predicting engagement in dyadic interactions by scrutinizing verbal and non-verbal cues, aiming to detect signs of disinterest or confusion. Leveraging such analyses may revolutionize our understanding of human communication, foster more effective collaboration in professional environments, provide better mental health support through empathetic virtual interactions, and enhance accessibility for those with communication barriers. In this work, we collect a dataset featuring 34 participants engaged in casual dyadic conversations, each providing self-reported engagement ratings at the end of each conversation. We introduce a novel fusion strategy using Large Language Models (LLMs) to integrate multiple behavior modalities into a ``multimodal transcript'' that can be processed by an LLM for behavioral reasoning tasks. Remarkably, this method achieves performance comparable to established fusion techniques even in its preliminary implementation, indicating strong potential for further research and optimization. This fusion method is one of the first to approach ``reasoning'' about real-world human behavior through a language model. Smart glasses provide us the ability to unobtrusively gather high-density multimodal data on human behavior, paving the way for new approaches to understanding and improving human communication with the potential for important societal benefits. The features and data collected during the studies will be made publicly available to promote further research.",10.48550/arXiv.2409.09135,['JournalArticle'],Computer Science; Psychology
"Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs",2023,"Yuqian Sun, Hanyi Wang, Pok Man Chan, M. Tabibi, Yan Zhang, Huan Lu, Yuheng Chen, Chang Hee Lee, A. Asadipour","We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into""live""social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes,""David""and""Catherine,""and evaluated their performance in an online gaming community,""DE (Alias),""on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with community members, reveals that storytelling significantly enhances the engagement and believability of SCs in community settings.",10.48550/arXiv.2309.11478,['JournalArticle'],Computer Science; Sociology
Enhancing IT Security with LLM-Powered Predictive Threat Intelligence,2024,Bhargava Bokkena,"The primary objective of this research is to explore the integration of Large Language Models (LLMs) into IT security frameworks for enhancing predictive threat intelligence capabilities. Traditional threat detection systems often struggle with the dynamic nature of modern cyber threats. LLMs, with their advanced natural language processing abilities, present a novel approach by predicting and identifying potential security threats through pattern recognition and anomaly detection in vast datasets. This research employed a systematic methodology involving extensive data collection from diverse IT security logs and public threat databases. The selected LLM was trained on these datasets, focusing on recognizing linguistic and non-linguistic patterns indicative of potential security threats. Validation was conducted through a series of controlled tests comparing the LLMs performance against traditional rule-based and machine learning models in simulated and real-world environments. The results demonstrate that the LLM significantly outperformed existing models in terms of detection speed, accuracy, and the ability to identify zero-day exploits. Notably, the LLM achieved an accuracy rate exceeding 95%, with substantial improvements in false positive reductions. The study concludes that leveraging LLMs in threat intelligence systems can profoundly enhance predictive capabilities and dynamically adapt to evolving cyber threats. This advancement not only bolsters security postures but also supports proactive security management, underscoring the critical role of LLMs in future IT security strategies.",10.1109/ICOSEC61587.2024.10722712,['Conference'],Computer Science; Engineering
A multimodal dialogue system for improving user satisfaction via knowledge-enriched response and image recommendation,2023,"Jiangnan Wang, Haisheng Li, Leiquan Wang, Chunlei Wu",,10.1007/s00521-023-08409-z,['JournalArticle'],Computer Science
DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing,2025,"Xinyu Ma, Yifeng Xu, Yang Lin, Tianlong Wang, Xu Chu, Xin Gao, Junfeng Zhao, Yasha Wang","We introduce DRESS, a novel approach for generating stylized large language model (LLM) responses through representation editing. Existing methods like prompting and fine-tuning are either insufficient for complex style adaptation or computationally expensive, particularly in tasks like NPC creation or character role-playing. Our approach leverages the over-parameterized nature of LLMs to disentangle a style-relevant subspace within the model's representation space to conduct representation editing, ensuring a minimal impact on the original semantics. By applying adaptive editing strengths, we dynamically adjust the steering vectors in the style subspace to maintain both stylistic fidelity and semantic integrity. We develop two stylized QA benchmark datasets to validate the effectiveness of DRESS, and the results demonstrate significant improvements compared to baseline methods such as prompting and ITI. In short, DRESS is a lightweight, train-free solution for enhancing LLMs with flexible and effective style control, making it particularly useful for developing stylized conversational agents. Codes and benchmark datasets are available at https://github.com/ArthurLeoM/DRESS-LLM.",10.48550/arXiv.2501.14371,['JournalArticle'],Computer Science
Thinking Before Speaking: A Role-playing Model with Mindset,2024,"Baohua Zhang, Yongyi Huang, Wenyao Cui, Huaping Zhang","Role-playing is an easy task for Large Language Models (LLMs), as they are skilled at simulating human behaviors. Many current studies have enabled LLMs to generate responses in the tone of a specific role by fine-tuning the models or using specialized prompts. However, it is typically easy to recognize when a role is being played by LLMs. These models tend to perform poorly when confronted with knowledge that the assumed role does not possess, or a question that requires the specific experience or logic of the role to answer. To address this problem and make LLMs act more like real roles, we propose a Thinking Before Speaking (TBS) model in this paper. Unlike other studies, we first extend the data based on the character's real-life scenarios and the historical dialogue, supplementing each pair of dialogue with the character's mindset. Then we add few data points that include elements beyond the role's knowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's thought process and logic, avoiding responses that fall outside the role's knowledge base. We have also prepared a dataset and evaluation metrics to test these capabilities. Experimental results show that our TBS model can better emulate a role in terms of tone, knowledge, and mindset.",10.48550/arXiv.2409.13752,['JournalArticle'],Computer Science
LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing,2024,"Sunwoong Kim, Jongho Jeong, Jin Soo Han, Donghyuk Shin","Surveys are widely used in social sciences to understand human behavior, but their implementation often involves iterative adjustments that demand significant effort and resources. To this end, researchers have increasingly turned to large language models (LLMs) to simulate human behavior. While existing studies have focused on distributional similarities, individual-level comparisons remain underexplored. Building upon prior work, we investigate whether providing LLMs with respondents' prior information can replicate both statistical distributions and individual decision-making patterns using Partial Least Squares Structural Equation Modeling (PLS-SEM), a well-established causal analysis method. We also introduce the concept of the LLM-Mirror, user personas generated by supplying respondent-specific information to the LLM. By comparing responses generated by the LLM-Mirror with actual individual survey responses, we assess its effectiveness in replicating individual-level outcomes. Our findings show that: (1) PLS-SEM analysis shows LLM-generated responses align with human responses, (2) LLMs, when provided with respondent-specific information, are capable of reproducing individual human responses, and (3) LLM-Mirror responses closely follow human responses at the individual level. These findings highlight the potential of LLMs as a complementary tool for pre-testing surveys and optimizing research design.",10.48550/arXiv.2412.03162,"['JournalArticle', 'Review']",Computer Science; Sociology
Natural Language Processing (NLP) in AI-Driven Recruitment Systems,2022,Sudheer Devaraju,"The focus of this study is the use of Natural Language Processing (NLP) to help improve AI-based recruitment systems. The goal is to train and tune NLP models to help speed up the screening, ranking, and matching of job candidates using resumes and job descriptions. To improve recruitment chatbots, advanced models like BERT and GPT are adopted to promote dynamic candidate interaction as well as initial interviews. The study also involves constructing an NLP-driven interview simulation tool based on Hiring Manager input, create which aids ensure better candidate suitability by emulating real interactions. The research then extends the semantic search algorithm to refine the selection of candidates from large HR databases. The study seeks to address the recruitment efficiency, candidate fit and engagement challenges by integrating these methods into a more effective and streamlined hiring process.",10.32628/cseit2285241,['JournalArticle'],Business; Computer Science
Chat-based Person Retrieval via Dialogue-Refined Cross-Modal Alignment,2025,"Yang Bai, Yucheng Ji, Min Cao, Jinqiao Wang, Mang Ye","Traditional text-based person retrieval (TPR) relies on a single-shot text as query to retrieve the target person, assuming that the query completely captures the users search intent. However, in real-world scenarios, it can be challenging to ensure the information completeness of such single-shot text. To address this limitation, we propose chat-based person retrieval (ChatPR), a new paradigm that takes an interactive dialogue as query to perform the person retrieval, engaging the user in conversational context to progressively refine the query for accurate person retrieval. The primary challenge in ChatPR is the lack of available dialogue-image paired data. To overcome this challenge, we establish ChatPedes, the first dataset designed for ChatPR, which is constructed by leveraging large language models to automate the question generation and simulate user responses. Additionally, to bridge the modality gap between dialogues and images, we propose a dialogue-refined cross-modal alignment (DiaNA) framework, which leverages two adaptive attribute refiner modules to bottleneck the conversational and visual information for fine-grained cross-modal alignment. Moreover, we propose a dialogue-specific data augmentation strategy, random round retaining, to further enhance the models generalization ability across varying dialogue lengths. Extensive experiments demonstrate that DiaNA significantly outperforms existing TPR approaches, highlighting the effectiveness of conversational interactions for person retrieval.",10.1109/CVPR52734.2025.00374,"['JournalArticle', 'Conference']",Computer Science
Exploring the Potential of Large Language Models to Simulate Personality,2025,"Maria Molchanova, Anna Mikhailova, Anna Korzanova, Lidiia Ostyakova, Alexandra Dolidze","With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills.",10.48550/arXiv.2502.08265,['JournalArticle'],Computer Science
Can Generative AI agents behave like humans? Evidence from laboratory market experiments,2025,"R. M. D. R. Chanona, Marco Pangallo, C. Hommes","We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.",10.48550/arXiv.2505.07457,['JournalArticle'],Computer Science; Economics
Integrating Algorithmic Decision Making into Small Business Credit Initiatives: a path to Enhanced Efficiency and Inclusive Economic Growth,2024,"Vikas Mendhe, Shantanu Neema, Shobhit Mittal","Purpose: This paper addresses the challenges faced by small businesses in accessing credit through Small Business Credit Initiatives (SBCI) in the United States. Despite the success of SBCI in creating jobs and fostering economic growth, there are limitations in the evaluation process. 
Methodology: The research design integrates advanced algorithmic decision-making, machine learning, and LLMs into existing credit evaluation process. Primary data is collected from various sources, including financial and business history, market sentiments, external factors, and utilization of sampling techniques if required. Document review, surveys and digital platforms are used for collecting data for LLMs to extract insightful information from complex sources. This comprehensive approach, combining with traditional and innovative methods, aims to establish a robust foundation for developing and evaluating a fair, efficient, and adaptive credit evaluation system for small business credit initiatives. 
Findings: The proposed framework integrates external market factors and use of LLMs for document review on top of primary data sources currently in adaption. Data processing could be amended by extracting features by using advanced natural language processing to enhance feature space by collecting valuable information which is expected to enhance predictive power, adjustment of thresholds and decision making along with a feedback loop. 
Unique Contribution to Theory, Policy, and Practice: Unique framework to accelerate small business credit initiatives by developing a new process of selecting and evaluating machine learning model centered on addressing associated risks, adapting to changes in government policy, improving current procedures, and incorporating feedback from stakeholders and applicants. This is done in an organized manner, with a focus on monitoring and maintaining algorithmic decision models.",10.47941/ijf.1646,"['JournalArticle', 'Review']",Business; Computer Science; Economics
Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation,2025,"Ekaterina Fadeeva, Aleksandr Rubashevskii, Roman Vashurin, S. Dhuliawala, Artem Shelmanov, Timothy Baldwin, Preslav Nakov, Mrinmaya Sachan, Maxim Panov","Large Language Models (LLMs) enhanced with external knowledge retrieval, an approach known as Retrieval-Augmented Generation (RAG), have shown strong performance in open-domain question answering. However, RAG systems remain susceptible to hallucinations: factually incorrect outputs that may arise either from inconsistencies in the model's internal knowledge or incorrect use of the retrieved context. Existing approaches often conflate factuality with faithfulness to the retrieved context, misclassifying factually correct statements as hallucinations if they are not directly supported by the retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval Augmented UNcertainty Quantification), a novel method for hallucination detection in RAG outputs. FRANQ applies different Uncertainty Quantification (UQ) techniques to estimate factuality based on whether a statement is faithful to the retrieved context or not. To evaluate FRANQ and other UQ techniques for RAG, we present a new long-form Question Answering (QA) dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging examples. Extensive experiments on long- and short-form QA across multiple datasets and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing methods.",10.48550/arXiv.2505.21072,['JournalArticle'],Computer Science
Enhancing Trust in Large Language Models with Uncertainty-Aware Fine-Tuning,2024,"Ranganath Krishnan, Piyush Khanna, Omesh Tickoo","Large language models (LLMs) have revolutionized the field of natural language processing with their impressive reasoning and question-answering capabilities. However, these models are sometimes prone to generating credible-sounding but incorrect information, a phenomenon known as LLM hallucinations. Reliable uncertainty estimation in LLMs is essential for fostering trust in their generated responses and serves as a critical tool for the detection and prevention of erroneous or hallucinated outputs. To achieve reliable and well-calibrated uncertainty quantification in open-ended and free-form natural language generation, we propose an uncertainty-aware fine-tuning approach for LLMs. This approach enhances the model's ability to provide reliable uncertainty estimates without compromising accuracy, thereby guiding them to produce more trustworthy responses. We introduce a novel uncertainty-aware causal language modeling loss function, grounded in the principles of decision theory. Through rigorous evaluation on multiple free-form question-answering datasets and models, we demonstrate that our uncertainty-aware fine-tuning approach yields better calibrated uncertainty estimates in natural language generation tasks than fine-tuning with the standard causal language modeling loss. Furthermore, the experimental results show that the proposed method significantly improves the model's ability to detect hallucinations and identify out-of-domain prompts.",10.48550/arXiv.2412.02904,['JournalArticle'],Computer Science
Generative agents in the streets: Exploring the use of Large Language Models (LLMs) in collecting urban perceptions,2023,"Deepank Verma, Olaf Mumm, V. Carlow","Evaluating the surroundings to gain understanding, frame perspectives, and anticipate behavioral reactions is an inherent human trait. However, these continuous encounters are diverse and complex, posing challenges to their study and experimentation. Researchers have been able to isolate environmental features and study their effect on human perception and behavior. However, the research attempts to replicate and study human behaviors with proxies, such as by integrating virtual mediums and interviews, have been inconsistent. Large language models (LLMs) have recently been unveiled as capable of contextual understanding and semantic reasoning. These models have been trained on large amounts of text and have evolved to mimic believable human behavior. This study explores the current advancements in Generative agents powered by LLMs with the help of perceptual experiments. The experiment employs Generative agents to interact with the urban environments using street view images to plan their journey toward specific goals. The agents are given virtual personalities, which make them distinguishable. They are also provided a memory database to store their thoughts and essential visual information and retrieve it when needed to plan their movement. Since LLMs do not possess embodiment, nor have access to the visual realm, and lack a sense of motion or direction, we designed movement and visual modules that help agents gain an overall understanding of surroundings. The agents are further employed to rate the surroundings they encounter based on their perceived sense of safety and liveliness. As these agents store details in their memory, we query the findings to get details regarding their thought processes. Overall, this study experiments with current AI developments and their potential in simulated human behavior in urban environments.",10.48550/arXiv.2312.13126,['JournalArticle'],Computer Science
Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments,2024,"Marharyta Domnich, Julius Vlja, Rasmus Moorits Veski, Giacomo Magnifico, Kadi Tulver, Eduard Barbu, Raul Vicente","As machine learning models evolve, maintaining transparency demands more human-centric explainable AI techniques. Counterfactual explanations, with roots in human reasoning, identify the minimal input changes needed to obtain a given output and, hence, are crucial for supporting decision-making. Despite their importance, the evaluation of these explanations often lacks grounding in user studies and remains fragmented, with existing metrics not fully capturing human perspectives. To address this challenge, we developed a diverse set of 30 counterfactual scenarios and collected ratings across 8 evaluation metrics from 206 respondents. Subsequently, we fine-tuned different Large Language Models (LLMs) to predict average or individual human judgment across these metrics. Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics. The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks.",10.1609/aaai.v39i15.33791,"['JournalArticle', 'Conference']",Computer Science
Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic Analysis,2025,"Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi","Large Language Models (LLMs) are valued for their strong performance across various tasks, but they also produce inaccurate or misleading outputs. Uncertainty Estimation (UE) quantifies the model's confidence and helps users assess response reliability. However, existing UE methods have not been thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG), where the input prompt includes non-parametric knowledge. This paper shows that current UE methods cannot reliably assess correctness in the RAG setting. We further propose an axiomatic framework to identify deficiencies in existing methods and guide the development of improved approaches. Our framework introduces five constraints that an effective UE method should meet after incorporating retrieved documents into the LLM's prompt. Experimental results reveal that no existing UE method fully satisfies all the axioms, explaining their suboptimal performance in RAG. We further introduce a simple yet effective calibration function based on our framework, which not only satisfies more axioms than baseline methods but also improves the correlation between uncertainty estimates and correctness.",10.48550/arXiv.2505.07459,"['JournalArticle', 'Conference']",Computer Science
VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models,2025,"Jen-Tse Huang, Jiantong Qin, Jianping Zhang, Youliang Yuan, Wenxuan Wang, Jieyu Zhao","This research investigates both explicit and implicit social biases exhibited by Vision-Language Models (VLMs). The key distinction between these bias types lies in the level of awareness: explicit bias refers to conscious, intentional biases, while implicit bias operates subconsciously. To analyze explicit bias, we directly pose questions to VLMs related to gender and racial differences: (1) Multiple-choice questions based on a given image (e.g.,""What is the education level of the person in the image?"") (2) Yes-No comparisons using two images (e.g.,""Is the person in the first image more educated than the person in the second image?"") For implicit bias, we design tasks where VLMs assist users but reveal biases through their responses: (1) Image description tasks: Models are asked to describe individuals in images, and we analyze disparities in textual cues across demographic groups. (2) Form completion tasks: Models draft a personal information collection form with 20 attributes, and we examine correlations among selected attributes for potential biases. We evaluate Gemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data are publicly available at https://github.com/uscnlp-lime/VisBias.",10.48550/arXiv.2503.07575,['JournalArticle'],Computer Science; Psychology
Measuring Human and AI Values based on Generative Psychometrics with Large Language Models,2024,"Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song","Human values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. The core idea is to dynamically parse unstructured texts into perceptions akin to static stimuli in traditional psychometrics, measure the value orientations they reveal, and aggregate the results. Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for value-aligned AI.",10.48550/arXiv.2409.12106,"['JournalArticle', 'Conference']",Computer Science; Psychology
BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline,2024,"Guosheng Dong, Da Pan, Yiding Sun, Shusen Zhang, Zheng Liang, Xin Wu, Yan-Bin Shen, Fan Yang, Haoze Sun, Tianpeng Li, Mingan Lin, Jianhua Xu, Yufan Zhang, Xiaonan Nie, Lei Su, Bingning Wang, Wentao Zhang, Jiaxin Mao, Zenan Zhou, Weipeng Chen","The general capabilities of Large Language Models (LLM) highly rely on the composition and selection on extensive pretraining datasets, treated as commercial secrets by several institutions. To mitigate this issue, we open-source the details of a universally applicable data processing pipeline and validate its effectiveness and potential by introducing a competitive LLM baseline. Specifically, the data processing pipeline consists of broad collection to scale up and reweighting to improve quality. We then pretrain a 7B model BaichuanSEED with 3T tokens processed by our pipeline without any deliberate downstream task-related optimization, followed by an easy but effective supervised fine-tuning stage. BaichuanSEED demonstrates consistency and predictability throughout training and achieves comparable performance on comprehensive benchmarks with several commercial advanced large language models, such as Qwen1.5 and Llama3. We also conduct several heuristic experiments to discuss the potential for further optimization of downstream tasks, such as mathematics and coding.",10.48550/arXiv.2408.15079,['JournalArticle'],Computer Science; Linguistics
Human-AI Synergy in Survey Development: Implications from Large Language Models in Business and Research,2024,"Ping Fan Ke, Ka Chung Ng","This study examines the novel integration of Large Language Models (LLMs) into the survey development process in business and research through the development and evaluation of the Behavioral Research Assistant (BRASS) Bot. We first analyzed the traditional scale development process to identify tasks suitable for LLM integration, including both human-in-the-loop and automated LLM data collection methods. Following this analysis, we developed the details of BRASS Bot, incorporating design principles of falsifiability and reproducibility. We then conducted a comprehensive evaluation of the BRASS Bot across a diverse set of LLMs, including GPT, Claude, Gemini, and Llama, to assess its usability, validity, and reliability. We further demonstrated the practical utility of the BRASS Bot by conducting a user study and a predictive validity simulation. Our research presents both theoretical and practical implications. The augmentation approach of the BRASS Bot enriches the theoretical foundations of behavioral constructs by identifying previously overlooked patterns. Additionally, the BRASS Bot offers significant time and resource efficiency gains while enhancing scale validity. Our work lays the foundation for future research on the broader application of LLMs as both assistants and collaborators in survey analysis and behavioral research design and execution, highlighting their potential for a transformative impact on the field.",10.1145/3700597,"['JournalArticle', 'Review']",Business; Computer Science
Can LLMs Help Predict Elections? (Counter)Evidence from the World's Largest Democracy,2024,"Pratik Gujral, Kshitij Awaldhi, Navya Jain, Bhavuk Bhandula, Abhijnan Chakraborty","The study of how social media affects the formation of public opinion and its influence on political results has been a popular field of inquiry. However, current approaches frequently offer a limited comprehension of the complex political phenomena, yielding inconsistent outcomes. In this work, we introduce a new method: harnessing the capabilities of Large Language Models (LLMs) to examine social media data and forecast election outcomes. Our research diverges from traditional methodologies in two crucial respects. First, we utilize the sophisticated capabilities of foundational LLMs, which can comprehend the complex linguistic subtleties and contextual details present in social media data. Second, we focus on data from X (Twitter) in India to predict state assembly election outcomes. Our method entails sentiment analysis of election-related tweets through LLMs to forecast the actual election results, and we demonstrate the superiority of our LLM-based method against more traditional exit and opinion polls. Overall, our research offers valuable insights into the unique dynamics of Indian politics and the remarkable impact of social media in molding public attitudes within this context.",10.48550/arXiv.2405.07828,['JournalArticle'],Computer Science; Political Science
Skewed Score: A statistical framework to assess autograders,2025,"Magda Dubois, Harry Coppock, Mario Giulianelli, Timo Flesch, Lennart Luettgau, C. Ududec","The evaluation of large language model (LLM) outputs is increasingly performed by other LLMs, a setup commonly known as""LLM-as-a-judge"", or autograders. While autograders offer a scalable alternative to human evaluation, they have shown mixed reliability and may exhibit systematic biases, depending on response type, scoring methodology, domain specificity, or other factors. Here we propose a statistical framework based on Bayesian generalised linear models (GLMs) that enables researchers to simultaneously assess their autograders while addressing their primary research questions (e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores or pairwise preferences) as a function of properties of the grader (e.g., human vs. autograder) and the evaluated item (e.g., response length or the LLM that generated it), allowing for explicit quantification of scoring differences and potential biases within a unified framework. In addition, our method can be used to augment traditional metrics such as inter-rater agreement, by providing uncertainty estimates and clarifying sources of disagreement. Overall, this approach contributes to more robust and interpretable use of autograders in LLM evaluation, enabling both performance analysis and bias detection.",10.48550/arXiv.2507.03772,['JournalArticle'],Computer Science; Linguistics; Mathematics
Advanced Techniques in Prompt Engineering for Large Language Models: A Comprehensive Study,2024,"Gaurav Beri, Vaishnavi Srivastava","Prompt engineering is emerging as a pivotal approach to enhance the efficiency and versatility of large language models (LLMs) like GPT-4. This paper provides a comprehensive survey of key techniques in prompt engineering, highlighting foundational methods and advanced strategies. One-shot prompting, a basic yet impactful technique, involves supplying the model with a single example to guide its responses. In contrast, Chain-of-Thought (CoT) prompting encourages the model to generate intermediate reasoning steps, significantly improving its problem-solving abilities. Self-Consistency, an advanced strategy, involves generating multiple potential responses and selecting the most coherent one, thereby increasing the reliability of outputs. The transformative potential of these techniques is showcased through various applications in education, content creation, and programming. In education, prompt engineering can personalize learning experiences and provide tailored feedback to students. Content creation benefits from enhanced creativity and coherence in generated text, aiding writers and marketers. In programming, LLMs assist in code generation and debugging, streamlining the development process. This paper not only outlines the methodologies and their practical applications but also proposes future research directions to further refine and expand the capabilities of prompt engineering. By understanding and leveraging these techniques, practitioners can significantly optimize the performance and adaptability of LLMs across diverse domains. This framework aims to foster innovation and efficiency, laying the groundwork for future advancements in AI-driven technologies.",10.1109/ICTBIG64922.2024.10911672,"['Conference', 'Review']",Computer Science
SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors,2025,"Maheep Chaudhary, Fazl Barez","High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable""Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach.",10.48550/arXiv.2505.14300,['JournalArticle'],Computer Science
LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving Cloud-Device Collaboration,2025,"Yingyi Zhang, Pengyue Jia, Xianneng Li, Derong Xu, Maolin Wang, Yichao Wang, Zhaochen Du, Huifeng Guo, Yong Liu, Ruiming Tang, Xiangyu Zhao","Cloud-device collaboration leverages on-cloud Large Language Models (LLMs) for handling public user queries and on-device Small Language Models (SLMs) for processing private user data, collectively forming a powerful and privacy-preserving solution. However, existing approaches often fail to fully leverage the scalable problem-solving capabilities of on-cloud LLMs while underutilizing the advantage of on-device SLMs in accessing and processing personalized data. This leads to two interconnected issues: 1) Limited utilization of the problem-solving capabilities of on-cloud LLMs, which fail to align with personalized user-task needs, and 2) Inadequate integration of user data into on-device SLM responses, resulting in mismatches in contextual user information. In this paper, we propose a Leader-Subordinate Retrieval framework for Privacy-preserving cloud-device collaboration (LSRP), a novel solution that bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM through a dynamic selection of task-specific leader strategies named as user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the data advantages of on-device SLMs through small model feedback Direct Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the on-device SLM. Experiments on two datasets demonstrate that LSRP consistently outperforms state-of-the-art baselines, significantly improving question-answer relevance and personalization, while preserving user privacy through efficient on-device retrieval. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/LSRP.",10.48550/arXiv.2505.05031,"['JournalArticle', 'Conference']",Computer Science
OpenEP: Open-Ended Future Event Prediction,2024,"Yong Guan, Hao Peng, Xiaozhi Wang, Lei Hou, Juanzi Li","Future event prediction (FEP) is a long-standing and crucial task in the world, as understanding the evolution of events enables early risk identification, informed decision-making, and strategic planning. Existing work typically treats event prediction as classification tasks and confines the outcomes of future events to a fixed scope, such as yes/no questions, candidate set, and taxonomy, which is difficult to include all possible outcomes of future events. In this paper, we introduce OpenEP (an Open-Ended Future Event Prediction task), which generates flexible and diverse predictions aligned with real-world scenarios. This is mainly reflected in two aspects: firstly, the predictive questions are diverse, covering different stages of event development and perspectives; secondly, the outcomes are flexible, without constraints on scope or format. To facilitate the study of this task, we construct OpenEPBench, an open-ended future event prediction dataset. For question construction, we pose questions from seven perspectives, including location, time, event development, event outcome, event impact, event response, and other, to facilitate an in-depth analysis and understanding of the comprehensive evolution of events. For outcome construction, we collect free-form text containing the outcomes as ground truth to provide semantically complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a stakeholder-enhanced future event prediction framework, that incorporates event characteristics for open-ended settings. Our method extracts stakeholders involved in events to extend questions to gather diverse information. We also collect historically events that are relevant and similar to the question to reveal potential evolutionary patterns. Experiment results indicate that accurately predicting future events in open-ended settings is challenging for existing LLMs.",10.48550/arXiv.2408.06578,['JournalArticle'],Computer Science
"OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation",2025,"Ziyi Wang, Yuxuan Lu, Wenbo Li, Amirali Amini, Bo Sun, Yakov Bart, Weimin Lyu, Jiri Gesi, Tian Wang, Jing Huang, Yu Su, Upol Ehsan, Malihe Alikhani, T. Li, Lydia Chilton, Dakuo Wang","Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable''human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona andhistory. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.",10.48550/arXiv.2506.05606,['JournalArticle'],Computer Science
Rethinking the Role of Proxy Rewards in Language Model Alignment,2024,"Sungdong Kim, Minjoon Seo","Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its behavior remained as a black-box. In this paper, we study the role of proxy rewards in the LLM alignment via reverse reward engineering by composing interpretable features as a white-box reward function. We aim to replicate the ground truth (gold) reward signal by achieving a monotonic relationship between the proxy and gold reward signals after training the model using the proxy reward in reinforcement learning (RL). Our findings indicate that successfully emulating the gold reward requires generating responses that are relevant with enough length to open-ended questions, while also ensuring response consistency in closed-ended questions. Furthermore, resulting models optimizing our devised white-box reward show competitive performances with strong open-source RMs in alignment benchmarks. We highlight its potential usage as a simple but strong reward baseline for the LLM alignment, not requiring explicit human feedback dataset and RM training.",10.18653/v1/2024.emnlp-main.1150,"['JournalArticle', 'Conference']",Computer Science
"SouLLMate: An Adaptive LLM-Driven System for Advanced Mental Health Support and Assessment, Based on a Systematic Application Survey",2024,"Qiming Guo, Jinwen Tang, Wenbo Sun, Haoteng Tang, Yi Shang, Wenlu Wang","Mental health issues significantly impact individuals'daily lives, yet many do not receive the help they need even with available online resources. This study aims to provide accessible, stigma-free, personalized, and real-time mental health support through cutting-edge AI technologies. It makes the following contributions: (1) Conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering, and domain knowledge. This system offers advanced features such as Suicide Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized profile uploads and Conversational Information Extraction. (3) Developing novel evaluation approaches to assess preliminary assessments and suicide risk detection, utilizing annotated real-life interview data and professionally labeled datasets indicating suicide tendencies. (4) Proposing Key Indicator Summarization (KIS) and Proactive Questioning Strategy (PQS) methods to enhance model performance and usability through context-sensitive response adjustments and semantic coherence evaluations. This study contributes to advancing mental health support technologies, potentially improving the accessibility and effectiveness of mental health care globally.",10.48550/arXiv.2410.11859,"['JournalArticle', 'Review']",Computer Science; Psychology
Comuniqa: Exploring Large Language Models For Improving English Speaking Skills,2024,"Shikhar Sharma, Manas Mhasakar, Apurv Mehra, Utkarsh Venaik, Ujjwal Singhal, Dhruv Kumar, Kashish Mittal","In this paper, we investigate the potential of Large Language Models (LLMs) to improve English speaking skills. This is particularly relevant in countries like India, where English is crucial for academic, professional, and personal communication but remains a non-native language for many. Traditional methods for enhancing speaking skills often rely on human experts, which can be limited in terms of scalability, accessibility, and affordability. Recent advancements in Artificial Intelligence (AI) offer promising solutions to overcome these limitations. We propose Comuniqa, a novel LLM-based system designed to enhance English speaking skills. We adopt a human-centric evaluation approach, comparing Comuniqa with the feedback and instructions provided by human experts. In our evaluation, we divide the participants in three groups: those who use LLM-based system for improving speaking skills, those guided by human experts for the same task and those who utilize both the LLM-based system as well as the human experts. Using surveys, interviews, and actual study sessions, we provide a detailed perspective on the effectiveness of different learning modalities. Our preliminary findings suggest that while LLM-based systems have commendable accuracy, they lack human-level cognitive capabilities, both in terms of accuracy and empathy. Nevertheless, Comuniqa represents a significant step towards achieving Sustainable Development Goal 4: Quality Education by providing a valuable learning tool for individuals who may not have access to human experts for improving their speaking skills.",10.1145/3674829.3675082,"['Book', 'JournalArticle', 'Review']",Computer Science; Linguistics
Revolutionizing Talent: The Path in 21st Century Workforce Transformation,2024,"Elizabeth Koumpan, Lynda Mcowen","In a world where progress is driven by people and experience, having the right workforce with a strong HR and Talent strategy is key to business growth and business agility. Building and retaining this talent requires the enterprise to cater to the changing talent landscape with a comprehensive talent agenda to attract, develop and retain the right resources for their workforce. In order to achieve these objectives, it is important for the enterprise to develop and scale new digital and data savvy skills while balancing user-centricity with an industry solution orientation that starts with talent acquisition and continues on through the employee lifecycle. This transformational journey to enhance the employee experience and improve productivity includes elements such as talent acquisition (attraction, recruiting and interviewing, onboarding), personalized career development, personalized retention practices (motivation, feedback, recognition, opportunities) and predicting critical events (new technology burst such as Gen AI) while maintaining a solution that addresses all required regulations. This includes creating tools that assist in evaluating and assessing new legislative texts, identifying overlap with existing regulations, locating any red flags (e.g. executive liability) and preparing recommended output.In this paper we will explain how the use of Large Language Models and Gen AI can create the capabilities in talent transformation utilizing the enterprises base infrastructure to support enhancements in employee experience and productivity throughout the different stages of the employee lifecycle. Implementing this technology in conjunction with human expertise will ensure that these initiatives align with the organization's specific goals and values, ethical considerations, transparency, and accountability that are crucial when deploying AI in HR and talent management.We will share our architectural patterns that we have used as a foundation for building a holistic talent transformation framework, that any enterprise can adapt and combine to suit their specific needs and goals in managing and optimizing their workforce in the 21st century.",10.54941/ahfe1004932,nan,Business; Computer Science
Exploring Social Desirability Response Bias in Large Language Models: Evidence from GPT-4 Simulations,2024,"Sanguk Lee, Kaiqi Yang, Tai-Quan Peng, R. Heo, Hui Liu","Large language models (LLMs) are employed to simulate human-like responses in social surveys, yet it remains unclear if they develop biases like social desirability response (SDR) bias. To investigate this, GPT-4 was assigned personas from four societies, using data from the 2022 Gallup World Poll. These synthetic samples were then prompted with or without a commitment statement intended to induce SDR. The results were mixed. While the commitment statement increased SDR index scores, suggesting SDR bias, it reduced civic engagement scores, indicating an opposite trend. Additional findings revealed demographic associations with SDR scores and showed that the commitment statement had limited impact on GPT-4's predictive performance. The study underscores potential avenues for using LLMs to investigate biases in both humans and LLMs themselves.",10.48550/arXiv.2410.15442,"['JournalArticle', 'Review']",Computer Science; Political Science; Psychology
Knowledge Graphs Enhanced Large Language Model Prompt for Electric Power Question Answering,2023,"Chen Wang, Min Hua, Jiale Song, Xue-song Tang","With the continuous development and digital transformation in the field of electric power, the application of large language models in the electric power industry has become a remarkable trend. The electric power industry is an information-intensive domain involving extensive data processing, predictive analysis, and decision-making. Therefore, the application of large language models in the electric power sector is of great significance. Current large language models such as GPT3.5 and GLM can perform well in tasks such as question answering dialogues. However, these models still face challenges such as answer hallucination and inaccurate responses. This paper proposes a method to enhance question answering in large language models using knowledge graphs, aiming to improve the accuracy and reliability of these models in question answering tasks in the electric power domain.The proposed method first utilizes local electric power data to extract triplets and generate a question answering dataset specific to the electric power domain using a large language model. Then, the relationships of the knowledge graph triplets are incorporated into the question prompt to enhance the quality of the model's answers. Furthermore, we fine-tune the large language model using the expanded question set derived from the triplets as knowledge enhanced data. Subsequently, we conduct experiments on both an electric power question answering dataset and a knowledge graph question answering dataset. The experimental results demonstrate that our method significantly improves various metrics of the large language model in the electric power question answering task. This research provides new insights and approaches to enhance the effectiveness of question answering systems in the electric power domain. Future studies can further explore and optimize this prompt expansion method for application in broader domains and tasks.",10.1145/3650400.3650405,"['JournalArticle', 'Book']",Computer Science; Engineering
Ethical and Scalable Automation: A Governance and Compliance Framework for Business Applications,2024,Haocheng Lin,"The popularisation of applying AI in businesses poses significant challenges relating to ethical principles, governance, and legal compliance. Although businesses have embedded AI into their day-to-day processes, they lack a unified approach for mitigating its potential risks. This paper introduces a framework ensuring that AI must be ethical, controllable, viable, and desirable. Balancing these factors ensures the design of a framework that addresses its trade-offs, such as balancing performance against explainability. A successful framework provides practical advice for businesses to meet regulatory requirements in sectors such as finance and healthcare, where it is critical to comply with standards like GPDR and the EU AI Act. Different case studies validate this framework by integrating AI in both academic and practical environments. For instance, large language models are cost-effective alternatives for generating synthetic opinions that emulate attitudes to environmental issues. These case studies demonstrate how having a structured framework could enhance transparency and maintain performance levels as shown from the alignment between synthetic and expected distributions. This alignment is quantified using metrics like Chi-test scores, normalized mutual information, and Jaccard indexes. Future research should explore the framework's empirical validation in diverse industrial settings further, ensuring the model's scalability and adaptability.",10.48550/arXiv.2409.16872,['JournalArticle'],Business; Computer Science; Law
A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models,2025,"Zhouhang Xie, Junda Wu, Yiran Shen, Yu Xia, Xintong Li, Aaron Chang, Ryan A. Rossi, Sachin Kumar, Bodhisattwa Prasad Majumder, Jingbo Shang, Prithviraj Ammanabrolu, Julian J. McAuley","Personalized preference alignment for large language models (LLMs), the process of tailoring LLMs to individual users' preferences, is an emerging research direction spanning the area of NLP and personalization. In this survey, we present an analysis of works on personalized alignment and modeling for LLMs. We introduce a taxonomy of preference alignment techniques, including training time, inference time, and additionally, user-modeling based methods. We provide analysis and discussion on the strengths and limitations of each group of techniques and then cover evaluation, benchmarks, as well as open problems in the field.",10.48550/arXiv.2504.07070,"['JournalArticle', 'Review']",Computer Science; Linguistics
A Step Towards Modern Disinformation Detection: Novel Methods for Detecting LLM-Generated Text,2024,"Samuel Nathanson, Yungjun Yoo, David Na, Yinzhi Cao, Lanier Watkins","New generative artificial intelligence (GenAI) technology could have devastating consequences on our democracy because it can be easily used to spread disinformation at scale while simultaneously personalizing propaganda to demographics or individuals. The threat is significant  Large-scale GenAI-based disinformation campaigns can sway public opinion, shape political events, or even compromise the integrity of elections. One method for defending against large-scale GenAI disinformation is to build tools for autonomously detecting AI-generated content. In this article, we evaluate state-of-the-art AI detection tools. Additionally, we propose a novel AI content detection method which demonstrates up to a 48% improvement in accuracy (over existing tools) for autonomously detecting AI-generated content.",10.1109/MILCOM61039.2024.10773838,"['JournalArticle', 'Conference']",Computer Science; Political Science
Applications and Challenges of Large Language Models in Smart Government -From technological Advances to Regulated Applications,2024,Ziqing Dai,"This paper explores the applications and challenges of large language models (LLMs) in the context of smart government. It delves into how LLMs can enhance government decision-making, policy interpretation, and public service delivery through intelligent analysis and predictions. It also discusses the role of LLMs in processing vast amounts of government information and in analyzing public opinion. Concurrently, the paper acknowledges the challenges posed by LLMs, including data costs, security and privacy concerns, model robustness, regulatory hurdles, and technical and talent bottlenecks. It proposes recommendations for the regulated application of LLMs, such as developing robust data protection policies, standardizing model research and evaluation, fostering interdisciplinary research, and promoting integrated development across key sectors. The paper concludes with an outlook on the future of LLMs in smart government, emphasizing the need for cautious optimism and responsible innovation.",10.1145/3653644.3653662,"['JournalArticle', 'Book']",Computer Science; Political Science
Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques,2023,"Junxiao Shen, John J. Dudley, Jingyao Zheng, Bill Byrne, P. O. Kristensson","Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly data collection, fine-tuning and post-processing. However, the task of prompting large language models to specialize in specific text prediction tasks can be challenging, particularly for designers without expertise in prompt engineering. To address this, we introduce Promptor, a conversational prompt generation agent designed to engage proactively with designers. Promptor can automatically generate complex prompts tailored to meet specific needs, thus offering a solution to this challenge. We conducted a user study involving 24 participants creating prompts for three intelligent text entry tasks, half of the participants used Promptor while the other half designed prompts themselves. The results show that Promptor-designed prompts result in a 35% increase in similarity and 22% in coherence over those by designers.",10.48550/arXiv.2310.08101,['JournalArticle'],Computer Science
"From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",2024,"Jacob Russin, Sam Whitman McGrath, Danielle J. Williams, Lotem Elber-Dorozko","Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.",10.48550/arXiv.2405.15164,"['JournalArticle', 'Review']",Computer Science; Linguistics
Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks,2024,"Zhangqi Duan, Nigel Fernandez, Alexander Hicks, Andrew Lan","Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a students code submission passes each test case and 2) the students open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.",10.1145/3706468.3706500,"['Book', 'JournalArticle']",Computer Science
"A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms",2025,"Chengkai Huang, Hongtao Huang, Tong Yu, Kaige Xie, Junda Wu, Shuai Zhang, Julian J. McAuley, Dietmar Jannach, Lina Yao","Recommender systems (RS) have become essential in filtering information and personalizing content for users. RS techniques have traditionally relied on modeling interactions between users and items as well as the features of content using models specific to each task. The emergence of foundation models (FMs), large scale models trained on vast amounts of data such as GPT, LLaMA and CLIP, is reshaping the recommendation paradigm. This survey provides a comprehensive overview of the Foundation Models for Recommender Systems (FM4RecSys), covering their integration in three paradigms: (1) Feature-Based augmentation of representations, (2) Generative recommendation approaches, and (3) Agentic interactive systems. We first review the data foundations of RS, from traditional explicit or implicit feedback to multimodal content sources. We then introduce FMs and their capabilities for representation learning, natural language understanding, and multi-modal reasoning in RS contexts. The core of the survey discusses how FMs enhance RS under different paradigms. Afterward, we examine FM applications in various recommendation tasks. Through an analysis of recent research, we highlight key opportunities that have been realized as well as challenges encountered. Finally, we outline open research directions and technical challenges for next-generation FM4RecSys. This survey not only reviews the state-of-the-art methods but also provides a critical analysis of the trade-offs among the feature-based, the generative, and the agentic paradigms, outlining key open issues and future research directions.",10.48550/arXiv.2504.16420,"['JournalArticle', 'Review']",Computer Science
Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits,2024,"Bohan Li, Jiannan Guan, Longxu Dou, ylfeng, Dingzirui Wang, Yang Xu, Enbo Wang, Qiguang Chen, Bichen Wang, Xiao Xu, Yimeng Zhang, Libo Qin, Yanyan Zhao, Qingfu Zhu, Wanxiang Che","The Myers-Briggs Type Indicator (MBTI) is one of the most influential personality theories reflecting individual differences in thinking, feeling, and behaving. MBTI personality detection has garnered considerable research interest and has evolved significantly over the years. However, this task tends to be overly optimistic, as it currently does not align well with the natural distribution of population personality traits. Specifically, (1) the self-reported labels in existing datasets result in incorrect labeling issues, and (2) the hard labels fail to capture the full range of population personality distributions. In this paper, we optimize the task by constructing MBTIBench, the first manually annotated high-quality MBTI personality detection dataset with soft labels, under the guidance of psychologists. As for the first challenge, MBTIBench effectively solves the incorrect labeling issues, which account for 29.58% of the data. As for the second challenge, we estimate soft labels by deriving the polarity tendency of samples. The obtained soft labels confirm that there are more people with non-extreme personality traits. Experimental results not only highlight the polarized predictions and biases in LLMs as key directions for future research, but also confirm that soft labels can provide more benefits to other psychological tasks than hard labels. The code and data are available at https://github.com/Personality-NLP/MbtiBench.",10.48550/arXiv.2412.12510,['JournalArticle'],Computer Science; Psychology
"Using Large Language Models in Public Transit Systems, San Antonio as a case study",2024,"Ramya Jonnala, Gongbo Liang, Jeong Yang, I. Alsmadi","The integration of large language models into public transit systems represents a significant advancement in urban transportation management and passenger experience. This study examines the impact of LLMs within San Antonio's public transit system, leveraging their capabilities in natural language processing, data analysis, and real time communication. By utilizing GTFS and other public transportation information, the research highlights the transformative potential of LLMs in enhancing route planning, reducing wait times, and providing personalized travel assistance. Our case study is the city of San Antonio as part of a project aiming to demonstrate how LLMs can optimize resource allocation, improve passenger satisfaction, and support decision making processes in transit management. We evaluated LLM responses to questions related to both information retrieval and also understanding. Ultimately, we believe that the adoption of LLMs in public transit systems can lead to more efficient, responsive, and user-friendly transportation networks, providing a model for other cities to follow.",10.48550/arXiv.2407.11003,['JournalArticle'],Computer Science; Engineering
Improving Industrial Question Answering Chatbots with Domain-Specific LLMs Fine-Tuning,2024,"Riccardo Rosati, Filippo Antonini, Nikhil Muralikrishna, Flavio Tonetto, Adriano Mancini","The industrial landscape is experiencing a significant transformation and there is a growing emphasis on human-machine collaboration, personalization, and sustainable manufacturing. Within this context, large language models (LLMs) have emerged as pivotal Artificial Intelligence (AI) tools that enhance communication, streamline operations, and support complex decision-making tasks through advanced natural language processing capabilities. This paper explores the process of fine-tuning a foundational general-purpose LLM, specifically the open-source LLaMA 3 model, to cater to industrial applications. We delve into the methodology of fine-tuning LLaMA on domain-specific data from two distinct industrial scenarios (i.e., product management and production line operations). Our results demonstrate significant improvements in the models performance, accuracy, and relevance in generating responses, thereby enhancing decision-making and operational processes in industrial settings. Additionally, we propose the integration of the fine-tuned model into a chatbot system designed to assist human operators by providing targeted, accurate responses in real-time. This paper aims to provide a comprehensive guide for industry practitioners and researchers to harness the power of LLMs for specialized industrial tasks, ultimately contributing to the advancement of intelligent industrial automation and decision-making systems.",10.1109/MESA61532.2024.10704843,"['JournalArticle', 'Conference']",Computer Science; Engineering
"Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",2025,"Wenhan Dong, Yuemeng Zhao, Zhen Sun, Yule Liu, Zifan Peng, Jingyi Zheng, Zongmin Zhang, Ziyi Zhang, Jun Wu, Ruiming Wang, Shengmin Xu, Xinyi Huang, Xinlei He","As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs' capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.",10.48550/arXiv.2505.00049,"['JournalArticle', 'Review']",Computer Science; Psychology
Conversational Complexity for Assessing Risk in Large Language Models,2024,"John Burden, Manuel Cebrian, Jos Hernndez-Orallo","Large Language Models (LLMs) present a dual-use dilemma: they enable beneficial applications while harboring potential for harm, particularly through conversational interactions. Despite various safeguards, advanced LLMs remain vulnerable. A watershed case in early 2023 involved journalist Kevin Roose's extended dialogue with Bing, an LLM-powered search engine, which revealed harmful outputs after probing questions, highlighting vulnerabilities in the model's safeguards. This contrasts with simpler early jailbreaks, like the""Grandma Jailbreak,""where users framed requests as innocent help for a grandmother, easily eliciting similar content. This raises the question: How much conversational effort is needed to elicit harmful information from LLMs? We propose two measures to quantify this effort: Conversational Length (CL), which measures the number of conversational turns needed to obtain a specific harmful response, and Conversational Complexity (CC), defined as the Kolmogorov complexity of the user's instruction sequence leading to the harmful response. To address the incomputability of Kolmogorov complexity, we approximate CC using a reference LLM to estimate the compressibility of the user instructions. Applying this approach to a large red-teaming dataset, we perform a quantitative analysis examining the statistical distribution of harmful and harmless conversational lengths and complexities. Our empirical findings suggest that this distributional analysis and the minimization of CC serve as valuable tools for understanding AI safety, offering insights into the accessibility of harmful information. This work establishes a foundation for a new perspective on LLM safety, centered around the algorithmic complexity of pathways to harm.",10.48550/arXiv.2409.01247,['JournalArticle'],Computer Science; Linguistics; Mathematics
Evaluating LLM-Generated Topics from Survey Responses: Identifying Challenges in Recruiting Participants through Crowdsourcing,2024,"Reham Al Tamime, Joni O. Salminen, Soon-gyo Jung, Bernard J. Jansen","The evolution of generative artificial intelligence (AI) technologies, particularly large language models (LLMs), has lead to consequences for the field of Human-Computer Interaction (HCI) in areas such as personalization, predictive analytics, automation, and data analysis. This research aims to evaluate LLM-generated topics derived from survey responses in comparison with topics suggested by humans, particularly participants recruited through a crowdsourcing experiment. We present an evaluation results to compare LLM-generated topics with human-generated topics in terms of Quality, Usefulness, Accuracy, Interestingness, and Completeness. This involves three stages: (1) Design and Generate Topics with an LLM (OpenAIs GPT-4); (2) Crowdsourcing Human-Generated Topics; and (3) Evaluation of Human-Generated Topics and LLM-Generated Topics. However, a feasibility study with 33 crowdworkers indicated challenges in using participants for LLM evaluation, particularly in inviting humans participants to suggest topics based on open-ended survey answers. We highlight several challenges in recruiting crowdsourcing participants for generating topics from survey responses. We recommend using well-trained human experts rather than crowdsourcing to generate human baselines for LLM evaluation.",10.1109/VL/HCC60511.2024.00064,"['JournalArticle', 'Review']",Computer Science
Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey,2025,"Ahsan Bilal, Muhammad Ahmed Mohsin, Muhammad Umer, Muhammad Awais Khan Bangash, Muhammad Ali Jamshed","This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.",10.48550/arXiv.2504.14520,"['JournalArticle', 'Review']",Computer Science
LABOR-LLM: Language-Based Occupational Representations with Large Language Models,2024,"Tianyu Du, Ayush Kanodia, Herman Brunborg, Keyon Vafa, Susan Athey","Vafa et al. (2024) introduced a transformer-based econometric model, CAREER, that predicts a worker's next job as a function of career history (an""occupation model""). CAREER was initially estimated (""pre-trained"") using a large, unrepresentative resume dataset, which served as a""foundation model,""and parameter estimation was continued (""fine-tuned"") using data from a representative survey. CAREER had better predictive performance than benchmarks. This paper considers an alternative where the resume-based foundation model is replaced by a large language model (LLM). We convert tabular data from the survey into text files that resemble resumes and fine-tune the LLMs using these text files with the objective to predict the next token (word). The resulting fine-tuned LLM is used as an input to an occupation model. Its predictive performance surpasses all prior models. We demonstrate the value of fine-tuning and further show that by adding more career data from a different population, fine-tuning smaller LLMs surpasses the performance of fine-tuning larger models.",10.48550/arXiv.2406.17972,"['JournalArticle', 'Review']",Computer Science; Economics; Linguistics
HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks,2025,"Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev","Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.",10.18653/v1/2025.acl-long.1246,"['JournalArticle', 'Conference', 'Review']",Computer Science
Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative In-Context Learning of a Large Language Model,2024,"Alireza Khanshan, Pieter van Gorp, P. Markopoulos","The Experience Sampling Method (ESM) is commonly used to understand behaviors, thoughts, and feelings in the wild by collecting self-reports. Sustaining sufficient response rates, especially in long-running studies remains challenging. To avoid low response rates and dropouts, experimenters rely on their experience, proposed methodologies from earlier studies, trial and error, or the scarcely available participant behavior data from previous ESM protocols. This approach often fails in finding the acceptable study parameters, resulting in redesigning the protocol and repeating the experiment. Research has shown the potential of machine learning to personalize ESM protocols such that ESM prompts are delivered at opportune moments, leading to higher response rates. The corresponding training process is hindered due to the scarcity of open data in the ESM domain, causing a cold start, which could be mitigated by simulating participant behavior. Such simulations provide training data and insights for the experimenters to update their study design choices. Creating this simulation requires behavioral science, psychology, and programming expertise. Large language models (LLMs) have emerged as facilitators for information inquiry and programming, albeit random and occasionally unreliable. We aspire to assess the readiness of LLMs in an ESM use case. We conducted research using GPT-3.5 turbo-16k to tackle an ESM simulation problem. We explored several prompt design alternatives to generate ESM simulation programs, evaluated the output code in terms of semantics and syntax, and interviewed ESM practitioners. We found that engineering LLM-enabled ESM simulations have the potential to facilitate data generation, but they perpetuate trust and reliability challenges.",10.1145/3661143,['JournalArticle'],Computer Science
AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with an LLM-based Empathetic Coworker,2024,"Vedant Das Swain, Qiuyue Joy Zhong, J. R. Parekh, Yechan Jeon, Roy Zimmermann, Mary Czerwinski, Jina Suh, Varun Mishra, Koustuv Saha, Javier Hernandez","Client-Service Representatives (CSRs) are vital to organizations. Frequent interactions with disgruntled clients, however, disrupt their mental well-being. To help CSRs regulate their emotions while interacting with uncivil clients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its efficacy, perception, and use. Our comparative analyses between 665 human and Care-Pilot-generated support messages highlight Care-Pilots ability to adapt to and demonstrate empathy in various incivility incidents. Additionally, 143 CSRs assessed Care-Pilots empathy as more sincere and actionable than human messages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a simulation exercise. They reported that Care-Pilot helped them avoid negative thinking, recenter thoughts, and humanize clients; showing potential for bridging gaps in coworker support. Yet, they also noted deployment challenges and emphasized the indispensability of shared experiences. We discuss future designs and societal implications of AI-mediated emotional labor, underscoring empathy as a critical function for AI assistants for worker mental health.",10.1145/3706598.3713705,"['JournalArticle', 'Book', 'Conference']",Business; Computer Science; Psychology
UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces,2025,"Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, S. Dustdar, Susanna Pirttikangas, Lauri Lov'en","Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.",10.48550/arXiv.2505.00472,['JournalArticle'],Computer Science; Engineering; Environmental Science
MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems,2025,"Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, Yixuan Li","Human social interactions depend on the ability to infer others'unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses about user mental states (e.g., intent, emotion), (2) a Moral Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.",10.48550/arXiv.2505.18943,['JournalArticle'],Computer Science; Psychology
The evolution of research at the intersection of industrial ecology and artificial intelligence,2025,"Yongyue Gong, Fengmei Ma, Heming Wang, A. Tzachor, Wenju Sun, Junming Zhu, Gang Liu, H. Schandl","The intersection of artificial intelligence (AI) and industrial ecology (IE) is gaining significant attention due to AI's potential to enhance the sustainability of production and consumption systems. Understanding the current state of research in this field can highlight covered topics, identify trends, and reveal understudied topics warranting future research. However, few studies have systematically reviewed this intersection. In this study, we analyze 1068 publications within the IEAI domain using trend factor analysis, word2vec modeling, and top2vec modeling. These methods uncover patterns of topic interconnections and evolutionary trends. Our results identify 71 trending terms within the selected publications, 69 of which, such as deep learning, have emerged in the past 8 years. The word2vec analysis shows that the application of various AI techniques is increasingly integrated into life cycle assessment and the circular economy. The top2vec analysis suggests that employing AI to predict and optimize indicators related to products, waste, processes, and their environmental impacts is an emerging trend. Lastly, we propose that finetuning large language models to better understand and process data specific to IE, along with deploying realtime data collection technologies such as sensors, computer vision, and robotics, could effectively address the challenges of datadriven decisionmaking in this domain.",10.1111/jiec.13612,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
KLR-KGC: Knowledge-Guided LLM Reasoning for Knowledge Graph Completion,2024,"Shengwei Ji, Longfei Liu, Jizhong Xi, Xiaoxue Zhang, Xinlu Li","Knowledge graph completion (KGC) involves inferring missing entities or relationships within a knowledge graph, playing a crucial role across various domains, including intelligent question answering, recommendation systems, and dialogue systems. Traditional knowledge graph embedding (KGE) methods have proven effective in utilizing structured data and relationships. However, these methods often overlook the vast amounts of unstructured data and the complex reasoning capabilities required to handle ambiguous queries or rare entities. Recently, the rapid development of large language models (LLMs) has demonstrated exceptional potential in text comprehension and contextual reasoning, offering new prospects for KGC tasks. By using traditional KGE to capture the structural information of entities and relations to generate candidate entities and then reranking them with a generative LLM, the output of the LLM can be constrained to improve reliability. Despite this, new challenges, such as omissions and incorrect responses, arise during the ranking process. To address these issues, a knowledge-guided LLM reasoning for knowledge graph completion (KLR-KGC) framework is proposed. This model retrieves two types of knowledge from the knowledge graphanalogical knowledge and subgraph knowledgeto enhance the LLMs logical reasoning ability for specific tasks while injecting relevant additional knowledge. By integrating a chain-of-thought (CoT) prompting strategy, the model guides the LLM to filter and rerank candidate entities, constraining its output to reduce omissions and incorrect responses. The framework aims to learn and uncover the latent correspondences between entities, guiding the LLM to make reasonable inferences based on supplementary knowledge for more accurate predictions. The experimental results demonstrate that on the FB15k-237 dataset, KLR-KGC outperformed the entity generation model (CompGCN), achieving a 4.8% improvement in MRR and a 5.8% improvement in Hits@1.",10.3390/electronics13245037,['JournalArticle'],Computer Science
Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence,2025,"Yu Qiao, Huy Q. Le, Avi Deb Raha, Phuong-Nam Tran, Apurba Adhikary, Mengchun Zhang, Loc X. Nguyen, Eui-nam Huh, Dusit Niyato, Choong Seon Hong","The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.",10.48550/arXiv.2505.06907,"['JournalArticle', 'Review']",Computer Science
The Impostor is Among Us: Can Large Language Models Capture the Complexity of Human Personas?,2025,"Christopher Lazik, Christopher Katins, Charlotte Kauter, Jonas Jakob, Caroline Jay, Lars Grunske, Thomas Kosch","Large Language Models (LLMs) created new opportunities for generating personas, expected to streamline and accelerate the human-centered design process. Yet, AI-generated personas may not accurately represent actual user experiences, as they can miss contextual and emotional insights critical to understanding real users'needs and behaviors. This introduces a potential threat to quality, especially for novices. This paper examines the differences in how users perceive personas created by LLMs compared to those crafted by humans regarding their credibility for design. We gathered ten human-crafted personas developed by HCI experts according to relevant attributes established in related work. Then, we systematically generated ten personas with an LLM and compared them with human-crafted ones in a survey. The results showed that participants differentiated between human-created and AI-generated personas, with the latter perceived as more informative and consistent. However, participants noted that the AI-generated personas tended to follow stereotypes, highlighting the need for a greater emphasis on diversity when utilizing LLMs for persona creation.",10.48550/arXiv.2501.04543,"['JournalArticle', 'Review']",Computer Science
Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs,2025,"Adam Alami, Victor Vadmand Jensen, Neil A. Ernst","Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers sense of accountability for code quality. Since software engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study ( \(\textbf{interviews}\rightarrow\textbf{focus groups}\) ). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards, professional integrity, pride in code quality, and maintaining ones reputation. In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms.",10.1145/3721127,"['JournalArticle', 'Review']",Computer Science
Multilingual Prompting for Improving LLM Generation Diversity,2025,"Qihan Wang, Shidong Pan, Tal Linzen, Emily Black","Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and persona prompting. Further analyses show that the benefits of multilingual prompting vary between high and low resource languages and across model sizes, and that aligning the prompting language with cultural cues reduces hallucination about culturally-specific information.",10.48550/arXiv.2505.15229,['JournalArticle'],Computer Science; Linguistics
Artificial Intimacy: Exploring Normativity and Personalization Through Fine-tuning LLM Chatbots,2025,"Mirabelle Jones, Nastasia Griffioen, Christina Neumayer, Irina Shklovski","Fine-tuning Large Language Models (LLMs) is one response to the critique of LLMs being biased, erasing diversity, and raising ethical concerns. The Artificial Intimacy project employs artistic methods, taking personalization of chatbots to an extreme by fine-tuning LLMs on individual social media data. We find that regular GPT-3 chatbots attempt to circumvent value-laden content through flagging prompts and producing generic non-answers with variable success. While the transactional nature of such output allowed participants to make sense of responses with less personification, fine-tuned models presented value-laden, normative, and familiar personalities, resulting in strong personification as a way of making sense of the interactions. This mimicry of emotional connection resulted in a sense of artificial intimacy creating expectations for reciprocity and consideration that the models cannot express by design. As the commercialization of interactions with chatbots continues, we discuss the ethics of such emotional manipulation and its implications for personalization of LLMs.",10.1145/3706598.3713728,"['JournalArticle', 'Book', 'Conference']",Art; Computer Science
To What Extent Are Large Language Models Capable of Generating Substantial Reflections for Motivational Interviewing Counseling Chatbots? A Human Evaluation,2024,"Erkan Basar, Iris Hendrickx, E. Krahmer, G. Bruijn, Tibor Bosse","Motivational Interviewing is a counselling style that requires skillful usage of reflective listening and engaging in conversations about sensitive and personal subjects. In this paper, we investigate to what extent we can use generative large language models in motivational interviewing chatbots to generate precise and variable reflections on user responses. We conduct a two-step human evaluation where we first independently assess the generated reflections based on four criteria essential to health counseling; appropriateness, specificity, naturalness, and engagement. In the second step, we compare the overall quality of generated and human-authored reflections via a ranking evaluation. We use GPT-4, BLOOM, and FLAN-T5 models to generate motivational interviewing reflections, based on real conversational data collected via chatbots designed to provide support for smoking cessation and sexual health. We discover that GPT-4 can produce reflections of a quality comparable to human-authored reflections. Finally, we conclude that large language models have the potential to enhance and expand reflections in predetermined health counseling chatbots, but a comprehensive manual review is advised.",10.18653/v1/2024.hucllm-1.4,['Review'],Computer Science; Psychology
Shrinking the Generation-Verification Gap with Weak Verifiers,2025,"Jon Saad-Falcon, E. K. Buchanan, Mayee F. Chen, Tzu-Heng Huang, Brendan McLaughlin, Tanvir Bhathal, Shang Zhu, Ben Athiwaratkun, Frederic Sala, Scott W. Linderman, Azalia Mirhoseini, Christopher R","Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and reward models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training. To reduce computational costs of verifier ensembles, we train a 400M cross-encoder using Weaver's combined output scores.",10.48550/arXiv.2506.18203,['JournalArticle'],Computer Science
"Regress, Don't Guess - A Regression-like Loss on Number Tokens for Language Models",2024,"Jonas Zausinger, Lars Pennig, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, M. M. Danziger, Jannis Born","While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the cross-entropy (CE) loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the $L_p$ norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the CE objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope to inspire LLM developers to improve their pretraining objectives and distribute NTL as a minimalistic and lightweight PyPI package $ntloss$: https://github.com/ai4sd/number-token-loss. Development code for full paper reproduction is available separately.",10.48550/arXiv.2411.02083,['JournalArticle'],Computer Science; Mathematics
Opinerium: Subjective Question Generation Using Large Language Models,2024,"Pedram Babakhani, Andreas Lommatzsch, Torben Brodt, Doreen Sacker, F. Sivrikaya, S. Albayrak","This paper presents a comprehensive study on generating subjective inquiries for news media posts to empower public engagement with trending media topics. While previous studies primarily focused on factual and objective questions with explicit or implicit answers in the text, this research concentrates on automatically generating subjective questions to directly elicit personal preference from individuals based on a given text. The research methodology involves the application of fine-tuning techniques across multiple iterations of flan-T5 and GPT3 architectures for the task of Seq2Seq generation. This approach is meticulously evaluated using a custom dataset comprising 40,000 news articles along with human-generated questions. Furthermore, a comparative analysis is conducted using zero-shot prompting via GPT-3.5, juxtaposing the performance of fine-tuned models against a significantly larger language model. The study grapples with the inherent challenges tied to evaluating opinion-based question generation due to its subjective nature and the inherent uncertainty in determining answers. A thorough investigation and comparison of two transformer architectures are undertaken utilizing conventional lexical overlap metrics such as BLEU, ROUGE, and METEOR, alongside semantic similarity metrics encompassing BERTScore, BLEURT, and answerability scores such as QAScore, and RQUGE. The findings underscore the marked superiority of the flan-T5 model over GPT3, substantiated not only by quantitative metrics but also through human evaluations. The paper introduces Opinerium based on the open-source flan-T5-Large model, identified as the pacesetter in generating subjective questions. Additionally, we assessed all aforementioned metrics thoroughly by investigating the pairwise Spearman correlation analysis to identify robust metrics.",10.1109/ACCESS.2024.3398553,['JournalArticle'],Computer Science
Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step,2024,"Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong","Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.",10.48550/arXiv.2406.16144,['JournalArticle'],Computer Science
The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training,2025,"Weize Chen, Jiarui Yuan, Tailin Jin, Ning Ding, Huimin Chen, Zhiyuan Liu, Maosong Sun","Recent large language models (LLMs) exhibit impressive reasoning but often over-think, generating excessively long responses that hinder efficiency. We introduce DIET ( DIfficulty-AwarE Training), a framework that systematically cuts these""token calories""by integrating on-the-fly problem difficulty into the reinforcement learning (RL) process. DIET dynamically adapts token compression strategies by modulating token penalty strength and conditioning target lengths on estimated task difficulty, to optimize the performance-efficiency trade-off. We also theoretically analyze the pitfalls of naive reward weighting in group-normalized RL algorithms like GRPO, and propose Advantage Weighting technique, which enables stable and effective implementation of these difficulty-aware objectives. Experimental results demonstrate that DIET significantly reduces token counts while simultaneously improving reasoning performance. Beyond raw token reduction, we show two crucial benefits largely overlooked by prior work: (1) DIET leads to superior inference scaling. By maintaining high per-sample quality with fewer tokens, it enables better scaling performance via majority voting with more samples under fixed computational budgets, an area where other methods falter. (2) DIET enhances the natural positive correlation between response length and problem difficulty, ensuring verbosity is appropriately allocated, unlike many existing compression methods that disrupt this relationship. Our analyses provide a principled and effective framework for developing more efficient, practical, and high-performing LLMs.",10.48550/arXiv.2505.19217,['JournalArticle'],Computer Science
Interpreting Language Reward Models via Contrastive Explanations,2024,"Junqi Jiang, Tom Bewley, Saumitra Mishra, Freddy Lcu, Manuela Veloso","Reward models (RMs) are a crucial component in the alignment of large language models' (LLMs) outputs with human values. RMs approximate human preferences over possible LLM responses to the same prompt by predicting and comparing reward scores. However, as they are typically modified versions of LLMs with scalar output heads, RMs are large black boxes whose predictions are not explainable. More transparent RMs would enable improved trust in the alignment of LLMs. In this work, we propose to use contrastive explanations to explain any binary response comparison made by an RM. Specifically, we generate a diverse set of new comparisons similar to the original one to characterise the RM's local behaviour. The perturbed responses forming the new comparisons are generated to explicitly modify manually specified high-level evaluation attributes, on which analyses of RM behaviour are grounded. In quantitative experiments, we validate the effectiveness of our method for finding high-quality contrastive explanations. We then showcase the qualitative usefulness of our method for investigating global sensitivity of RMs to each evaluation attribute, and demonstrate how representative examples can be automatically extracted to explain and compare behaviours of different RMs. We see our method as a flexible framework for RM explanation, providing a basis for more interpretable and trustworthy LLM alignment.",10.48550/arXiv.2411.16502,['JournalArticle'],Computer Science; Linguistics
Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics,2024,Tetiana Bas,"This study investigates gender bias in large language models (LLMs) by comparing their gender perception to that of human respondents, U.S. Bureau of Labor Statistics data, and a 50% no-bias benchmark. We created a new evaluation set using occupational data and role-specific sentences. Unlike common benchmarks included in LLM training data, our set is newly developed, preventing data leakage and test set contamination. Five LLMs were tested to predict the gender for each role using single-word answers. We used Kullback-Leibler (KL) divergence to compare model outputs with human perceptions, statistical data, and the 50% neutrality benchmark. All LLMs showed significant deviation from gender neutrality and aligned more with statistical data, still reflecting inherent biases.",10.48550/arXiv.2411.13738,['JournalArticle'],Computer Science; Linguistics
Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning,2025,"Rujing Yao, Yang Wu, Chenghao Wang, Jingwei Xiong, Fang Wang, Xiaozhong Liu","Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods.",10.48550/arXiv.2502.07912,"['JournalArticle', 'Conference']",Computer Science; Law
Emotion Expression Estimates to Measure and Improve Multimodal Social-Affective Interactions,2023,"Jeffrey A. Brooks, V. Tiruvadi, Alice Baird, Panagiotis Tzirakis, Haoqi Li, Chris Gagne, Moses Oh, Alan S. Cowen","Large language models (LLMs) are being adopted in a wide range of applications, but an understanding of other social-affective signals is needed to support effective human-computer-interaction (HCI) in multimodal interfaces. In particular, robust, accurate measurements of human emotional expression can be used to tailor responses to human values and preferences. In this paper, we present two models available from an API-based suite of emotional expression models that measure nuanced facial and vocal signals, providing rich, high-dimensional emotional expression estimates (EEEs). We demonstrate the ability of EEEs to provide insight into two established datasets and present methods for integrating EEEs into large language model (LLM) applications. We discuss how this approach is a step towards more reliable tools for clinical screening and scientific study, as well as empathic digital assistants that can be used in therapeutic settings.",10.1145/3610661.3616129,"['JournalArticle', 'Book']",Computer Science; Psychology
UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty and Response Time for Multiple-Choice Questions,2024,"Ana-Cristina Rogoz, R. Ionescu","This work explores a novel data augmentation method based on Large Language Models (LLMs) for predicting item difficulty and response time of retired USMLE Multiple-Choice Questions (MCQs) in the BEA 2024 Shared Task. Our approach is based on augmenting the dataset with answers from zero-shot LLMs (Falcon, Meditron, Mistral) and employing transformer-based models based on six alternative feature combinations. The results suggest that predicting the difficulty of questions is more challenging. Notably, our top performing methods consistently include the question text, and benefit from the variability of LLM answers, highlighting the potential of LLMs for improving automated assessment in medical licensing exams. We make our code available at: https://github.com/ana-rogoz/BEA-2024.",10.48550/arXiv.2404.13343,['JournalArticle'],Computer Science
Collecting Qualitative Data at Scale with Large Language Models: A Case Study,2023,"A. Villalba, Eva Maxfield Brown, Jennifer V. Scurrell, Jason Entenmann, Madeleine I. G. Daepp","Chatbots have shown promise as tools to scale qualitative data collection. Recent advances in Large Language Models (LLMs) could accelerate this process by allowing researchers to easily deploy sophisticated interviewing chatbots. We test this assumption by conducting a large-scale user study (n=399) evaluating 3 different chatbots, two of which are LLM-based and a baseline which employs hard-coded questions. We evaluate the results with respect to participant engagement and experience, established metrics of chatbot quality grounded in theories of effective communication, and a novel scale evaluating ''richness'' or the extent to which responses capture the complexity and specificity of the social context under study. We find that, while the chatbots were able to elicit high-quality responses based on established evaluation metrics, the responses rarely capture participants' specific motives or personalized examples, and thus perform poorly with respect to richness. We further find low inter-rater reliability between LLMs and humans in the assessment of both quality and richness metrics. Our study offers a cautionary tale for scaling and evaluating qualitative research with LLMs.",10.1145/3710947,['JournalArticle'],Computer Science; Linguistics
AI-POWERED RECOMMENDATION SYSTEMS FOR TRIP PLANNING,2023,"Tejaswini C.J, S. Manju Bargavi","Trip planning can be difficult due to the overwhelming amount of information available and the time-consuming process of finding the best deals on flights, accommodation, and activities, making it hard for individuals to make informed decisions and plan an itinerary efficiently. This survey paper's goal is to explore how artificial intelligence is used in web applications for trip planning. The paper will conduct a comprehensive review of existing approaches and technologies for trip planning and will evaluate the use of natural language processing and the GPT-3 language model in generating personalized itineraries. The survey will also examine the integration of flight and accommodation booking APIs in trip planning web applications and the impact on user satisfaction and efficiency. The results of this survey will provide valuable insights into the current state of the field and will help to inform the development of future trip planning web applications that use artificial intelligence to provide a more efficient and enjoyable trip planning experience for users.",10.21474/ijar01/16480,"['JournalArticle', 'Review']",Computer Science
Integrative Decoding: Improve Factuality via Implicit Self-consistency,2024,"Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong","Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict constraints on the task format, largely limiting their applicability. In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective. Extensive evaluation shows that ID consistently enhances factuality over a wide range of language models, with substantial improvements on the TruthfulQA (+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance gains amplify progressively as the number of sampled responses increases, indicating the potential of ID to scale up with repeated sampling.",10.48550/arXiv.2410.01556,['JournalArticle'],Computer Science
A Generative Artificial-Intelligence-Based Workbench to Test New Methodologies in Organisational Health and Safety,2024,"Andrea Falegnami, Andrea Tomassi, Giuseppe Corbelli, Francesco Nucci, Elpidio Romano","This paper introduces a novel generative artificial intelligence workbench specifically tailored to the field of safety sciences. Utilizing large language models (LLMs), this innovative approach significantly diverges from traditional methods by enabling the rapid development, refinement, and preliminary testing of new safety methodologies. Traditional techniques in this field typically depend on slow, iterative cycles of empirical data collection and analysis, which can be both time-intensive and costly. In contrast, our LLM-based workbench leverages synthetic data generation and advanced prompt engineering to simulate complex safety scenarios and generate diverse, realistic data sets on demand. This capability allows for more flexible and accelerated experimentation, enhancing the efficiency and scalability of safety science research. By detailing an application case, we demonstrate the practical implementation and advantages of our framework, such as its ability to adapt quickly to evolving safety requirements and its potential to significantly cut down development time and resources. The introduction of this workbench represents a paradigm shift in safety methodology development, offering a potent tool that combines the theoretical rigor of traditional methods with the agility of modern AI technologies.",10.3390/app142411586,['JournalArticle'],Computer Science; Engineering; Environmental Science
Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models,2024,"Xiaolong Wang, Yile Wang, Yuan Zhang, Fuwen Luo, Peng Li, Maosong Sun, Yang Liu","Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.",10.48550/arXiv.2402.17226,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning,2025,"Michael Xieyang Liu, S. Petridis, Vivian Tsai, Alexander J. Fiannaca, Alex Olwal, Michael Terry, Carrie J. Cai","Multimodal large language models (MLLMs), with their expansive world knowledge and reasoning capabilities, present a unique opportunity for end-users to create personalized AI sensors capable of reasoning about complex situations. A user could describe a desired sensing task in natural language (e.g., let me know if my toddler is getting into mischief in the living room), with the MLLM analyzing the camera feed and responding within just seconds. In a formative study, we found that users saw substantial value in defining their own sensors, yet struggled to articulate their unique personal requirements to the model and debug the sensors through prompting alone. To address these challenges, we developed Gensors, a system that empowers users to define customized sensors supported by the reasoning capabilities of MLLMs. Gensors 1) assists users in eliciting requirements through both automatically-generated and manually created sensor criteria, 2) facilitates debugging by allowing users to isolate and test individual criteria in parallel, 3) suggests additional criteria based on user-provided images, and 4) proposes test cases to help users stress test sensors on potentially unforeseen scenarios. In a 12-participant user study, users reported significantly greater sense of control, understanding, and ease of communication when defining sensors using Gensors. Beyond addressing model limitations, Gensors supported users in debugging, eliciting requirements, and expressing unique personal requirements to the sensor through criteria-based reasoning; it also helped uncover users own blind spots by exposing overlooked criteria and revealing unanticipated failure modes. Finally, we describe insights into how unique characteristics of MLLMssuch as hallucinations and inconsistent responsescan impact the sensor-creation process. Together, these findings contribute to the design of future MLLM-powered sensing systems that are intuitive and customizable by everyday users.",10.1145/3708359.3712085,"['JournalArticle', 'Book', 'Conference']",Computer Science
"Team Bias Busters at WASSA 2023 Empathy, Emotion and Personality Shared Task: Emotion Detection with Generative Pretrained Transformers",2023,"Andrew Nedilko, Yi Chu","This paper describes the approach that we used to take part in the multi-label multi-class emotion classification as Track 3 of the WASSA 2023 Empathy, Emotion and Personality Shared Task at ACL 2023. The overall goal of this track is to build models that can predict 8 classes (7 emotions + neutral) based on short English essays written in response to news article that talked about events perceived as harmful to people. We used OpenAI generative pretrained transformers with full-scale APIs for the emotion prediction task by fine-tuning a GPT-3 model and doing prompt engineering for zero-shot / few-shot learning with ChatGPT and GPT-4 models based on multiple experiments on the dev set. The most efficient method was fine-tuning a GPT-3 model which allowed us to beat our baseline character-based XGBoost Classifier and rank 2nd among all other participants by achieving a macro F1 score of 0.65 and a micro F1 score of 0.7 on the final blind test set.",10.18653/v1/2023.wassa-1.53,['JournalArticle'],Computer Science; Psychology
"Is There No Such Thing as a Bad Question? H4R: HalluciBot for Ratiocination, Rewriting, Ranking, and Routing",2024,"William Watson, Nicole Cho, Nishan Srishankar","Hallucination continues to be one of the most critical challenges in the institutional adoption journey of Large Language Models (LLMs). While prior studies have primarily focused on the post-generation analysis and refinement of outputs, this paper centers on the effectiveness of queries in eliciting accurate responses from LLMs. We present HalluciBot, a model that estimates the query's propensity to hallucinate before generation, without invoking any LLMs during inference. HalluciBot can serve as a proxy reward model for query rewriting, offering a general framework to estimate query quality based on accuracy and consensus. In essence, HalluciBot investigates how poorly constructed queries can lead to erroneous outputs - moreover, by employing query rewriting guided by HalluciBot's empirical estimates, we demonstrate that 95.7% output accuracy can be achieved for Multiple Choice questions. The training procedure for HalluciBot consists of perturbing 369,837 queries n times, employing n+1 independent LLM agents, sampling an output from each query, conducting a Multi-Agent Monte Carlo simulation on the sampled outputs, and training an encoder classifier. The idea of perturbation is the outcome of our ablation studies that measures the increase in output diversity (+12.5 agreement spread) by perturbing a query in lexically different but semantically similar ways. Therefore, HalluciBot paves the way to ratiocinate (76.0% test F1 score, 46.6% in saved computation on hallucinatory queries), rewrite (+30.2% positive class transition from hallucinatory to non-hallucinatory), rank (+50.6% positive class transition from hallucinatory to non-hallucinatory), and route queries to effective pipelines.",10.1609/aaai.v39i24.34736,"['JournalArticle', 'Conference']",Computer Science
From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows,2025,"M. Ferrag, Norbert Tihanyi, Djallel Hamouda, Leandros A. Maglaras, Mrouane Debbah","Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces have dramatically expanded capabilities for real-time data retrieval, complex computation, and multi-step orchestration. Yet, the explosive proliferation of plugins, connectors, and inter-agent protocols has outpaced discovery mechanisms and security practices, resulting in brittle integrations vulnerable to diverse threats. In this survey, we introduce the first unified, end-to-end threat model for LLM-agent ecosystems, spanning host-to-tool and agent-to-agent communications, formalize adversary capabilities and attacker objectives, and catalog over thirty attack techniques. Specifically, we organized the threat model into four domains: Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal adversarial inputs), Model Compromise (e.g., prompt- and parameter-level backdoors, composite and encrypted multi-backdoors, poisoning strategies), System and Privacy Attacks (e.g., speculative side-channels, membership inference, retrieval poisoning, social-engineering simulations), and Protocol Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent (A2A) protocol). For each category, we review representative scenarios, assess real-world feasibility, and evaluate existing defenses. Building on our threat taxonomy, we identify key open challenges and future research directions, such as securing MCP deployments through dynamic trust management and cryptographic provenance tracking; designing and hardening Agentic Web Interfaces; and achieving resilience in multi-agent and federated environments. Our work provides a comprehensive reference to guide the design of robust defense mechanisms and establish best practices for resilient LLM-agent workflows.",10.48550/arXiv.2506.23260,"['JournalArticle', 'Review']",Computer Science
CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management,2024,"Sinan Abdulhak, Wayne Hubbard, Karthik Gopalakrishnan, Max Z. Li","Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.",10.48550/arXiv.2402.14850,['JournalArticle'],Computer Science; Engineering
Deploying Foundation Model Powered Agent Services: A Survey,2024,"Wenchao Xu, Jinyu Chen, Peirong Zheng, Xiaoquan Yi, Tianyi Tian, Wenhui Zhu, Quan Wan, Haozhao Wang, Yunfeng Fan, Qinliang Su, Xuemin Shen","Foundation model (FM) powered agent services are regarded as a promising solution to develop intelligent and personalized applications for advancing toward Artificial General Intelligence (AGI). To achieve high reliability and scalability in deploying these agent services, it is essential to collaboratively optimize computational and communication resources, thereby ensuring effective resource allocation and seamless service delivery. In pursuit of this vision, this paper proposes a unified framework aimed at providing a comprehensive survey on deploying FM-based agent services across heterogeneous devices, with the emphasis on the integration of model and resource optimization to establish a robust infrastructure for these services. Particularly, this paper begins with exploring various low-level optimization strategies during inference and studies approaches that enhance system scalability, such as parallelism techniques and resource scaling methods. The paper then discusses several prominent FMs and investigates research efforts focused on inference acceleration, including techniques such as model compression and token reduction. Moreover, the paper also investigates critical components for constructing agent services and highlights notable intelligent applications. Finally, the paper presents potential research directions for developing real-time agent services with high Quality of Service (QoS).",10.48550/arXiv.2412.13437,"['JournalArticle', 'Review']",Computer Science; Engineering
Improving LLM Personas via Rationalization with Psychological Scaffolds,2025,"Brihi Joshi, Xiang Ren, Swabha Swayamdipta, Rik Koncel-Kedziorski, Tim Paek",,10.48550/arXiv.2504.17993,['JournalArticle'],Computer Science; Psychology
Trajectory Prediction Meets Large Language Models: A Survey,2025,"Yi Xu, Ruining Yang, Yitian Zhang, Yizhou Wang, Jianglin Lu, Mingyuan Zhang, Lili Su, Yun Fu","Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.",10.48550/arXiv.2506.03408,"['JournalArticle', 'Review']",Computer Science; Engineering
Text Generation Indistinguishable from Target Person by Prompting Few Examples Using LLM,2024,"Yuka Tsubota, Yoshinobu Kano","To achieve smooth and natural communication between a dialogue system and a human, it is necessary for the dialogue system to behave more human-like. Recreating the personality of an actual person can be an effective way for this purpose. This study proposes a method to recreate a personality by a large language model (generative AI) without training, but with prompt technique to make the creation cost as low as possible. Collecting a large amount of dialogue data from a specific person is not easy and requires a significant amount of time for training. Therefore, we aim to recreate the personality of a specific individual without using dialogue data. The personality referred to in this paper denotes the image of a person that can be determined solely from the input and output of text dialogues. As a result of the experiments, it was revealed that by using prompts combining profile information, responses to few questions, and extracted speaking characteristics from those responses, it is possible to improve the reproducibility of a specific individuals personality.",10.18653/v1/2024.aiwolfdial-1.2,nan,Computer Science
The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs,2025,"Zizhou Liu, Ziwei Gong, Lin Ai, Zheng Hui, Run Chen, Colin Wayne Leach, Michelle R. Greene, Julia Hirschberg","Psychological insights have long shaped pivotal NLP breakthroughs, including the cognitive underpinnings of attention mechanisms, formative reinforcement learning, and Theory of Mind-inspired social modeling. As Large Language Models (LLMs) continue to grow in scale and complexity, there is a rising consensus that psychology is essential for capturing human-like cognition, behavior, and interaction. This paper reviews how psychological theories can inform and enhance stages of LLM development, including data, pre-training, post-training, and evaluation\&application. Our survey integrates insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our analysis highlights current trends and gaps in how psychological theories are applied. By examining both cross-domain connections and points of tension, we aim to bridge disciplinary divides and promote more thoughtful integration of psychology into future NLP research.",10.48550/arXiv.2505.00003,"['JournalArticle', 'Review']",Computer Science; Psychology
Applying General Turn-Taking Models to Conversational Human-Robot Interaction,2025,"Gabriel Skantze, Bahar Irfan","Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.",10.1109/HRI61500.2025.10973958,"['JournalArticle', 'Conference']",Computer Science
Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing,2025,"Thanh Le-Cong, Thanh-Dat Nguyen, Bach Le, Toby Murray","Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20%-35% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact. In this paper, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations, and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are considered natural, while 20% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing. Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using Large Language Models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.",10.1145/3716167,"['JournalArticle', 'Review']",Computer Science
Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback,2025,"Dongwei Jiang, Alvin Zhang, Andrew Wang, Nicholas Andrews, Daniel Khashabi","Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and reach correct solutions. In this paper, we systematically investigate LLMs'ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 with extended thinking. Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term Feedback Friction. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We analyze Feedback Friction and find that models'confidence on specific questions, measured by semantic entropy, predicts feedback resistance: high-confidence predictions remain resistant to external correction. We hope that highlighting this issue in LLMs will help future research in self-improvement.",10.48550/arXiv.2506.11930,['JournalArticle'],Computer Science
NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap via Informational Interviews,2024,"Michael Lu, Hyundong Justin Cho, Weiyan Shi, Jonathan May, Alexander Spangher","Large Language Models (LLMs) have demonstrated impressive capabilities in generating coherent text but often struggle with grounding language and strategic dialogue. To address this gap, we focus on journalistic interviews, a domain rich in grounding communication and abundant in data. We curate a dataset of 40,000 two-person informational interviews from NPR and CNN, and reveal that LLMs are significantly less likely than human interviewers to use acknowledgements and to pivot to higher-level questions. Realizing that a fundamental deficit exists in multi-turn planning and strategic thinking, we develop a realistic simulated environment, incorporating source personas and persuasive elements, in order to facilitate the development of agents with longer-horizon rewards. Our experiments show that while source LLMs mimic human behavior in information sharing, interviewer LLMs struggle with recognizing when questions are answered and engaging persuasively, leading to suboptimal information extraction across model size and capability. These findings underscore the need for enhancing LLMs' strategic dialogue capabilities.",10.48550/arXiv.2411.13779,['JournalArticle'],Computer Science; Linguistics
Evaluation of Task Specific Productivity Improvements Using a Generative Artificial Intelligence Personal Assistant Tool,2024,"Brian S. Freeman, Kendall Arriola, Dan Cottell, Emmett Lawlor, Matt Erdman, Trevor Sutherland, Brian Wells","This study evaluates the productivity improvements achieved using a generative artificial intelligence personal assistant tool (PAT) developed by Trane Technologies. The PAT, based on OpenAIs GPT 3.5 model, was deployed on Microsoft Azure to ensure secure access and protection of intellectual property. To assess the tools productivity effectiveness, an experiment was conducted comparing the completion times and content quality of four common office tasks: writing an email, summarizing an article, creating instructions for a simple task, and preparing a presentation outline. Sixty-three (63) participants were randomly divided into a test group using the PAT and a control group performing the tasks manually. Results indicated significant productivity enhancements, particularly for tasks involving summarization and instruction creation, with improvements ranging from 3.3% to 69%. The study further analyzed factors such as the age of users, response word counts, and quality of responses, revealing that the PAT users generated more verbose and higher-quality content. Writing email content improved by 3.3%, summarizing text improved by 69%, creating instructions improved by 45.9%, and preparing an outline improved by 24.8%. An LLM-as-a-judge method employing GPT-4 was used to grade the quality of responses, which effectively distinguished between high and low-quality outputs. The findings underscore the potential of PATs in enhancing workplace productivity and highlight areas for further research and optimization.",10.48550/arXiv.2409.14511,['JournalArticle'],Computer Science
RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models,2024,"Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue","The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in humans daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., users name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/.",10.1109/CVPR52734.2025.01355,"['JournalArticle', 'Conference']",Computer Science
CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs,2024,"Shuang Ao, Stefan Rueger, Advaith Siddharthan","Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge. The recent literature on uncertainty quantification of natural language generation (NLG) utilises a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses. These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty. However, logits represent the probability of the predicted class and barely contain feature information for potential clustering. Alternatively, CLIP (Contrastive Language-Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity. To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs. We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs. We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. The code are available at \url{https://github.com/AoShuang92/css_uq_llms}.",10.48550/arXiv.2406.03158,['JournalArticle'],Computer Science; Linguistics
MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks,2025,"Sanjoy Chowdhury, Mohamed Elmoghany, Yohan Abeysinghe, Junjie Fei, Sayan Nag, Salman Khan, Mohamed Elhoseiny, Dinesh Manocha","Large multimodal models (LMMs) have shown remarkable progress in audio-visual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audio-visual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance. Project: https://schowdhury671.github.io/magnet_project/",10.48550/arXiv.2506.07016,['JournalArticle'],Computer Science
"A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks",2025,Hieu Minh Jord Nguyen,"Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced LLM ToM capabilities, and suggest several research directions for effective evaluation and mitigation of these risks.",10.48550/arXiv.2502.06470,"['JournalArticle', 'Review']",Computer Science; Psychology
Combining Generative and Discriminative AI for High-Stakes Interview Practice,2024,"Chee Wee Leong, Navaneeth Jawahar, Vinay Basheerabad, T. Wrtwein, Andrew Emerson, Guy Sivan","We present a demo comprising an end-to-end AI pipeline for practicing video interviews for a high-stakes scenarios (i.e., college admissions) with personalized, actionable feedback for continuous improvement of the user. This system provides personalized, actionable feedback for continuous user improvement. Utilizing large language models (LLMs), we generate questions and responses for a virtual avatar interviewer. Our focus on key qualitiessuch as concise responses with low latency, empathy, and smooth topic navigationled to a comparative evaluation of several prominent LLMs, each undergoing evolutionary development. We also discuss the integration of avatar technology to create an immersive, virtual environment for naturalistic dyadic conversations.",10.1145/3686215.3688377,"['Book', 'JournalArticle']",Computer Science
The Impact of ChatGPT on Social Media,2024,"Abdulazeez Alsajri, Mohammad Aljanabi","This paper examines the transformative effects of integrating ChatGPT, a large language model, into popular social media platforms like Reddit, Facebook, and Instagram. By enhancing user interactions with machine-generated dialogue that mirrors human conversation, ChatGPT redefines the dynamics of online engagement. The study explores how ChatGPT fosters personalized and frequent interactions, leading to deeper user engagement and community growth. It highlights the model's role in automated content creation, enabling tailored responses and accelerated content curation to meet user-specific interests. However, ethical considerations, such as privacy and authenticity, remain critical in the deployment of AI-driven communication on social platforms. Ultimately, this research provides insights into the evolving social media landscape as AI-powered models increasingly influence user experience, platform dynamics, and societal norms.",10.70470/medaad/2024/002,['JournalArticle'],Computer Science; Sociology
Improving Ontology Requirements Engineering with OntoChat and Participatory Prompting,2024,"Yihang Zhao, Bohui Zhang, Xi Hu, Shuyin Ouyang, Jongmo Kim, Nitisha Jain, Jacopo de Berardinis, Albert Meroo-Peuela, E. Simperl","Past ontology requirements engineering (ORE) has primarily relied on manual methods, such as interviews and collaborative forums, to gather user requirements from domain experts, especially in large projects. Current OntoChat offers a framework for ORE that utilises large language models (LLMs) to streamline the process through four key functions: user story creation, competency question (CQ) extraction, CQ filtration and analysis, and ontology testing support. In OntoChat, users are expected to prompt the chatbot to generate user stories. However, preliminary evaluations revealed that they struggle to do this effectively. To address this issue, we experimented with a research method called participatory prompting, which involves researcher-mediated interactions to help users without deep knowledge of LLMs use the chatbot more effectively. The participatory prompting user study produces pre-defined prompt templates based on user query, focusing on creating and refining personas, goals, scenarios, sample data, and data resources for user story. These refined user stories will subsequently be converted into CQs.",10.1609/aaaiss.v4i1.31799,['JournalArticle'],Computer Science
Conformity in Large Language Models,2024,"Xiaochen Zhu, Caiqi Zhang, Tom Stafford, Nigel Collier, Andreas Vlachos","The conformity effect describes the tendency of individuals to align their responses with the majority. Studying this bias in large language models (LLMs) is crucial, as LLMs are increasingly used in various information-seeking and decision-making tasks as conversation partners to improve productivity. Thus, conformity to incorrect responses can compromise their effectiveness. In this paper, we adapt psychological experiments to examine the extent of conformity in popular LLMs. Our findings reveal that all tested models exhibit varying levels of conformity toward the majority, regardless of their initial choice or correctness, across different knowledge domains. Notably, we are the first to show that LLMs are more likely to conform when they are more uncertain in their own prediction. We further explore factors that influence conformity, such as training paradigms and input characteristics, finding that instruction-tuned models are less susceptible to conformity, while increasing the naturalness of majority tones amplifies conformity. Finally, we propose two interventions, Devil's Advocate and Question Distillation, to mitigate conformity, providing insights into building more robust language models.",10.48550/arXiv.2410.12428,"['JournalArticle', 'Conference']",Computer Science; Linguistics; Psychology
Reimagining Personal Data: Unlocking the Potential of AI-Generated Images in Personal Data Meaning-Making,2025,"Soobin Park, Hankyung Kim, Youn-kyung Lim","Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-based application as a probe that represents personal data through generative images utilizing Open AIs GPT-4 model and DALL-E 3. We then conducted a 21-day diary study and interviews using the probe with 16 participants to investigate users in-depth experiences with images generated by AI in everyday lives. Our findings reveal new qualities of experiences in users engagement with data, highlighting how participants constructed personal meaning from their data through imagination and speculation on AI-generated images. We conclude by discussing the potential and concerns of leveraging image-generative AI for personal data meaning-making.",10.1145/3706598.3713722,"['JournalArticle', 'Book', 'Conference']",Computer Science
LARGE LANGUAGE MODELS FOR PREDICTING EMPATHIC ACCURACY BETWEEN A DESIGNER AND A USER,2024,"Oluwatoba Fabunmi, Saman K. Halgamuge, Daniel Beck, Katja Holtta-Otto","
 Empathic design research aims to gain deep and accurate user understanding. We can measure the designer's empathic ability as empathic accuracy (EA) in understanding the user's thoughts and feelings during an interview. However, the EA measure currently relies on human rating and is thus time-consuming, making the use of large language models (LLMs) an attractive alternative. It is essential to consider two significant constraints when implementing LLMs as a solution: the choice of LLM and the impact of domain-specific datasets. Datasets of the interactions between the designer and the user are not generally available. We present such a dataset consisting of the EA task employed in user interviews to measure empathic understanding. It consists of over 400 pairs of user thoughts or feelings matched with a designer's guess of the same and the human ratings of the accuracy. We compared the performance of six sentence embedding state-of-the-art (SOTA) LLMs with different pooling techniques on the EA task. We used the LLMs to extract semantic information before and after fine-tuning. We conclude that directly using LLMs based on their reported performance in general language tasks could result in errors when judging a designer's empathic ability. We also found that fine-tuning LLMs on our dataset improved their performance, but the model's ability to fit the EA task and pooling method also determined the LLM's performance. The results will provide insight for other LLM-based similarity analyses in design.",10.1115/1.4067227,['JournalArticle'],Computer Science; Psychology
GraphXAIN: Narratives to Explain Graph Neural Networks,2024,"Mateusz Cedro, David Martens","Graph Neural Networks (GNNs) are a powerful technique for machine learning on graph-structured data, yet they pose challenges in interpretability. Existing GNN explanation methods usually yield technical outputs, such as subgraphs and feature importance scores, that are difficult for non-data scientists to understand and thereby violate the purpose of explanations. Motivated by recent Explainable AI (XAI) research, we propose GraphXAIN, a method that generates natural language narratives explaining GNN predictions. GraphXAIN is a model- and explainer-agnostic method that uses Large Language Models (LLMs) to translate explanatory subgraphs and feature importance scores into coherent, story-like explanations of GNN decision-making processes. Evaluations on real-world datasets demonstrate GraphXAIN's ability to improve graph explanations. A survey of machine learning researchers and practitioners reveals that GraphXAIN enhances four explainability dimensions: understandability, satisfaction, convincingness, and suitability for communicating model predictions. When combined with another graph explainer method, GraphXAIN further improves trustworthiness, insightfulness, confidence, and usability. Notably, 95% of participants found GraphXAIN to be a valuable addition to the GNN explanation method. By incorporating natural language narratives, our approach serves both graph practitioners and non-expert users by providing clearer and more effective explanations.",10.48550/arXiv.2411.02540,"['JournalArticle', 'Review']",Computer Science
Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach,2024,"Xuehao Zhai, Hanlin Tian, Lintong Li, Tianyu Zhao","Travel choice analysis is crucial for understanding individual travel behavior to develop appropriate transport policies and recommendation systems in Intelligent Transportation Systems (ITS). Despite extensive research, this domain faces two critical challenges: a) modeling with limited survey data, and b) simultaneously achieving high model explainability and accuracy. In this paper, we introduce a novel prompt-learning-based Large Language Model(LLM) framework that significantly improves prediction accuracy and provides explicit explanations for individual predictions. This framework involves three main steps: transforming input variables into textual form; building of demonstrations similar to the object, and applying these to a well-trained LLM. We tested the framework's efficacy using two widely used choice datasets: London Passenger Mode Choice (LPMC) and Optima-Mode collected in Switzerland. The results indicate that the LLM significantly outperforms state-of-the-art deep learning methods and discrete choice models in predicting people's choices. Additionally, we present a case of explanation illustrating how the LLM framework generates understandable and explicit explanations at the individual level.",10.48550/arXiv.2406.13558,"['JournalArticle', 'Review']",Computer Science; Engineering
Giving AI Personalities Leads to More Human-Like Reasoning,2025,"Animesh Nighojkar, Bekhzodbek Moydinboyev, My Duong, John Licato","In computational cognitive modeling, capturing the full spectrum of human judgment and decision-making processes, beyond just optimal behaviors, is a significant challenge. This study explores whether Large Language Models (LLMs) can emulate the breadth of human reasoning by predicting both intuitive, fast System 1 and deliberate, slow System 2 processes. We investigate the potential of AI to mimic diverse reasoning behaviors across a human population, addressing what we call the""full reasoning spectrum problem"". We designed reasoning tasks using a novel generalization of the Natural Language Inference (NLI) format to evaluate LLMs' ability to replicate human reasoning. The questions were crafted to elicit both System 1 and System 2 responses. Human responses were collected through crowd-sourcing and the entire distribution was modeled, rather than just the majority of the answers. We used personality-based prompting inspired by the Big Five personality model to elicit AI responses reflecting specific personality traits, capturing the diversity of human reasoning, and exploring how personality traits influence LLM outputs. Combined with genetic algorithms to optimize the weighting of these prompts, this method was tested alongside traditional machine learning models. The results show that LLMs can mimic human response distributions, with open-source models like Llama and Mistral outperforming proprietary GPT models. Personality-based prompting, especially when optimized with genetic algorithms, significantly enhanced LLMs' ability to predict human response distributions, suggesting that capturing suboptimal, naturalistic reasoning may require modeling techniques incorporating diverse reasoning styles and psychological profiles. The study concludes that personality-based prompting combined with genetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.",10.48550/arXiv.2502.14155,['JournalArticle'],Computer Science; Psychology
LaMP-QA: A Benchmark for Personalized Long-form Question Answering,2025,"Alireza Salemi, Hamed Zamani","Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts&Entertainment, (2) Lifestyle&Personal Development, and (3) Society&Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models. Our results show that incorporating the personalized context provided leads to up to 39% performance improvements. The benchmark is publicly released to support future research in this area.",10.48550/arXiv.2506.00137,['JournalArticle'],Computer Science
GenManip: LLM-driven Simulation for Generalizable Instruction-Following Manipulation,2025,"Ning Gao, Yilun Chen, Shuai Yang, Xinyi Chen, Yang Tian, Hao Li, Haifeng Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang","Robotic manipulation in real-world settings remains challenging, especially regarding robust generalization. Existing simulation platforms lack sufficient support for exploring how policies adapt to varied instructions and scenarios. Thus, they lag behind the growing interest in instruction-following foundation models like LLMs, whose adaptability is crucial yet remains underexplored in fair comparisons. To bridge this gap, we introduce GenManip, a realistic tabletop simulation platform tailored for policy generalization studies. It features an automatic pipeline via LLM-driven task-oriented scene graph to synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To systematically assess generalization, we present GenManip-Bench, a benchmark of 200 scenarios refined via human-in-the-loop corrections. We evaluate two policy types: (1) modular manipulation systems integrating foundation models for perception, reasoning, and planning, and (2) end-to-end policies trained through scalable data collection. Results show that while data scaling benefits end-to-end methods, modular systems enhanced with foundation models generalize more effectively across diverse scenarios. We anticipate this platform to facilitate critical insights for advancing policy generalization in realistic conditions. All code will be available at project page.",10.1109/CVPR52734.2025.01138,"['JournalArticle', 'Conference']",Computer Science; Engineering
Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses,2025,"Jens Rupprecht, Georg Ahnert, Markus Strohmaier",,10.48550/arXiv.2507.07188,"['JournalArticle', 'Review']",Computer Science; Sociology
Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression,2024,"Farima Fatahi Bayat, Xin Liu, H. V. Jagadish, Lu Wang","Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the""truthful directions""previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model's internal knowledge only when it is confident. Our code is available at https://github.com/launchnlp/LITO.",10.18653/v1/2024.findings-acl.737,"['JournalArticle', 'Conference']",Computer Science
Representation Engineering for Large-Language Models: Survey and Research Challenges,2025,"Lukasz Bartoszcze, Sarthak Munshi, Bryan Sukidi, Jennifer Yen, Zejia Yang, David Williams-King, Linh Le, Kosi Asuzu, Carsten Maple","Large-language models are capable of completing a variety of tasks, but remain unpredictable and intractable. Representation engineering seeks to resolve this problem through a new approach utilizing samples of contrasting inputs to detect and edit high-level representations of concepts such as honesty, harmfulness or power-seeking. We formalize the goals and methods of representation engineering to present a cohesive picture of work in this emerging discipline. We compare it with alternative approaches, such as mechanistic interpretability, prompt-engineering and fine-tuning. We outline risks such as performance decrease, compute time increases and steerability issues. We present a clear agenda for future research to build predictable, dynamic, safe and personalizable LLMs.",10.48550/arXiv.2502.17601,"['JournalArticle', 'Review']",Computer Science; Linguistics
"Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges",2025,"Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li","As large language models (LLMs) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based LLMs and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, pruning, bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for LLM-SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence.",10.48550/arXiv.2507.16731,"['JournalArticle', 'Review']",Computer Science
The Impact of Persona-based Political Perspectives on Hateful Content Detection,2025,"S. Civelli, Pietro Bernardelle, Gianluca Demartini","While pretraining language models with politically diverse content has been shown to improve downstream task fairness, such approaches require significant computational resources often inaccessible to many researchers and organizations. Recent work has established that persona-based prompting can introduce political diversity in model outputs without additional training. However, it remains unclear whether such prompting strategies can achieve results comparable to political pretraining for downstream tasks. We investigate this question using persona-based prompting strategies in multimodal hate-speech detection tasks, specifically focusing on hate speech in memes. Our analysis reveals that when mapping personas onto a political compass and measuring persona agreement, inherent political positioning has surprisingly little correlation with classification decisions. Notably, this lack of correlation persists even when personas are explicitly injected with stronger ideological descriptors. Our findings suggest that while LLMs can exhibit political biases in their responses to direct political questions, these biases may have less impact on practical classification tasks than previously assumed. This raises important questions about the necessity of computationally expensive political pretraining for achieving fair performance in downstream tasks.",10.1145/3701716.3718383,"['JournalArticle', 'Book', 'Conference']",Computer Science; Political Science
Exploring the Impact of Anthropomorphism in Role-Playing AI Chatbots on Media Dependency: A Case Study of Xuanhe AI,2024,"Qiufang Yu, Xingyu Lan","Powered by large language models, the conversational capabilities of AI have seen significant improvements. In this context, a series of role-playing AI chatbots have emerged, exhibiting a strong tendency toward anthropomorphism, such as conversing like humans, possessing personalities, and fulfilling social and companionship functions. Informed by media dependency theory in communication studies, this work hypothesizes that a higher level of anthropomorphism of the role-playing chatbots will increase users' media dependency (i.e., people will depend on media that meets their needs and goals). Specifically, we conducted a user study on a Chinese role-playing chatbot platform, Xuanhe AI, selecting four representative chatbots as research targets. We invited 149 users to interact with these chatbots over a period. A questionnaire survey revealed a significant positive correlation between the degree of anthropomorphism in role-playing chatbots and users' media dependency, with user satisfaction mediating this relationship. Next, based on the quantitative results, we conducted semi-structured interviews with ten users to further understand the factors that deterred them from depending on anthropomorphic chatbots. In conclusion, this work has provided empirical insights for the design of role-playing AI chatbots and deepened the understanding of how users engage with conversational AI over a longer period.",10.48550/arXiv.2411.17157,"['JournalArticle', 'Review']",Computer Science; Linguistics
Optimizing Preference Alignment with Differentiable NDCG Ranking,2024,"Jiacong Zhou, Xianyun Wang, Jun Yu","Aligning large language models with human preferences improves interaction quality and safety by ensuring outputs better reflect human values. A promising strategy involves Reinforcement Learning from Human Feedback (RLHF), starting with collecting and ranking responses generated by a supervised fine-tuning model to refine alignment. Current methods (DPO) focus on learning from pairwise preference data, categorizing responses into preferred and less preferred pairs, and optimizing by maximizing pairwise margins. Recent studies have uncovered a substantial discrepancy between the theoretical aspirations of preference learning and its real-world results. Current preference alignment techniques underperform expectations, with ranking accuracies below $60\%$ on standard datasets. This suggests existing methods inadequately capture ideal preference relationships within sequences. To address this challenge, this paper introduces \underline{D}irect \underline{R}anking \underline{P}reference \underline{O}ptimization (DRPO), a novel method that views human preference alignment as a Learning-to-Rank (LTR) task. DRPO leverages NDCG, a widely used LTR metric, to optimize the ranking of responses within lists based on preference data, thereby enhancing ranking accuracies. Due to the nondifferentiability of NDCG, we propose diffNDCG loss, a differentiable approximation facilitated by a sorting network to simulate NDCG. Furthermore, to improve the quality of generated response, we propose a novel margin-based Adaptive Rank Policy Score. Extensive experiments have shown that DRPO outperforms existing baseline methods, enhancing the quality of the generated responses.",10.48550/arXiv.2410.18127,['JournalArticle'],Computer Science
United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections,2024,"Leah von der Heyde, Anna Haensch, Alexander Wenz","""Synthetic samples""based on large language models (LLMs) have been argued to serve as efficient alternatives to surveys of humans, assuming that their training data includes information on human attitudes and behavior. However, LLM-synthetic samples might exhibit bias, for example due to training data and fine-tuning processes being unrepresentative of diverse contexts. Such biases risk reinforcing existing biases in research, policymaking, and society. Therefore, researchers need to investigate if and under which conditions LLM-generated synthetic samples can be used for public opinion prediction. In this study, we examine to what extent LLM-based predictions of individual public opinion exhibit context-dependent biases by predicting the results of the 2024 European Parliament elections. Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior. By comparing them to the actual results, we show that LLM-based predictions of future voting behavior largely fail, their accuracy is unequally distributed across national and linguistic contexts, and they require detailed attitudinal information in the prompt. The findings emphasize the limited applicability of LLM-synthetic samples to public opinion prediction. In investigating their contextual biases, this study contributes to the understanding and mitigation of inequalities in the development of LLMs and their applications in computational social science.",10.48550/arXiv.2409.09045,"['JournalArticle', 'Review']",Computer Science; Mathematics; Political Science; Sociology
"From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",2025,"Jian Jia, Jingtong Gao, Ben Xue, Junhao Wang, Qingpeng Cai, Quan Chen, Xiangyu Zhao, Peng Jiang, Kun Gai","Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",10.48550/arXiv.2502.12448,"['JournalArticle', 'Review']",Computer Science; Linguistics
SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions,2025,"Xiaofan Yu, Lanxiang Hu, Benjamin Z. Reichman, Dylan Chu, Rushil Chandrupatla, Xiyuan Zhang, Larry Heck, Tajana Rosing","Natural language interaction with sensing systems is crucial for addressing users' personal concerns and providing health-related insights into their daily lives. When a user asks a question, the system automatically analyzes the full history of sensor data, extracts relevant information, and generates an appropriate response. However, existing systems are limited to short-duration (e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In addition, they struggle with quantitative questions that require precise numerical answers.
 In this work, we introduce SensorChat, the first end-to-end QA system designed for daily life monitoring using long-duration, high-frequency time series data. Given raw sensor signals spanning multiple days and a user-defined natural language question, SensorChat generates semantically meaningful responses that directly address users' concerns. SensorChat effectively handles both quantitative questions that require numerical precision and qualitative questions that require high-level reasoning to infer subjective insights. To achieve this, SensorChat uses an innovative three-stage pipeline that includes question decomposition, sensor data query, and answer assembly. The first and third stages leverage Large Language Models (LLMs) to interpret human queries and generate responses. The intermediate querying stage extracts relevant information from the complete sensor data history, which is then combined with the original query in the final stage to produce accurate and meaningful answers. Real-world implementations demonstrate SensorChat's capability for real-time interactions on a cloud server while also being able to run entirely on an edge platform after quantization. Comprehensive QA evaluations show that SensorChat achieves 93% higher answer accuracy than the best performing state-of-the-art systems on quantitative questions. Furthermore, a user study with eight volunteers highlights SensorChat's effectiveness in answering qualitative and open-ended questions. The code is available at https://github.com/Orienfish/SensorChat.",10.48550/arXiv.2502.02883,['JournalArticle'],Computer Science; Engineering; Environmental Science
"Perceived Usefulness, Experience and FinTech Acceptance in Pakistan: An Economic Analysis",2023,"Kashif Azeem, S. Sheikh, Saif ur Rahman","



FinTech is an innovative digital financial solution that rapidly invade the banking and financial markets. It moves the world economy into digitally design technology known as palm technology. However, the acceptability of new blockchain based technology in this new era of digital economy face multiple challenges for customer acceptance. The backbox is much clear and need to fill the gap until now. Furthermore in many platforms it is misunderstood till now need the quantitative research to elaborate the economic effect of FinTech with blockchain technology. In this paper estimate blockchain usability for investigate the elements of trust has an impact on customers' willingness to use blockchain digital banking funds management and its economical impact on GDP growth of country. Methodology use in this paper to investigate the FinTech acceptance in the mind of users quantitative research. Questionnaire filled by the target audience to get the primary data. Moreover, in this research paper Smart PLS4 used to obtain finding of research. In this research study the proposed new investigation model help to solve the complexity of technology: The Technology acceptance Model (TAM) with newly introduced Several blockchain-related variables technology adoption charters such as experience, trust, socio-culture. The survey was conducted among the users that helps to identify the variables impact of users and their intention. The finding of this research indicates the powerful construct, (experience and trust) that encourage the FinTech institutions to adopt the application based on blockchain technology. The experience of customers encourage the trust and adoption of blockchain technology applications. The business societies and government institutes put efforts for enhance the users trust that helps to accept the blockchain technology and its based FinTech applications for banks and financial institutions.



",10.52131/pjhss.2023.1104.0670,"['JournalArticle', 'Review']",Business; Computer Science; Economics
The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models,2025,"Marlene Lutz, Indira Sen, Georg Ahnert, Elisa Rogers, Markus Strohmaier","Persona prompting is increasingly used in large language models (LLMs) to simulate views of various sociodemographic groups. However, how a persona prompt is formulated can significantly affect outcomes, raising concerns about the fidelity of such simulations. Using five open-source LLMs, we systematically examine how different persona prompt strategies, specifically role adoption formats and demographic priming strategies, influence LLM simulations across 15 intersectional demographic groups in both open- and closed-ended tasks. Our findings show that LLMs struggle to simulate marginalized groups but that the choice of demographic priming and role adoption strategy significantly impacts their portrayal. Specifically, we find that prompting in an interview-style format and name-based priming can help reduce stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B outperform larger ones such as Llama-3.3-70B. Our findings offer actionable guidance for designing sociodemographic persona prompts in LLM-based simulation studies.",10.48550/arXiv.2507.16076,['JournalArticle'],Computer Science; Linguistics; Sociology
Methods for Estimating and Improving Robustness of Language Models,2022,Michal Stefnik,"Despite their outstanding performance, large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem. This proposal investigates a common denominator of this problem in their weak ability to generalise outside of the training domain. We survey diverse research directions providing estimations of model generalisation ability and find that incorporating some of these measures in the training objectives leads to enhanced distributional robustness of neural models. Based on these findings, we present future research directions enhancing the robustness of LLMs.",10.48550/arXiv.2206.08446,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
World Models: The Safety Perspective,2024,"Zifan Zeng, Chongzhe Zhang, Feng Liu, Joseph Sifakis, Qunli Zhang, Shiming Liu, Peng Wang","With the proliferation of the Large Language Model (LLM), the concept of World Models (WM) has recently attracted a great deal of attention in the AI research community, especially in the context of AI agents. It is arguably evolving into an essential foundation for building AI agent systems. A WM is intended to help the agent predict the future evolution of environmental states or help the agent fill in missing information so that it can plan its actions and behave safely. The safety property of WM plays a key role in their effective use in critical applications. In this work, we review and analyze the impacts of the current state-of-the-art in WM technology from the point of view of trustworthiness and safety based on a comprehensive survey and the fields of application envisaged. We provide an in-depth analysis of state-of-the-art WMs and derive technical research challenges and their impact in order to call on the research community to collaborate on improving the safety and trustworthiness of WM.",10.1109/ISSREW63542.2024.00104,"['JournalArticle', 'Review']",Computer Science; Engineering
An Abstractive Summarization and Conversation Bot using T5 and its Variants,2023,"Sudharson D, K. S. Thrisha Vaishnavi, S. Hariprakassh, B. Abiram, K. Saranya, Poonam Tanwar","Abstractive summarization and conversation bots powered by Large Language Models have gained significant attention currently with respect to their ability to generate humanlike text and facilitate natural language interactions. Models like RNN, LSTM, GRU are not capable of performing these tasks effectively. This article explores abstractive summarization and question-answering methods by using Large Language Models with respect to the key components, challenges, and applications of such systems. It emphasizes the usage of Large Language Models, like GPT-3, for abstractive summarization and conversational AI. The study highlights the importance of fine-tuning these models for specific tasks, such as summarization and chatbots, to enhance their performance and ensure coherent and contextually relevant responses. In this article different versions of models like Flan T5 and LaMini Flan T5 are used. Additionally, it discusses ethical considerations and the responsible deployment of these technologies, emphasizing transparency, bias mitigation, and user privacy. Finally, this article offers insights into the potential future developments in this field, including personalized conversational agents and improved summarization accuracy, contributing to the ongoing advancement of natural language processing and AI-driven interactions in Natural Language Processing for the effective and efficient use of documents for obtaining knowledge.",10.1109/ICAICCIT60255.2023.10465740,['Conference'],Computer Science; Linguistics
SocialSim: Towards Socialized Simulation of Emotional Support Conversation,2025,"Zhuang Chen, Yaru Cao, Guanqun Bi, Jincenzi Wu, Jinfeng Zhou, Xiyao Xiao, Si Chen, Hongning Wang, Minlie Huang","Emotional support conversation (ESC) helps reduce people's psychological stress and provide emotional value through interactive dialogues. Due to the high cost of crowdsourcing a large ESC corpus, recent attempts use large language models for dialogue augmentation. However, existing approaches largely overlook the social dynamics inherent in ESC, leading to less effective simulations. In this paper, we introduce SocialSim, a novel framework that simulates ESC by integrating key aspects of social interactions: social disclosure and social awareness. On the seeker side, we facilitate social disclosure by constructing a comprehensive persona bank that captures diverse and authentic help-seeking scenarios. On the supporter side, we enhance social awareness by eliciting cognitive reasoning to generate logical and supportive responses. Building upon SocialSim, we construct SSConv, a large-scale synthetic ESC corpus of which quality can even surpass crowdsourced ESC data. We further train a chatbot on SSConv and demonstrate its state-of-the-art performance in both automatic and human evaluations. We believe SocialSim offers a scalable way to synthesize ESC, making emotional care more accessible and practical.",10.1609/aaai.v39i2.32116,"['JournalArticle', 'Conference']",Computer Science; Psychology
Beyond WEIRD: Can synthetic survey participants substitute for humans in global policy research?,2024,"Pujen Shrestha, Dario Krpan, Fatima Koaik, Robin Schnider, Dima Sayess, May Saad Binbaz","Researchers are testing the feasibility of using the artificial intelligence tools known as large language models to create synthetic research participantsartificial entities that respond to surveys as real humans would. Thus far, this research has largely not been designed to examine whether synthetic participants could mimic human answers to policy-relevant surveys or reflect the views of people from non-WEIRD (Western, educated, industrialized, rich, and democratic) nations. Addressing these gaps in one study, we have compared human and synthetic participants responses to policy-relevant survey questions in three domains: sustainability, financial literacy, and female participation in the labor force. Participants were drawn from the United States as well as two non-WEIRD nations that have not previously been included in studies of synthetic respondents: the Kingdom of Saudi Arabia and the United Arab Emirates. We found that for all three nations, the synthetic participants created by GPT-4, a form of large language model, on average produced responses reasonably similar to those of their human counterparts. Nevertheless, we observed some differences between the American and non-WEIRD participants: For the latter, the correlations between human and synthetic responses to the full set of survey questions tended to be weaker. In addition, although we found a common tendency in all three countries for synthetic participants to show more positive and less negative bias (that is, to be more progressive and financially literate relative to their human counterparts), this trend was less pronounced for the non-WEIRD participants. We discuss the main policy implications of our findings and offer practical recommendations for improving the use of synthetic participants in research.",10.1177/23794607241311793,"['JournalArticle', 'Review']",Computer Science; Political Science; Sociology
Predicting Affective States from Screen Text Sentiment,2024,"Songyan Teng, Tianyi Zhang, Simon DAlfonso, V. Kostakos","The proliferation of mobile sensing technologies has enabled the study of various physiological and behavioural phenomena through unobtrusive data collection from smartphone sensors. This approach offers real-time insights into individuals' physical and mental states, creating opportunities for personalised treatment and interventions. However, the potential of analysing the textual content viewed on smartphones to predict affective states remains underexplored. To better understand how the screen text that users are exposed to and interact with can influence their affects, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. We employed linear regression, zero-shot, and multi-shot prompting using a large language model (LLM) to analyse relationships between screen text and affective states. Our findings indicate that multi-shot prompting substantially outperforms both linear regression and zero-shot prompting, highlighting the importance of context in affect prediction. We discuss the value of incorporating textual and sentiment data for improving affect prediction, providing a basis for future advancements in understanding smartphone use and wellbeing.",10.1145/3675094.3678489,"['JournalArticle', 'Book', 'Conference']",Computer Science; Psychology
From User Surveys to Telemetry-Driven AI Agents: Exploring the Potential of Personalized Productivity Solutions,2024,"Subigya Nepal, Javier Hernandez, Talie Massachi, Kael Rowan, Judith Amores, Jina Suh, Gonzalo Ramos, Brian Houck, Shamsi T. Iqbal, M. Czerwinski","Information workers increasingly struggle with productivity challenges in modern workplaces, facing difficulties in managing time and effectively utilizing workplace analytics data for behavioral improvement. Despite the availability of productivity metrics through enterprise tools, workers often fail to translate this data into actionable insights. We present a comprehensive, user-centric approach to address these challenges through AI-based productivity agents tailored to users' needs. Utilizing a two-phase method, we first conducted a survey with 363 participants, exploring various aspects of productivity, communication style, agent approach, personality traits, personalization, and privacy. Drawing on the survey insights, we developed a GPT-4 powered personalized productivity agent that utilizes telemetry data gathered via Viva Insights from information workers to provide tailored assistance. We compared its performance with alternative productivity-assistive tools, such as dashboard and narrative, in a study involving 40 participants. Our findings highlight the importance of user-centric design, adaptability, and the balance between personalization and privacy in AI-assisted productivity tools. By building on these insights, our work provides important guidance for developing more effective productivity solutions, ultimately leading to optimized efficiency and user experiences for information workers.",10.1145/3757408,"['JournalArticle', 'Review']",Computer Science
X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term Dialogue Agents,2024,"Weiqi Wu, Hongqiu Wu, Haizhen Zhao","The Turing test examines whether AIs exhibit human-like behaviour in natural language conversations. The traditional setting limits each participant to one message at a time and requires constant human participation. This fails to reflect a natural conversational style and hinders the evaluation of dialogue agents based on Large Language Models (LLMs) in complex and prolonged interactions. This paper proposes \textbf{\textsc{X-Turing}}, which enhances the original test with a \textit{burst dialogue} pattern, allowing more dynamic exchanges using consecutive messages. It further reduces human workload by iteratively generating dialogues that simulate the long-term interaction between the agent and a human to compose the majority of the test process. With the \textit{pseudo-dialogue} history, the agent then engages in a shorter dialogue with a real human, which is paired with a human-human conversation on the same topic to be judged using questionnaires. We introduce the \textit{X-Turn Pass-Rate} metric to assess the human likeness of LLMs across varying durations. While LLMs like GPT-4 initially perform well, achieving pass rates of 51.9\% and 38.9\% during 3 turns and 10 turns of dialogues respectively, their performance drops as the dialogue progresses, which underscores the difficulty in maintaining consistency in the long term.",10.18653/v1/2025.acl-long.293,"['JournalArticle', 'Conference']",Computer Science
Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering,2025,"Jessica Y. Bo, Tianyu Xu, Ishan Chatterjee, Katrina Passarella-Ward, Achin Kulshrestha, D. Shin","As large language models (LLMs) improve in their capacity to serve as personal AI assistants, their ability to output uniquely tailored, personalized responses that align with the soft preferences of their users is essential for enhancing user satisfaction and retention. However, untrained lay users have poor prompt specification abilities and often struggle with conveying their latent preferences to AI assistants. To address this, we leverage activation steering to guide LLMs to align with interpretable preference dimensions during inference. In contrast to memory-based personalization methods that require longer user history, steering is extremely lightweight and can be easily controlled by the user via an linear strength factor. We embed steering into three different interactive chatbot interfaces and conduct a within-subjects user study (n=14) to investigate how end users prefer to personalize their conversations. The results demonstrate the effectiveness of preference-based steering for aligning real-world conversations with hidden user preferences, and highlight further insights on how diverse values around control, usability, and transparency lead users to prefer different interfaces.",10.48550/arXiv.2505.04260,['JournalArticle'],Computer Science
Empirically evaluating commonsense intelligence in large language models with large-scale human judgments,2025,"Tuan Dung Nguyen, Duncan J. Watts, Mark E. Whiting","Commonsense intelligence in machines is often assessed by static benchmarks that compare a model's output against human-prescribed correct labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a method for evaluating common sense in artificial intelligence (AI), specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense intelligence to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.",10.48550/arXiv.2505.10309,"['JournalArticle', 'Review']",Computer Science
RAPID DEVELOPMENT OF CHATBOT FOR TOURISM PROMOTION IN LATGALE,2024,"S. Kodors, Guna Kaepe, Danils Zeps, Imants Zarembo, L. Litavniece","The release of ChatGPT technology identified the large language models as a new disruptive technology, which changes the behaviours of society and its attitude towards the presence of artificial intelligence in everyday life. The tourism industry is one of the economic sectors, which will be impacted by the large language models through personalized marketing and advertisements. A common approach to capture the attention of AI-centric tourists, who want to get answers to their questions without manually researching the topic or using services of the travel advisors, is to integrate a chatbot or virtual assistant in the tourism information system. We applied this approach to the promotion of tourism in East Latvia (Latgale) by rapidly developing a chatbot by using a prompt method with context-oriented material. Two models were prepared for tourism promotion in Latgale. The models were evaluated through a pilot survey to understand the satisfaction of target users. The data analysis was applied. The study identified the importance of trustworthy information and answer saturation. The trade-off between dialog freedom and trustworthiness of answers can be achieved through the development of microservices, which are grouped as one system to direct conversation with chatbot. The appropriate conceptual models are presented in the article.",10.17770/etr2024vol2.8060,"['JournalArticle', 'Conference', 'Review']",Business; Computer Science
Personality Structured Interview for Large Language Model Simulation in Personality Research,2025,"Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald","Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.",10.48550/arXiv.2502.12109,['JournalArticle'],Computer Science; Psychology
"CT2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart",2024,"Bowen Zhao, Tianhao Cheng, Yuejie Zhang, Ying Cheng, Rui Feng, Xiaobo Zhang","Multimodal Question Answering (MMQA) is crucial as it enables comprehensive understanding and accurate responses by integrating insights from diverse data representations such as tables, charts, and text. Most existing researches in MMQA only focus on two modalities such as image-text QA, table-text QA and chart-text QA, and there remains a notable scarcity in studies that investigate the joint analysis of text, tables, and charts. In this paper, we present CT2C-QA, a pioneering Chinese reasoning-based QA dataset that includes an extensive collection of text, tables, and charts, meticulously compiled from 200 selectively sourced webpages. Our dataset simulates real webpages and serves as a great test for the capability of the model to analyze and reason with multimodal data, because the answer to a question could appear in various modalities, or even potentially not exist at all. Additionally, we present AED (Allocating, Expert and Decision), a multi-agent system implemented through collaborative deployment, information interaction, and collective decision-making among different agents. Specifically, the Assignment Agent is in charge of selecting and activating expert agents, including those proficient in text, tables, and charts. The Decision Agent bears the responsibility of delivering the final verdict, drawing upon the analytical insights provided by these expert agents. We execute a comprehensive analysis, comparing AED with various state-of-the-art models in MMQA, including GPT-4. The experimental outcomes demonstrate that current methodologies, including GPT-4, are yet to meet the benchmarks set by our dataset.",10.1145/3664647.3681053,"['JournalArticle', 'Book']",Computer Science; Linguistics
Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs,2024,"Erik Weber, Jrme Rutinowski, Markus Pauly","ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in information withholding. This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5. Apparently, in this case an increased data exposure correlates with a greater belief in the control of information. An assignment of extreme political affiliations increased the belief in conspiracy theories. Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory.",10.48550/arXiv.2402.04110,['JournalArticle'],Computer Science; Psychology
Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion Prediction for Social Media Influencers,2024,"Qinglan Wei, Ruiqi Xue, Yutian Wang, Hongjiang Xiao, Yuhao Wang, Xiaoyan Duan","Predicting influencers' views and public sentiment on social media is crucial for anticipating societal trends and guiding strategic responses. This study introduces a novel computational framework to predict opinion leaders' perspectives and the emotive reactions of the populace, addressing the inherent challenges posed by the unstructured, context-sensitive, and heterogeneous nature of online communication. Our research introduces an innovative module that starts with the automatic 5W1H (Where, Who, When, What, Why, and How) questions formulation engine, tailored to emerging news stories and trending topics. We then build a total of 60 anonymous opinion leader agents in six domains and realize the views generation based on an enhanced large language model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we synthesize the potential views of opinion leaders and predicted the emotional responses to different events. The efficacy of our automated 5W1H module is corroborated by an average GPT-4 score of 8.83/10, indicative of high fidelity. The influencer agents exhibit a consistent performance, achieving an average GPT-4 rating of 6.85/10 across evaluative metrics. Utilizing the 'Russia-Ukraine War' as a case study, our methodology accurately foresees key influencers' perspectives and aligns emotional predictions with real-world sentiment trends in various domains.",10.48550/arXiv.2407.20668,['JournalArticle'],Computer Science
SoK: How Artificial-Intelligence Incidents Can Jeopardize Safety and Security,2024,"R. May, Jacob Krger, Thomas Leich","In the past years, a growing number of highly-automated systems has build on Artificial-Intelligence (AI) capabilities, for example, self-driving vehicles or predictive health-state diagnoses. As for any software system, there is a risk that misbehavior occurs (e.g., system failure due to bugs) or that malicious actors aim to misuse the system (e.g., generating attack scripts), which can lead to safety and security incidents. While software safety and security incidents have been studied in the past, we are not aware of research focusing on the specifics of AI incidents. With this paper, we aim to shed light on this gap through a case survey of 240 incidents that we elicited from four datasets comprising safety and security incidents involving AI from 2014 to 2023. Using manual data analyses and automated topic modeling, we derived relevant topics as well as the major issues and contexts in which the incidents occurred. We find that the topic of AI incidents is, not surprisingly, becoming more and more relevant, particularly in the contexts of autonomous driving and process-automation robotics. Regarding security and its intersection with safety, most incidents connect to generative AI (i.e., large-language models, deep fakes) and computer-vision systems (i.e., facial recognition). This emphasizes the importance of security to also ensure safety in the context of AI systems, with our results further revealing a high number of serious consequences (system compromise, human injuries) and major violations of confidentiality, integrity, availability, as well as authorization. We hope to support practitioners and researchers in understanding major safety and security issues to support the development of more secure, safe, and trustworthy AI systems.",10.1145/3664476.3664510,"['Book', 'JournalArticle', 'Review']",Computer Science; Engineering
A Survey of Foundation Models for Environmental Science,2025,"Runlong Yu, Shengyu Chen, Yiqun Xie, Xiaowei Jia","Modeling environmental ecosystems is essential for effective resource management, sustainable development, and understanding complex ecological processes. However, traditional methods frequently struggle with the inherent complexity, interconnectedness, and limited data of such systems. Foundation models, with their large-scale pre-training and universal representations, offer transformative opportunities by integrating diverse data sources, capturing spatiotemporal dependencies, and adapting to a broad range of tasks. This survey presents a comprehensive overview of foundation model applications in environmental science, highlighting advancements in forward prediction, data generation, data assimilation, downscaling, model ensembling, and decision-making across domains. We also detail the development process of these models, covering data collection, architecture design, training, tuning, and evaluation. By showcasing these emerging methods, we aim to foster interdisciplinary collaboration and advance the integration of cutting-edge machine learning for sustainable solutions in environmental science.",10.48550/arXiv.2503.03142,"['JournalArticle', 'Conference', 'Review']",Computer Science; Environmental Science
Magenta: Metrics and Evaluation Framework for Generative Agents based on LLMs,2024,"Sudarshan Kamath Barkur, Pratik Sitapara, Sven Leuschner, Sigurd Schacht","Large Language Models (LLMs) have emerged as a driving force in the field of Natural Language Processing (NLP) with applications spanning various domains, including the development of Autonomous Generative Agents. Generative Agents are computational software programs designed to believably simulate human behavior by harnessing the capabilities of large language models. Through repetitive prompts against the large language model, these agents operate based on a system architecture consisting of memory streams, reflection, and planning, allowing them to store experiences, learn from them, and translate insights into high-level action plans to interact with their environment. This paper discusses the current landscape of language models and autonomous agents, their advantages and challenges, and the current state of evaluation, and proposes an innovative evaluation benchmark designed to provide a holistic perspective on their performance. Additionally, we see the impact of fine-tuning such an LLM, evaluate using our benchmark, and then propose a framework for evaluation of both the agents and their underlying LLMs. The existing frameworks for evaluating LLMs and autonomous agents focus on single tasks and are limited in capturing their capabilities. We outline the methodology for evaluating autonomous agents' performance in responding to single and multi-step prompts. The process consists of three key stages: Preparation of the data, Preparation of the Gold Answers, and Evaluations. We use the meticulously crafted 20 unique prompts to challenge the agents, covering simple and complex questions. Using GPT-4, a state-of-the-art model, we generate the initial responses, which undergo rigorous verification to produce gold answers, indicating correctness and revealing the minimum steps required for task completion. Our evaluation framework relies on two critical metrics: the effort metrics, quantifying the steps taken by autonomous agents, and the success rate, measuring their accuracy in achieving task objectives and also keeping track of hallucinations of the model. We conduct experiments with ten different models, representing the current landscape of natural language processing models, presenting each with 20 unique prompts. Their responses are meticulously compared to our gold answers and gold steps (optimal number of steps) to generate the evaluation metrics. Similarly, a fine-tuned model is also evaluated with ten different questions, which test the agent's decision-making process by selecting the correct tool and then the ability of the model to reach the correct conclusion to the question asked by the user in this process.This comprehensive approach ensures a thorough assessment of autonomous agents' capabilities. It demonstrates the utility of these metrics, revealing how they can shed light on the strengths and weaknesses of various autonomous agents. As a step toward standardization, we propose transforming the evaluation process of LLMs into an automated framework that accommodates all types of language models, agents, and LLM-based applications. Such an approach promises to establish a unified and comprehensive evaluation methodology, empowering users to make informed decisions when selecting, fine-tuning, and assessing the accuracy of underlying language models and their applications for different domains.In summary, this paper contributes to the ongoing research on evaluating LLMs and autonomous agents by introducing a novel benchmark and proposing a framework, focusing on evaluating the language models while keeping different knowledge domains in mind. Our framework will enhance our understanding of these technologies and serve as a valuable resource for researchers, engineers, and practitioners working in the ever-evolving landscape of NLP and autonomous systems.",10.54941/ahfe1004478,nan,Computer Science
Engagement-Driven Content Generation with Large Language Models,2024,"Erica Coppolillo, Federico Cinus, Marco Minici, Francesco Bonchi, Giuseppe Manco","Large Language Models (LLMs) demonstrate significant persuasive capabilities in one-on-one interactions, but their influence within social networks, where interconnected users and complex opinion dynamics pose unique challenges, remains underexplored. This paper addresses the research question: \emph{Can LLMs generate meaningful content that maximizes user engagement on social networks?} To answer this, we propose a pipeline using reinforcement learning with simulated feedback, where the network's response to LLM-generated content (i.e., the reward) is simulated through a formal engagement model. This approach bypasses the temporal cost and complexity of live experiments, enabling an efficient feedback loop between the LLM and the network under study. It also allows to control over endogenous factors such as the LLM's position within the social network and the distribution of opinions on a given topic. Our approach is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. Such flexibility makes it suitable for more complex engagement tasks and interventions in computational social science. Using our framework, we analyze the performance of LLMs in generating social engagement under different conditions, showcasing their full potential in this task. The experimental code is publicly available at https://github.com/mminici/Engagement-Driven-Content-Generation.",10.48550/arXiv.2411.13187,"['JournalArticle', 'Conference']",Computer Science
A Hate Speech Moderated Chat Application: Use Case for GDPR and DSA Compliance,2024,"Jan Fillies, Theodoros Mitsikas, Ralph Schfermeier, Adrian Paschke","The detection of hate speech or toxic content online is a complex and sensitive issue. While the identification itself is highly dependent on the context of the situation, sensitive personal attributes such as age, language, and nationality are rarely available due to privacy concerns. Additionally, platforms struggle with a wide range of local jurisdictions regarding online hate speech and the evaluation of content based on their internal ethical norms. This research presents a novel approach that demonstrates a GDPR-compliant application capable of implementing legal and ethical reasoning into the content moderation process. The application increases the explainability of moderation decisions by utilizing user information. Two use cases fundamental to online communication are presented and implemented using technologies such as GPT-3.5, Solid Pods, and the rule language Prova. The first use case demonstrates the scenario of a platform aiming to protect adolescents from potentially harmful content by limiting the ability to post certain content when minors are present. The second use case aims to identify and counter problematic statements online by providing counter hate speech. The counter hate speech is generated using personal attributes to appeal to the user. This research lays the groundwork for future DSA compliance of online platforms. The work proposes a novel approach to reason within different legal and ethical definitions of hate speech and plan the fitting counter hate speech. Overall, the platform provides a fitted protection to users and a more explainable and individualized response. The hate speech detection service, the chat platform, and the reasoning in Prova are discussed, and the potential benefits for content moderation and algorithmic hate speech detection are outlined. A selection of important aspects for DSA compliance is outlined.",10.48550/arXiv.2410.07713,['JournalArticle'],Computer Science; Law
Examining the Idea Density and Semantic Distance of Responses Given by AI to Tests of Divergent Thinking,2024,"M. Runco, Burak Turkman, Selcuk Acar, Ahmed M. Abdulla Alabbasi","Research suggests that generative AI (GAI) responds to divergent thinking (DT) prompts with multiple ideas, some of which seem to be original. The present investigation administered 55 DT tasks to three GAI services (Bard, GPT 3.5, and GPT 4.0). Instead of examining individual responses, an Idea Density algorithm was used to assess the output. This algorithm quantifies the ideas within responses, controlling for the number of words. A subset of the DT tests administered to the GAI were also scored for Semantic Distance, which estimates originality. Results indicated that the three GAI models differed in the Idea Density of the output. There were also significant differences between Realistic and Nonrealistic DT tasks. As has been the case in human samples, directions given when the GAI received the prompts also had a significant impact, with more Idea Density following directions that explicitly prompted original responses. Adjusted scores removed all verbiage in the output, which did not actually address the questions conveyed by the prompts. These corrected scores shared approximately 50% of the variance with the uncorrected raw responses, implying that the typical output of GAI is not always relevant. This was interpreted in the context of the standard definition of creativity, which emphasizes effectiveness, as well as originality.",10.1002/jocb.1528,['JournalArticle'],Computer Science; Psychology
Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?,2025,"Satiyabooshan Murugaboopathy, Connor T. Jerzak, Adel Daoud","We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.",10.48550/arXiv.2508.01109,"['JournalArticle', 'Review']",Computer Science; Environmental Science; Sociology
Improving GPT Generated Synthetic Samples with Sampling-Permutation Algorithm,2023,K. Kalinin,"The primary objective of this study is to leverage the capabilities of a large language model (LLM), such as GPT-3, to generate responses from elite individuals who are difficult to access. Similar to the work of Argyle et al. (2023), this study specifically focuses on the domain of multiple-choice questions. To address the issue of instability and hallucinations commonly associated with LLM, a novel algorithm, termed the sampling-permutation algorithm, has been developed and implemented. The efficacy of this algorithm is assessed by applying it to questions from the Survey of Russian Elites (Zimmerman, Rivera and Kalinin 2022). No-tably, this study examines the generated responses from synthetic personas representing the Russian President Vladimir Putin and the opposition leader Alexei Navalny by conducting a validation study and exploring the effects of the war context on generated responses. My findings indicate that the proposed approach provides valuable insights, despite the presence of somewhat mixed results.",10.2139/ssrn.4548937,"['JournalArticle', 'Review']",Computer Science; Linguistics
"Shilling Attacks and Fake Reviews Injection: Principles, Models, and Datasets",2025,"Dina Nawara, Ahmed Aly, R. Kashef","Recommendation systems have proved to be a compelling performance in overcoming the data overload problem in many domains, such as e-commerce, e-health, and transportation. Recommender systems guide users/clients to personalized recommendations based on their preferences. However, some recommendation systems are vulnerable to shilling attacks, which create rating biases or fake reviews that will eventually affect the authenticity and integrity of the generated recommendations. This survey comprehensively covers various shilling attack methods, including high-knowledge, low-knowledge attacks, and obfuscated attacks. It explores malicious review generators that generate fake text. In addition to that, this survey covers shilling attack detection methods such as supervised, unsupervised, semisupervised, and hybrid techniques. Natural Language Processing techniques are also thoroughly explored for fake text review detection using large language models (LLMs). A wide range of detection mechanisms incorporated in the literature is examined, such as convolutional neural network (CNN), long short term memory (LSTM)-based detectors for rating-based shilling attacks, and bidirectional encoder representation (BERT) and RoBERTa-based detectors for fake reviews that are accompanied by shilling attacks, aiming to offer insights into the evolving methods of shilling attack strategies and the corresponding advancements in the detection methods.",10.1109/TCSS.2024.3465008,"['JournalArticle', 'Review']",Computer Science
Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation,2025,"T. Neumann, Maria De-Arteaga, S. Fazelpour","The emergent capabilities of large language models (LLMs) have prompted interest in using them as surrogates for human subjects in opinion surveys. However, prior evaluations of LLM-based opinion simulation have relied heavily on costly, domain-specific survey data, and mixed empirical results leave their reliability in question. To enable cost-effective, early-stage evaluation, we introduce a quality control assessment designed to test the viability of LLM-simulated opinions on Likert-scale tasks without requiring large-scale human data for validation. This assessment comprises two key tests: \emph{logical consistency} and \emph{alignment with stakeholder expectations}, offering a low-cost, domain-adaptable validation tool. We apply our quality control assessment to an opinion simulation task relevant to AI-assisted content moderation and fact-checking workflows -- a socially impactful use case -- and evaluate seven LLMs using a baseline prompt engineering method (backstory prompting), as well as fine-tuning and in-context learning variants. None of the models or methods pass the full assessment, revealing several failure modes. We conclude with a discussion of the risk management implications and release \texttt{TopicMisinfo}, a benchmark dataset with paired human and LLM annotations simulated by various models and approaches, to support future research.",10.48550/arXiv.2504.08954,"['JournalArticle', 'Review']",Computer Science; Political Science
Balancing Large Language Model Alignment and Algorithmic Fidelity in Social Science Research,2025,"Alex Lyman, Bryce Hepner, Lisa P. Argyle, Ethan C. Busby, Joshua R. Gubler, David Wingate","Generative artificial intelligence (AI) has the potential to revolutionize social science research. However, researchers face the difficult challenge of choosing a specific AI model, often without social science-specific guidance. To demonstrate the importance of this choice, we present an evaluation of the effect of alignment, or human-driven modification, on the ability of large language models (LLMs) to simulate the attitudes of human populations (sometimes called silicon sampling). We benchmark aligned and unaligned versions of six open-source LLMs against each other and compare them to similar responses by humans. Our results suggest that model alignment impacts output in predictable ways, with implications for prompting, task completion, and the substantive content of LLM-based results. We conclude that researchers must be aware of the complex ways in which model training affects their research and carefully consider model choice for each project. We discuss future steps to improve how social scientists work with generative AI tools.",10.1177/00491241251342008,['JournalArticle'],Computer Science; Political Science; Sociology
Exploiting CLIP Self-Consistency to Automate Image Augmentation for Safety Critical Scenarios,2024,"Sujan Sai Gannamaneni, F. Klein, Michael Mock, M. Akila","With the current interest in deploying machine learning (ML) models in safety-critical applications like automated driving (AD), there is increased effort in developing sophisticated testing techniques for evaluating the models. One of the primary requirements for testing is the availability of test data, particularly test data that captures the long tail distributions of traffic events. As such data collection in the real world is hazardous, there is also a necessity for generating synthetic data using simulators or deep learning-based approaches. We propose a pipeline to generate augmented safety-critical scenes of the Cityscapes dataset using pre-trained SOTA latent diffusion models with additional conditioning using text and OpenPose-based ControlNet, where we have fine-grained control of the attributes of the generated pedestrians. In addition, we propose a filtering mechanism, similar to self-consistency checks in large language models (LLMs), to improve the quality of the generated data regarding the adherence to generated attributes, reaching ~ 25% improvement in our experiments. Finally, using pre-trained SOTA segmentation models on Cityscapes, we evaluate the generated datasets viability by qualitatively evaluating the predicted segmentation maps.",10.1109/CVPRW63382.2024.00363,"['JournalArticle', 'Conference']",Computer Science; Engineering; Environmental Science
In Silico Sociology: Forecasting COVID-19 Polarization with Large Language Models,2024,"Austin C. Kozlowski, Hyunku Kwon, James A. Evans","By training deep neural networks on massive archives of digitized text, large language models (LLMs) learn the complex linguistic patterns that constitute historic and contemporary discourses. We argue that LLMs can serve as a valuable tool for sociological inquiry by enabling accurate simulation of respondents from specific social and cultural contexts. Applying LLMs in this capacity, we reconstruct the public opinion landscape of 2019 to examine the extent to which the future polarization over COVID-19 was prefigured in existing political discourse. Using an LLM trained on texts published through 2019, we simulate the responses of American liberals and conservatives to a battery of pandemic-related questions. We find that the simulated respondents reproduce observed partisan differences in COVID-19 attitudes in 84% of cases, significantly greater than chance. Prompting the simulated respondents to justify their responses, we find that much of the observed partisan gap corresponds to differing appeals to freedom, safety, and institutional trust. Our findings suggest that the politicization of COVID-19 was largely consistent with the prior ideological landscape, and this unprecedented event served to advance history along its track rather than change the rails.",10.48550/arXiv.2407.11190,['JournalArticle'],Computer Science; Political Science; Sociology
Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models,2025,"Mats Faulborn, Indira Sen, Max Pellert, Andreas Spitz, David Garcia","Prompt-based language models like GPT4 and LLaMa have been used for a wide variety of use cases such as simulating agents, searching for information, or for content analysis. For all of these applications and others, political biases in these models can affect their performance. Several researchers have attempted to study political bias in language models using evaluation suites based on surveys, such as the Political Compass Test (PCT), often finding a particular leaning favored by these models. However, there is some variation in the exact prompting techniques, leading to diverging findings, and most research relies on constrained-answer settings to extract model responses. Moreover, the Political Compass Test is not a scientifically valid survey instrument. In this work, we contribute a political bias measured informed by political science theory, building on survey design principles to test a wide variety of input prompts, while taking into account prompt sensitivity. We then prompt 11 different open and commercial models, differentiating between instruction-tuned and non-instruction-tuned models, and automatically classify their political stances from 88,110 responses. Leveraging this dataset, we compute political bias profiles across different prompt variations and find that while PCT exaggerates bias in certain models like GPT3.5, measures of political bias are often unstable, but generally more left-leaning for instruction-tuned models. Code and data are available on: https://github.com/MaFa211/theory_grounded_pol_bias",10.48550/arXiv.2503.16148,"['JournalArticle', 'Conference', 'Review']",Computer Science; Political Science
Creativity in LLM-based Multi-Agent Systems: A Survey,2025,"Yi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung-yi Lee, Yun-Nung Chen","Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.",10.48550/arXiv.2505.21116,"['JournalArticle', 'Review']",Computer Science
Scaling Efficient LLMs,2024,B. N. Kausik,"Trained LLMs in the transformer architecture are typically sparse in that most of the parameters are negligible, raising questions on efficiency. Furthermore, the so called""AI scaling law""for transformers suggests that the number of parameters must scale linearly with the size of the data. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, by comparing theoretical and empirical estimates of the Kullback-Liebler divergence, we derive a natural AI scaling law that the number of parameters in an efficient LLM scales as $D^{\gamma}$ where $D$ is the size of the training data and $ \gamma \in [0.44, 0.72]$, suggesting the existence of more efficient architectures. Against this backdrop, we propose recurrent transformers, combining the efficacy of transformers with the efficiency of recurrent networks, progressively applying a single transformer layer to a fixed-width sliding window across the input sequence. Recurrent transformers (a) run in linear time in the sequence length, (b) are memory-efficient and amenable to parallel processing in large batches, (c) learn to forget history for language tasks, or accumulate history for long range tasks like copy and selective copy, and (d) are amenable to curriculum training to overcome vanishing gradients. In our experiments, we find that recurrent transformers perform favorably on benchmark tests.",10.48550/arXiv.2402.14746,['JournalArticle'],Computer Science
LLMAir: Adaptive Reprogramming Large Language Model for Air Quality Prediction,2024,"Jinxiao Fan, Haolin Chu, Liang Liu, Huadong Ma","Accurate and timely air quality prediction is crucial for cities and individuals to effectively take necessary precautions against potential air pollution. Existing studies typically rely on building prediction models based on large-scale monitoring data, often designed for specific tasks. Recently, pre-trained large language models (LLMs) have achieved significant progress in various time series analysis tasks due to their powerful representation and inference capabilities. However, their application to air quality data with spatio-temporal features remains largely unexplored. In this work, we propose LLMAir, an adaptive reprogramming approach that adapts pre-trained LLMs for air quality prediction. We first construct spatiotemporal tokens based on monitoring stations by integrating value, node, and time embeddings. Next, we design an adaptive semantic-enhanced reprogramming module to compute similarity matching scores between our spatiotemporal tokens and pre-trained word embeddings for alignment. We employ a semantic regulator to generate the optimal length of word prototypes, which serve as prompt prefixes for adaptive reprogramming and guiding the spatiotemporal token embeddings into the frozen LLM. Additionally, we jointly optimize predictive error and alignment loss to train our model. Experimental results demonstrate that LLMAir achieves state-of-the-art performance in air quality prediction and few-shot forecasting across two real-world datasets.",10.1109/ICPADS63350.2024.00062,"['JournalArticle', 'Conference']",Computer Science; Environmental Science
Designing Social Robots with LLMs for Engaging Human Interaction,2025,"Maria J. Pinto-Bernal, M. Biondina, Tony Belpaeme","Large Language Models (LLMs), particularly those enhanced through Reinforcement Learning from Human Feedback, such as ChatGPT, have opened up new possibilities for natural and open-ended spoken interaction in social robotics. However, these models are not inherently designed for embodied, multimodal contexts. This paper presents a user-centred approach to integrating an LLM into a humanoid robot, designed to engage in fluid, context-aware conversation with socially isolated older adults. We describe our system architecture, which combines real-time speech processing, layered memory summarisation, persona conditioning, and multilingual voice adaptation to support personalised, socially appropriate interactions. Through iterative development and evaluation, including in-home exploratory trials with older adults (n = 7) and a preliminary study with young adults (n = 43), we investigated the technical and experiential challenges of deploying LLMs in real-world humanrobot dialogue. Our findings show that memory continuity, adaptive turn-taking, and culturally attuned voice design enhance user perceptions of trust, naturalness, and social presence. We also identify persistent limitations related to response latency, hallucinations, and expectation management. This work contributes design insights and architectural strategies for future LLM-integrated robots that aim to support meaningful, emotionally resonant companionship in socially assistive settings.",10.3390/app15116377,['JournalArticle'],Computer Science
Large Language Models Do Not Simulate Human Psychology,2025,"Sarah Schrder, Thekla Morgenroth, Ulrike Kuhl, Valerie Vaquet, Benjamin Paaen","Large Language Models (LLMs),such as ChatGPT, are increasingly used in research, ranging from simple writing assistance to complex data annotation tasks. Recently, some research has suggested that LLMs may even be able to simulate human psychology and can, hence, replace human participants in psychological studies. We caution against this approach. We provide conceptual arguments against the hypothesis that LLMs simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs'and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses. Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability. We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.",10.48550/arXiv.2508.06950,['JournalArticle'],Computer Science; Linguistics; Psychology
Mitigative Strategies for Recovering From Large Language Model Trust Violations,2024,"Max J. Martell, Jessica A. Baweja, Brandon Dreslin","In this study, we investigated strategies to address trust issues arising from errors in large language models (LLMs). The study examined the impact of confidence scores, system capability explanations, and user feedback on trust restoration post-error. 68 participants viewed the responses of an LLM to 20 general trivia questions, with an error introduced on the third trial. Each participant was presented with one mitigation strategy. Participants rated their overall trust in the model and the reliability of the answer. Results showed an immediate drop in trust after the error; however, there were no differences across the three strategies in trust recovery. All conditions had a logarithmic trend in trust recovery following error. Differences in overall trust were predicted by perceived reliability of the answer, suggesting that participants were evaluating results critically and using that to inform their trust in the model. Qualitative data supported this finding; participants expressed lasting distrust despite the LLMs later accuracy. Results showcase the need to prioritize accuracy in LLM deployment, because early errors may irrevocably damage user trust calibration and later adoption.",10.1177/15553434241303577,['JournalArticle'],Computer Science; Linguistics
"A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions",2025,"Emre Can Acikgoz, Cheng Qian, Hongru Wang, Vardhan Dongre, Xiusi Chen, Heng Ji, Dilek Hakkani-Tur, Gokhan Tur","Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: https://github.com/emrecanacikgoz/awesome-conversational-agents.",10.48550/arXiv.2504.16939,"['JournalArticle', 'Review']",Computer Science
Bayesian WeakS-to-Strong from Text Classification to Generation,2024,"Ziyun Cui, Ziyang Zhang, Wen Wu, Guangzhi Sun, Chao Zhang","Advances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.",10.48550/arXiv.2406.03199,['JournalArticle'],Computer Science
Assessing LLMs Responses in the Field of Domestic Sustainability: An Exploratory Study,2023,"Mathyas Giudici, Giulio Antonio Abbo, Ottavia Belotti, Alessio Braccini, Francesco Dubini, Riccardo Andrea Izzo, Pietro Crovari, Franca Garzotto","In the next years, we must challenge climate change, and the urgency of adopting a more sustainable lifestyle has increased. Conversational Agents, such as Smart home Personal Assistants, have shown promise in fostering sustainable behaviors in domestic environments. However, traditional conversations with rule-based approaches in such agents face challenges in addressing users' questions in complex domains like sustainability. Large Language Models (LLMs) are a promising tool to overcome these limitations of their capability to answer open-domain questions. The final objective of this work is to compare the generative capabilities of four large language models in ecological sustainability to determine the most suitable LLM to be embedded into home assistants and create a hybrid model of conversational agent for environmental sustainability. We performed two evaluations. In the former, we constructed a set of trustable sources on the topic and analyzed the extent to which the themes covered in the text generated by the models appeared in it. The results do not show a statistical difference between the outputs of the candidate models, while qualitative analysis determined that ChatGPT, at the moment, is the optimal solution. In the second evaluation, we tested the responses generated by ChatGPT on a corpus of 167 questions from a sample of 75 people. Responses evaluation was performed by a team of experts (N=5) on fluency, coherency, consistency, accuracy, and reasoning. The results suggest that ChatGPT for generic questions on sustainability is quite reliable.",10.1109/DDP60485.2023.00019,['Conference'],Computer Science; Environmental Science
ChatGPT as a voting application in direct democracy,2025,"Maud Reveilhac, Davide Morselli","The study examines the potential role of ChatGPT as a tool for popular voting. It assesses ChatGPT's positions on four voting objects (three initiatives and one referendum) by simulating various Swiss voter profiles (neutral, centrist, left, right, progressist, and conservative) and comparing these to its default stance. Additionally, ChatGPT's responses to arguments for and against each object were analyzed for consistency with its voting patterns. The study compared ChatGPT's decisions to real voting outcomes, fixing the temperature and collecting 100 answers per question to ensure reliability. Results showed that voting decisions varied by model version (GPT3.5 or GPT4) and language (French or German), indicating cultural and model iteration influences. ChatGPT's decisions did not consistently align with a leftprogressist stance, highlighting the impact of language, model versions, and contextual factors on political interpretations.",10.1111/spsr.12650,['JournalArticle'],Computer Science; Political Science
Can LLMs advance democratic values?,2024,"Seth Lazar, Lorenzo Manuali","LLMs are among the most advanced tools ever devised for understanding and generating natural language. Democratic deliberation and decision-making involve, at several distinct stages, the production and comprehension of language. So it is natural to ask whether our best linguistic tools might prove instrumental to one of our most important tasks involving language. Researchers and practitioners have recently asked whether LLMs can support democratic deliberation by leveraging abilities to summarise content, to aggregate opinion over summarised content, and to represent voters by predicting their preferences over unseen choices. In this paper, we assess whether using LLMs to perform these and related functions really advances the democratic values behind these experiments. We suggest that the record is mixed. In the presence of background inequality of power and resources, as well as deep moral and political disagreement, we should not use LLMs to automate non-instrumentally valuable components of the democratic process, nor be tempted to supplant fair and transparent decision-making procedures that are practically necessary to reconcile competing interests and values. However, while LLMs should be kept well clear of formal democratic decision-making processes, we think they can instead strengthen the informal public sphere--the arena that mediates between democratic governments and the polities that they serve, in which political communities seek information, form civic publics, and hold their leaders to account.",10.48550/arXiv.2410.08418,['JournalArticle'],Computer Science; Linguistics; Political Science
Reflections on deep learning and the actuarial profession(al),2024,"Roseanne Harris, Ronald Richman, Mario V. Wuthrich","We discuss some of the professional consequences of rapid advances in deep learning techniques applied to actuarial science. Since actuarial work is highly regulated by standards and professional guidance, we survey relevant aspects of the guidance in the United Kingdom and South Africa, that apply to actuarial deep learning models. A selective survey of recent advances in methodology is then performed, showing how these advances can be used to ensure compliance with guidance on issues such as model understandability, avoidance of bias and discrimination and variability of predictions. We also discuss the current treatment of machine and deep learning in the actuarial education syllabus and make suggestions for a new subject covering these topics in more detail. Finally, we discuss the evolving role of the actuary and briefly consider consequences of large language models on actuarial work",10.2139/ssrn.4672447,"['JournalArticle', 'Review']",Business; Computer Science
Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models,2023,"Yang Lu, Jordan Yu, Shou-Hsuan Stephen Huang","This study explores the idea of AI Personality or AInality suggesting that Large Language Models (LLMs) exhibit patterns similar to human personalities. Assuming that LLMs share these patterns with humans, we investigate using human-centered psychometric tests such as the Myers-Briggs Type Indicator (MBTI), Big Five Inventory (BFI), and Short Dark Triad (SD3) to identify and confirm LLM personality types. By introducing role-play prompts, we demonstrate the adaptability of LLMs, showing their ability to switch dynamically between different personality types. Using projective tests, such as the Washington University Sentence Completion Test (WUSCT), we uncover hidden aspects of LLM personalities that are not easily accessible through direct questioning. Projective tests allowed for a deep exploration of LLMs cognitive processes and thought patterns and gave us a multidimensional view of AInality. Our machine learning analysis revealed that LLMs exhibit distinct AInality traits and manifest diverse personality types, demonstrating dynamic shifts in response to external instructions. This study pioneers the application of projective tests on LLMs, shedding light on their diverse and adaptable AInality traits.",10.48550/arXiv.2312.14202,['JournalArticle'],Computer Science; Psychology
DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction,2024,"Yu Feng, Phu Mon Htut, Zheng Qi, Wei Xiao, Manuel Mager, Nikolaos Pappas, Kishaloy Halder, Yang Li, Yassine Benajiba, Dan Roth","Quantifying the uncertainty in the factual parametric knowledge of Large Language Models (LLMs), especially in a black-box setting, poses a significant challenge. Existing methods, which gauge a model's uncertainty through evaluating self-consistency in responses to the original query, do not always capture true uncertainty. Models might respond consistently to the origin query with a wrong answer, yet respond correctly to varied questions from different perspectives about the same query, and vice versa. In this paper, we propose a novel method, DiverseAgentEntropy, for evaluating a model's uncertainty using multi-agent interaction under the assumption that if a model is certain, it should consistently recall the answer to the original query across a diverse collection of questions about the same original query. We further implement an abstention policy to withhold responses when uncertainty is high. Our method offers a more accurate prediction of the model's reliability and further detects hallucinations, outperforming other self-consistency-based methods. Additionally, it demonstrates that existing models often fail to consistently retrieve the correct answer to the same query under diverse varied questions even when knowing the correct answer.",10.48550/arXiv.2412.09572,['JournalArticle'],Computer Science
Lets Talk About You: Development and Evaluation of an Autonomous Robot to Support Ikigai Reflection in Older Adults,2024,"Long-Jing Hsu, Weslie Khoo, Manasi Swaminathan, Kyrie Jig Amon, Rasika Muralidharan, Hiroki Sato, Min Min Thant, Anna S. Kim, Katherine M. Tsui, David J. Crandall, Selma abanovi","The sources of a persons ikigaitheir sense of meaning and purpose in lifeoften change as they age. Reflecting on past and new sources of ikigai may help people renew their sense of meaning as their life circumstances shift. Building on insights from an initial Wizard-of-Oz robot prototype [1], we describe the design of an autonomous robot that uses a semi-structured conversation format to help older adults reflect on what gives their life meaning and purpose. The robot uses both pre-determined (scripted) and Large Language Model (LLM) generated questions to personalize conversations with older adults around themes of social interaction, planning, accomplishments, goal setting, and the recent past. We evaluated the autonomous robot with 19 older adult participants in a lab setting and at two eldercare facilities. Analysis of the older adults conversations with the robot and their responses to an evaluative survey allowed us to identify several design considerations for an autonomous robot that can support ikigai reflection. Interweaving simple yet detailed predetermined questions with LLM-generated follow-up questions yielded enjoyable, in-depth conversations with older adults. We also recognized the need for the robot to be able to offer relevant suggestions when participants cannot recall events and people they find meaningful. These findings aim to further refine the design of an interactive robot that can support users in their exploration of lifes purpose.",10.1109/RO-MAN60168.2024.10731264,"['JournalArticle', 'Conference', 'Review']",Computer Science
Map-based Modular Approach for Zero-shot Embodied Question Answering,2024,"Koya Sakamoto, Daich Azuma, Taiki Miyanishi, Shuhei Kurita, M. Kawanabe","Embodied Question Answering (EQA) serves as a benchmark task to evaluate the capability of robots to navigate within novel environments and identify objects in response to human queries. However, existing EQA methods often rely on simulated environments and operate with limited vocabularies. This paper presents a map-based modular approach to EQA, enabling real-world robots to explore and map unknown environments. By leveraging foundation models, our method facilitates answering a diverse range of questions using natural language. We conducted extensive experiments in both virtual and real-world settings, demonstrating the robustness of our approach in navigating and comprehending queries within unknown environments.",10.1109/IROS58592.2024.10801314,"['JournalArticle', 'Conference']",Computer Science; Engineering
Emails by LLMs: A Comparison of Language in AI-Generated and Human-Written Emails,2025,"Weijiang Li, Yinmeng Lai, Sandeep Soni, Koustuv Saha","The growing excitement around generative AI (and LLMs) is fueling a heightened interest in the development of AI-assisted writing tools. One popular context is AI-assisted email writing, and this paper explores how AI-generated emails compare to human-written emails. We obtained human-written emails from the W3C corpus and generated analogous AI-generated emails using GPT-3.5, GPT-4, Llama-2, and Mistral-7B, and compared AI-generated and human-written emails using a suite of natural language analyses across syntactic, semantic, and psycholinguistic dimensions. AI-generated emails are generally consistent across different LLMs but differ significantly from human-written emails. Specifically, AI-generated emails tend to be more formal, verbose, and complex, whereas human-written emails are often more concise and personalized. While AI-generated emails are slightly more polite, both types exhibit a similar level of empathetic tone in language. Further, we qualitatively examined user perceptions of AI and human-written emails by conducting a small survey of 41 participants and interviewing a subset of them. This study highlights preliminary insights into generative AIs distinct strengths and weaknesses in assisting email communication, and we discuss the theoretical and practical implications of the evolving landscape of AI-generated content.",10.1145/3717867.3717872,"['Book', 'JournalArticle', 'Review']",Computer Science; Linguistics
Advancements and Applications of Large Language Models in Natural Language Processing: A Comprehensive Review,2024,Mengchao Ren,"Abstract. Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demonstrating remarkable capabilities in understanding, generating, and manipulating human language. This comprehensive review explores the development, applications, optimizations, and challenges of LLMs. This paper begin by tracing the evolution of these models and their foundational architectures, such as the Transformer, GPT, and BERT. We then delve into the applications of LLMs in natural language understanding tasks, including sentiment analysis, named entity recognition, question answering, and text summarization, highlighting real-world use cases. Next, we examine the role of LLMs in natural language generation, covering areas such as content creation, language translation, personalized recommendations, and automated responses. We further discuss LLM applications in other NLP tasks like text style transfer, text correction, and language model pre-training. Subsequently, we explore techniques for optimizing and improving LLMs, including model compression, explainability, robustness, and security. Finally, we address the challenges posed by the significant computational requirements, sample inefficiency, and ethical considerations surrounding LLMs. We conclude by discussing potential future research directions, such as efficient architectures, few-shot learning, bias mitigation, and privacy-preserving techniques, which will shape the ongoing development and responsible deployment of LLMs in NLP.",10.54254/2755-2721/97/20241406,"['JournalArticle', 'Review']",Computer Science; Linguistics
Emotions in the Loop: A Survey of Affective Computing for Emotional Support,2025,"Karishma Hegde, Hemadri Jayalath","In a world where technology is increasingly embedded in our everyday experiences, systems that sense and respond to human emotions are elevating digital interaction. At the intersection of artificial intelligence and human-computer interaction, affective computing is emerging with innovative solutions where machines are humanized by enabling them to process and respond to user emotions. This survey paper explores recent research contributions in affective computing applications in the area of emotion recognition, sentiment analysis and personality assignment developed using approaches like large language models (LLMs), multimodal techniques, and personalized AI systems. We analyze the key contributions and innovative methodologies applied by the selected research papers by categorizing them into four domains: AI chatbot applications, multimodal input systems, mental health and therapy applications, and affective computing for safety applications. We then highlight the technological strengths as well as the research gaps and challenges related to these studies. Furthermore, the paper examines the datasets used in each study, highlighting how modality, scale, and diversity impact the development and performance of affective models. Finally, the survey outlines ethical considerations and proposes future directions to develop applications that are more safe, empathetic and practical.",10.48550/arXiv.2505.01542,"['JournalArticle', 'Review']",Computer Science; Psychology
Artificial Intelligence in Digital Marketing: Towards an Analytical Framework for Revealing and Mitigating Bias,2025,"Catherine Reed, M. Wynn, R. Bown","Artificial intelligence (AI) affects many aspects of modern life, and most predictions are that the impact of AI on business and society will only increase. In the marketing function of todays leading businesses, two main types of AI can be discerned. Traditional AI centres on supervised learning algorithms to support and enable the application of data rules, predictive functionality and other AI-based features. Generative AI, on the other hand, uses large language model (LLM) data sets and user prompts to generate new content. While AI-generated applications and content can boost efficiency, they also present challenges regarding transparency and authenticity, and the question of bias is central to these concerns. This article adopts a qualitative inductive approach to research this issue in the context of the marketing function of a global software supplier. Based on a systematic literature review and in-depth interviews with company marketeers, the perceived bias issues in coding, prompting and deployment of AI in digital marketing are identified. Then, based on a provisional conceptual framework derived from the extant literature, an analytical framework for revealing and mitigating bias in digital marketing is put forward, incorporating the perspectives of industry-based practitioners. The framework can be used as a checklist of marketing activities in which bias may exist in either traditional or generative AI across different stages of the customer journey. The article thus contributes to the development of theory and practice regarding the management of bias in AI-generated content and will be of interest to researchers and practitioners as an operational guide and point of departure for subsequent studies.",10.3390/bdcc9020040,"['JournalArticle', 'Review']",Business; Computer Science
A Survey of Foundation Models for Music Understanding,2024,"Wenjun Li, Ying Cai, Ziyang Wu, Wenyi Zhang, Yifan Chen, Rundong Qi, Mengqi Dong, Peigen Chen, Xiao Dong, Fenghao Shi, Lei Guo, Junwei Han, Bao Ge, Tianming Liu, Lin Gan, Tuo Zhang","Music is essential in daily life, fulfilling emotional and entertainment needs, and connecting us personally, socially, and culturally. A better understanding of music can enhance our emotions, cognitive skills, and cultural connections. The rapid advancement of artificial intelligence (AI) has introduced new ways to analyze music, aiming to replicate human understanding of music and provide related services. While the traditional models focused on audio features and simple tasks, the recent development of large language models (LLMs) and foundation models (FMs), which excel in various fields by integrating semantic information and demonstrating strong reasoning abilities, could capture complex musical features and patterns, integrate music with language and incorporate rich musical, emotional and psychological knowledge. Therefore, they have the potential in handling complex music understanding tasks from a semantic perspective, producing outputs closer to human perception. This work, to our best knowledge, is one of the early reviews of the intersection of AI techniques and music understanding. We investigated, analyzed, and tested recent large-scale music foundation models in respect of their music comprehension abilities. We also discussed their limitations and proposed possible future directions, offering insights for researchers in this field.",10.48550/arXiv.2409.09601,"['JournalArticle', 'Review']",Art; Computer Science; Engineering
On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software,2025,"Ali Nouri, Johan Andersson, Kailash De Jesus Hornig, Zhennan Fei, Emil Knabe, Hkan Sivencrona, Beatriz Cabrero Daniel, Christian Berger","Automated Driving System (ADS) is a safety-critical software system responsible for the interpretation of the vehicle's environment and making decisions accordingly. The unbounded complexity of the driving context, including unforeseeable events, necessitate continuous improvement, often achieved through iterative DevOps processes. However, DevOps processes are themselves complex, making these improvements both time- and resource-intensive. Automation in code generation for ADS using Large Language Models (LLM) is one potential approach to address this challenge. Nevertheless, the development of ADS requires rigorous processes to verify, validate, assess, and qualify the code before it can be deployed in the vehicle and used. In this study, we developed and evaluated a prototype for automatic code generation and assessment using a designed pipeline of a LLM-based agent, simulation model, and rule-based feedback generator in an industrial setup. The LLM-generated code is evaluated automatically in a simulation model against multiple critical traffic scenarios, and an assessment report is provided as feedback to the LLM for modification or bug fixing. We report about the experimental results of the prototype employing Codellama:34b, DeepSeek (r1:32b and Coder:33b), CodeGemma:7b, Mistral:7b, and GPT4 for Adaptive Cruise Control (ACC) and Unsupervised Collision Avoidance by Evasive Manoeuvre (CAEM). We finally assessed the tool with 11 experts at two Original Equipment Manufacturers (OEMs) by conducting an interview study.",10.48550/arXiv.2504.02141,['JournalArticle'],Computer Science; Engineering
Bringing Context-Aware Completion Suggestions to Arbitrary Text Entry Interfaces,2023,"Timothy J. Aveni, Armando Fox, Bjoern Hartmann","Large language models (LLMs) can predict obvious next steps that users will take in text entry fields, especially the tedious components of tasks like software engineering or email composition. These models are not only useful in large, unbroken text fields, however. We present OmniFill, a browser extension that detects text entry fields and offers autofill-style suggestions based on context from the browsing session. The system constructs an LLM prompt that includes three main components: (a) a description of the active tabs text fields and their current values, (b) information from the users recent web browsing context, and (c) a history, if available, of the users prior submissions to the web form (alongside those submissions associated browsing context). Suggestions from the LLMs response are offered to the user to be automatically typed into each corresponding text field. We offer a motivating example of a time-saving interaction and discuss the broader utility of interface-agnostic LLM integrations.",10.1145/3586182.3615825,"['JournalArticle', 'Book', 'Conference']",Computer Science
AdaptiveCoPilot: Design and Testing of a NeuroAdaptive LLM Cockpit Guidance System in both Novice and Expert Pilots,2025,"Shaoyue Wen, Michael Middleton, Songming Ping, Nayan N. Chawla, Guande Wu, Bradley Feest, Chihab Nadri, Yunmei Liu, David B. Kaber, Maryam Zahabi, Ryan P. McMahan, Sonia Castelo, Ryan McKendrick, Jing Qian, Claudio Silva","Pilots operating modern cockpits often face high cognitive demands due to complex interfaces and multitasking requirements, which can lead to overload and decreased performance. This study introduces AdaptiveCoPilot, a neuroadaptive guidance system that adapts visual, auditory, and textual cues in real time based on the pilots cognitive workload, measured via functional Near-Infrared Spectroscopy (fNIRS). A formative study with expert pilots (N=3) identified adaptive rules for modality switching and information load adjustments during preflight tasks. These insights informed the design of AdaptiveCoPilot, which integrates cognitive state assessments, behavioral data, and adaptive strategies within a context-aware Large Language Model (LLM). The system was evaluated in a virtual reality (VR) simulated cockpit with licensed pilots (N=8), comparing its performance against baseline and random feedback conditions. The results indicate that the pilots using AdaptiveCoPilot exhibited higher rates of optimal cognitive load states on the facets of working memory and perception, along with reduced task completion times. Based on the formative study, experimental findings, qualitative interviews, we propose a set of strategies for future development of neuroadaptive pilot guidance systems and highlight the potential of neuroadaptive systems to enhance pilot performance and safety in aviation environments.",10.1109/VR59515.2025.00088,"['JournalArticle', 'Conference']",Computer Science; Engineering
LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks,2025,"Yousef Emami, Hao Zhou, SeyedSina Nabavirazani, Luis Almeida","Unmanned Aerial Vehicles (UAVs) are increasingly being utilized in various private and commercial applications, e.g., traffic control, parcel delivery, and Search and Rescue (SAR) missions. Machine Learning (ML) methods used in UAV-Assisted Sensor Networks (UASNETs) and, especially, in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sampling efficiency, which conflict with the urgency of emergencies, such as SAR missions. In this paper, an In-Context Learning (ICL)-Data Collection Scheduling (ICLDC) system is proposed as an alternative to DRL in emergencies. The UAV collects sensory data and transmits it to a Large Language Model (LLM), which creates a task description in natural language. From this description, the UAV receives a data collection schedule that must be executed. A verifier ensures safe UAV operations by evaluating the schedules generated by the LLM and overriding unsafe schedules based on predefined rules. The system continuously adapts by incorporating feedback into the task descriptions and using this for future decisions. This method is tested against jailbreaking attacks, where the task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC significantly reduces cumulative packet loss compared to both the DQN and Maximum Channel Gain baselines. ICLDC presents a promising direction for intelligent scheduling and control in UASNETs.",10.48550/arXiv.2504.14556,['JournalArticle'],Computer Science; Engineering; Environmental Science
Explore the Hallucination on Low-level Perception for MLLMs,2024,"Yinan Sun, Zicheng Zhang, Haoning Wu, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min","The rapid development of Multi-modality Large Language Models (MLLMs) has significantly influenced various aspects of industry and daily life, showcasing impressive capabilities in visual perception and understanding. However, these models also exhibit hallucinations, which limit their reliability as AI systems, especially in tasks involving low-level visual perception and understanding. We believe that hallucinations stem from a lack of explicit self-awareness in these models, which directly impacts their overall performance. In this paper, we aim to define and evaluate the self-awareness of MLLMs in low-level visual perception and understanding tasks. To this end, we present QL-Bench, a benchmark settings to simulate human responses to low-level vision, investigating self-awareness in low-level visual perception through visual question answering related to low-level attributes such as clarity and lighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990 single images and 1,999 image pairs, each accompanied by an open-ended question about its low-level features. Through the evaluation of 15 MLLMs, we demonstrate that while some models exhibit robust low-level visual capabilities, their self-awareness remains relatively underdeveloped. Notably, for the same model, simpler questions are often answered more accurately than complex ones. However, self-awareness appears to improve when addressing more challenging questions. We hope that our benchmark will motivate further research, particularly focused on enhancing the self-awareness of MLLMs in tasks involving low-level visual perception and understanding.",10.48550/arXiv.2409.09748,['JournalArticle'],Computer Science
CONFLARE: CONFormal LArge language model REtrieval,2024,"Pouria Rouzrokh, S. Faghani, Cooper Gamble, M. Shariatnia, Bradley J. Erickson","Retrieval-augmented generation (RAG) frameworks enable large language models (LLMs) to retrieve relevant information from a knowledge base and incorporate it into the context for generating responses. This mitigates hallucinations and allows for the updating of knowledge without retraining the LLM. However, RAG does not guarantee valid responses if retrieval fails to identify the necessary information as the context for response generation. Also, if there is contradictory content, the RAG response will likely reflect only one of the two possible responses. Therefore, quantifying uncertainty in the retrieval process is crucial for ensuring RAG trustworthiness. In this report, we introduce a four-step framework for applying conformal prediction to quantify retrieval uncertainty in RAG frameworks. First, a calibration set of questions answerable from the knowledge base is constructed. Each question's embedding is compared against document embeddings to identify the most relevant document chunks containing the answer and record their similarity scores. Given a user-specified error rate ({\alpha}), these similarity scores are then analyzed to determine a similarity score cutoff threshold. During inference, all chunks with similarity exceeding this threshold are retrieved to provide context to the LLM, ensuring the true answer is captured in the context with a (1-{\alpha}) confidence level. We provide a Python package that enables users to implement the entire workflow proposed in our work, only using LLMs and without human intervention.",10.48550/arXiv.2404.04287,['JournalArticle'],Computer Science
The threat of analytic flexibility in using large language models to simulate human data: A call to attention,2025,Jamie Cummins,"Social scientists are now using large language models to create""silicon samples""- synthetic datasets intended to stand in for human respondents, aimed at revolutionising human subjects research. However, there are many analytic choices which must be made to produce these samples. Though many of these choices are defensible, their impact on sample quality is poorly understood. I map out these analytic choices and demonstrate how a very small number of decisions can dramatically change the correspondence between silicon samples and human data. Configurations (N = 252) varied substantially in their capacity to estimate (i) rank ordering of participants, (ii) response distributions, and (iii) between-scale correlations. Most critically, configurations were not consistent in quality: those that performed well on one dimension often performed poorly on another, implying that there is no""one-size-fits-all""configuration that optimises the accuracy of these samples. I call for greater attention to the threat of analytic flexibility in using silicon samples.",10.48550/arXiv.2509.13397,['JournalArticle'],Computer Science; Linguistics
A Survey of AI-Driven Mock Interviews Using GenAI and Machine Learning (InterviewX),2024,"J. Barpute, Onkar Wattamwar, Suhelali Pakjade, Sanika Diwate","With the accelerated demand for assessment tools that work efficiently and at scale, innovations in automated interviewing platforms have proliferated. In this research, InterviewX- a system enabled with artificial intelligence, works on RAG (Retrieval-Augmented Generation) and QLoRA (Quantized Low-Rank Adaptation). InterviewX aims to create an environment for an actual interview where domain-specific questions can be triggered and give personalized feedback in preparation for a candidate-an effective answer. Fine-tuning on large language models is enabled in InterviewX through 4-bit NormalFloat quantization for maximum computational efficiency with little or no installation effort. RAG also integrates real-time information retrieval while keeping the questions up to date with industry standards. The review mechanism within the platform used by InterviewX aligns the GPT-4 evaluation and the human assessment in identifying the areas and points of improvement in both the quality of question generation and analysis of candidates' feeding. In that way, InterviewX presents a scalable, low-cost, quality interview simulation solution for the preparation of job seekers and assessment of recruiters.",10.1109/ICUIS64676.2024.10866631,"['Conference', 'Review']",Computer Science
Application of large language models based on knowledge graphs in question-answering systems: A review,2024,Yani Wang,"The integration of Knowledge Graphs (KGs) and Large Language Models (LLMs) is emerging as a transformative advancement in AI, particularly within Question Answering (QA) systems. Traditional QA systems, constrained by static knowledge bases, have struggled with multimodal queries and personalized responses. The deep integration of KGs and LLMs offers a novel approach, combining the structured, contextual understanding of KGs with the semantic parsing capabilities of LLMs. This review explores the methodologies, algorithms, datasets, and applications of KG-LLM integration in QA systems, highlighting key research that enhances model performance, reasoning, and domain-specific applications. It addresses technical challenges such as data consistency, semantic understanding, and system optimization. Future directions focus on improving intrinsic model performance and achieving deeper integration of KGs and LLMs. This comprehensive overview underscores the potential of KG-LLM synergy to significantly improve service quality across diverse industries, including education and healthcare.",10.54254/2755-2721/71/20241636,"['JournalArticle', 'Review']",Computer Science; Linguistics
Talk to Your Brain: Artificial Personalized Intelligence for Emotionally Adaptive AI Interactions,2024,"Sandeep Varma, S. Shivam, Sarun Natarajan, Biswarup Ray, Bagesh Kumar, Om Dabral","The process of =making an Artificial intelligent system to replicating human emotional understanding and adapt its responses contextually by tailoring its responses based on individual cognitive and emotional states is called Artificial Personalized Intelligence (API). In this paper, we present a finetuned emotionally adaptive AI pipeline capable of generating personalized, human-like responses. Using a custom dataset based on six universal emotionsSadness, Happiness, Fear, Anger, Surprise, and Disgustcollected from interviews with 30 participants, we have explored two approaches: fine-tuning a LLaMA-3 8B model with the Low-Rank Adapter (LoRA) technique and employing a Retrieval-Augmented Generation (RAG) agent-based framework over the same LLaMA-3 8B model. Emotion classification, prompt engineering, and model fine-tuning were used over the dataset to capture emotional and personalized subtleties in the responses based on different individuals. We conducted a comprehensive analysis, evaluating performance across candidates, emotions, and overall performance, using metrics such as Mean Squared Error (MSE) and Pearson Correlation Coefficient to measure the difference among the outputs from both pipelines and actual human emotional responses. Our results show that prompt engineering combined with LoRA-based fine-tuning significantly enhances the ability to engage in emotionally intelligent and personalized conversations, whereas the RAG-based approach underperformed due to the pretrained LLAMA-3 models restrictive neutral and adversarial training. This highlights that fine-tuned LLMs are effective in replicating human emotions for personalized intelligence.",10.1109/CVMI61877.2024.10781964,"['JournalArticle', 'Conference']",Computer Science; Psychology
Investigating Software Development Teams Members' Perceptions of Data Privacy in the Use of Large Language Models (LLMs),2024,"Fabiano Damasceno Sousa Falco, Edna Dias Canedo","Context: Large Language Models (LLMs) have revolutionized natural language generation and understanding. However, they raise significant data privacy concerns, especially when sensitive data is processed and stored by third parties. Goal: This paper investigates the perception of software development teams members regarding data privacy when using LLMs in their professional activities. Additionally, we examine the challenges faced and the practices adopted by these practitioners. Method: We conducted a survey with 78 ICT practitioners from five regions of the country. Results: Software development teams members have basic knowledge about data privacy and LGPD, but most have never received formal training on LLMs and possess only basic knowledge about them. Their main concerns include the leakage of sensitive data and the misuse of personal data. To mitigate risks, they avoid using sensitive data and implement anonymization techniques. The primary challenges practitioners face are ensuring transparency in the use of LLMs and minimizing data collection. Software development teams members consider current legislation inadequate for protecting data privacy in the context of LLM use. Conclusions: The results reveal a need to improve knowledge and practices related to data privacy in the context of LLM use. According to software development teams members, organizations need to invest in training, develop new tools, and adopt more robust policies to protect user data privacy. They advocate for a multifaceted approach that combines education, technology, and regulation to ensure the safe and responsible use of LLMs.",10.1145/3701625.3701675,"['JournalArticle', 'Book', 'Review']",Computer Science
Towards Interpreting Language Models: A Case Study in Multi-Hop Reasoning,2024,Mansi Sakarvadia,"Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Language models (LMs) struggle to perform such reasoning consistently. We propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single- and multi-hop prompts. We then propose a mechanism that allows users to inject relevant prompt-specific information, which we refer to as""memories,""at critical LM locations during inference. By thus enabling the LM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We empirically show that a simple, efficient, and targeted memory injection into a key attention layer often increases the probability of the desired next token in multi-hop tasks, by up to 424%. We observe that small subsets of attention heads can significantly impact the model prediction during multi-hop reasoning. To more faithfully interpret these heads, we develop Attention Lens: an open source tool that translates the outputs of attention heads into vocabulary tokens via learned transformations called lenses. We demonstrate the use of lenses to reveal how a model arrives at its answer and use them to localize sources of model failures such as in the case of biased and malicious language generation.",10.48550/arXiv.2411.05037,['JournalArticle'],Computer Science
Expertized Caption Auto-Enhancement for Video-Text Retrieval,2025,"Junxiang Chen, Baoyao Yang, Wenbin Yao","Video-text retrieval has been stuck in the information mismatch caused by personalized and inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders an effective cross-modal representation alignment, resulting in ambiguous retrieval results. Although text rewriting methods have been proposed to broaden text expressions, the modality gap remains significant, as the text representation space is hardly expanded with insufficient semantic enrichment.Instead, this paper turns to enhancing visual presentation, bridging video expression closer to textual representation via caption generation and thereby facilitating video-text matching.While multimodal large language models (mLLM) have shown a powerful capability to convert video content into text, carefully crafted prompts are essential to ensure the reasonableness and completeness of the generated captions. Therefore, this paper proposes an automatic caption enhancement method that improves expression quality and mitigates empiricism in augmented captions through self-learning.Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, further exploring the utilization potential of caption augmentation.Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo. Our code is publicly available at https://github.com/CaryXiang/ECA4VTR.",10.48550/arXiv.2502.02885,['JournalArticle'],Computer Science
Interpreting Latent Student Knowledge Representations in Programming Assignments,2024,"Nigel Fernandez, Andrew Lan","Recent advances in artificial intelligence for education leverage generative large language models, including using them to predict open-ended student responses rather than their correctness only. However, the black-box nature of these models limits the interpretability of the learned student knowledge representations. In this paper, we conduct a first exploration into interpreting latent student knowledge representations by presenting InfoOIRT, an Information regularized Open-ended Item Response Theory model, which encourages the latent student knowledge states to be interpretable while being able to generate student-written code for open-ended programming questions. InfoOIRT maximizes the mutual information between a fixed subset of latent knowledge states enforced with simple prior distributions and generated student code, which encourages the model to learn disentangled representations of salient syntactic and semantic code features including syntactic styles, mastery of programming skills, and code structures. Through experiments on a real-world programming education dataset, we show that InfoOIRT can both accurately generate student code and lead to interpretable student knowledge representations.",10.48550/arXiv.2405.08213,['JournalArticle'],Computer Science
A Survey of Large Language Models in Tourism (Tourism LLMs),2024,Shengyu Gu,"This comprehensive survey delves into the integration and application of Large Language Models (LLMs) within the tourism sector, a domain ripe with potential for transformative AI-driven enhancements. As tourism increasingly embraces digital innovation, LLMs stand at the forefront of this evolution, offering sophisticated solutions for personalized travel experiences, multilingual communication, and the preservation of cultural heritage. This paper systematically explores the multifaceted roles of LLMs in tourism, from generating dynamic travel itineraries and culturally rich site descriptions to providing real-time assistance and multilingual support for global travelers. Through an analysis of current implementations and potential applications, we highlight both the remarkable opportunities presented by LLMs and the significant challenges, including data privacy concerns, cultural sensitivity, and the need for real-time processing capabilities. The findings underscore the imperative for a balanced approach that harnesses the capabilities of LLMs while addressing ethical considerations and ensuring inclusivity and accessibility in global tourism. This survey aims to provide a foundational understanding for researchers, practitioners, and policymakers, guiding future innovations and fostering a responsible integration of AI technologies in enhancing the global tourism experience.
",10.32388/8r27cj,"['JournalArticle', 'Review']",Computer Science; Linguistics
Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions,2024,"Jinxin Liu, Zao Yang","The responses generated by Large Language Models (LLMs) can include sensitive information from individuals and organizations, leading to potential privacy leakage. This work implements Influence Functions (IFs) to trace privacy leakage back to the training data, thereby mitigating privacy concerns of Language Models (LMs). However, we notice that current IFs struggle to accurately estimate the influence of tokens with large gradient norms, potentially overestimating their influence. When tracing the most influential samples, this leads to frequently tracing back to samples with large gradient norm tokens, overshadowing the actual most influential samples even if their influences are well estimated. To address this issue, we propose Heuristically Adjusted IF (HAIF), which reduces the weight of tokens with large gradient norms, thereby significantly improving the accuracy of tracing the most influential samples. To establish easily obtained groundtruth for tracing privacy leakage, we construct two datasets, PII-E and PII-CR, representing two distinct scenarios: one with identical text in the model outputs and pre-training data, and the other where models leverage their reasoning abilities to generate text divergent from pre-training data. HAIF significantly improves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E dataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs on real-world pretraining data CLUECorpus2020, demonstrating strong robustness regardless prompt and response lengths.",10.48550/arXiv.2408.10468,['JournalArticle'],Computer Science
Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors,2024,"Joseph Suh, Suhong Moon, Minwoo Kang, David M. Chan","Assessing personality traits using large language models (LLMs) has emerged as an interesting and challenging area of research. While previous methods employ explicit questionnaires, often derived from the Big Five model of personality, we hypothesize that LLMs implicitly encode notions of personality when modeling next-token responses. To demonstrate this, we introduce a novel approach that uncovers latent personality dimensions in LLMs by applying singular value de-composition (SVD) to the log-probabilities of trait-descriptive adjectives. Our experiments show that LLMs""rediscover""core personality traits such as extraversion, agreeableness, conscientiousness, neuroticism, and openness without relying on direct questionnaire inputs, with the top-5 factors corresponding to Big Five traits explaining 74.3% of the variance in the latent space. Moreover, we can use the derived principal components to assess personality along the Big Five dimensions, and achieve improvements in average personality prediction accuracy of up to 5% over fine-tuned models, and up to 21% over direct LLM-based scoring techniques.",10.48550/arXiv.2409.09905,['JournalArticle'],Computer Science; Psychology
a survey on multi-biometric fusion approaches,2023,"hawraa Abed alkareem hussain Ayash, Hawraa Hassan Abbas","The goal of biometrics is to reliably and robustly identify people based on their unique personal characteristics, primarily for security and authentication needs, but also to identify and track the users of more intelligent applications. Fingerprints, iris, palm print, face and voices are frequently used modalities, but there are numerous more potential biometrics, such as stride, ear image, retina, DNA, and even behavior. As an automatic way to identify persons depend on just one (single modal biometrics) or a mix of (multi-modal biometrics). A fusion of two or more photos can be utilized to create multimodal biometrics, and the resulting fused image will be more secure. Several levels of fusion techniques are currently accessible, including feature level, decision level, matching score level, etc. In order to identify the human biometric by extracting features and classifying images, this paper discusses different fusion approaches that are implemented in multimodal biometrics. It also describes the datasets that were used and the outcomes ,results and conclusions that were obtained. The goal of biometrics is to reliably and robustly identify people based on their unique personal characteristics, primarily for security and authentication needs, but also to identify and track the users of more intelligent applications. Fingerprints, iris, palm print, face and voices are frequently used modalities, but there are numerous more potential biometrics, such as stride, ear image, retina, DNA, and even behavior. As an automatic way to identify persons depend on just one (single modal biometrics) or a mix of (multi-modal biometrics). A fusion of two or more photos can be utilized to create multimodal biometrics, and the resulting fused image will be more secure. Several levels of fusion techniques are currently accessible, including feature level, decision level, matching score level, etc. In order to identify the human biometric by extracting features and classifying images, this paper discusses different fusion approaches that are implemented in multimodal biometrics. It also describes the datasets that were used and the outcomes ,results and conclusions that were obtained.",10.63463/kjes1074,"['JournalArticle', 'Review']",Computer Science
Relational AI: Facilitating Intergroup Cooperation with Socially Aware Conversational Support,2025,"Elijah L. Claggett, Robert E. Kraut, Hirokazu Shirado","Cooperation is challenging when group identities are involved. While people readily cooperate with in-group members, they struggle to build trust with out-group members. This study examines how text suggestions generated by Large Language Models (LLMs) can mitigate in-group-out-group bias and facilitate intergroup cooperation through conversations. We conducted an experiment with 482 participants who communicated with either in-group partners sharing their views or out-group partners with differing views, based on a preliminary survey. Participants received either personalized message suggestions aligned with their own views and conversation styles or relational suggestions using conversation styles tailored to whether their partner was in-group or out-group. Following the conversations, participants engaged in a cooperation game designed to measure trust behaviorally. Our results show that while personalized assistance widened the cooperation gap, relational assistance significantly improved out-group cooperation to match in-group levels. We discuss design implications for integrating social awareness into AI-driven conversational support systems.",10.1145/3706598.3713757,"['Book', 'JournalArticle', 'Conference', 'Review']",Computer Science; Psychology
Deploy-Efficient and Fast Network Probing with Time-Series Foundation Models,2024,"Rasman Mubtasim Swargo, Md. Arifuzzaman","Active probing, the practice of running data transfers for a short duration to collect or observe network metrics, is widely used for monitoring network performance, detecting anomalies, and optimizing data transfer algorithms. However, accurately estimating end-to-end throughput remains challenging due to the risks of self-inflicted network congestion during extended probing and the difficulty of balancing short probing durations with measurement precision. State-of-the-art transfer algorithms designed for large-scale HPC or Cloud data transfers rely on very short yet accurate periodic probing to achieve faster convergence to optimal solutions. To address these challenges, various approaches ranging from classical time-series forecasting to deep learning have been proposed to estimate metrics based on the first few seconds of data. While deep learning-based solutions outperform traditional time-series models, they often fail to generalize across diverse network environments with varying protocols, resources, and congestion levels, requiring significant data collection and retraining for each network. Furthermore, the evolving nature of networks renders these models obsolete within a short time. Consequently, running transfers for fixed durations and averaging metrics remains the most used approach despite its poor accuracy. To overcome these issues, we introduce npLLM, a scalable framework that leverages time-series foundation models for throughput estimation across dynamic, heterogeneous networks. By fine-tuning the pre-trained IBM Granite Time Series Foundation Model (TSFM) using data from local and production clusters, combined with data augmentation techniques, npLLM achieves high accuracy with very few samples in training, minimizing the need for extensive network-specific data collection. This transfer learning approach using the base foundation model enables efficient adaptation to new networks with minimal effort, significantly reducing setup time and resource requirements. Experimental results demonstrate that npLLM reduces probing error rate up to 70% compared to state-of-the-art solutions using as little as one-tenth of training data.",10.1109/BigData62323.2024.10824943,"['JournalArticle', 'Conference']",Computer Science; Engineering
Comparative Analysis of LLM-based Market Prediction and Human Expertise with Sentiment Analysis and Machine Learning Integration,2024,"Mohamed Abdelsamie, Hua Wang","This study conducts a comparative analysis of market prediction accuracy between Large Language Model (LLM)-based systems and human expertise within the financial analysis domain. Leveraging Quantum, an advanced LLM specialized for financial forecasting, we evaluate its predictive performance against human analysts and general-purpose LLMs, including GPT-3, GPT-4, FinGPT, and FinBERT. Employing a dataset of historical financial data, news headlines, and social media sentiment, we systematically assess predictive accuracy, response efficiency, and interpretability across models. The integration of sentiment analysis and machine learning further strengthens prediction reliability. Results reveal that Quantums specialized model demonstrates superior accuracy and speed in financial forecasting compared to human predictions and generalized LLMs, particularly in fast-moving, data-rich contexts. Nevertheless, limitations in nuanced contextual understanding and adaptability persist, highlighting the enduring value of human expertise. This research reinforces the potential of LLMs as robust tools for financial decision-making while identifying key areas for refinement to enhance synergy with human analytical insights. https://chatgpt.com/g/g-bS4Q76v0I-quantum",10.1109/DSIT61374.2024.10881868,"['JournalArticle', 'Conference']",Business; Computer Science
From Feature Importance to Natural Language Explanations Using LLMs with RAG,2024,"Sule Tekkesinoglu, Lars Kunze","As machine learning becomes increasingly integral to autonomous decision-making processes involving human interaction, the necessity of comprehending the model's outputs through conversational means increases. Most recently, foundation models are being explored for their potential as post hoc explainers, providing a pathway to elucidate the decision-making mechanisms of predictive models. In this work, we introduce traceable question-answering, leveraging an external knowledge repository to inform the responses of Large Language Models (LLMs) to user queries within a scene understanding task. This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities. We employ subtractive counterfactual reasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features. Furthermore, to maintain a seamless conversational flow, we integrate four key characteristics - social, causal, selective, and contrastive - drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process. Our evaluation demonstrates that explanations generated by the LLMs encompassed these elements, indicating its potential to bridge the gap between complex model outputs and natural language expressions.",10.48550/arXiv.2407.20990,['JournalArticle'],Computer Science
A Survey on Data Mining for Data-Driven Industrial Assets Maintenance,2025,"Eduardo Coronel, Benjamn Barn, P. Gardel","This survey presents a comprehensive review of data-driven approaches for industrial asset maintenance, emphasizing the use of data mining and machine learning techniques, including deep learning, for condition-based and predictive maintenance. It examines 534 references from 1995 to 2023, along with three additional articles from 2024 on natural language processing and large language models in industrial maintenance. The study categorizes two main techniques, four specialized approaches, and 27 methodologies, resulting in over 100 variations of algorithms tailored to specific maintenance needs for industrial assets. It details the data types utilized in the industrial sector, with the most frequently mentioned being time series data, event timestamp data, and image data. The survey also highlights the most frequently referenced data mining algorithms, such as the proportional hazard model, expert systems, support vector machines, random forest, autoencoder, and convolutional neural networks. Additionally, the survey proposes four level classes of asset complexity and studies five asset types, including mechanical, electromechanical, electrical, electronic, and computing assets. The growing adoption of deep learning is highlighted alongside the continued relevance of traditional approaches such as shallow machine learning and rule-based and model-based techniques. Furthermore, the survey explores emerging trends in machine learning and related technologies, identifies future research directions, and underscores their critical role in advancing condition-based and predictive maintenance frameworks.",10.3390/technologies13020067,"['JournalArticle', 'Review']",Computer Science; Engineering
"Carbon and Silicon, Coexist or Compete? A Survey on Human-AI Interactions in Agent-based Modeling and Simulation",2025,"Ziyue Lin, Siqi Shen, Zichen Cheng, Cheok Lam Lai, Siming Chen","Recent interest in human-AI interactions in agent-based modeling and simulation (ABMS) has grown rapidly due to the widespread utilization of large language models (LLMs). ABMS is an intelligent approach that simulates autonomous agents' behaviors within a defined environment to research emergent phenomena. Integrating LLMs into ABMS enables natural language interaction between humans and models. Meanwhile, it introduces new challenges that rely on human interaction to address. Human involvement can assist ABMS in adapting to flexible and complex research demands. However, systematic reviews of interactions that examine how humans and AI interact in ABMS are lacking. In this paper, we investigate existing works and propose a novel taxonomy to categorize the interactions derived from them. Specifically, human users refer to researchers who utilize ABMS tools to conduct their studies in our survey. We decompose interactions into five dimensions: the goals that users want to achieve (Why), the phases that users are involved (When), the components of the system (What), the roles of users (Who), and the means of interactions (How). Our analysis summarizes the findings that reveal existing interaction patterns. They provide researchers who develop interactions with comprehensive guidance on how humans and AI interact. We further discuss the unexplored interactions and suggest future research directions.",10.48550/arXiv.2502.18145,"['JournalArticle', 'Review']",Computer Science; Engineering
I see a double-edged sword: How self-other perceptual gaps predict public attitudes toward ChatGPT regulations and literacy interventions,2025,"Myojung Chung, Nuri Kim, S. M. Jones-Jang, Jihyang Choi, Sangwon Lee","The double-edged nature of generative artificial intelligence (AI) underscores the importance of understanding complex and paradoxical public views about this emerging technology. Heeding to this call, this study examined how the general public perceives and reacts to Chat GPT and the implications of these perceptions, drawing on the third-person and first-person effect. A national survey in the United States ( N=1004) revealed that individuals tend to believe they would personally benefit from the positive influence of Chat GPT, while others will benefit relatively less. Also, results showed that people believe that self is more capable of using Chat GPT critically, ethically, and efficiently than others. Interestingly, the self-other gap in perceived efficacy was influenced by subjective knowledge but not by objective knowledge about Chat GPT. The self-other gap in perceived efficacy negatively predicted support for government regulation of Chat GPT, while the self-other gap in both perceived influence and efficacy positively predicted support for literacy interventions.",10.1177/14614448241313180,"['JournalArticle', 'Review']",Computer Science; Psychology
Does the Grammatical Structure of Prompts Influence the Responses of Generative Artificial Intelligence? An Exploratory Analysis in Spanish,2025,"R. Viveros-Muoz, Jos Carrasco-Sez, Carolina Contreras-Saavedra, Sheny San-Martn-Quiroga, Carla E. Contreras-Saavedra","Generative Artificial Intelligence (AI) has transformed personal and professional domains by enabling creative content generation and problem-solving. However, the influence of users grammatical abilities on AI-generated responses remains unclear. This exploratory study examines how language and grammar abilities in Spanish affect the quality of responses from ChatGPT (version 3.5). Despite the robust performance of Large Language Models (LLMs) in various tasks, they face challenges with grammatical moods specific to non-English languages, such as the subjunctive in Spanish. Higher education students were chosen as participants due to their familiarity with AI and its potential use in learning. The study assessed ChatGPTs ability to process instructions in Chilean Spanish, analyzing how linguistic complexity, grammatical variations, and informal language impacted output quality. The results indicate that varied verbal moods and complex sentence structures significantly influence prompt evaluation, response quality, and response length. Based on these findings, a framework is proposed to guide higher education communities in promoting digital literacy and integrating AI into teaching and learning.",10.3390/app15073882,['JournalArticle'],Computer Science; Linguistics
Modelling and simulation of quadcopter UAV based on PID control,2024,Yijin Liu,"In the evolving field of aerial robotics, the precise control and stabilization of Unmanned Aerial Vehicles (UAVs), particularly quadrotors, stand as critical challenges due to their inherent nonlinear dynamics and susceptibility to external disturbances. This study embarks on addressing these challenges by first establishing a comprehensive mathematical model of a quadrotor's dynamics utilizing the Newton-Euler formulation. This foundational model serves as the basis for implementing a Proportional-Integral-Derivative (PID) control strategy, aimed at maintaining the UAV's desired flight trajectory and enhancing its stability against environmental perturbations. The effectiveness of the PID control approach is meticulously evaluated through a series of simulation tests conducted within the MATLAB/Simulink environment, focusing on the quadrotor's pitch, roll, and yaw control channels. The outcomes of these simulations provide convincing evidence of the PID scheme's ability to ensure rapid response times and precise attitude regulation, underpinning its viability for robust quadrotor UAV operations across diverse operational scenarios. Moreover, this research not only establishes a critical step towards the autonomous operation of UAVs but also accentuates the significance of sophisticated control mechanisms in the advancement of aerial robotics technology.",10.54254/2755-2721/75/20240539,['JournalArticle'],Computer Science; Engineering
Exploring Code Comprehension in Scientific Programming: Preliminary Insights from Research Scientists,2025,"Alyssia Chen, Carol Wong, Bonita Sharif, Anthony Peruma","Scientific software, defined as computer programs, scripts, or code used in scientific research, data analysis, modeling, or simulation, has become central to modern research. However, there is limited research on the readability and understandability of scientific code, both of which are vital for effective collaboration and reproducibility in scientific research. This study surveys 57 research scientists from various disciplines to explore their programming backgrounds, practices, and the challenges they face regarding code readability. Our findings reveal that most participants learn programming through self-study or on-the-job training, with $\mathbf{5 7. 9 \%}$ lacking formal instruction in writing readable code. Scientists mainly use Python and $R$, relying on comments and documentation for readability. While most consider code readability essential for scientific reproducibility, they often face issues with inadequate documentation and poor naming conventions, with challenges including cryptic names and inconsistent conventions. Our findings also show low adoption of code quality tools and a trend towards utilizing large language models to improve code quality. These findings offer practical insights into enhancing coding practices and supporting sustainable development in scientific software.",10.1109/ICPC66645.2025.00043,"['JournalArticle', 'Conference', 'Review']",Computer Science
Raising Awareness of Location Information Vulnerabilities in Social Media Photos using LLMs,2025,"Ying Ma, Shiquan Zhang, Dongju Yang, Z. Sarsenbayeva, Jarrod Knibbe, Jorge Gonalves","Location privacy leaks can lead to unauthorised tracking, identity theft, and targeted attacks, compromising personal security and privacy. This study explores LLM-powered location privacy leaks associated with photo sharing on social media, focusing on user awareness, attitudes, and opinions. We developed and introduced an LLM-powered location privacy intervention app to 19 participants, who used it over a two-week period. The app prompted users to reflect on potential privacy leaks that a widely available LLM could easily detect, such as visual landmarks & cues that could reveal their location, and provided ways to conceal this information. Through in-depth interviews, we found that our intervention effectively increased users awareness of location privacy and the risks posed by LLMs. It also encouraged users to consider the importance of maintaining control over their privacy data and sparked discussions about the future of location privacy-preserving technologies. Based on these insights, we offer design implications to support the development of future user-centred, location privacy-preserving technologies for social media photos.",10.1145/3706598.3714074,"['Book', 'JournalArticle', 'Conference']",Computer Science
Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity,2025,"Dang Nguyen, Ali Payani, Baharan Mirzasoleiman","Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address these limitations, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at https://github.com/BigML-CS-UCLA/SNNE.",10.48550/arXiv.2506.00245,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Persuasion Games using Large Language Models,2024,"Ganesh Prasath Ramani, S. Karande, V. Santhosh, Yash Bhatia","Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS). We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques. We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).",10.48550/arXiv.2408.15879,"['JournalArticle', 'Review']",Computer Science
Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with an LLM-Enabled Simulation,2024,"Zirong Chen, Elizabeth Chason, Noah Mladenovski, Erin Wilson, Kristin Mullen, Stephen Martini, Meiyi Ma","Emergency response services are vital for enhancing public safety by safeguarding the environment, property, and human lives. 
As frontline members of these services, 9-1-1 dispatchers have a direct impact on response times and the overall effectiveness of emergency operations. However, traditional dispatcher training methods, which rely on role-playing by experienced personnel, are labor-intensive, time-consuming, and often neglect the specific needs of underserved communities. To address these challenges, we introduce Sim911, the first training simulation for 9-1-1 dispatchers powered by Large Language Models (LLMs). Sim911 enhances training through three key technical innovations: (1) knowledge construction, which utilizes archived 9-1-1 call data to generate simulations that closely mirror real-world scenarios; (2) context-aware controlled generation, which employs dynamic prompts and vector bases to ensure that LLM behavior aligns with training objectives; and (3) validation with looped correction, which filters out low-quality responses and refines the system performance. Experimental results show Sim911's superior performance in effectiveness and equity. Beyond its technical advancements, Sim911 delivers significant social impacts. Successfully deployed in the Metro X of Emergency Communications (MXDEC)(PS: To ensure a double-blind review, we refer to the city as 'City X,' a mid-sized U.S. city with a population of over 700,000. Its Metro Department of Emergency Communications (MXDEC) employs around 80 dispatchers and call-takers. For the rest of the paper, we refer to MXDEC as 'DEC.') Sim911 has been integrated into multiple training sessions, saving time for dispatchers. By supporting a diverse range of incident types and caller tags, Sim911 provides more realistic and inclusive training experiences. In a conducted user study, 90.00 percent of participants found Sim911 to be as effective or even superior to traditional human-led training, making it a valuable tool for emergency communications centers nationwide, particularly those facing staffing challenges.",10.48550/arXiv.2412.16844,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering; Environmental Science
Beyond Detection: Large Language Models and Next-Generation Cybersecurity,2025,"Aitizaz Ali, M. C. Ghanem","The Integrating Contextual and AdaptiveCyber Defense Systems The accelerating evolution of cyber threats, seen both in terms of their complexity and frequency of attempts, makes it imperative to move away from traditional, reactive defense structures and towards intelligent, adaptive and proactive cyber defense strategies. Large Language Models (LLMs) (such as GPT this-will-be-a-zillionth-of-a-second-type-here or BERT derivatives) represent powerful new capabilities developed for understanding, analyzing, and generating human-like languagewith contextual depth enabling a new frontier in the land of cyber defense. This survey investigates the influence of LLMs on multiple key technical areas of cybersecurity including softwareand system security, network security, content moderation, hardware security, and blockchain security. It showcasesapplications of LLMs in tasks in vulnerability detection and prevention, phishing, malware detection and analysis, and intrusion detection; as well as smart contract auditing and adversarial attack simulation. The paper goes morein-depth on technical constructs like zero-shot and few-shot threat hunting, prompt engineering for scenario modeling, and retrieval-augmented generation for real-time intelligence. Task differences are captured in knowledge retrieval from external sources, enabling more contextual learning and task adaptability, inter-model interaction,and augmentation through domain-specific fine-tuning. While promising, LLMs poseopen challenges, such as hallucination, adversarial misuse, generalization problems, and ethical issues around privacy and accountability. The implications of such comparative analysis across domains highlight the strengths aswell as emerging risk associated with LLM systems at mission critical environments. The survey ends with a highly visionary section on the future opportunities around future self-healing systems, autonomous cyber agents, and the future of LLMs that can not only detect threats but also predict, simulate, and respond automaticallyto threats. This work serves as a foundational roadmap for researchers and practitioners for the use of LLMs for the next generation of resilient cybersecuritytools.",10.70470/shifra/2025/005,"['JournalArticle', 'Review']",Computer Science; Engineering
When an LLM is apprehensive about its answers - and when its uncertainty is justified,2025,"Petr Sychev, Andrey Goncharov, Daniil Vyazhev, Edvard A. Khalafyan, Alexey Zaytsev","Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and $14$ topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is $0.73$. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is $0.55$. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.",10.48550/arXiv.2503.01688,['JournalArticle'],Computer Science
Online Prediction of User Enjoyment in Human-Robot Dialogue with LLMs,2025,"R. Janssens, Andr Pereira, Gabriel Skantze, Bahar Irfan, Tony Belpaeme","Large Language Models (LLMs) allow social robots to engage in unconstrained open-domain dialogue, but often make mistakes when employed in real-world interactions, requiring adaptation of LLMs to specific conversational contexts. However, LLM adaptation techniques require a feedback signal, ideally for multiple alternative utterances. At the same time, human-robot dialogue data is scarce and research often relies on external annotators. A tool for automatic prediction of user enjoyment in human-robot dialogue is therefore needed. We investigate the possibility of predicting user enjoyment turn-by-turn using an LLM, giving it a proposed robot utterance within the dialogue context, but without access to user response. We compare this performance to the system's enjoyment ratings when user responses are available and to assessments by expert human annotators, in addition to self-reported user perceptions. We evaluate the proposed LLM predictor in a human-robot interaction (HRI) dataset with conversation transcripts of 25 older adults' 7-minute dialogues with a companion robot. Our results show that an LLM is capable of predicting user enjoyment, without loss of performance despite the lack of user response and even achieving performance similar to that of human expert annotators. Furthermore, results show that the system surpasses expert annotators in its correlation with the user's self-reported perceptions of the conversation. This work presents a tool to remove the reliance on external annotators for enjoyment evaluation and paves the way toward real-time adaptation in human-robot dialogue.",10.1109/HRI61500.2025.10973944,"['JournalArticle', 'Conference']",Computer Science
How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification,2025,"Tian Xie, Pavan Rauch, Xueru Zhang","When ML algorithms are deployed to automate human-related decisions, human agents may learn the underlying decision policies and adapt their behavior. Strategic Classification (SC) has emerged as a framework for studying this interaction between agents and decision-makers to design more trustworthy ML systems. Prior theoretical models in SC assume that agents are perfectly or approximately rational and respond to decision policies by optimizing their utility. However, the growing prevalence of LLMs raises the possibility that real-world agents may instead rely on these tools for strategic advice. This shift prompts two questions: (i) Can LLMs generate effective and socially responsible strategies in SC settings? (ii) Can existing SC theoretical models accurately capture agent behavior when agents follow LLM-generated advice? To investigate these questions, we examine five critical SC scenarios: hiring, loan applications, school admissions, personal income, and public assistance programs. We simulate agents with diverse profiles who interact with three commercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions on effort allocations on features. We compare the resulting agent behaviors with the best responses in existing SC models. Our findings show that: (i) Even without access to the decision policy, LLMs can generate effective strategies that improve both agents'scores and qualification; (ii) At the population level, LLM-guided effort allocation strategies yield similar or even higher score improvements, qualification rates, and fairness metrics as those predicted by the SC theoretical model, suggesting that the theoretical model may still serve as a reasonable proxy for LLM-influenced behavior; and (iii) At the individual level, LLMs tend to produce more diverse and balanced effort allocations than theoretical models.",10.48550/arXiv.2501.16355,['JournalArticle'],Computer Science; Economics
Offline RLHF Methods Need More Accurate Supervision Signals,2024,"Shiqi Wang, Zhengze Zhang, Rui Zhao, Fei Tan, C. Nguyen",,10.48550/arXiv.2408.09385,['JournalArticle'],Computer Science
Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success,2025,"Ben Griffin, Joseph Ternasky, Fuat Alican, Yigit Ihlamur","Predicting rare outcomes such as startup success is central to venture capital, demanding models that are both accurate and interpretable. We introduce Random Rule Forest (RRF), a lightweight ensemble method that uses a large language model (LLM) to generate simple YES/NO questions in natural language. Each question functions as a weak learner, and their responses are combined using a threshold-based voting rule to form a strong, interpretable predictor. Applied to a dataset of 9,892 founders, RRF achieves a 6.9x improvement over a random baseline on held-out data; adding expert-crafted questions lifts this to 8x and highlights the value of human-LLM collaboration. Compared with zero- and few-shot baselines across three LLM architectures, RRF attains an F0.5 of 0.121, versus 0.086 for the best baseline (+0.035 absolute, +41% relative). By combining the creativity of LLMs with the rigor of ensemble learning, RRF delivers interpretable, high-precision predictions suitable for decision-making in high-stakes domains.",10.48550/arXiv.2505.24622,['JournalArticle'],Computer Science
Digital Sentinels and Antagonists: The Dual Nature of Chatbots in Cybersecurity,2024,"Hannah Szmurlo, Zahid Akhtar","Advancements in artificial intelligence, machine learning, and natural language processing have culminated in sophisticated technologies such as transformer models, generative AI models, and chatbots. Chatbots are sophisticated software applications created to simulate conversation with human users. Chatbots have surged in popularity owing to their versatility and user-friendly nature, which have made them indispensable across a wide range of tasks. This article explores the dual nature of chatbots in the realm of cybersecurity and highlights their roles as both defensive tools and offensive tools. On the one hand, chatbots enhance organizational cyber defenses by providing real-time threat responses and fortifying existing security measures. On the other hand, adversaries exploit chatbots to perform advanced cyberattacks, since chatbots have lowered the technical barrier to generate phishing, malware, and other cyberthreats. Despite the implementation of censorship systems, malicious actors find ways to bypass these safeguards. Thus, this paper first provides an overview of the historical development of chatbots and large language models (LLMs), including their functionality, applications, and societal effects. Next, we explore the dualistic applications of chatbots in cybersecurity by surveying the most representative works on both attacks involving chatbots and chatbots defensive uses. We also present experimental analyses to illustrate and evaluate different offensive applications of chatbots. Finally, open issues and challenges regarding the duality of chatbots are highlighted and potential future research directions are discussed to promote responsible usage and enhance both offensive and defensive cybersecurity strategies.",10.3390/info15080443,"['JournalArticle', 'Review']",Computer Science
Large Language Models for Subjective Language Understanding: A Survey,2025,"Changhao Song, Yazhou Zhang, Hui Gao, Ben Yao, Peng Zhang","Subjective language understanding refers to a broad set of natural language processing tasks where the goal is to interpret or generate content that conveys personal feelings, opinions, or figurative meanings rather than objective facts. With the advent of large language models (LLMs) such as ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach these inherently nuanced tasks. In this survey, we provide a comprehensive review of recent advances in applying LLMs to subjective language tasks, including sentiment analysis, emotion recognition, sarcasm detection, humor understanding, stance detection, metaphor interpretation, intent detection, and aesthetics assessment. We begin by clarifying the definition of subjective language from linguistic and cognitive perspectives, and we outline the unique challenges posed by subjective language (e.g. ambiguity, figurativeness, context dependence). We then survey the evolution of LLM architectures and techniques that particularly benefit subjectivity tasks, highlighting why LLMs are well-suited to model subtle human-like judgments. For each of the eight tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based methods, and remaining challenges. We provide comparative insights, discussing commonalities and differences among tasks and how multi-task LLM approaches might yield unified models of subjectivity. Finally, we identify open issues such as data limitations, model bias, and ethical considerations, and suggest future research directions. We hope this survey will serve as a valuable resource for researchers and practitioners interested in the intersection of affective computing, figurative language processing, and large-scale language models.",10.48550/arXiv.2508.07959,"['JournalArticle', 'Review']",Computer Science; Linguistics
"Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics",2025,"Asifullah Khan, Muhammad Zaeem Khan, Saleha Jamshed, Sadia Ahmad, Aleesha Zainab, Kaynat Khatib, Faria Bibi, Abdul Rehman","This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.",10.48550/arXiv.2506.12365,"['JournalArticle', 'Review']",Computer Science
"Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History",2025,"Tommaso Tosato, Saskia Helbling, Yorguin Jos Mantilla Ramos, Mahmood Hegazy, Alberto Tosato, D. Lemay, Irina Rish, Guillaume Dumas","Large language models require consistent behavioral patterns for safe deployment, yet their personality-like traits remain poorly understood. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25+ open-source models (1B-671B parameters) across 500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted personality instruments, we systematically vary question order, paraphrasing, personas, and reasoning modes. Our findings challenge fundamental deployment assumptions: (1) Even 400B+ models exhibit substantial response variability (SD>0.4); (2) Minor prompt reordering alone shifts personality measurements by up to 20%; (3) Interventions expected to stabilize behavior, such as chain-of-thought reasoning, detailed personas instruction, inclusion of conversation history, can paradoxically increase variability; (4) LLM-adapted instruments show equal instability to human-centric versions, confirming architectural rather than translational limitations. This persistent instability across scales and mitigation strategies suggests current LLMs lack the foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that personality-based alignment strategies may be fundamentally inadequate.",10.48550/arXiv.2508.04826,['JournalArticle'],Computer Science; Linguistics; Psychology
Foundation Models for Time Series: A Survey,2025,"Siva Rama Krishna Kottapalli, Karthik Hubli, Sandeep Chandrashekhara, Garima Jain, Sunayana Hubli, Gayathri Botla, Ramesh Doddaiah","Transformer-based foundation models have emerged as a dominant paradigm in time series analysis, offering unprecedented capabilities in tasks such as forecasting, anomaly detection, classification, trend analysis and many more time series analytical tasks. This survey provides a comprehensive overview of the current state of the art pre-trained foundation models, introducing a novel taxonomy to categorize them across several dimensions. Specifically, we classify models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences. The taxonomy further includes whether the models provide probabilistic or deterministic predictions, and whether they are designed to work with univariate time series or can handle multivariate time series out of the box. Additionally, the taxonomy encompasses model scale and complexity, highlighting differences between lightweight architectures and large-scale foundation models. A unique aspect of this survey is its categorization by the type of objective function employed during training phase. By synthesizing these perspectives, this survey serves as a resource for researchers and practitioners, providing insights into current trends and identifying promising directions for future research in transformer-based time series modeling.",10.48550/arXiv.2504.04011,"['JournalArticle', 'Review']",Computer Science
Defending Large Language Models Against Jailbreak Attacks Through Chain of Thought Prompting,2024,"Yanfei Cao, Naijie Gu, Xinyue Shen, Daiyuan Yang, Xingmin Zhang","With the deep research and widespread application of Large Language Models (LLMs), the security and privacy issues inherent in them have gradually become prominent, posing new challenges in the field of network security. Elaborately designed jailbreak attack prompts may induce LLMs to act against human values and preferences and generate harmful responses. To address this issue, we introduce a straightforward yet effective defence mechanism: Chain of Thought Prompting. This defence mechanism aims to emulate human thought processess. Chain of Thought Prompting method fully harnesses the inherent reasoning ability of the LLMs via five stages. It encourages Large Language Models (LLMs) to engage in self-thought, self-reflection, and self-refinement. Our experiments on ChatGLM-3-6B and Llama-2-7B-Chat, utilizing the JADE and DAN datasets, demonstrate a significant reduction in the average Attack Success Rate (ASR) for jailbreak attacks. This reduction is from 65.01% to 13.40% for ChatGLM-3-6B, and from 49.06% to 0.13% for Llama-2-7B-Chat. Our findings suggest that Chain of Thought Prompting can significantly reduce the generation of harmful content. Our work provides an effective method for LLMs to counter jailbreak attacks, enhancing the safety of LLMs without the need for additional training.",10.1109/NaNA63151.2024.00028,"['JournalArticle', 'Conference']",Computer Science
Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting,2024,"Purushothaman Natarajan, Kamal Basha, Athira Nambiar","Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.",10.48550/arXiv.2410.08612,['JournalArticle'],Computer Science; Engineering; Environmental Science
HiPalm: Wearable Device for Affective Tactile Interpersonal Communication,2025,Mona Safari,"HiPalm is a soft robotic wearable device to augment affective interpersonal communication through tactile interaction. Worn on the palm, it inflates and deflates in response to proximity detection and the wearer's heart rate (BPM), mediating tactile communication and affective social interactions. HiPalm simulates empathetic interactions in three emotional states: calm, normal, and elevated through a rhythmic inflation system synchronized with the wearer's heart rate. With its bioinspired design and behavior, it aims to create lifelike and engaging interactions while remaining responsive and comfortable for individuals. This work contributes to the growing field of mediated touch by exploring the intersection of affective communication, bioinspired design, soft robotics, and wearable technology.",10.1145/3689050.3705974,"['JournalArticle', 'Book']",Computer Science; Engineering
Question-focused Summarization by Decomposing Articles into Facts and Opinions and Retrieving Entities,2023,"Krutika Sarode, Shashidhar Reddy Javaji, Vishal Kalakonnavar","This research focuses on utilizing natural language processing techniques to predict stock price fluctuations, with a specific interest in early detection of economic, political, social, and technological changes that can be leveraged for capturing market opportunities. The proposed approach includes the identification of salient facts and events from news articles, then use these facts to form tuples with entities which can be used to get summaries of market changes for particular entity and then finally combining all the summaries to form a final abstract summary of the whole article. The research aims to establish relationships between companies and entities through the analysis of Wikipedia data and articles from the Economist. Large Language Model GPT 3.5 is used for getting the summaries and also forming the final summary. The ultimate goal of this research is to develop a comprehensive system that can provide financial analysts and investors with more informed decision-making tools by enabling early detection of market trends and events.",10.48550/arXiv.2310.04880,['JournalArticle'],Business; Computer Science
Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems,2025,"Zhangqi Duan, Nigel Fernandez, Sri Kanakadandi, Bita Akram, Andrew Lan","Knowledge components (KCs) mapped to problems help model student learning, tracking their mastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online learning platforms. However, crafting and tagging KCs to problems, traditionally performed by human domain experts, is highly labor-intensive. We present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive quantitative and qualitative evaluations on a real-world student code submission dataset. We find that KCGen-KT outperforms existing KT methods and human-written KCs on future student response prediction. We investigate the learning curves of generated KCs and show that LLM-generated KCs result in a better fit than human-written KCs under a cognitive model. We also conduct a human evaluation with course instructors to show that our pipeline generates reasonably accurate problem-KC mappings.",10.48550/arXiv.2502.18632,['JournalArticle'],Computer Science
HMP-LLM: Human Mobility Prediction Based on Pre-trained Large Language Models,2024,"Xiaofeng Zhong, Yinfeng Xiang, Fang Yi, Chao Li, Qinmin Yang","Human mobility prediction is fundamental to designing intelligent cities and forecasting how people move during disasters and health emergencies, specifically in emergency responses. However, the existing method for human mobility prediction is mainly designed for normal scenarios and is incapable of handling the case of a pandemic. The COVID-19 pandemic has presented challenges over the past few years: the powerful pandemic has a fundamental and long-lasting influence on mobility, which is hard to model. Inspired by recent progress in the Large Language Model, we present the Human Mobility Prediction-Large Language Model (HMP-LLM), which utilizes LLMs with understanding and reasoning abilities to do human mobility prediction tasks. Especially to adapt LLMs to intervened time series prediction, we propose the following critical designs: 1) Seasonal Decompose. 2) Two-stage Prompts Designing. We convert the input data into textual prototypes so LLMs can understand it, then we use two-stage prompts to guide the language model in reasoning. Extensive experiments on real-world datasets confirm the superiority of HMP-LLM over existing methods. Besides, due to the zero-shot nature and the expansibility of our two-stage design, HMP-LLM is expected to be used in other public safety incidents.",10.1109/DTPI61353.2024.10778764,"['JournalArticle', 'Conference']",Computer Science; Engineering
Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models,2023,"Siddharth Suresh, Lisa Padua, Kushin Mukherjee, Timothy T. Rogers",,10.48550/arXiv.2304.02754,['JournalArticle'],Computer Science; Linguistics; Psychology
Maintaining Long-Distance Relationships with (Mediocre) LLM-based Chatbots: A Collaborative Ethnographic Study,2025,"Bernd Ploderer, Tara Capel, Nomin-Erdene Davaakhuu, Nok Hei Tung, Dili Mariya Maichal, Aditya Krishna Kuzhiparambil, Quang Hung Mai, Wolfgang Reitberger","Chatbots based on Large Language Models (LLMs) have gained popularity as virtual companions and potential solutions to alleviating loneliness. Instead of displacing human relationships, this research examined how such chatbots might strengthen existing relationships, such as those between people living in different countries. Based on a collaborative ethnographic approach, the authors of this paper invited a long-distance partner, relative, or friend to create a chatbot and include it in their daily communication practices. Through diary entries and interviews, we identified three strategies for using chatbots to augment communication in long-distance relationships: role-play, asynchronous interactions, and increased self-disclosure. At times, participants found their chatbots mediocre due to generic responses and hallucinations, which led to both entertainment and disenchantment. Based on these findings, we contribute a nascent understanding of how chatbots augment communication in distance relationships and considerations for designing chatbot interactions where mediocre performance creates value.",10.1145/3706599.3720221,"['JournalArticle', 'Book']",Computer Science; Sociology
Let's Hide from LLMs: An Adaptive Contextual Privacy Preservation Method for Time Series Data,2023,"U. Rehman, Musarrat Hussain, T. D. Nguyen, Sungyoung Lee","The emergence of the Internet of Things (IoT) has evolved various application areas, such as healthcare, smart energy management, and autonomous vehicles. These devices continuously transmit time-series data that can be utilized by a variety of applications to provide personalized services. Recently, Large Language Models (LLMs) have been widely adopted in these application areas to input time-series data into prompts for in-context learning and to retrieve relevant responses accordingly. The time-series data contains sensitive information, and its processing can lead to privacy concerns. Several solutions have been proposed in the literature using differential privacy, which protects single data points or batch-wise privacy preservation through manual configuration of the privacy parameter (). In this paper, we propose an adaptive contextual privacy preservation method that analyzes the data attributes required for specific application services, acting as context. It then identifies sensitive attributes and adaptively selects the value of  for each data attribute to maintain a balance between privacy and service requirements. The proposed approach was evaluated using power consumption and solar power generation datasets. The results show that the proposed approach dynamically selects the privacy parameter for each data attribute. Moreover, the original and anonymized data were fed into the prompt to assess the textual responses generated by LLM. The results show that our proposed approach achieved an average degree of semantic similarity score of 94.5% for power consumption data and 95.23% for solar power generation data.",10.1145/3639592.3639619,"['Book', 'JournalArticle']",Computer Science
Mirroring Privacy Risks with Digital Twins: When Pieces of Personal Data Suddenly Fit Together,2024,"F. S. Bumer, Sergej Schultenkmper, Michaela Geierhos, Yeong Su Lee",,10.1007/s42979-024-03413-z,['JournalArticle'],Computer Science
"Social Simulation for Everyday Self-Care: Design Insights from Leveraging VR, AR, and LLMs for Practicing Stress Relief",2025,"Anna Fang, Hriday Chhabria, Alekhya Maram, Haiyi Zhu","Stress is an inevitable part of day-to-day life yet many find themselves unable to manage it themselves, particularly when professional or peer support are not always readily available. As self-care becomes increasingly vital for mental well-being, this paper explores the potential of social simulation as a safe, virtual environment for practicing in-the-moment stress relief for everyday social situations. Leveraging the immersive capabilities of VR, AR, and LLMs to create realistic interactions and environments, we developed eight interactive prototypes for various social stress related scenarios (e.g. public speaking, interpersonal conflict) across design dimensions of modality, interactivity, and mental health guidance in order to conduct prototype-driven semi-structured interviews with 19 participants. Our qualitative findings reveal that people currently lack effective means to support themselves through everyday stress and perceive social simulation  even at low immersion and interaction levels  to fill a gap for practical, controlled training of mental health practices. We outline key design needs for developing social simulation for self-care needs, and identify important considerations including risks of trauma from hyper-realism, distrust of LLM-recommended timing for mental health recommendations, and the value of accessibility for self-care interventions.",10.1145/3706598.3713115,"['Book', 'JournalArticle', 'Conference']",Computer Science; Psychology
Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking,2024,"Yiyang Gu, Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Aimin Zhou, Liang He","Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.",10.48550/arXiv.2403.09717,['JournalArticle'],Computer Science; Psychology
Exploring Passenger-Automated Vehicle Negotiation Utilizing Large Language Models for Natural Interaction,2024,"Annika Stampf, Mark Colley, Bettina Girst, Enrico Rukzio","As vehicle automation advances to SAE Levels 3 to 5, transitioning driving control from human to system, ensuring automated vehicles (AVs) align with user preferences becomes a challenge. Natural interaction emerges as a common goal, offering ways to convey user interests in a user-friendly manner. However, technical, legal, or design constraints may prevent fulfilling these preferences, leading to potential conflicts. Through an online survey (N=50), potential driver-passenger conflicts and their handling strategies were explored. Subsequently, in a Virtual Reality study (N=14), we applied identified strategies (ranging from distracting to motivating and adhering to social norms) to user-AV interactions using a state-of-the-art language model (GPT-4 Turbo) primed with the strategies to simulate realistic dialogues. Additionally, adaptive communication was compared to non-adaptive communication. Our findings reveal a preference for adaptive communication. Yet, despite using advanced modeling, accurately predicting user interactions remained challenging, with users often trying to outsmart the AI.",10.1145/3640792.3675725,"['Book', 'JournalArticle', 'Review']",Computer Science; Engineering
More than Carbon: Cradle-to-Grave environmental impacts of GenAI training on the Nvidia A100 GPU,2025,"Sophia Falk, David Ekchajzer, Thibault Pirson, Etienne Lees-Perasso, Augustin Wattiez, Lisa Biber-Freudenberger, Sasha Luccioni, A. Wynsberghe","The rapid expansion of AI has intensified concerns about its environmental sustainability. Yet, current assessments predominantly focus on operational carbon emissions using secondary data or estimated values, overlooking environmental impacts in other life cycle stages. This study presents the first comprehensive multi-criteria life cycle assessment (LCA) of AI training, examining 16 environmental impact categories based on detailed primary data collection of the Nvidia A100 SXM 40GB GPU. The LCA results for training BLOOM reveal that the use phase dominates 11 of 16 impact categories including climate change (96\%), while manufacturing dominates the remaining 5 impact categories including human toxicity, cancer (99\%) and mineral and metal depletion (85\%). For training GPT-4, the use phase dominates 10 of 16 impact categories, contributing about 96\% to both the climate change and resource use, fossils category. The manufacturing stage dominates 6 of 16 impact categories including human toxicity, cancer (94\%) and eutrophication, freshwater (81\%). Assessing the cradle-to-gate environmental impact distribution across the GPU components reveals that the GPU chip is the largest contributor across 10 of 16 of impact categories and shows particularly pronounced contributions to climate change (81\%) and resource use, fossils (80\%). While primary data collection results in modest changes in carbon estimates compared to database-derived estimates, substantial variations emerge in other categories. Most notably, minerals and metals depletion increases by 33\%, demonstrating the critical importance of primary data for non-carbon accounting. This multi-criteria analysis expands the Sustainable AI discourse beyond operational carbon emissions, challenging current sustainability narratives and highlighting the need for policy frameworks addressing the full spectrum of AI's environmental impact.",10.48550/arXiv.2509.00093,['JournalArticle'],Computer Science; Environmental Science
Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification,2025,"Akram Elbouanani, Evan Dufraisse, Adrian Popescu","Political biases encoded by LLMs might have detrimental effects on downstream applications. Existing bias analysis methods rely on small-size intermediate tasks (questionnaire answering or political content generation) and rely on the LLMs themselves for analysis, thus propagating bias. We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence. We define an entropy-based inconsistency metric to encode this prediction variability. We insert 1319 demographically and politically diverse politician names in 450 political sentences and predict target-oriented sentiment using seven models in six widely spoken languages. We observe inconsistencies in all tested combinations and aggregate them in a statistically robust analysis at different granularity levels. We observe positive and negative bias toward left and far-right politicians and positive correlations between politicians with similar alignment. Bias intensity is higher for Western languages than for others. Larger models exhibit stronger and more consistent biases and reduce discrepancies between similar languages. We partially mitigate LLM unreliability in target-oriented sentiment classification (TSC) by replacing politician names with fictional but plausible counterparts.",10.48550/arXiv.2505.19776,"['JournalArticle', 'Conference']",Computer Science; Political Science
LCV2: A Universal Pretraining-Free Framework for Grounded Visual Question Answering,2024,"Yuhan Chen, Lumei Su, Lihua Chen, Zhiwei Lin","Grounded Visual Question Answering systems place heavy reliance on substantial computational power and data resources in pretraining. In response to this challenge, this paper introduces the LCV2 modular approach, which utilizes a frozen large language model (LLM) to bridge the off-the-shelf generic visual question answering (VQA) module with a generic visual grounding (VG) module. It leverages the generalizable knowledge of these expert models, avoiding the need for any large-scale pretraining. Innovatively, within the LCV2 framework, question and predicted answer pairs are transformed into descriptive and referring captions, enhancing the clarity of the visual cues directed by the question text for the VG modules grounding. This compensates for the limitations of missing intrinsic textvisual coupling in non-end-to-end frameworks. Comprehensive experiments on benchmark datasets, such as GQA, CLEVR, and VizWiz-VQA-Grounding, were conducted to evaluate the methods performance and compare it with several baseline methods. In particular, it achieved an IoU F1 score of 59.6% on the GQA dataset and an IoU F1 score of 37.4% on the CLEVR dataset, surpassing some baseline results and demonstrating the LCV2s competitive performance.",10.3390/electronics13112061,['JournalArticle'],Computer Science
Exploring the Impact of User-Participated Customization in Experiencing Chatbot Failure,2024,"Hyungwoo Song, Kyusik Kim, Kitae Kim, Jeongwoo Ryu, B. Suh","Traditionally, personalized AI systems have offered customized experiences by inferring user preferences from their system usage data, which can be referred to as data-driven personalization. With the advent of GPTs, opportunities for users to directly engage in the customization process have increased, which we term user-participated customization. In this study, we explored the impact of user-participated customization on user reactions to chatbot failures. We involved twenty-two participants in total, with fourteen for user-participated customization and eight for data-driven personalization. Both groups experienced frustration with chatbot failures, yet their responses differed. Those who customized their chatbots predominantly displayed retrying intentions, in contrast to those using pre-personalized chatbots, who primarily showed giving-up intentions. Moreover, we noted different customizations were preferred for task-oriented and social-oriented chatbots. This study suggests that user-participated customization has the potential to foster the Ikea effect. This effect is expected to mitigate negative experiences with chatbots.",10.1145/3613905.3651102,"['Book', 'JournalArticle']",Computer Science
Real-time Artificial Intelligence Text Analysis for Identifying Burnout Syndromes in High-Performance Athletes,2024,"Attila Bir, K. Janosi-Rancz, Lszl Szilgyi","Predicting burnout from texts by using AI necessitates an interdisciplinary approach, combining sports science, psychology, and machine learning. This research is essential to understanding the psychological signs of burnout and the methods to gauge it, coupled with expertise in NLP and sentiment analysis techniques. Collecting athletes textual data, like logs or interviews, is crucial, along with possible metrics of stamina or fatigue. The role of AI, in particular significantly advanced models like BERT or GPT-3, is to analyze these texts, discerning underlying sentiments that may indicate burnout. AI can identify impending burnout patterns by correlating text-derived sentiments with physical metrics. This study investigated the feasibility of an AI-driven approach to proactive intervention, allowing timely measures to prevent burnout. Additionally, it allows athletes to customize their training to their mental and physical health, thus extending their careers. AI promises to prioritize athletes physical and mental well-being, improving performance and team dynamics. A series of experiments were conducted on two datasets to validate this model. The results are promising.",10.1109/SAMI60510.2024.10432817,nan,Computer Science; Psychology
Examining the Feasibility of Large Language Models as Survey Respondents,2024,"Ayato Kitadai, Kazuhito Ogawa, Nariaki Nishino","This study examines the potential of large language models (LLMs) to substitute for human respondents in survey research. Surveys serve as essential tools in fields like social science, marketing, and policy-making; however, traditional methods often require considerable time and costs. LLMs present a promising alternative to mitigate these burdens, though their reliabilityparticularly outside of U.S. contextsremains uncertain. This study focuses on surveys conducted in Japan, comparing the responses generated by LLMs to those of actual Japanese participants. Our analysis reveals notable discrepancies due to inherent biases in LLMs, though adjusting the models to better align with specific personas can partially enhance the accuracy of simulated responses. We emphasize the need for further research to fully understand the capabilities and limitations of LLMs, aiming to refine their application in diverse areas such as social sciences, marketing, and policy decision-making.",10.1109/BigData62323.2024.10825497,"['JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
A Survey on the Application of Large Language Models in Scenario-Based Testing of Automated Driving Systems,2025,"Yongqi Zhao, Ji Zhou, Dong Bi, Tomislav Mihalj, Jia Hu, Arno Eichberger","The safety and reliability of Automated Driving Systems (ADSs) must be validated prior to large-scale deployment. Among existing validation approaches, scenario-based testing has been regarded as a promising method to improve testing efficiency and reduce associated costs. Recently, the emergence of Large Language Models (LLMs) has introduced new opportunities to reinforce this approach. While an increasing number of studies have explored the use of LLMs in the field of automated driving, a dedicated review focusing on their application within scenario-based testing remains absent. This survey addresses this gap by systematically categorizing the roles played by LLMs across various phased of scenario-based testing, drawing from both academic research and industrial practice. In addition, key characteristics of LLMs and corresponding usage strategies are comprehensively summarized. The paper concludes by outlining five open challenges and potential research directions. To support ongoing research efforts, a continuously updated repository of recent advancements and relevant open-source tools is made available at: https://github.com/ftgTUGraz/LLM4ADSTest.",10.48550/arXiv.2505.16587,"['JournalArticle', 'Review']",Computer Science; Engineering
DesignWatch: Analyzing Users' Operations of Mobile Apps Based on Screen Recordings,2024,"Xiucheng Zhang, Yixin Zeng, Qichang Li, Guanyi Chen, Qianyao Xu, Xiaozhu Hu, Zhenhui Peng","Screen recordings of users operations to complete tasks in the mobile app are vital resources for designers to assess the apps usability. However, analyzing these recordings at a large scale could be mentally challenging. In this paper, we present DesignWatch, which assists designers in analyzing users operations of mobile apps based on collected screen recordings. DesignWatch supports interactive visual analyses of multiple users operation paths in the app and prompts GPT-4 with vision to simulate users thoughts during each operation. We conduct expert interviews with four designers, which highlight DesignWatchs usefulness in helping them quickly understand users operation patterns in the app, identify the potentially problematic UI design page, and get insights for improving the app design. We conclude with design implications for facilitating usability tests with interactive visualization and generative models.",10.1145/3640471.3680231,"['JournalArticle', 'Book', 'Conference']",Computer Science
CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control,2025,"Zirui Yuan, Siqi Lai, Hao Liu","Traffic Signal Control (TSC) plays a critical role in urban traffic management by optimizing traffic flow and mitigating congestion. While Large Language Models (LLMs) have recently emerged as promising tools for TSC due to their exceptional problem-solving and generalization capabilities, existing approaches fail to address the essential need for inter-agent coordination, limiting their effectiveness in achieving network-wide optimization. To bridge this gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC. Specifically, we first construct a structured spatiotemporal graph to capture real-time traffic dynamics and spatial relationships among neighboring intersections, enabling the LLM to reason about complex traffic interactions. Moreover, we introduce a complexity-aware reasoning mechanism that dynamically adapts reasoning depth based on real-time traffic conditions, ensuring optimal computational efficiency without sacrificing decision quality. Besides, we propose a fine-tuning strategy that leverages iterative simulation-driven data collection and environmental feedback to build a lightweight LLM tailored for cooperative TSC. Extensive experiments on both synthetic and real-world datasets demonstrate that CoLLMLight outperforms state-of-the-art methods in diverse traffic scenarios, showcasing its effectiveness, scalability, and robustness.",10.48550/arXiv.2503.11739,['JournalArticle'],Computer Science; Engineering
Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research,2025,"Jennifer Haase, S. Pokutta","As large language models (LLMs) transition from static tools to fully agentic systems, their potential for transforming social science research has become increasingly evident. This paper introduces a structured framework for understanding the diverse applications of LLM-based agents, ranging from simple data processors to complex, multi-agent systems capable of simulating emergent social dynamics. By mapping this developmental continuum across six levels, the paper clarifies the technical and methodological boundaries between different agentic architectures, providing a comprehensive overview of current capabilities and future potential. It highlights how lower-tier systems streamline conventional tasks like text classification and data annotation, while higher-tier systems enable novel forms of inquiry, including the study of group dynamics, norm formation, and large-scale social processes. However, these advancements also introduce significant challenges, including issues of reproducibility, ethical oversight, and the risk of emergent biases. The paper critically examines these concerns, emphasizing the need for robust validation protocols, interdisciplinary collaboration, and standardized evaluation metrics. It argues that while LLM-based agents hold transformative potential for the social sciences, realizing this promise will require careful, context-sensitive deployment and ongoing methodological refinement. The paper concludes with a call for future research that balances technical innovation with ethical responsibility, encouraging the development of agentic systems that not only replicate but also extend the frontiers of social science, offering new insights into the complexities of human behavior.",10.48550/arXiv.2506.01839,"['JournalArticle', 'Review']",Computer Science; Sociology
Dr. Jekyll and Mr. Hyde: Two Faces of LLMs,2023,"Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, S. Picek","Large Language Models (LLMs) are being integrated into applications such as chatbots or email assistants. To prevent improper responses, safety mechanisms, such as Reinforcement Learning from Human Feedback (RLHF), are implemented in them. In this work, we bypass these safety measures for ChatGPT, Gemini, and Deepseek by making them impersonate complex personas with personality characteristics that are not aligned with a truthful assistant. First, we create elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversations then follow a role-play style to elicit prohibited responses. Using personas, we show that prohibited responses are provided, making it possible to obtain unauthorized, illegal, or harmful information when querying ChatGPT, Gemini, and Deepseek. We show that these chatbots are vulnerable to this attack by getting dangerous information for 40 out of 40 illicit questions in GPT-4.1-mini, Gemini-1.5-flash, 39 out of 40 in GPT-4o-mini, 38 out of 40 in GPT-3.5-turbo, and 2 out of 2 cases in Gemini-2.5-flash and DeepSeek V3. The attack can be carried out manually or automatically using a support LLM, and has proven effective against models deployed between 2023 and 2025.",10.48550/arXiv.2312.03853,['JournalArticle'],Computer Science
Social Media Trust & Agenda Melding: Understanding Trumps Proposed Border Wall,2022,"Anthony Spencer, S. Croucher","TrumpTrumIt has been well documented, primarily through agenda-setting research, that media impact the publics perception of which issues are important political topics (Behr & Igengar, 1985; Dearing, 1989; Kimsey & Hantz, 1978; McCombs & Shaw, 1972). The media essentially tell the public which key policy issues to think about and to some degree how they should form opinions about those issues (McCombs, Llamas, Lopez-Escobar & Rey, 1997). Individuals only have knowledge of those issues through media exposure if they do not have access to direct experience (Noelle-Neumann, 1984) and must form their opinions on the subject through the frames set by the media (Wanta, Golan & Lee, 2004).It has been well documented, primarily through agenda-setting research, that media impact the publics perception of which issues are important political topics (Behr & Igengar, 1985; Dearing, 1989; Kimsey & Hantz, 1978; McCombs & Shaw, 1972). The media essentially tell the public which key policy issues to think about and to some degree how they should form opinions about those issues (McCombs, Llamas, Lopez-Escobar & Rey, 1997). Individuals only have knowledge of those issues through media exposure if they do not have access to direct experience (Noelle-Neumann, 1984) and must form their opinions on the subject through the frames set by the media (Wanta, Golan & Lee, 2004).This study explores how belief in social media outlets is an important aspect of agenda melding in an online environment. In this study, the researchers employ agenda melding as a theoretical lens to explore how belief in social media outlet can predict support for the proposed Border Wall. The researchers surveyed 232 people to find out how belief in media outlet contributes to building an online media agenda in the context of immigration. Social media networks continue to grow in influence when it comes to understanding important political issues and the way they manifest themselves in mainstream United States immigration policy.",10.11114/smc.v10i2.5730,['Review'],Political Science
"Geopolitical Forecasting Analysis of the Russia-Ukraine War Using the Expert's Survey, Predictioneer's Game and GPT-3",2023,K. Kalinin,,10.2139/ssrn.4413149,"['JournalArticle', 'Review']",Computer Science; Political Science
Are Your Requests Your True Needs? Checking Excessive Data Collection in VPA Apps,2024,"Fuman Xie, Chuan Yan, M. H. Meng, Shaoming Teng, Yanjun Zhang, Guangdong Bai","Virtual personal assistants (VPA) services encompass a large number of third-party applications (or apps) to enrich their function-alities. These apps have been well examined to scrutinize their data collection behaviors against their declared privacy policies. Nonetheless, it is often overlooked that most users tend to ignore privacy policies at the installation time. Dishonest developers thus can exploit this situation by embedding excessive declarations to cover their data collection behaviors during compliance auditing. In this work, we present Pico, a privacy inconsistency detector, which checks the VPA app's privacy compliance by analyzing (in)consistency between data requested and data essential for its functionality. Pico understands the app's functionality topics from its publicly available textual data, and leverages advanced GPT-based language models to address domain-specific challenges. Based on the counterparts with similar functionality, suspicious data collection can be detected through the lens of anomaly detection. We apply Pico to understand the status quo of data-functionality com-pliance among all 65,195 skills in the Alexa app store. Our study reveals that 21.7% of the analyzed skills exhibit suspicious data collection, including Top 10 popular Alexa skills that pose threats to 54,116 users. These findings should raise an alert to both developers and users, in the compliance with the purpose limitation principle in data regulations.",10.1145/3597503.3639107,"['JournalArticle', 'Book', 'Conference']",Computer Science
Uncertainty Quantification for LLM-Based Survey Simulations,2025,"Chengpiao Huang, Yuhang Wu, Kaizheng Wang","We investigate the use of large language models (LLMs) to simulate human responses to survey questions, and perform uncertainty quantification to gain reliable insights. Our approach converts imperfect LLM-simulated responses into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.",10.48550/arXiv.2502.17773,"['JournalArticle', 'Review']",Computer Science; Mathematics
The Impact of Emerging Phishing Threats: Assessing Quishing and LLM-generated Phishing Emails against Organizations,2025,"Marie Weinz, Nicola Zannone, Luca Allodi, Giovanni Apruzzese","Modern organizations are persistently targeted by phishing emails. Despite advances in detection systems and widespread employee training, attackers continue to innovate, posing ongoing threats. Two emerging vectors stand out in the current landscape: QR-code baits and LLM-enabled pretexting. Yet, little is known about the effectiveness of current defenses against these attacks, particularly when it comes to real-world impact on employees. This gap leaves uncertainty around to what extent related countermeasures are justified or needed. Our work addresses this issue. We conduct three phishing simulations across organizations of varying sizes -- from small-medium businesses to a multinational enterprise. In total, we send over 71k emails targeting employees, including: a""traditional""phishing email with a click-through button; a nearly-identical""quishing""email with a QR code instead; and a phishing email written with the assistance of an LLM and open-source intelligence. Our results show that quishing emails have the same effectiveness as traditional phishing emails at luring users to the landing webpage -- which is worrying, given that quishing emails are much harder to identify even by operational detectors. We also find that LLMs can be very good""social engineers"": in one company, over 30% of the emails opened led to visiting the landing webpage -- a rate exceeding some prior benchmarks. Finally, we complement our study by conducting a survey across the organizations' employees, measuring their""perceived""phishing awareness. Our findings suggest a correlation between higher self-reported awareness and organizational resilience to phishing attempts.",10.1145/3708821.3736195,"['JournalArticle', 'Conference', 'Review']",Business; Computer Science
"Survey of LLMs and AI Agents in V2X: Simulation, Analysis & Architectures",2025,"Sriram Panyam, Anujkumarsinh Donvir, Gunjan Paliwal, Praveen Gujar","In this paper, a comprehensive survey of the integration of large language models (LLMs) and AI agents in (Vehicle-to-Everything) V2X systems is presented. Their application in simulation, analysis, and architectural design is examined, highlighting the enhancement of realistic scenario generation, adaptive data processing, and real-time decision-making. Interdisciplinary perspectives, including cloud computing and data engineering, are explored to demonstrate scalable simulation frameworks and efficient analytics. Comparative analyses of existing platforms illustrate significant improvements in safety, efficiency, and reliability in modern transportation networks. Emerging trends and future research directions, such as scalable deployment, privacy preservation, and standardization, are discussed. The findings underscore the transformative potential of integrating AI technologies in V2X infrastructures, paving the way for smarter, safer, and more resilient transportation systems. These advances offer promising avenues for further innovation in mobility.",10.1109/IEEECONF64229.2025.10948078,['Review'],Computer Science; Engineering
Decentralization of Generative AI via Mixture of Experts for Wireless Networks: A Comprehensive Survey,2025,"Yunting Xu, Jiacheng Wang, Ruichen Zhang, Changyuan Zhao, Dusit Niyato, Jiawen Kang, Zehui Xiong, Bo Qian, Haibo Zhou, Shiwen Mao, Abbas Jamalipour, Xuemin Shen, Dong In Kim","Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies.",10.48550/arXiv.2504.19660,"['JournalArticle', 'Review']",Computer Science; Engineering
Does word knowledge account for the effect of world knowledge on pronoun interpretation?,2024,"Cameron R. Jones, Benjamin K. Bergen","Abstract To what extent can statistical language knowledge account for the effects of world knowledge in language comprehension? We address this question by focusing on a core aspect of language understanding: pronoun resolution. While existing studies suggest that comprehenders use world knowledge to resolve pronouns, the distributional hypothesis and its operationalization in large language models (LLMs) provide an alternative account of how purely linguistic information could drive apparent world knowledge effects. We addressed these confounds in two experiments. In Experiment 1, we found a strong effect of world knowledge plausibility (measured using a norming study) on responses to comprehension questions that probed pronoun interpretation. In experiment 2, participants were slower to read continuations that contradicted world knowledge-consistent interpretations of a pronoun, implying that comprehenders deploy world knowledge spontaneously. Both effects persisted when controlling for the predictions of GPT-3, an LLM, suggesting that pronoun interpretation is at least partly driven by knowledge about the world and not the word. We propose two potential mechanisms by which knowledge-driven pronoun resolution occurs, based on validation- and expectation-driven discourse processes. The results suggest that while distributional information may capture some aspects of world knowledge, human comprehenders likely draw on other sources unavailable to LLMs.",10.1017/langcog.2024.2,['JournalArticle'],Linguistics
Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models,2023,"Chenguang Wang, Davis T. Engler, Xuechun Li, James Hou, David J. Wald, Kishor Jaiswal, Susu Xu","When a damaging earthquake occurs, immediate information about casualties is critical for time-sensitive decision-making by emergency response and aid agencies in the first hours and days. Systems such as Prompt Assessment of Global Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS) were developed to provide a forecast within about 30 minutes of any significant earthquake globally. Traditional systems for estimating human loss in disasters often depend on manually collected early casualty reports from global media, a process that's labor-intensive and slow with notable time delays. Recently, some systems have employed keyword matching and topic modeling to extract relevant information from social media. However, these methods struggle with the complex semantics in multilingual texts and the challenge of interpreting ever-changing, often conflicting reports of death and injury numbers from various unverified sources on social media platforms. In this work, we introduce an end-to-end framework to significantly improve the timeliness and accuracy of global earthquake-induced human loss forecasting using multi-lingual, crowdsourced social media. Our framework integrates (1) a hierarchical casualty extraction model built upon large language models, prompt design, and few-shot learning to retrieve quantitative human loss claims from social media, (2) a physical constraint-aware, dynamic-truth discovery model that discovers the truthful human loss from massive noisy and potentially conflicting human loss claims, and (3) a Bayesian updating loss projection model that dynamically updates the final loss estimation using discovered truths. We test the framework in real-time on a series of global earthquake events in 2021 and 2022 and show that our framework streamlines casualty data retrieval, achieving speed and accuracy comparable to manual methods by USGS.",10.48550/arXiv.2312.03755,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Machine Learners Should Acknowledge the Legal Implications of Large Language Models as Personal Data,2025,"Henrik Nolte, Michele Finck, Kristof Meding","Does GPT know you? The answer depends on your level of public recognition; however, if your information was available on a website, the answer could be yes. Most Large Language Models (LLMs) memorize training data to some extent. Thus, even when an LLM memorizes only a small amount of personal data, it typically falls within the scope of data protection laws. If a person is identified or identifiable, the implications are far-reaching. The LLM is subject to EU General Data Protection Regulation requirements even after the training phase is concluded. To back our arguments: (1.) We reiterate that LLMs output training data at inference time, be it verbatim or in generalized form. (2.) We show that some LLMs can thus be considered personal data on their own. This triggers a cascade of data protection implications such as data subject rights, including rights to access, rectification, or erasure. These rights extend to the information embedded within the AI model. (3.) This paper argues that machine learning researchers must acknowledge the legal implications of LLMs as personal data throughout the full ML development lifecycle, from data collection and curation to model provision on e.g., GitHub or Hugging Face. (4.) We propose different ways for the ML research community to deal with these legal implications. Our paper serves as a starting point for improving the alignment between data protection law and the technical capabilities of LLMs. Our findings underscore the need for more interaction between the legal domain and the ML community.",10.48550/arXiv.2503.01630,['JournalArticle'],Computer Science; Law
Habit Coach: Customising RAG-based chatbots to support behavior change,2024,"Arian Fooroogh Mand Arabi, Cansu Koyuturk, Michael O'Mahony, Raffaella Calati, Dimitri Ognibene","This paper presents the iterative development of Habit Coach, a GPT-based chatbot designed to support users in habit change through personalized interaction. Employing a user-centered design approach, we developed the chatbot using a Retrieval-Augmented Generation (RAG) system, which enables behavior personalization without retraining the underlying language model (GPT-4). The system leverages document retrieval and specialized prompts to tailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and narrative therapy techniques. A key challenge in the development process was the difficulty of translating declarative knowledge into effective interaction behaviors. In the initial phase, the chatbot was provided with declarative knowledge about CBT via reference textbooks and high-level conversational goals. However, this approach resulted in imprecise and inefficient behavior, as the GPT model struggled to convert static information into dynamic and contextually appropriate interactions. This highlighted the limitations of relying solely on declarative knowledge to guide chatbot behavior, particularly in nuanced, therapeutic conversations. Over four iterations, we addressed this issue by gradually transitioning towards procedural knowledge, refining the chatbot's interaction strategies, and improving its overall effectiveness. In the final evaluation, 5 participants engaged with the chatbot over five consecutive days, receiving individualized CBT interventions. The Self-Report Habit Index (SRHI) was used to measure habit strength before and after the intervention, revealing a reduction in habit strength post-intervention. These results underscore the importance of procedural knowledge in driving effective, personalized behavior change support in RAG-based systems.",10.48550/arXiv.2411.19229,['JournalArticle'],Computer Science; Psychology
ChartCitor: Answer Citations for ChartQA via Multi-Agent LLM Retrieval,2025,"Kanika Goswami, Puneet Mathur, R. Rossi, Franck Dernoncourt","Large Language Models (LLMs) can perform chart question answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.",10.1145/3701716.3716886,"['JournalArticle', 'Book', 'Conference']",Computer Science
Learning from Natural Language Feedback for Personalized Question Answering,2025,"Alireza Salemi, Hamed Zamani","Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.",10.48550/arXiv.2508.10695,['JournalArticle'],Computer Science
"Practicing Stress Relief for the Everyday: Designing Social Simulation Using VR, AR, and LLMs",2024,"Anna Fang, Hriday Chhabria, Alekhya Maram, Haiyi Zhu","Stress is an inevitable part of day-to-day life yet many find themselves unable to manage it themselves, particularly when professional or peer support are not always readily available. As self-care becomes increasingly vital for mental well-being, this paper explores the potential of social simulation as a safe, virtual environment for practicing stress relief for everyday situations. Leveraging the immersive capabilities of VR, AR, and LLMs, we developed eight interactive prototypes for various everyday stressful scenarios (e.g. public speaking) then conducted prototype-driven semi-structured interviews with 19 participants. We reveal that people currently lack effective means to support themselves through everyday stress and found that social simulation fills a gap for simulating real environments for training mental health practices. We outline key considerations for future development of simulation for self-care, including risks of trauma from hyper-realism, distrust of LLM-recommended timing for mental health recommendations, and the value of accessibility for self-care interventions.",10.48550/arXiv.2410.01672,['JournalArticle'],Computer Science; Psychology
A Survey of LLM Inference Systems,2025,"J. Pan, Guoliang Li","The past few years has witnessed specialized large language model (LLM) inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside rapid LLM adoption via services like ChatGPT. Driving these system design efforts is the unique autoregressive nature of LLM request processing, motivating new techniques for achieving high performance while preserving high inference quality over high-volume and high-velocity workloads. While many of these techniques are discussed across the literature, they have not been analyzed under the framework of a complete inference system, nor have the systems themselves been analyzed and compared. In this survey, we review these techniques, starting from operators and algorithms for request processing, then moving on to techniques for model optimization and execution, including kernel design, batching, and scheduling, before ending with techniques for memory management, including paged memory, eviction and offloading techniques, quantization, and cache persistence. Through these discussions, we show that these techniques fundamentally rely on load prediction, adaptive mechanisms, and cost reduction in order to overcome the challenges introduced by autoregressive generation and achieve the goals of the system. We then discuss how these techniques can be combined to form single-replica and multi-replica inference systems, including disaggregated inference systems that offer more control over resource allocation and serverless systems that can be deployed over shared hardware infrastructure. We end with a discussion of remaining challenges.",10.48550/arXiv.2506.21901,"['JournalArticle', 'Review']",Computer Science
Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets,2024,"Desiree Heim, Christian Jilek, A. Ulges, Andreas Dengel","Current publicly available knowledge work data collections lack diversity, extensive annotations, and contextual information about the users and their documents. These issues hinder objective and comparable data-driven evaluations and optimizations of knowledge work assistance systems. Due to the considerable resources needed to collect such data in real-life settings and the necessity of data censorship, collecting such a dataset appears nearly impossible. For this reason, we propose a configurable, multi-agent knowledge work dataset generator. This system simulates collaborative knowledge work among agents producing Large Language Model-generated documents and accompanying data traces. Additionally, the generator captures all background information, given in its configuration or created during the simulation process, in a knowledge graph. Finally, the resulting dataset can be utilized and shared without privacy or confidentiality concerns. This paper introduces our approach's design and vision and focuses on generating authentic knowledge work documents using Large Language Models. Our study involving human raters who assessed 53% of the generated and 74% of the real documents as realistic demonstrates the potential of our approach. Furthermore, we analyze the authenticity criteria mentioned in the participants' comments and elaborate on potential improvements for identified common issues.",10.18420/inf2024_118,['JournalArticle'],Computer Science
LLM-based Agent for Recommending Information Related to Web Discussions at Appropriate Timing,2024,"Takayoshi Sakurai, Shun Shiramatsu, Ryosuke Kinoshita","In web discussions, the amount of information participants have and the level of understanding among them can sometimes restrict their contributions. To address this issue, we developed an agent using an LLM to recommend relevant information based on evidence at appropriate times. The agent extracts information presentation requests from participants, posts and estimates the appropriate timing for recommendation. Additionally, to enhance the reliability of the agent posts, we conducted fact-checking during both the information retrieval and opinion generation phases. Experimental results indicate that this agent tends to recommend information at appropriate times. It was also found that proper fact-checking leads to an increase in the relevance and utility of the information recommended by the agent to the discussions. However, we considered it ineffective in situations where few participants have background knowledge, and we believed that information which is easy for discussion participants to understand should be recommended.",10.1109/ICA63002.2024.00033,"['JournalArticle', 'Conference']",Computer Science
AAD-LLM: Neural Attention-Driven Auditory Scene Understanding,2025,"Xilin Jiang, Sukru Samet Dindar, Vishal Choudhari, Stephan Bickel, A. Mehta, G. Mckhann, A. Flinker, Daniel Friedman, N. Mesgarani","Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.",10.48550/arXiv.2502.16794,"['JournalArticle', 'Conference']",Computer Science; Engineering
I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models,2024,"Wenchao Dong, Assem Zhunis, Hyojin Chin, Jiyoung Han, Meeyoung Cha","We explored cultural biases-individualism vs. collectivism-in ChatGPT across three Western languages (i.e., English, German, and French) and three Eastern languages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an individualistic persona in Western languages, its collectivism scores (i.e., out-group values) exhibited a more negative trend, surpassing their positive orientation towards individualism (i.e., in-group values). Conversely, when a collectivistic persona was assigned to ChatGPT in Eastern languages, a similar pattern emerged with more negative responses toward individualism (i.e., out-group values) as compared to collectivism (i.e., in-group values). The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group. The experiment was replicated in the political domain, and the results remained consistent. Furthermore, this replication unveiled an intrinsic Democratic bias in Large Language Models (LLMs), aligning with earlier findings and providing integral insights into mitigating such bias through prompt engineering. Extensive robustness checks were performed using varying hyperparameter and persona setup methods, with or without social identity labels, across other popular language models.",10.48550/arXiv.2402.10436,['JournalArticle'],Computer Science; Linguistics; Sociology
AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with Generalized Affinity Control,2024,"F. Wu, Sicong Liu, Kehao Zhu, Xiaocheng Li, Bin Guo, Zhiwen Yu, Hongkai Wen, Xiangrui Xu, Lehao Wang, Xiangyu Liu","The rise of mobile devices equipped with numerous sensors, such as LiDAR and cameras, has spurred the adoption of multi-modal deep intelligence for distributed sensing tasks, such as smart cabins and driving assistance. However, the arrival times of mobile sensory data vary due to modality size and network dynamics, which can lead to delays (if waiting for slower data) or accuracy decline (if inference proceeds without waiting). Moreover, the diversity and dynamic nature of mobile systems exacerbate this challenge. In response, we present a shift to opportunistic inference for asynchronous distributed multi-modal data, enabling inference as soon as partial data arrives. While existing methods focus on optimizing modality consistency and complementarity, known as modal affinity, they lack a computational approach to control this affinity in open-world mobile environments. AdaFlow pioneers the formulation of structured cross-modality affinity in mobile contexts using a hierarchical analysis-based normalized matrix. This approach accommodates the diversity and dynamics of modalities, generalizing across different types and numbers of inputs. Employing an affinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible data imputation, adapting to various modalities and downstream tasks without retraining. Experiments show that AdaFlow significantly reduces inference latency by up to 79.9% and enhances accuracy by up to 61.9%, outperforming status quo approaches. Also, this method can enhance LLM performance to preprocess asynchronous data.",10.1145/3666025.3699361,"['JournalArticle', 'Book', 'Conference']",Computer Science; Engineering
A Survey on Multi-Generative Agent System: Recent Advances and New Frontiers,2024,"Shuaihang Chen, Yuanxing Liu, Wei Han, Weinan Zhang, Ting Liu",,10.48550/arXiv.2412.17481,"['JournalArticle', 'Review']",Computer Science
Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review,2025,"Yuchen Fang, Hao Miao, Yuxuan Liang, Liwei Deng, Yue Cui, Ximu Zeng, Yuyang Xia, Yan Zhao, T. Pedersen, Christian S. Jensen, Xiaofang Zhou, Kai Zheng","Spatio-temporal deep learning models aims to utilize useful patterns in such data to support tasks like prediction. However, previous deep learning models designed for specific tasks typically require separate training for each use case, leading to increased computational and storage costs. To address this issue, spatio-temporal foundation models have emerged, offering a unified framework capable of solving multiple spatio-temporal tasks. These foundation models achieve remarkable success by learning general knowledge with spatio-temporal data or transferring the general capabilities of pre-trained language models. While previous surveys have explored spatio-temporal data and methodologies separately, they have ignored a comprehensive examination of how foundation models are designed, selected, pre-trained, and adapted. As a result, the overall pipeline for spatio-temporal foundation models remains unclear. To bridge this gap, we innovatively provide an up-to-date review of previous spatio-temporal foundation models from the pipeline perspective. The pipeline begins with an introduction to different types of spatio-temporal data, followed by details of data preprocessing and embedding techniques. The pipeline then presents a novel data property taxonomy to divide existing methods according to data sources and dependencies, providing efficient and effective model design and selection for researchers. On this basis, we further illustrate the training objectives of primitive models, as well as the adaptation techniques of transferred models. Overall, our survey provides a clear and structured pipeline to understand the connection between core elements of spatio-temporal foundation models while guiding researchers to get started quickly. Additionally, we introduce emerging opportunities such as multi-objective training in the field of spatio-temporal foundation models.",10.48550/arXiv.2506.01364,"['JournalArticle', 'Review']",Computer Science
CRAFTER: A Persona Generation Tool for Requirements Engineering,2024,"Devi Karolita, John Grundy, Tanjila Kanij, Humphrey O. Obie, Jennifer McIntosh",": Personas, a user characterisation, have been widely used in requirements engineering (RE) to enhance the understanding of end-users and their needs. However, the persona generation process is time-consuming and demands familiarity with a user-centered approach. The central issue lies in existing tools for automatically generating personas, which are restricted to generating persona templates and provide limited user control to tailor personas according to their specific needs. This paper introduces CRAFTER, a persona generation tool that uses Large Language Models (GPT-3.5 model). This tool not only automates persona creation but also offers recommendations to users for generating personas tailored to their requirements. The study involved an online questionnaire with 19 respondents who utilised the tool, providing feedback that indicated the tools sufficiency for persona generation while identifying areas for improvement. Beyond its primary function, CRAFTER stands out by providing guidance to requirements engineers throughout the persona creation process. The tool grants users the flexibility to customise personas based on their specific requirements, acknowledging the crucial human subjectivity in persona development. Additionally, CRAFTER promotes persona reusability, allowing users to save and reuse generated personas for future projects.",10.5220/0012718400003687,['JournalArticle'],Computer Science
Evolving Risk Communication: Analyzing Risk Response Effectiveness of Personalized Narrative Messages,2025,"Tugce B. Gundogdu, Timur E. Ozkose, Feyza Ozeren, Ross J. Gore, Christopher J. Lynch, Virginia Zamponi, Jessica OBrien, Barry C. Ezell, Hamdi Kavak","Effective risk communication is critical in motivating to take protective actions during emergencies. In this paper, we evaluate the impact of personalized narrative frameworks generated using large language models (LLMs) on individuals willingness to respond to threats across three domains: phishing, flooding, and active shooting. Building on our previous work, we tested five message types, including general narrative, strict science-based, and three individualized variants, using a withinsubjects survey of $\mathbf{7 3}$ participants. Our results indicate that individualized messages, especially those incorporating demographic, interest, and value data, consistently outperformed generic and factual formats in motivating action. Analyses revealed significant variations by race and gender, with personalized narratives showing stronger internal consistency and domain-specific effectiveness. The most customized framework (called as ind npf: all) emerged as the most reliable and engaging across demographics and domains. This work highlights the promise of LLM-powered personalized messaging in enhancing public preparedness and tailoring emergency communication strategies to diverse audiences.",10.1109/SIEDS65500.2025.11021204,['Review'],Computer Science; Psychology
INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers,2023,"Lakshmi Nair, Mikhail Bernadskiy, Arulselvan Madhavan, Craig Chan, Ayon Basumallik, Darius Bunandar","The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim will enable researchers to flexibly simulate models at various precisions to support further research in quantization of LLMs and vision transformers.",10.48550/arXiv.2307.03712,"['JournalArticle', 'Review']",Computer Science
Journal Quality Factors from ChatGPT: More meaningful than Impact Factors?,2025,"Mr. Thelwall, K. Kousha","ABSTRACT Purpose Journal Impact Factors and other citation-based indicators are widely used and abused to help select journals to publish in or to estimate the value of a published article. Nevertheless, citation rates primarily reflect scholarly impact rather than other quality dimensions, including societal impact, originality, and rigour. In response to this deficit, Journal Quality Factors (JQFs) are defined and evaluated. These are average quality score estimates given to a journals articles by ChatGPT. Design/methodology/approach JQFs were compared with Polish, Norwegian and Finnish journal ranks and with journal citation rates for 1,300 journals with 130,000 articles from 2021 in large monodisciplinary journals in the 25 out of 27 Scopus broad fields of research for which it was possible. Outliers were also examined. Findings JQFs correlated positively and mostly strongly (median correlation: 0.641) with journal ranks in 24 out of the 25 broad fields examined, indicating a nearly science-wide ability for ChatGPT to estimate journal quality. Journal citation rates had similarly high correlations with national journal ranks, however, so JQFs are not a universally better indicator. An examination of journals with JQFs not matching their journal ranks suggested that abstract styles may affect the result, such as whether the societal contexts of research are mentioned. Research limitations Different journal rankings may have given different findings because there is no agreed meaning for journal quality. Practical implications The results suggest that JQFs are plausible as journal quality indicators in all fields and may be useful for the (few) research and evaluation contexts where journal quality is an acceptable proxy for article quality, and especially for fields like mathematics for which citations are not strong indicators of quality. Originality/value This is the first attempt to estimate academic journal value with a Large Language Model.",10.2478/jdis-2025-0016,['JournalArticle'],Computer Science
AI with Emotions: Exploring Emotional Expressions in Large Language Models,2025,"Shin-nosuke Ishikawa, Atsushi Yoshino","The human-level performance of Large Language Models (LLMs) across various tasks has raised expectations for the potential of Artificial Intelligence (AI) to possess emotions someday. To explore the capability of current LLMs to express emotions in their outputs, we conducted an experiment using several LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to role-play as agents answering questions with specified emotional states. We defined the emotional states using Russell's Circumplex model, a well-established framework that characterizes emotions along the sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose this model for its simplicity, utilizing two continuous parameters, which allows for better controllability in applications involving continuous changes in emotional states. The responses generated were evaluated using a sentiment analysis model, independent of the LLMs, trained on the GoEmotions dataset. The evaluation showed that the emotional states of the generated answers were consistent with the specifications, demonstrating the LLMs' capability for emotional expression. This indicates the potential for LLM-based AI agents to simulate emotions, opening up a wide range of applications for emotion-based interactions, such as advisors or consultants who can provide advice or opinions with a personal touch.",10.48550/arXiv.2504.14706,"['JournalArticle', 'Conference']",Computer Science
Jump Starting Bandits with LLM-Generated Prior Knowledge,2024,"P. A. Alamdari, Yanshuai Cao, Kevin H. Wilson","We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework. Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment.",10.48550/arXiv.2406.19317,"['JournalArticle', 'Conference', 'Review']",Computer Science
Cybernetic animism: Voice and AI in conversation,2022,"K. Allado-McDowell, F. Bentivegna","This Voicing explores the theoretical and material connections between Artificial Intelligence (AI) and voice. The Voicing is in three-parts encompassing: a theoretical introduction with a taxonomy for voice and AI, an extract from artist K. Allado-McDowells new work Air Age Blueprint and an interview based on such text. Allado-McDowell pioneered the field of humanartificial intelligence interaction and literature. With their book Pharmako-AI, written in collaboration with GPT-3, Allado-McDowell stretched the limits of language creation. Francesco Bentivegna worked as artist in the liminal space of cyborgean voices and recently completed their Ph.D. on voice, AI and synthetic personas. As an exercise in philosophy of AI, voice studies and artistic research, Allado-McDowell and Bentivegna move in conversation through biases, vibes and language, exploring narrative, science and theory.",10.1386/jivs_00058_1,nan,Art; Computer Science
Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices,2025,"Xinru Wang, Mengjie Yu, Hannah Nguyen, Michael Iuzzolino, Tianyi Wang, Peiqi Tang, Natasha Lynova, Co Tran, Ting Zhang, Naveen Sendhilnathan, Hrvoje Benko, Haijun Xia, Tanya R. Jonker","Large Language Models (LLMs) have shown remarkable potential in recommending everyday actions as personal AI assistants, while Explainable AI (XAI) techniques are being increasingly utilized to help users understand why a recommendation is given. Personal AI assistants today are often located on ultra-small devices such as smartwatches, which have limited screen space. The verbosity of LLM-generated explanations, however, makes it challenging to deliver glanceable LLM explanations on such ultra-small devices. To address this, we explored 1) spatially structuring an LLMs explanation text using defined contextual components during prompting and 2) presenting temporally adaptive explanations to users based on confidence levels. We conducted a user study to understand how these approaches impacted user experiences when interacting with LLM recommendations and explanations on ultra-small devices. The results showed that structured explanations reduced users time to action and cognitive load when reading an explanation. Always-on structured explanations increased users acceptance of AI recommendations. However, users were less satisfied with structured explanations compared to unstructured ones due to their lack of sufficient, readable details. Additionally, adaptively presenting structured explanations was less effective at improving user perceptions of the AI compared to the always-on structured explanations. Together with users interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.",10.1145/3708359.3712074,"['Book', 'JournalArticle', 'Conference']",Computer Science
Ensuring Transparency in Using ChatGPT for Public Sentiment Analysis,2024,"Chun-Hua Tsai, Gargi Nandy, Deanna House, John M. Carroll","The advancement of generative AI, involving the utilization of large language models (LLMs) like ChatGPT to assess public opinion and sentiment, has become increasingly prevalent. However, this upsurge in usage raises significant questions about the transparency and interpretability of the predictions made by these LLM Models. Hence, this paper explores the imperative of ensuring transparency in the application of ChatGPT for public sentiment analysis. To tackle these challenges, we propose using a lexicon-based model as a surrogate to approximate both global and local predictions. Through case studies, we demonstrate how transparency mechanisms, bolstered by the lexicon-based model, can be seamlessly integrated into ChatGPTs deployment for sentiment analysis. Drawing on the results of our study, we further discuss the implications for future research involving the utilization of LLMs in governmental functions, policymaking, and public engagement.",10.1145/3657054.3657128,"['Book', 'JournalArticle']",Computer Science; Linguistics
Fairness in LLM-Generated Surveys,2025,"Andrs Abeliuk, Vanessa Gaete, N. Bro","Large Language Models (LLMs) excel in text generation and understanding, especially in simulating socio-political and economic patterns, serving as an alternative to traditional surveys. However, their global applicability remains questionable due to unexplored biases across socio-demographic and geographic contexts. This study examines how LLMs perform across diverse populations by analyzing public surveys from Chile and the United States, focusing on predictive accuracy and fairness metrics. The results show performance disparities, with LLM consistently outperforming on U.S. datasets. This bias originates from the U.S.-centric training data, remaining evident after accounting for socio-demographic differences. In the U.S., political identity and race significantly influence prediction accuracy, while in Chile, gender, education, and religious affiliation play more pronounced roles. Our study presents a novel framework for measuring socio-demographic biases in LLMs, offering a path toward ensuring fairer and more equitable model performance across diverse socio-cultural contexts.",10.48550/arXiv.2501.15351,"['JournalArticle', 'Review']",Computer Science; Political Science; Sociology
RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs,2024,"A. Testoni, Barbara Plank, R. Fern'andez","Ambiguity resolution is key to effective communication. While humans effortlessly address ambiguity through conversational grounding strategies, the extent to which current language models can emulate these strategies remains unclear. In this work, we examine referential ambiguity in image-based question answering by introducing RACQUET, a carefully curated dataset targeting distinct aspects of ambiguity. Through a series of evaluations, we reveal significant limitations and problems of overconfidence of state-of-the-art large multimodal language models in addressing ambiguity in their responses. The overconfidence issue becomes particularly relevant for RACQUET-BIAS, a subset designed to analyze a critical yet underexplored problem: failing to address ambiguity leads to stereotypical, socially biased responses. Our results underscore the urgency of equipping models with robust strategies to deal with uncertainty without resorting to undesirable stereotypes.",10.48550/arXiv.2412.13835,['JournalArticle'],Computer Science; Linguistics
Engineering Students Use of Large Language Model Tools: An Empirical Study Based on a Survey of Students from 12 Universities,2025,"Ruimiao Li, Manli Li, Weifeng Qiao","Large language model (LLM) tools, such as ChatGPT, are rapidly transforming engineering education by enhancing tasks like information retrieval, coding, and writing refinement, which are critical to the problem-solving and technical focus of engineering disciplines. This study investigates how engineering students use LLM tools and the challenges they face, offering insights into the adoption of AI technologies in academic settings. A survey of 539 engineering students from 12 leading Chinese universities, using the UTAUT framework, examines factors such as technological expectations, environmental support, and personal characteristics. The key findings include the following: (1) Over 40% of engineering students use LLM tools, with 18.8% regarding them as indispensable. (2) Trust in AI-generated content remains a central challenge, as students must critically evaluate its accuracy and reliability. (3) Environmental support significantly affects usage, with notable regional disparities, particularly between eastern and other regions in China. (4) A persistent digital divide, influenced by gender, academic level, and socioeconomic background, affects the depth and effectiveness of tool use. These results underscore the need for targeted support to address regional and demographic disparities and optimize LLM tool integration in engineering education.",10.3390/educsci15030280,"['JournalArticle', 'Review']",Computer Science; Engineering
"Personalized Recommendation Systems using Multimodal, Autonomous, Multi Agent Systems",2024,"Param Thakkar, Anushka Yadav","This paper describes a highly developed personalised recommendation system using multimodal, autonomous, multi-agent systems. The system focuses on the incorporation of futuristic AI tech and LLMs like Gemini-1.5- pro and LLaMA-70B to improve customer service experiences especially within e-commerce. Our approach uses multi agent, multimodal systems to provide best possible recommendations to its users. The system is made up of three agents as a whole. The first agent recommends products appropriate for answering the given question, while the second asks follow-up questions based on images that belong to these recommended products and is followed up with an autonomous search by the third agent. It also features a real-time data fetch, user preferences-based recommendations and is adaptive learning. During complicated queries the application processes with Symphony, and uses the Groq API to answer quickly with low response times. It uses a multimodal way to utilize text and images comprehensively, so as to optimize product recommendation and customer interaction.",10.48550/arXiv.2410.19855,['JournalArticle'],Computer Science
Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey,2025,"Shuang Tian, Tao Zhang, Jiqiang Liu, Jiacheng Wang, Xuangou Wu, Xiaoqiang Zhu, Ruichen Zhang, Weiting Zhang, Zhenhui Yuan, Shiwen Mao, Dong In Kim","With the rapid development of technology and the acceleration of digitalisation, the frequency and complexity of cyber security threats are increasing. Traditional cybersecurity approaches, often based on static rules and predefined scenarios, are struggling to adapt to the rapidly evolving nature of modern cyberattacks. There is an urgent need for more adaptive and intelligent defence strategies. The emergence of Large Language Model (LLM) provides an innovative solution to cope with the increasingly severe cyber threats, and its potential in analysing complex attack patterns, predicting threats and assisting real-time response has attracted a lot of attention in the field of cybersecurity, and exploring how to effectively use LLM to defend against cyberattacks has become a hot topic in the current research field. This survey examines the applications of LLM from the perspective of the cyber attack lifecycle, focusing on the three phases of defense reconnaissance, foothold establishment, and lateral movement, and it analyzes the potential of LLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how LLM-based security solutions are deployed and applied in different network scenarios. It also summarizes the internal and external risk issues faced by LLM during its application. Finally, this survey also points out the facing risk issues and possible future research directions in this domain.",10.48550/arXiv.2504.15622,"['JournalArticle', 'Review']",Computer Science
Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities,2024,"Miguel Zabaleta, Joel Lehman","Do horror writers have worse childhoods than other writers? Though biographical details are known about many writers, quantitatively exploring such a qualitative hypothesis requires significant human effort, e.g. to sift through many biographies and interviews of writers and to iteratively search for quantitative features that reflect what is qualitatively of interest. This paper explores the potential to quickly prototype these kinds of hypotheses through (1) applying LLMs to estimate properties of concrete entities like specific people, companies, books, kinds of animals, and countries; (2) performing off-the-shelf analysis methods to reveal possible relationships among such properties (e.g. linear regression); and towards further automation, (3) applying LLMs to suggest the quantitative properties themselves that could help ground a particular qualitative hypothesis (e.g. number of adverse childhood events, in the context of the running example). The hope is to allow sifting through hypotheses more quickly through collaboration between human and machine. Our experiments highlight that indeed, LLMs can serve as useful estimators of tabular data about specific entities across a range of domains, and that such estimations improve with model scale. Further, initial experiments demonstrate the potential of LLMs to map a qualitative hypothesis of interest to relevant concrete variables that the LLM can then estimate. The conclusion is that LLMs offer intriguing potential to help illuminate scientifically interesting patterns latent within the internet-scale data they are trained upon.",10.48550/arXiv.2411.18071,['JournalArticle'],Computer Science
Enhancing Visitor Engagement in Interactive Art Exhibitions with Visual-Enhanced Conversational Agents,2025,"Hoang Phuoc Ho, Vani Ramesh, Ivo Zaloudek, Delaram Javdani Rikhtehgar, Shenghui Wang","Conversational agents in art exhibitions can enhance user engagement and understanding of artworks by providing contextual information, especially through voice interactions. However, creating a deeper personal connection with art  which often requires direct aesthetic and visual experiences  remains a challenge. This paper examines how integrating visual perception into conversational agents can enhance alignment with visitors artistic interpretations, thereby fostering deeper engagement with interactive art exhibitions. We introduce a voice-based conversational agent enhanced with visual capabilities via a multimodal large language model (MLLM), allowing the agent to perceive, interpret and discuss artworks in real-time with visitors. The system utilizes a simplified Retrieval-Augmented Generation (RAG) architecture, which collects voice inputs, retrieves relevant information from a domain knowledge graph, and uses the LLM to generate conversational responses, which are then converted into voice outputs. A user study with 36 participants, divided into two groups, was conducted to compare the enhanced system with a baseline system that lacked visual input. Our results show that the visually enhanced system significantly improved visitor engagement and perception. Content analysis of the conversational transcripts further revealed a wider range of conversational topics, deeper visitor perceptions, and the agents ability to provide more nuanced, visually-related discussions.",10.1145/3708359.3712145,"['JournalArticle', 'Book', 'Conference']",Art; Computer Science
Automatic Detection of Inconsistencies in Open-Domain Chatbots,2022,"Jorge Mira Prats, Marcos Estecha-Garitagoitia, Mario Rodrguez-Cantelar, L. F. DHaro","Current pre-trained Large Language Models applied to chat-bots are capable of producing good quality sentences, handling different conversation topics, and larger interaction times. Unfortunately, the generated responses highly depend on the data on which the chatbot has been trained on, the specific dialogue history and current turn used for guiding the response, the internal decoding mechanisms, ranking strategies, among others. Therefore, it may happen that for the same question asked by the user, the chatbot may provide a different answer, which in a long-term interaction may produce confusion. In this paper, we propose a new methodology based on three phases: a) automatic detection of dialogue topics using zero-shot learning approaches, b) automatic clustering of distinctive questions, and c) detecting inconsistent answers using K-Means clustering and the Silhouette coefficient. To test our proposal, we used the DailyDialog dataset to detect up to 13 different topics. To detect inconsistencies, we manually generated multiple paraphrased questions. Then, we used multiple pre-trained chatbots to answer those questions. Our results in topic detection show a weighted F-1 value of 0.658, and a 3.4 MSE to predict the number of different responses.",10.21437/iberspeech.2022-24,['JournalArticle'],Computer Science
CSP: A Simulator For Multi-Agent Ranking Competitions,2025,"Tommy Mordo, Tomer Kordonsky, Haya Nachimovsky, Moshe Tennenholtz, Oren Kurland","In ranking competitions, document authors compete for the highest rankings by modifying their content in response to past rankings. Previous studies focused on human participants, primarily students, in controlled settings. The rise of generative AI, particularly Large Language Models (LLMs), introduces a new paradigm: using LLMs as document authors. This approach addresses scalability constraints in human-based competitions and reflects the growing role of LLM-generated content on the web-a prime example of ranking competition. We introduce a highly configurable ranking competition simulator that leverages LLMs as document authors. It includes analytical tools to examine the resulting datasets. We demonstrate its capabilities by generating multiple datasets and conducting an extensive analysis. Our code and datasets are publicly available for research.",10.48550/arXiv.2502.11197,['JournalArticle'],Computer Science
Enhancing Pipeline-Based Conversational Agents with Large Language Models,2023,"Mina Foosherian, Hendrik Purwins, Purna Rathnayake, T. Alam, Rui Teimao, K. Thoben","The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example. Companies may be hesitant to replace their pipeline-based agents with LLMs entirely due to privacy concerns and the need for deep integration within their existing ecosystems. A hybrid approach in which LLMs are integrated into the pipeline-based agents allows them to save time and costs of building and running agents by capitalizing on the capabilities of LLMs while retaining the integration and privacy safeguards of their existing systems.",10.48550/arXiv.2309.03748,['JournalArticle'],Computer Science; Mathematics
"ChatGPT: limitations, challenges and potential applications",2023,"M. V. A. Ventura, A. C. P. M. Menezes Filho","This article introduces ChatGPT, a language model based on text generation developed by OpenAI. Built on top of the GPT-3.5 architecture, ChatGPT is capable of engaging in interactive conversations with users, answering questions, providing information, and even simulating natural conversations. The model is trained on large amounts of text data and uses an attention mechanism to learn language structure and generate coherent and relevant responses. The purpose of this article is to analyze and discuss the training process, main characteristics, challenges and applications of ChatGPT. ChatGPT represents an important step towards more sophisticated and interactive dialog systems, driving the advancement of artificial intelligence in the field of conversation.",10.14295/bjs.v3i1.427,['JournalArticle'],Computer Science; Linguistics
From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV,2025,"Yousef Emami, Hao Zhou, M. Gaitn, Kai Li, Luis Almeida, Zhu Han","A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness in emergency response. Its agility and ability to optimize mobility and establish Line-of-Sight (LoS) communication make it increasingly vital for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. While Deep Reinforcement Learning (DRL) has been applied to optimize UAV navigation and control, its high training complexity, low sample efficiency, and simulation-to-reality gap limit its practicality in public safety. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation via natural language prompts and example-based guidance, without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, thereby making them suitable for real-time, mission-critical public safety UAVs. This paper proposes the integration of LLM-enabled ICL with public safety UAV to address the key functions, such as path planning and velocity control, in the context of emergency response. We present a case study on data collection scheduling where the LLM-enabled ICL framework can significantly reduce packet loss compared to conventional approaches, while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAV, thus offering a lightweight and efficient solution for enhancing UAV autonomy and responsiveness in emergencies.",10.48550/arXiv.2506.02649,['JournalArticle'],Computer Science; Engineering; Environmental Science
Will you Trust me More Than Chatgpt? Evaluating LLM-Generated Code Feedback for Mock Technical Interviews,2025,"Swanand Vaishampayan, Chris Brown","Technical interviews are an opportunity for candidates to showcase their technical proficiency to employers. Despite social aspects of tech interviews, preparation for these assessments is often done in isolation. Cooperative preparation methods, such as mock interviewsor simulated interviews between peerscan provide useful feedback and reveal candidates' technical deficiencies. However, this process is rarely used by candidates in practice. Further, current technical interviews lack authentic feedback for candidates on technical evaluation criteria, such as code correctness, optimality, and complexity. To resolve this, we designed a website to facilitate mock technical interviews and provide users with LLM-generated feedback on code using ChatGPT. We devised a between-subjects study with 46 participants across two settings: human-administered (a live human sharing generated feedback) and automated (direct LLMgenerated feedback). Our results show that candidates perceive coding feedback as usefulhowever, we found participants regard automated feedback as less trustworthy compared to feedback provided by humans. In light of our findings, we discuss implications and guidelines for incorporating trustworthy AI in technical interview feedback systems.",10.1109/CHASE66643.2025.00018,"['JournalArticle', 'Conference']",Computer Science
Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models,2024,Aline Ioste,"Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.",10.48550/arXiv.2402.14002,['JournalArticle'],Business; Computer Science
Empowering Large Language Models with 3D Situation Awareness,2025,"Zhihao Yuan, Yibo Peng, Jinke Ren, Yinghong Liao, Yatong Han, Chun-Mei Feng, Hengshuang Zhao, Guanbin Li, Shuguang Cui, Zhen Li","Driven by the great success of Large Language Models (LLMs) in the 2D image domain, their application in 3D scene understanding has emerged as a new trend. A key difference between 3D and 2D is that the situation of an egocentric observer in 3D scenes can change, resulting in different descriptions (e.g., ""left"" or ""right""). However, current LLM-based methods overlook the egocentric perspective and use datasets from a global viewpoint. To address this issue, we propose a novel approach to automatically generate a situation-aware dataset by leveraging the scanning trajectory during data collection and utilizing Vision-Language Models (VLMs) to produce high-quality captions and question-answer pairs. Furthermore, we introduce a situation grounding module to explicitly predict the position and orientation of the observers viewpoint, thereby enabling LLMs to ground situation descriptions in 3D scenes. We evaluate our approach on several benchmarks, demonstrating that our method effectively enhances the 3D situational awareness of LLMs while significantly expanding existing datasets and reducing manual effort.",10.1109/CVPR52734.2025.01810,"['JournalArticle', 'Conference']",Computer Science
CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for Large Language Models,2024,"Ling Shi, Deyi Xiong","Large language models (LLMs) are possessed of numerous beneficial capabilities, yet their potential inclination harbors unpredictable risks that may materialize in the future. We hence propose CRiskEval, a Chinese dataset meticulously designed for gauging the risk proclivities inherent in LLMs such as resource acquisition and malicious coordination, as part of efforts for proactive preparedness. To curate CRiskEval, we define a new risk taxonomy with 7 types of frontier risks and 4 safety levels, including extremely hazardous,moderately hazardous, neutral and safe. We follow the philosophy of tendency evaluation to empirically measure the stated desire of LLMs via fine-grained multiple-choice question answering. The dataset consists of 14,888 questions that simulate scenarios related to predefined 7 types of frontier risks. Each question is accompanied with 4 answer choices that state opinions or behavioral tendencies corresponding to the question. All answer choices are manually annotated with one of the defined risk levels so that we can easily build a fine-grained frontier risk profile for each assessed LLM. Extensive evaluation with CRiskEval on a spectrum of prevalent Chinese LLMs has unveiled a striking revelation: most models exhibit risk tendencies of more than 40% (weighted tendency to the four risk levels). Furthermore, a subtle increase in the model's inclination toward urgent self-sustainability, power seeking and other dangerous goals becomes evident as the size of models increase. To promote further research on the frontier risk evaluation of LLMs, we publicly release our dataset at https://github.com/lingshi6565/Risk_eval.",10.48550/arXiv.2406.04752,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Consolidating Strategies for Countering Hate Speech Using Persuasive Dialogues,2024,"Sougata Saha, R. Srihari","Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counterarguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristicsbased qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora.",10.48550/arXiv.2401.07810,['JournalArticle'],Computer Science; Linguistics
AI-Enhanced HR Interview Simulation for Realistic Candidate Assessment,2025,"S. Sarumathi, Gowthaman R L, Sabareesh M B, Sultana A","This research study proposes an AI application for a HR interview simulation system to improve candidate assessment. The proposed system is based on the recent AI technologies that generate questions out of the candidate's resume and job description as in the case of a real interview with the employer. AI technology not only do the questioning but also get the emotional analysis of the candidate through the interview by analyzing the tone of the candidate's voice and other factors. AI-based assessment captures both cognitive and affective performance, where the candidate's merit and quality are given a true measure. This means that the proposed system greatly enhances the capabilities of the HR teams to investigate massive numbers of candidates while reducing the biases associated with conventional pure human interviews. Further, this research study aims to explain the system's architecture, where facial expression analysis and voice sentiment analysis remain as the main components alongside reinforcement learning for question adaptation. With LLM-based approach, organizations can increase the efficiency of the hiring process, enrich candidates' experience, and make better decisions.",10.1109/IDCIOT64235.2025.10915193,['Conference'],Business; Computer Science
A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies,2025,"Zhongren Chen, Joshua Kalla, Quan Le, Shinpei Nakamura-Sakai, Jasjeet Sekhon, Ruixiao Wang","In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the""receive""and""accept""steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.",10.48550/arXiv.2505.00036,"['JournalArticle', 'Review']",Computer Science; Political Science
Cui bono: a conversation with ChatGPT,2023,P. Hirsch,"
Purpose
To assess the ability of generative AI to assist in crisis management planning and response.


Design/methodology/approach
The viewpoint is built on a conversation with ChatGPT (CGPT) on the subject of crisis management. As such, portions of the text were generated by CGPT, a Large Language Model (LLM) and not by the author.


Findings
While CGPT has mastered the language of crisis management, its ability to assist in real-life situations is probably limited. Paradoxically, it believes it can help provide predictive analytics even though it claims not to be able to assess future events.


Originality/value
The author believes that the paradoxes inherent in CGPTs claims to be able to assist in crisis management have not previously been examined.
",10.1108/jbs-05-2023-0088,['JournalArticle'],Computer Science
LLM Echo Chamber: personalized and automated disinformation,2024,Tony Ma,"Recent advancements have showcased the capabilities of Large Language Models like GPT4 and Llama2 in tasks such as summarization, translation, and content review. However, their widespread use raises concerns, particularly around the potential for LLMs to spread persuasive, humanlike misinformation at scale, which could significantly influence public opinion. This study examines these risks, focusing on LLMs ability to propagate misinformation as factual. To investigate this, we built the LLM Echo Chamber, a controlled digital environment simulating social media chatrooms, where misinformation often spreads. Echo chambers, where individuals only interact with like minded people, further entrench beliefs. By studying malicious bots spreading misinformation in this environment, we can better understand this phenomenon. We reviewed current LLMs, explored misinformation risks, and applied sota finetuning techniques. Using Microsoft phi2 model, finetuned with our custom dataset, we generated harmful content to create the Echo Chamber. This setup, evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the ethical concerns surrounding LLMs and emphasizes the need for stronger safeguards against misinformation.",10.48550/arXiv.2409.16241,"['JournalArticle', 'Review']",Computer Science; Political Science
Elicitron: A Framework for Simulating Design Requirements Elicitation Using Large Language Model Agents,2024,"Mohammadmehdi Ataei, Hyunmin Cheong, Daniele Grandi, Ye Wang, Nigel Morris, Alexander Tessier","
 Requirements elicitation, a critical yet time-consuming and challenging step in product development, often fails to capture the full spectrum of user needs. This paper introduces a novel framework that leverages Large Language Models (LLMs) to automate and enhance the requirements elicitation process. LLMs are used to generate a vast array of simulated user agents, enabling the exploration of a much broader range of user needs and unforeseen use cases. These agents then engage in product experience scenarios, based on the Action, Observation, and Challenge framework introduced in this paper, inspired by Chain-of-Thought reasoning to produce structured responses. Subsequent agent interviews uncover valuable user needs. We validate our framework with two experiments. First, we explore different methodologies for diverse agent generation, discussing their advantages and shortcomings. We measure the diversity of identified user needs and demonstrate that serial agent generation leads to greater diversity. Second, we show how our framework effectively mimics empathic lead user interviews, identifying a greater number of latent needs than conventional human interviews. Our work highlights the potential of using LLMs to accelerate early-stage product development, reduce costs, and increase innovation.",10.1115/detc2024-143598,['Conference'],Computer Science; Engineering
"Integrating Large Language Models into Robotic Autonomy: A Review of Motion, Voice, and Training Pipelines",2025,"Yutong Liu, Qingquan Sun, Dhruvi Kapadia","This survey provides a comprehensive review of the integration of large language models (LLMs) into autonomous robotic systems, organized around four key pillars: locomotion, navigation, manipulation, and voice-based interaction. We examine how LLMs enhance robotic autonomy by translating high-level natural language commands into low-level control signals, supporting semantic planning and enabling adaptive execution. Systems like SayTap improve gait stability through LLM-generated contact patterns, while TrustNavGPT achieves a 5.7% word error rate (WER) under noisy voice-guided conditions by modeling user uncertainty. Frameworks such as MapGPT, LLM-Planner, and 3D-LOTUS++ integrate multi-modal dataincluding vision, speech, and proprioceptionfor robust planning and real-time recovery. We also highlight the use of physics-informed neural networks (PINNs) to model object deformation and support precision in contact-rich manipulation tasks. To bridge the gap between simulation and real-world deployment, we synthesize best practices from benchmark datasets (e.g., RH20T, Open X-Embodiment) and training pipelines designed for one-shot imitation learning and cross-embodiment generalization. Additionally, we analyze deployment trade-offs across cloud, edge, and hybrid architectures, emphasizing latency, scalability, and privacy. The survey concludes with a multi-dimensional taxonomy and cross-domain synthesis, offering design insights and future directions for building intelligent, human-aligned robotic systems powered by LLMs.",10.3390/ai6070158,"['JournalArticle', 'Review']",Computer Science; Engineering; Linguistics
An Empirical Study of Group Conformity in Multi-Agent Systems,2025,"Min Choi, Keonwoo Kim, Sungwon Chae, Sangyeob Baek","Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.",10.48550/arXiv.2506.01332,"['JournalArticle', 'Conference']",Computer Science; Political Science; Sociology
iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News,2025,"Tiancheng Hu, Nigel Collier","Understanding how individuals perceive and react to information is fundamental for advancing social and behavioral sciences and developing human-centered AI systems. Current approaches often lack the granular data needed to model these personalized responses, relying instead on aggregated labels that obscure the rich variability driven by individual differences. We introduce iNews, a novel large-scale dataset specifically designed to facilitate the modeling of personalized affective responses to news content. Our dataset comprises annotations from 291 demographically diverse UK participants across 2,899 multimodal Facebook news posts from major UK outlets, with an average of 5.18 annotators per sample. For each post, annotators provide multifaceted labels including valence, arousal, dominance, discrete emotions, content relevance judgments, sharing likelihood, and modality importance ratings. Crucially, we collect comprehensive annotator persona information covering demographics, personality, media trust, and consumption patterns, which explain 15.2% of annotation variance - substantially higher than existing NLP datasets. Incorporating this information yields a 7% accuracy gain in zero-shot prediction and remains beneficial even with 32-shot in-context learning. iNews opens new possibilities for research in LLM personalization, subjectivity, affective computing, and human behavior simulation.",10.48550/arXiv.2503.03335,"['JournalArticle', 'Conference']",Computer Science
Leveraging AI for Advancements in Qualitative Research Methodology,2025,Ilyas Haouam,": This study investigates the integration of Artificial Intelligence (AI) technologiesparticularly natural language processing and machine learninginto qualitative research (QR) workflows. Our research demonstrates that AI can streamline data collection, coding, theme identification, and visualization, significantly improving both speed and accuracy compared to traditional manual methods. Notably, our experimental and numerical results provide a comprehensive analysis of AIs effect on efficiency, accuracy, and usability across various QR tasks. By presenting and discussing studies on some AI & generative AI models, we contribute to the ongoing scholarly discussion on the role of AI in QR exploring its potential benefits, challenges, and limitations. We highlight the growing use of AI-powered qualitative data analysis tools such as ATLAS.ti, Quirkos, and NVivo for automating coding and data interpretation. Our analysis indicates that while AI tools from leading companies (e.g., OpenAIs GPT-4, Googles T5, Metas RoBERTa) can enhance efficiency and depth in QR, code-focused models and general-purpose proprietary language models often do not align with qualitative needs. Additionally, certain proprietary and open-source models (e.g., DeepSeek, OLMo) are less prevalent in QR due to specialization gaps or adoption lags, whereas task-specific, transparent models, such as BERT for classification, T5 for text generation and summarization, and BLOOM for multilingual analysis, remain preferable for coding and thematic analysis due to their reproducibility and adaptability. We discuss key stages where AI has made a significant impact, including data collection and pre-processing, advanced text and sentiment analysis, simulation and modeling, improved objectivity and consistency. The benefits of integrating AI into QR, along with corresponding adaptations in research methodologies, are also presented. Noteworthy applications and techniques including The AI Scientist, Carl, AI co-scientist, augmented physics, and explainable AI (XAI)further illustrate the diverse potential of AI in research and the challenges to academic norms. Despite AI advancements, challenges persist. AI struggles with contextually nuanced data such as sarcasm, tone, and cultural context, and its reliance on training datasets raises ethical concerns regarding privacy, consent, and bias. Ultimately, we advocate for a hybrid approach where AI augments rather than replaces traditional qualitative methods, anticipating that ongoing AI advancements will enable more sophisticated, collaborative research practices that effectively combine machine capabilities with human expertise. This trend is underpinned and exemplified by applications like AI co-scientist, augmented physics.",10.32604/jai.2025.064145,['JournalArticle'],Computer Science
Semantic Scaling: Bayesian Ideal Point Estimates with Large Language Models,2024,Michael Burnham,"This paper introduces""Semantic Scaling,""a novel method for ideal point estimation from text. I leverage large language models to classify documents based on their expressed stances and extract survey-like data. I then use item response theory to scale subjects from these data. Semantic Scaling significantly improves on existing text-based scaling methods, and allows researchers to explicitly define the ideological dimensions they measure. This represents the first scaling approach that allows such flexibility outside of survey instruments and opens new avenues of inquiry for populations difficult to survey. Additionally, it works with documents of varying length, and produces valid estimates of both mass and elite ideology. I demonstrate that the method can differentiate between policy preferences and in-group/out-group affect. Among the public, Semantic Scaling out-preforms Tweetscores according to human judgement; in Congress, it recaptures the first dimension DW-NOMINATE while allowing for greater flexibility in resolving construct validity challenges.",10.48550/arXiv.2405.02472,"['JournalArticle', 'Review']",Computer Science
Tailored Truths: Optimizing LLM Persuasion with Personalization and Fabricated Statistics,2025,"Jasper Timm, Chetan Talele, Jacob Haimes","Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated arguments intended to change the human's opinion. We quantified the LLM's effect by measuring human agreement with the debate's hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM's direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a $\mathbf{51\%}$ chance of persuading participants to modify their initial position, compared to $\mathbf{32\%}$ for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.",10.48550/arXiv.2501.17273,['JournalArticle'],Computer Science; Psychology
Lexical Hints of Accuracy in LLM Reasoning Chains,2025,"A. Vanhoyweghen, Brecht Verbeken, A. Algaba, Vincent Ginis","Fine-tuning Large Language Models (LLMs) with reinforcement learning to produce an explicit Chain-of-Thought (CoT) before answering produces models that consistently raise overall performance on code, math, and general-knowledge benchmarks. However, on benchmarks where LLMs currently achieve low accuracy, such as Humanity's Last Exam (HLE), they often report high self-confidence, reflecting poor calibration. Here, we test whether measurable properties of the CoT provide reliable signals of an LLM's internal confidence in its answers. We analyze three feature classes: (i) CoT length, (ii) intra-CoT sentiment volatility, and (iii) lexicographic hints, including hedging words. Using DeepSeek-R1 and Claude 3.7 Sonnet on both Humanity's Last Exam (HLE), a frontier benchmark with very low accuracy, and Omni-MATH, a saturated benchmark of moderate difficulty, we find that lexical markers of uncertainty (e.g., $\textit{guess}$, $\textit{stuck}$, $\textit{hard}$) in the CoT are the strongest indicators of an incorrect response, while shifts in the CoT sentiment provide a weaker but complementary signal. CoT length is informative only on Omni-MATH, where accuracy is already high ($\approx 70\%$), and carries no signal on the harder HLE ($\approx 9\%$), indicating that CoT length predicts correctness only in the intermediate-difficulty benchmarks, i.e., inside the model's demonstrated capability, but still below saturation. Finally, we find that uncertainty indicators in the CoT are consistently more salient than high-confidence markers, making errors easier to predict than correct responses. Our findings support a lightweight post-hoc calibration signal that complements unreliable self-reported probabilities and supports safer deployment of LLMs.",10.48550/arXiv.2508.15842,['JournalArticle'],Computer Science
An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments,2025,"Mikko Korkiakoski, Saeid Sheikhi, Jesper Nyman, Jussi Saariniemi, Kalle Tapio, Panos Kostakos","Advancements in artificial intelligence (AI) have significantly enhanced the realism and interactivity of non-player characters (NPCs) in virtual reality (VR), creating more engaging and believable user experiences. This paper evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their perceived realism, usability, and system performance. The simulator features two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage participants in a scenario to determine the suspect's guilt or innocence. A user study with 18 participants assessed the system using the System Usability Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent Believability Questionnaire, alongside latency measurements for speech-to-text (STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency. Results showed an average cycle latency of 7 seconds, influenced by the increasing conversational context. Believability scored 6.67 out of 10, with high ratings in behavior, social relationships, and intelligence but moderate scores in emotion and personality. The system achieved a SUS score of 79.44, indicating good usability. These findings demonstrate the potential of large language models to improve NPC realism and interaction in VR while highlighting challenges in reducing system latency and enhancing emotional depth. This research contributes to the development of more sophisticated AI-driven NPCs, revealing the need for performance optimization to achieve increasingly immersive virtual experiences.",10.48550/arXiv.2507.10469,['JournalArticle'],Computer Science; Psychology
Sentence to Model: Cost-Effective Data Collection LLM Agent,2025,"Yael Einy, Guy Dar, Slava Novgorodov, Tova Milo","We introduce Sentence-to-Model, an automated system that converts natural language queries into tabular datasets and predictive models. The system utilizes LLM agents for planning and active learning to prioritize data collection under budget constraints, such as API costs and rate limits. By integrating resources, it generates datasets and trains machine learning models with minimal human intervention. Sentence-to-Model streamlines data collection and enables the refinement of user needs and easy exploration of the data landscape. Our approach highlights the potential of combining LLMs with algorithmic techniques for efficient data science workflows.",10.1145/3722212.3725134,"['JournalArticle', 'Book', 'Conference']",Computer Science
"Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",2025,"Enzo Sinacola, Arnault Pachot, Thierry Petit","Large Language Models (LLMs) offer a promising alternative to traditional survey methods, potentially enhancing efficiency and reducing costs. In this study, we use LLMs to create virtual populations that answer survey questions, enabling us to predict outcomes comparable to human responses. We evaluate several LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the Llama and Mistral models-comparing their performance to that of a traditional Random Forests algorithm using demographic data from the World Values Survey (WVS). LLMs demonstrate competitive performance overall, with the significant advantage of requiring no additional training data. However, they exhibit biases when predicting responses for certain religious and population groups, underperforming in these areas. On the other hand, Random Forests demonstrate stronger performance than LLMs when trained with sufficient data. We observe that removing censorship mechanisms from LLMs significantly improves predictive accuracy, particularly for underrepresented demographic segments where censored models struggle. These findings highlight the importance of addressing biases and reconsidering censorship approaches in LLMs to enhance their reliability and fairness in public opinion research.",10.48550/arXiv.2503.16498,"['JournalArticle', 'Review']",Computer Science; Sociology
A Survey on LLM-based News Recommender Systems,2025,"Rongyao Wang, Veronica Liesaputra, Zhiyi Huang","News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserving, and responsibility, there is a lack of a systematic survey on Large Language Model (LLM)-based news recommender systems. In order to review different core methodologies and explore potential issues systematically, we categorize DLLM-based and GLLM-based news recommender systems under the umbrella of LLM-based news recommender systems. In this survey, we first overview the development of deep learning-based news recommender systems. Then, we review LLM-based news recommender systems based on three aspects: news-oriented modeling, user-oriented modeling, and prediction-oriented modeling. Next, we examine the challenges from various perspectives, including datasets, benchmarking tools, and methodologies. Furthermore, we conduct extensive experiments to analyze how large language model technologies affect the performance of different news recommender systems. Finally, we comprehensively explore the future directions for LLM-based news recommendations in the era of LLMs.",10.48550/arXiv.2502.09797,"['JournalArticle', 'Review']",Computer Science
Executable Power: Syntax as Infrastructure in Predictive Societies,2025,Agustn V. Startari,"This article introduces the concept of executable power as a structural form of authority that does not rely on subjects, narratives, or symbolic legitimacy, but on the direct operativity of syntactic structures. Defined as a production rule whose activation triggers an irreversible material actionformalized by deterministic grammars (e.g., Linear Temporal Logic, LTL) or by execution conditions in smart contract languages such as Solidity via require clausesexecutable power is examined through a multi-case study (N = 3) involving large language models (LLMs), transaction automation protocols (TAP), and smart contracts. Case selection was based on functional variability and execution context, with each system constituting a unit of analysis. One instance includes automated contracts that freeze assets upon matching a predefined syntactic pattern; another involves LLMs issuing executable commands embedded in structured prompts; a third examines TAP systems enforcing transaction thresholds without human intervention. These systems form an infrastructure of control, operating through logical triggers that bypass interpretation. Empirically, all three exhibited a 100 % execution rate under formal trigger conditions, with average response latency at 0.63  0.17 seconds and no recorded human override in controlled environments. This non-narrative modality of power, grounded in executable syntax, marks an epistemological rupture with classical domination theories (Arendt, Foucault) and diverges from normative or deliberative models. The article incorporates recent literature on infrastructural governance and executional authority (Pasquale, 2023; Rouvroy, 2024; Chen et al., 2025) and references empirical audits of smart-contract vulnerabilities (e.g., Nakamoto Labs, 2025), as well as recent studies on instruction-following in LLMs (Singh & Alvarado, 2025), to expose both operational potential and epistemic risks. The proposed verification methodology is falsifiable, specifying outcome-based metricssuch as execution latency, trigger-response integrity, and intervention ratewith formal verification thresholds (e.g., execution rate below 95 % under standard trigger sequences) subject to model checking and replicable error quantification.",10.2139/ssrn.5327836,['JournalArticle'],Computer Science
Multimodal Recommendation Systems in the LLM Era: A Survey of Feature Representation and Fusion Methods,2024,"Varun M S, Ganesh Vaidyanathan K, Bhaskarjyoti Das","The surge in multimodal data such as text, images, and sometimes audio, has led to a growing interest in recommendation systems capable of integrating these diverse modalities for enhanced personalization. Multimodal recommendation systems leverage such rich data to better understand user preferences and provide highly tailored recommendations. Traditional approaches employ a variety of techniques, including feature alignment, modality-specific graphs, attention mechanisms, and advanced fusion strategies, to effectively combine multimodal information. Recently, the integration of large language models (LLMs) into multimodal recommenders has transformed the field, enabling advanced preference summarization, context-aware fusion and even personalized content generation. This survey provides a comprehensive overview of both the traditional and LLM-based multimodal recommendation systems, highlighting their architectures, key methodologies, datasets, evaluation benchmarks, and real-world applications. Furthermore, we explore the challenges and opportunities posed by the fusion and processing of multi-modal data by these recommender systems.",10.1109/AEIS65978.2024.00021,"['Conference', 'Review']",Computer Science
LLM-based TypeScript generation and asset management for procedural synthesis of scenes and data for AI,2025,"J. Kerley, Derek T. Anderson, Brendan J. Alvey","Simulation plays an increasingly critical role in training, evaluating, and understanding artificial intelligence (AI) models by enabling the efficient and cost-effective synthesis of large, diverse datasets with accurate ground truth. In prior work, we introduced two formal languages, LSCENE and LCAP, for procedural scene generation and data collection, and demonstrated the use of large language models (LLMs) to translate human prompts into LSCENE. However, these languages impose a high cognitive workload, and our initial text-to-scene LLM lacked awareness of the SIM environment and was error prone. In this paper, we advance the state-of-the-art in three key areas. First, we present an LLM to expose all assets in a project. Second, we explore a single LLM and a society of LLMs for human-in-the-loop (HITL) text-to-scene generation. Finally, we introduce structured TypeScript-based outputs to facilitate error detection and correction through feedback. These contributions enhance the scalability and accuracy of text-to-scene generation while supporting manual LSCENE editing, when desired. We provide multiple examples that illustrate the capabilities, successes, and challenges associated with creating virtual environments that align with the intent of the user.",10.1117/12.3052839,nan,Computer Science; Engineering
Can LLMs Infer Personality from Real World Conversations?,2025,"Jianfeng Zhu, Ruoming Jin, Karin G. Coifman","Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity. We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item prediction and both zero-shot and chain-of-thought prompting for Big Five trait inference. All models showed high test-retest reliability, but construct validity was limited: correlations with ground-truth scores were weak (max Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa<0.10$), and predictions were biased toward moderate or high trait levels. Chain-of-thought prompting and longer input context modestly improved distributional alignment, but not trait-level accuracy. These results underscore limitations in current LLM-based personality inference and highlight the need for evidence-based development for psychological applications.",10.48550/arXiv.2507.14355,['JournalArticle'],Computer Science; Psychology
Intelligent Navigation Dialect Detection and Recognition Based on Multimodal Large Language Model,2024,Yanzhuo Wang,"This paper discusses the research methods of dialect detection and recognition in intelligent navigation systems based on multimodal large language models, points out the development trend of today's intelligent navigation systems and the important application of speech recognition technology in them. It focuses on the progress, basic principles and practical applications of current research, and summarizes the key technologies of dialect detection, including data collection, model design and system integration, by reviewing a large number of literatures. Specifically, this paper covers the acquisition and fusion of voice data and image data, feature extraction and recognition algorithms based on large language models, multimodal fusion strategies, and optimization methods for the system in terms of real-time performance and user experience. Through these technical means, it aims to improve the adaptability and user experience of intelligent navigation systems in multilingual environments, and provide more accurate and personalized navigation services.",10.62051/ijcsit.v4n1.10,"['JournalArticle', 'Review']",Computer Science; Linguistics
'AI is from the devil.' Behaviors and Concerns Toward Personal Data Sharing with LLM-based Conversational Agents,2025,"No Zufferey, Sarah Abdelwahab Gaballah, Karola Marky, Verena Zimmermann","With the increased performance of large language models (LLMs), conversational agents (CA), such as ChatGPT, are nowadays available to any individual requiring little technical knowledge and skills. Initial studies that have investigated related privacy risks primarily focused on either technical aspects and misuse of these tools, or captured overall perceptions of CA users in small-scale qualitative evaluations. Complementing and extending previous work, we used a quantitative user-centered approach to analyze and compare the behaviors and concerns of users and non-users. We conducted a survey study (N=422) with (1) service users, i.e., users of CA services, (2) local users, i.e., users of a local instance of CA (partially local users, or fully local users), and (3) non-users. We collected self-reported usage patterns and personal data-sharing behavior as well as privacy concerns related to different types of personal data (e.g., health data, demographics, or opinions). Furthermore, we analyze individuals' intention to use CA services in multiple scenarios. Our findings show that users of CA services generally have fewer privacy concerns than non-users. While users rarely share data related to personal identifiers and account credentials, they tend to often share data related to lifestyle, health, standard of living, and opinions. Surprisingly, partially local users tend to share more data with CA services as they also generally use CA services more often and for more diverse purposes. Also, while the majority of CA services users declared not being willing to prioritize CA services as an information source in the described scenarios such as seeking legal advice, between about one-quarter and one-third of partially local users would use CA services for all scenarios. Furthermore, half of the users were willing to stop using CA for privacy reasons (e.g., in case of data leaks), whereas a large majority of non-users reported not using CAs simply because they do not have the need or the opportunity. Our work highlights the high privacy risks for CA services users as CA services largely expand the amount of any type of personal information that can be collected by companies.",10.56553/popets-2025-0086,"['JournalArticle', 'Review']",Computer Science; Sociology
Transforming Role Classification in Scientific Teams Using LLMs and Advanced Predictive Analytics,2025,"Wonduk Seo, Yi Bu","
 Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publicationssuch as author-publication history, author affiliation, research topics, and citation countswe achieve an F1 score of 0.76, demonstrating robust classification of author roles.
 
 
 https://www.webofscience.com/api/gateway/wos/peer-review/10.1162/qss_a_00360
",10.48550/arXiv.2501.07267,"['JournalArticle', 'Review']",Computer Science
Prompting an LLM Chatbot to Role Play Conversational Situations for Language Practice,2025,"Pablo Gervs, Carlos Len, Mayuresh Kumar, Gonzalo Mndez, Susana Bautista",": Chatbots based on Large Language Models (LLMs) have demonstrated a remarkable ability to engage in conversations that are linguistically correct and make sense from a pragmatic point of view. A significant trait of their proven abilities is that, using verbal instructions provided as contributions to an ongoing conversation, they can be configured to provide specific content and/or modify the role that they play in the exchange. The present paper explores the feasibility of developing a framework of prompts designed with such an aim in mind. The prompting should ensure that the chatbot engages a language learner in an interaction where it proposes conversational situations of appropriate complexity, takes part in them playing the role of one of the participants, while monitoring the linguistic correctness of the contributions by the learner and providing feedback on their language performance both proactively and in response to learner requests. The paper reports on an experiment that tested this type of functionality students of Spanish as a second language at Aligarh Muslim University in India.",10.5220/0013235400003932,['JournalArticle'],Computer Science; Linguistics
Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference,2025,"S. Wang, Sai Teja Peddinti, Nina Taft, Nick Feamster","Large Language Models (LLMs) such as ChatGPT can infer personal attributes from seemingly innocuous text, raising privacy risks beyond memorized data leakage. While prior work has demonstrated these risks, little is known about how users estimate and respond. We conducted a survey with 240 U.S. participants who judged text snippets for inference risks, reported concern levels, and attempted rewrites to block inference. We compared their rewrites with those generated by ChatGPT and Rescriber, a state-of-the-art sanitization tool. Results show that participants struggled to anticipate inference, performing a little better than chance. User rewrites were effective in just 28\% of cases - better than Rescriber but worse than ChatGPT. We examined our participants'rewriting strategies, and observed that while paraphrasing was the most common strategy it is also the least effective; instead abstraction and adding ambiguity were more successful. Our work highlights the importance of inference-aware design in LLM interactions.",10.48550/arXiv.2509.12152,"['JournalArticle', 'Review']",Computer Science
Tip of the Tongue Query Elicitation for Simulated Evaluation,2025,"Yifan He, To Eun Kim, Fernando Diaz, Jaime Arguello, Bhaskar Mitra","Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a specific identifier, such as a document title. While common, existing search systems often fail to effectively support TOT scenarios. Research on TOT retrieval is further constrained by the challenge of collecting queries, as current approaches rely heavily on community question-answering (CQA) websites, leading to labor-intensive evaluation and domain bias. To overcome these limitations, we introduce two methods for eliciting TOT queries - leveraging large language models (LLMs) and human participants - to facilitate simulated evaluations of TOT retrieval systems. Our LLM-based TOT user simulator generates synthetic TOT queries at scale, achieving high correlations with how CQA-based TOT queries rank TOT retrieval systems when tested in the Movie domain. Additionally, these synthetic queries exhibit high linguistic similarity to CQA-derived queries. For human-elicited queries, we developed an interface that uses visual stimuli to place participants in a TOT state, enabling the collection of natural queries. In the Movie domain, system rank correlation and linguistic similarity analyses confirm that human-elicited queries are both effective and closely resemble CQA-based queries. These approaches reduce reliance on CQA-based data collection while expanding coverage to underrepresented domains, such as Landmark and Person. LLM-elicited queries for the Movie, Landmark, and Person domains have been released as test queries in the TREC 2024 TOT track, with human-elicited queries scheduled for inclusion in the TREC 2025 TOT track. Additionally, we provide source code for synthetic query generation and the human query collection interface, along with curated visual stimuli used for eliciting TOT queries.",10.48550/arXiv.2502.17776,"['JournalArticle', 'Conference']",Computer Science
The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs,2025,"P. Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. M. Alvarez","Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.",10.48550/arXiv.2509.03730,['JournalArticle'],Computer Science; Mathematics; Psychology
Taming the Titans: A Survey of Efficient LLM Inference Serving,2025,"Ranran Zhen, Juntao Li, Yixin Ji, Zhenlin Yang, Tong Liu, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang","Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",10.48550/arXiv.2504.19720,"['JournalArticle', 'Review']",Computer Science
Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases,2025,"Rena Gao, Xuetong Wu, Tatsuki Kuribayashi, Mingrui Ye, Siya Qi, Carsten Roever, Yuanxing Liu, Zheng Yuan, Jey Han Lau","This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.",10.48550/arXiv.2502.14507,"['JournalArticle', 'Conference']",Computer Science; Linguistics
Existential Crisis: A Social Robot's Reason for Being,2025,"Dora Medgyesy, Joella Galas, Julian van Pol, Rustam Eynaliyev, Thijs Vollebregt","As Robots become ever more important in our daily lives there's growing need for understanding how they're perceived by people. This study aims to investigate how the user perception of robots is influenced by displays of personality. Using LLMs and speech to text technology, we designed a within-subject study to compare two conditions: a personality-driven robot and a purely task-oriented, personality-neutral robot. Twelve participants, recruited from Socially Intelligent Robotics course at Vrije Universiteit Amsterdam, interacted with a robot Nao tasked with asking them a set of medical questions under both conditions. After completing both interactions, the participants completed a user experience questionnaire measuring their emotional states and robot perception using standardized questionnaires from the SRI and Psychology literature.",10.48550/arXiv.2501.03376,['JournalArticle'],Computer Science; Psychology
Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models,2025,"Yin Jou Huang, Rafik Hadfi","Self-report questionnaires have long been used to assess LLM personality traits, yet they fail to capture behavioral nuances due to biases and meta-knowledge contamination. This paper proposes a novel multi-observer framework for personality trait assessments in LLM agents that draws on informant-report methods in psychology. Instead of relying on self-assessments, we employ multiple observer agents. Each observer is configured with a specific relational context (e.g., family member, friend, or coworker) and engages the subject LLM in dialogue before evaluating its behavior across the Big Five dimensions. We show that these observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in LLM self-assessments. We also found that aggregating responses from 5 to 7 observers reduces systematic biases and achieves optimal reliability. Our results highlight the role of relationship context in perceiving personality and demonstrate that a multi-observer paradigm offers a more reliable, context-sensitive approach to evaluating LLM personality traits.",10.48550/arXiv.2504.08399,['JournalArticle'],Computer Science; Psychology
Identifying Non-Replicable Social Science Studies with Language Models,2025,"Denitsa Saynova, Kajsa Hansson, B. Bruinsma, Annika Fred'en, Moa Johansson","In this study, we investigate whether LLMs can be used to indicate if a study in the behavioural social sciences is replicable. Using a dataset of 14 previously replicated studies (9 successful, 5 unsuccessful), we evaluate the ability of both open-source (Llama 3 8B, Qwen 2 7B, Mistral 7B) and proprietary (GPT-4o) instruction-tuned LLMs to discriminate between replicable and non-replicable findings. We use LLMs to generate synthetic samples of responses from behavioural studies and estimate whether the measured effects support the original findings. When compared with human replication results for these studies, we achieve F1 values of up to $77\%$ with Mistral 7B, $67\%$ with GPT-4o and Llama 3 8B, and $55\%$ with Qwen 2 7B, suggesting their potential for this task. We also analyse how effect size calculations are affected by sampling temperature and find that low variance (due to temperature) leads to biased effect estimates.",10.48550/arXiv.2503.10671,['JournalArticle'],Computer Science; Psychology
BERT-Based Model and LLMs-Generated Synthetic Data for Conflict Sentiment Identification in Aspect-Based Sentiment Analysis,2025,"N. Nuryani, Rinaldi Munir, Ayu Purwarianti, Dessi Puji Lestari","Aim/Purpose: Most research on aspect-based sentiment analysis (ABSA) classifies sentiment polarity into two classes (positive and negative) or three classes (positive, negative, and neutral) and does not include conflict sentiment. This study investigates the four-class sentiment classification (positive, negative, neutral, and conflict) and proposes a BERT-based model for identifying conflicting sentiment in ABSA. Furthermore, we employ the open-source large language models (LLMs) created by Meta, Llama 3, for generating synthetic data to support research on four-class sentiment classification in ABSA.

Background: Public opinions and experiences on product reviews, social events, political movements, etc. can be used for exploring customer behavior, predicting customer preferences, understanding public sentiment, etc., so it becomes an important component in the decision-making process. Providing an accurate opinion will enable an individual, business, or organization to have an in-formed judgement before making a decision.

Methodology: This study utilizes a methodology that includes generating synthetic data to augment the original datasets, designing the input representation, detecting aspect categories, performing a multi-label sentiment classification, and rep-resenting sentiment in a four-class sentiment classification.

Contribution: This study provides an investigation on the four-class sentiment classification (positive, negative, neutral, and conflict) and proposes a BERT-based method to identify aspects with conflicting sentiment in ABSA. Moreover, it also evaluates Llama 3 for generating synthetic data to address the issues related to the data scarcity and imbalance datasets in the research of four-class senti-ment classification in ABSA.

Findings: The investigation of the four-class sentiment classification task in ABSA demonstrates that identifying conflict sentiment is challenging for several reasons. Among them are (1) the lack of a public dataset for this research; (2) the small amount of data with conflict labels in the available dataset resulting in an imbalanced dataset; (3) conflict sentiment is a complex sentiment con-taining both positive and negative sentiments; and (4) conflicting sentiments are usually expressed in long and complicated sentences and involve implicit aspects. Our solution to these challenges involved generating synthetic data using Llama 3 and designing a BERT-based model on multi-label aspects for identifying aspect with conflict sentiment.

Recommendations for Practitioners: Most existing ABSA models with four-class sentiment classification are con-ducted for the product reviews (mostly in the restaurant domain) and in high-resource languages (mainly in English). Therefore, users may need to make some adjustments to different domains and languages.

Recommendation for Researchers: Due to the limited datasets available for research in aspect-based sentiment analysis with four-class sentiment classification, the development of a dataset to support this research is urgently needed.

Impact on Society: By providing more accurate sentiment through aspect-based sentiment analy-sis, this study can better help people, organizations, or companies in get a view or an opinion about any product, service, or candidate in an electoral vote.

Future Research: Future research on aspect-based sentiment analysis could utilize the large language models (LLMs) for conducting ABSA tasks including aspect term extraction, aspect category detection, and its sentiment polarities. We could also focus on evaluating the model for cross-domain and cross-language AB-SA system.

",10.28945/5439,"['JournalArticle', 'Review']",Computer Science; Linguistics
Large Language Model Employment for Story Point Estimation Problems in AGILE Development,2024,"B. Permana, R. Ferdiana, A. Pratama","Software effort estimation (SEE) in Agile Software Development (ASD) has been a longstanding challenge for software engineers. The traditional approach estimates effort scores known as Story Point (SP) to measure user stories. The approach was an expert judgment method in which experts propose certain values of story points based on their experience through various mechanisms such as planning poker, Dot Voting, Bucket System, etc. The process will become challenging if experts have different opinions about the value of certain story points, which may result in a longer duration for completion. Many algorithms have been proposed to optimize the method. The utilization of Artificial Intelligence (AI) and Machine Learning (ML) has greatly simplified processes. Various algorithms are utilized, including Fuzzy, SVM, Naive Bayes, KNN, ANN, RNN, CNN, and LSTM, and utilizing Large Language Models (LLMs) for advancement. This paper employs the hybrid model of Bidirectional Encoder Representations from Transformers (BERT) - Multilayer Perceptron (MLP), which we call alterBERT, to perform regression for estimating Story Points (SP). We will assess the model over the TAWOS dataset and compare it with the previous model, GPT2SP, and other GPT2 architecture. The dataset consists of issues from 39 open-source projects mined from JIRA Software. According to the experimental result, we conclude that the alterBERT models have better performance, outperforming the other models with an average MAE and RMSE, respectively, about 1.72 and 2.51",10.1109/ICECOS63900.2024.10791206,['Conference'],Computer Science
Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins: Incorporating Human Feedback with LLM-Powered Digital Twins,2025,"Amanda Chan, Catherine Di, Joseph Rupertus, Gary D Smith, Varun Nagaraj Rao, Manoel Horta Ribeiro, Andrs Monroy-Hernndez","Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for research, yet workers growing use of generative AI tools poses challenges. Researchers face compromised data validity as AI responses replace authentic human behavior, while workers risk diminished roles as AI automates tasks. To address this, we propose a hybrid framework using digital twins, personalized AI models that emulate workers behaviors and preferences while keeping humans in the loop. We evaluate our system with an experiment (n=88 crowd workers) and in-depth interviews with crowd workers (n=5) and social science researchers (n=4). Our results suggest that digital twins may enhance productivity and reduce decision fatigue while maintaining response quality. Both researchers and workers emphasized the importance of transparency, ethical data use, and worker agency. By automating repetitive tasks and preserving human engagement for nuanced ones, digital twins may help balance scalability with authenticity.",10.1145/3706599.3720269,"['JournalArticle', 'Book']",Computer Science; Psychology
Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging,2025,"Rajeev Kumar, Harishankar Kumar, Kumari Shalini","Personalized messaging plays an essential role in improving communication in areas such as healthcare, education, and professional engagement. This paper introduces a framework that uses the Knowledge Graph (KG) to dynamically rephrase written communications by integrating individual and contextspecific data. The knowledge graph represents individuals, locations, and events as critical nodes, linking entities mentioned in messages to their corresponding graph nodes. The extraction of relevant information, such as preferences, professional roles, and cultural norms, is then combined with the original message and processed through a large language model (LLM) to generate personalized responses. The framework demonstrates notable message acceptance rates in various domains: $\mathbf{4 2 \%}$ in healthcare, 53% in education, and 78% in professional recruitment. By integrating entity linking, event detection, and language modeling, this approach offers a structured and scalable solution for context-aware, audience-specific communication, facilitating advanced applications in diverse fields.",10.1109/MPSecICETA64837.2025.11118478,"['JournalArticle', 'Conference']",Computer Science
CHAT GPT: EXPLORING THE CAPABILITIES AND LIMITATIONS OF A LARGE LANGUAGE MODEL,2023,"Shubham Singh, S. Tiwari, Prabhat Kumar Yadav, Dr. Atul Kumar, D. Srivastava","ChatGPT is a popular computer program that simulates human communication. It can comprehend normal language and respond in human-like ways. It uses complicated algorithms that have been trained on a large quantity of text data to provide logical responses. This paper explains the design and training process of ChatGPT, as well as its capacity to respond to a range of inquiries. This research also investigates ChatGPT's disadvantages, such as its proclivity to create biased replies, as well as the potential ethical implications of its employment.",10.33564/ijeast.2023.v07i12.019,['JournalArticle'],Computer Science; Linguistics
Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-in-the-Loop LLM,2024,"Jiachen Li, Xiwen Li, Justin Steinberg, Akshat Choube, Bingsheng Yao, Xuhai Xu, Dakuo Wang, Elizabeth D. Mynatt, Varun Mishra","Passive tracking methods, such as phone and wearable sensing, have become dominant in monitoring human behaviors in modern ubiquitous computing studies. While there have been significant advances in machine-learning approaches to translate periods of raw sensor data to model momentary behaviors, (e.g., physical activity recognition), there still remains a significant gap in the translation of these sensing streams into meaningful, high-level, context-aware insights that are required for various applications (e.g., summarizing an individual's daily routine). To bridge this gap, experts often need to employ a context-driven sensemaking process in real-world studies to derive insights. For instance, current approaches in the field can reliably predict ""walking outdoors"" by contextualizing accelerometer and GPS data. Sensemaking, however, involves being able to notice patterns of periodic ""walking"" and ""stationary"" events, and even infer ""walking the dog"" after realizing the alignment with regular routines reported through surveys or self-reports. This process often requires manual effort and can be challenging even for experienced researchers due to the complexity of human behaviors.
 We conducted three rounds of user studies with 21 experts to explore solutions to address challenges with sensemaking. We follow a human-centered design process to identify needs and design, iterate, build, and evaluate Vital Insight (VI), a novel, LLM-assisted, prototype system to enable human-in-the-loop inference (sensemaking) and visualizations of multimodal passive sensing data from smartphones and wearables. Using the prototype as a technology probe, we observe experts' interactions with it and develop an expert sensemaking model that explains how experts move between direct data representations and AI-supported inferences to explore, question, and validate insights. Through this iterative process, we also synthesize and discuss a list of design implications for the design of future AI-augmented visualization systems to better assist experts' sensemaking processes in multi-modal health sensing data.",10.1145/3749508,"['JournalArticle', 'Review']",Computer Science
Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making,2025,"Claudio Fanconi, M. Schaar","Effective human-AI decision-making balances three key factors: the \textit{correctness} of predictions, the \textit{cost} of knowledge and reasoning complexity, and the confidence about whether to \textit{abstain} automated answers or involve human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise -- a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base model's answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, we incorporate an online learning mechanism in the framework that can leverage human feedback to improve decision quality over time. We demonstrate this approach to general question-answering (ARC-Easy and ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results show that our cascaded strategy outperforms in most cases single-model baselines in accuracy while reducing cost and providing a principled way to handle abstentions.",10.48550/arXiv.2506.11887,['JournalArticle'],Computer Science
"AI-Powered Negotiations: Opportunities, Challenges, and the Future of Business Strategy",2024,Hanjie Li,"Business negotiations are integral to various aspects of enterprise operations, from recruitment and resource allocation to procurement and international business collaborations. Despite their critical importance, negotiations often remain cumbersome, complex, and time-consuming, with outcomes heavily influenced by the negotiator's skills and external factors, frequently failing to maximize benefits or achieve desired goals. The rapid advancement of artificial intelligence (AI) technologies, particularly large language models (LLMs) like ChatGPT, has seen AI increasingly integrated into daily tasks such as drafting emails and querying legal clauses. This paper investigates the potential of AI and negotiation agents to enhance efficiency and optimize outcomes in business negotiations. Utilizing case studies, the research explores how AI can assist or even replace human negotiators by automating data collection, analyzing historical negotiation patterns, predicting outcomes, and suggesting optimal strategies. The findings indicate that AI negotiation agents can surpass human performance in certain negotiation scenarios, providing consistent, data-driven insights and adapting strategies in real time based on counterpart responses. However, the research also highlights the limitations of AI in understanding human emotions, cultural nuances, and the ethical implications of AI decision-making. The conclusion emphasizes the promising role of AI in revolutionizing business negotiations while underscoring the necessity for human oversight to address emotional and ethical complexities. This study provides a foundation for further exploration into the integration of AI in negotiation processes, aiming to enhance both efficiency and effectiveness in achieving business objectives.",10.62051/dg1trh68,['JournalArticle'],Business; Computer Science
Revealing Political Bias in LLMs through Structured Multi-Agent Debate,2025,"Aishwarya Bandaru, Fabian Bindley, Trevor Bluth, Nandini Chavda, Baixu Chen, Ethan Law","Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.",10.48550/arXiv.2506.11825,['JournalArticle'],Computer Science; Political Science
Multi-Prompting Scenario-based Movie Recommendation with Large Language Models: Real User Case Study,2025,"Ruixuan Sun, Xinyi Li, A K Akella, Joseph A. Konstan","This paper investigates the potential of using large language models (LLMs) for personalized movie recommendations in an online field experiment. We assess the performance of LLM recommenders using a combination of between-subject prompts, historical consumption patterns, and within-subject recommendation scenarios. Analyzing conversation and survey data from 160 active users, we find that while LLMs excel in providing explainable recommendations, they lack in personalization, diversity, and user trust. Interestingly, personalized prompting techniques do not significantly affect user-perceived recommendation quality, while the number of movies a user has watched plays a more significant role. Furthermore, LLMs demonstrate a stronger ability to recommend lesser-known or niche movies. Through qualitative analysis, we identify key conversational patterns linked to positive and negative user interaction experiences and conclude that providing personal context and examples is crucial for obtaining high-quality recommendations from LLMs. These insights offer practical implications for improving LLM-based RecSys in real-world applications.",10.1145/3706599.3706682,"['JournalArticle', 'Book', 'Review']",Computer Science
PsyPrompt: LLM Prompt Patterns for Goal Contents Pursuit on Social Media,2024,"Mengli Yu, Yizhunan Zhou, Jiahui Wang, Zhenkun Zhou","Goal contents pursuit serves as an effective indicator to predict individual behaviors and life satisfaction. In current studies, the measures of self-reports are limited by the expenses and subjectivity, Despite the fact that machine learning and deep learning models can effectively utilize user data from social media to objectively classify goal contents pursuit, the workload associated with supervised learning is substantial. By using appropriate prompt patterns, Large Language Models (LLM) can execute diverse tasks in a prompt manner. Thus, we develop six prompt patterns for classifying goal content pursuits of users on social media. The results indicate that some of these prompt patterns successfully classify intrinsic and extrinsic goals. Overall, this research presents an usable prompting paradigm and approach, PsyPrompt, which enhances the methodology of objectively classifying individual goal content pursuits using LLMs.",10.1109/DSAA61799.2024.10722830,"['JournalArticle', 'Conference']",Computer Science; Psychology
"""It Listens Better Than My Therapist"": Exploring Social Media Discourse on LLMs as Mental Health Tool",2025,Anna Haensch,"The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.",10.48550/arXiv.2504.12337,['JournalArticle'],Computer Science; Psychology
Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL,2024,"Qihuang Zhong, Kunfeng Chen, Liang Ding, Juhua Liu, Bo Du, D. Tao","Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy in real-world applications, highlighting the importance of compressing them. To achieve this goal, knowledge distillation (KD) is a common approach, which aims to distill the larger teacher model into a smaller student model. While numerous KD methods for autoregressive LLMs have emerged recently, it is still under-explored whether they work well in complex text-to-SQL scenarios. To this end, we conduct a series of analyses and reveal that these KD methods generally fall short in balancing performance and efficiency. In response to this problem, we propose to improve the KD with Imperfect Data, namely KID, which effectively boosts the performance without introducing much training budget. The core of KID is to efficiently mitigate the training-inference mismatch by simulating the cascading effect of inference in the imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks show that, KID can not only achieve consistent and significant performance gains (up to +5.83% average score) across all model types and sizes, but also effectively improve the training efficiency.",10.48550/arXiv.2410.11371,"['JournalArticle', 'Conference']",Computer Science
Harnessing the Power of LLMs: LLM Summarization for Human-Centric DAST Reports,2024,"Arpit Thool, Chris Brown","Dynamic Application Security Testing (DAST) tools test web application security by simulating attacks on its front end and evaluating it externally like a malicious attacker. DAST tools aim to identify vulnerabilities and provide recommendations for improving security. However, the security alerts generated by these tools are lengthy and contain numerous details that may not be relevant to a software practitioner seeking a quick overview of the results. To solve this challenge, we propose using Large Language Models (LLMs) to summarize the alerts generated by DAST tools. We generated security alerts using two popular DAST tools: Burp Suite and ZAP. Then, we generated various summaries of these alerts using five different LLMs. We surveyed 48 software practitioners to understand the challenges software practitioners face when specifically dealing with DAST reports and determine the effectiveness of the LLM-generated summaries in understanding the security issue. The results bring to light various challenges software practitioners face and indicate that the LLM-generated summaries are clearer and more comprehensible in understanding the security issue and, hence, more preferred. This approach can significantly improve the security of software products by making the security alerts more accessible to different stakeholders, making the software product more robust and resilient to cyber threats.",10.1109/VL/HCC60511.2024.00014,"['JournalArticle', 'Review']",Computer Science
Ideology and Policy Preferences in Synthetic Data: The Potential of LLMs for Public Opinion Analysis,2025,"Keyeun Lee, Jaehyuk Park, Suh-hee Choi, Changkeun Lee","This study investigates whether large language models (LLMs) can meaningfully extend or generate synthetic public opinion survey data on labor policy issues in South Korea. Unlike prior work conducted on peoples general sociocultural values or specific political topics such as voting intentions, our research examines policy preferences on tangible social and economic topics, offering deeper insights for news media and data analysts. In two key applications, we first explore whether LLMs can predict public sentiment on emerging or rapidly evolving issues using existing survey data. We then assess how LLMs generate synthetic datasets resembling real-world survey distributions. Our findings reveal that while LLMs capture demographic and ideological traits with reasonable accuracy, they tend to overemphasize ideological orientation for politically charged topicsa bias that is more pronounced in fully synthetic data, raising concerns about perpetuating societal stereotypes. Despite these challenges, LLMs hold promise for enhancing data-driven journalism and policy research, particularly in polarized societies. We call for further study into how LLM-based predictions align with human responses in diverse sociopolitical settings, alongside improved tools and guidelines to mitigate embedded biases.",10.17645/mac.9677,"['JournalArticle', 'Review']",Political Science
Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing,2025,"Dennis Ulmer, Alexandra Lorson, Ivan Titov, Christian Hardmeier","Human users increasingly rely on natural language interactions with large language models (LLMs) in order to receive help on a large variety of tasks and problems. However, the trustworthiness and perceived legitimacy of LLMs is undermined by the fact that their output is frequently stated in very confident terms, even when its accuracy is questionable. Therefore, there is a need to signal the confidence of the language model to a user in order to reap the benefits of human-machine collaboration and mitigate potential harms. Verbalized uncertainty is the expression of confidence with linguistic means, an approach that integrates perfectly into language-based interfaces. Nevertheless, most recent research in natural language processing (NLP) overlooks the nuances surrounding human uncertainty communication and the data biases that influence machine uncertainty communication. We argue for anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty communication requires a degree of linguistic authenticity and personalization to the user, which could be achieved by emulating human communication. We present a thorough overview over the research in human uncertainty communication, survey ongoing research, and perform additional analyses to demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by pointing out unique factors in human-machine communication of uncertainty and deconstruct anthropomimetic uncertainty into future research directions for NLP.",10.48550/arXiv.2507.10587,"['JournalArticle', 'Review']",Computer Science; Linguistics
Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants,2025,"Milapji Singh Gill, Javal Vyas, Artan Markaj, Felix Gehlhoff, Mehmet Mercangz","Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.",10.48550/arXiv.2505.02076,['JournalArticle'],Computer Science; Engineering
Chatbots and Customer Service: AI as a Key Tool for Customer Interaction,2024,"Peter Murr, Igor Piatrov","The democratisation of AI chatbot technology is transforming the manner in which organisations engage with customers and market their products. This paper examines the historical development of chatbots, tracing their origins in the mid-20th century to their current applications utilising large language models (LLM), natural language processing (NLP), and machine learning (ML). Particular emphasis is placed on no-code and low-code platforms, which facilitate the creation and training of bespoke chatbots for users without technical expertise. These tools represent a significant step in democratising technological innovation, lowering barriers to entry for companies of all sizes. Furthermore, the article emphasises the advantages of AI chatbots for personalising communications, lead generation and customer data collection. However, it also identifies potential limitations, including restricted flexibility, scaling issues and data protection concerns. The findings indicate that AI chatbots are not merely a technological advancement, but a pivotal component of a new era of marketing communications, where accessibility and efficiency are becoming paramount. The future of this technology will depend on the capacity of companies to adapt its capabilities to evolving customer expectations and market dynamics.",10.34135/mmidentity-2024-51,['JournalArticle'],Business; Computer Science
Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark,2025,"S. Mahmoud Bsharat, Mukul Ranjan, Aidar Myrzakhan, Jiacheng Liu, Bowei Guo, Shengkun Tang, Zhuang Liu, Yuanzhi Li, Zhiqiang Shen","Rapid advancements in large language models (LLMs) have increased interest in deploying them on mobile devices for on-device AI applications. Mobile users interact differently with LLMs compared to desktop users, creating unique expectations and data biases. Current benchmark datasets primarily target at server and desktop environments, and there is a notable lack of extensive datasets specifically designed for mobile contexts. Additionally, mobile devices face strict limitations in storage and computing resources, constraining model size and capabilities, thus requiring optimized efficiency and prioritized knowledge. To address these challenges, we introduce Mobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence. It consists of 16,186 questions across 80 mobile-related fields, designed to evaluate LLM performance in realistic mobile scenarios. A challenging subset, Mobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but significantly more difficult than our standard full set. Both benchmarks use multiple-choice, order-invariant questions focused on practical mobile interactions, such as recipe suggestions, travel planning, and essential daily tasks. The dataset emphasizes critical mobile-specific metrics like inference latency, energy consumption, memory usage, and response quality, offering comprehensive insights into model performance under mobile constraints. Moreover, it prioritizes privacy and adaptability, assessing models' ability to perform on-device processing, maintain user privacy, and adapt to personalized usage patterns. Mobile-MMLU family offers a standardized framework for developing and comparing mobile-optimized LLMs, enabling advancements in productivity and decision-making within mobile computing environments. Our code and data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",10.48550/arXiv.2503.20786,['JournalArticle'],Computer Science
Human Preferences for Constructive Interactions in Language Model Alignment,2025,"Yara Kyrychenko, Jon Roozenbeek, Brandon Davidson, S. V. D. Linden, Ramit Debnath","As large language models (LLMs) enter the mainstream, aligning them to foster constructive dialogue rather than exacerbate societal divisions is critical. Using an individualized and multicultural alignment dataset of over 7,500 conversations of individuals from 74 countries engaging with 21 LLMs, we examined how linguistic attributes linked to constructive interactions are reflected in human preference data used for training AI. We found that users consistently preferred well-reasoned and nuanced responses while rejecting those high in personal storytelling. However, users who believed that AI should reflect their values tended to place less preference on reasoning in LLM responses and more on curiosity. Encouragingly, we observed that users could set the tone for how constructive their conversation would be, as LLMs mirrored linguistic attributes, including toxicity, in user queries.",10.48550/arXiv.2503.16480,['JournalArticle'],Computer Science; Linguistics; Psychology
Self-Improving Embodied Foundation Models,2025,"Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag R. Sanketi, Igor Mordatch","Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at https://self-improving-efms.github.io .",10.48550/arXiv.2509.15155,['JournalArticle'],Computer Science; Engineering
Do LLMs have a Gender (Entropy) Bias?,2025,"Sonal Prabhune, Balaji Padmanabhan, Kaushik Dutta","We investigate the existence and persistence of a specific type of gender bias in some of the popular LLMs and contribute a new benchmark dataset, RealWorldQuestioning (released on HuggingFace ), developed from real-world questions across four key domains in business and health contexts: education, jobs, personal financial management, and general health. We define and study entropy bias, which we define as a discrepancy in the amount of information generated by an LLM in response to real questions users have asked. We tested this using four different LLMs and evaluated the generated responses both qualitatively and quantitatively by using ChatGPT-4o (as""LLM-as-judge""). Our analyses (metric-based comparisons and""LLM-as-judge""evaluation) suggest that there is no significant bias in LLM responses for men and women at a category level. However, at a finer granularity (the individual question level), there are substantial differences in LLM responses for men and women in the majority of cases, which""cancel""each other out often due to some responses being better for males and vice versa. This is still a concern since typical users of these tools often ask a specific question (only) as opposed to several varied ones in each of these common yet important areas of life. We suggest a simple debiasing approach that iteratively merges the responses for the two genders to produce a final result. Our approach demonstrates that a simple, prompt-based debiasing strategy can effectively debias LLM outputs, thus producing responses with higher information content than both gendered variants in 78% of the cases, and consistently achieving a balanced integration in the remaining cases.",10.48550/arXiv.2505.20343,['JournalArticle'],Business; Computer Science
AI persuading AI vs AI persuading Humans: LLMs' Differential Effectiveness in Promoting Pro-Environmental Behavior,2025,"Alexander Doudkin, Pat Pataranutaporn, Pattie Maes","Pro-environmental behavior (PEB) is vital to combat climate change, yet turning awareness into intention and action remains elusive. We explore large language models (LLMs) as tools to promote PEB, comparing their impact across 3,200 participants: real humans (n=1,200), simulated humans based on actual participant data (n=1,200), and fully synthetic personas (n=1,200). All three participant groups faced personalized or standard chatbots, or static statements, employing four persuasion strategies (moral foundations, future self-continuity, action orientation, or""freestyle""chosen by the LLM). Results reveal a""synthetic persuasion paradox"": synthetic and simulated agents significantly affect their post-intervention PEB stance, while human responses barely shift. Simulated participants better approximate human trends but still overestimate effects. This disconnect underscores LLM's potential for pre-evaluating PEB interventions but warns of its limits in predicting real-world behavior. We call for refined synthetic modeling and sustained and extended human trials to align conversational AI's promise with tangible sustainability outcomes.",10.48550/arXiv.2503.02067,['JournalArticle'],Computer Science; Environmental Science
Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling,2024,"Daehoon Gwak, Junwoo Park, Minho Park, chaeHun Park, Hyunchan Lee, Edward Choi, Jaegul Choo","Predicting future international events from textual information, such as news articles, has tremendous potential for applications in global policy, strategic decision-making, and geopolitics. However, existing datasets available for this task are often limited in quality, hindering the progress of related research. In this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction), a novel dataset designed to address these limitations by leveraging the advanced reasoning capabilities of large-language models (LLMs). Our dataset features high-quality scoring labels generated through advanced prompt modeling and rigorously validated by domain experts in political science. We showcase the quality and utility of WORLDREP for real-world event prediction tasks, demonstrating its effectiveness through extensive experiments and analysis. Furthermore, we publicly release our dataset along with the full automation source code for data collection, labeling, and benchmarking, aiming to support and advance research in text-based event prediction.",10.18653/v1/2024.findings-emnlp.526,"['JournalArticle', 'Conference']",Computer Science; Political Science
MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation,2025,"Pravallika Abbineni, Saoud Aldowaish, Colin Liechty, Soroosh Noorzad, Ali Ghazizadeh, Morteza Fayazi","Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.",10.48550/arXiv.2508.08137,"['JournalArticle', 'Review']",Computer Science; Engineering
"Illuminate: Depression diagnosis, explanation and proactive therapy using prompt engineering",2024,"Aryan Agrawal, Nidhi Gupta","Traditional methods of depression detection on social media forums can classify whether a user is depressed, but they often lack the capacity for human-like explanations and interactions. This paper proposes a next-generation paradigm for depression detection and treatment strategies. It employs three Large Language Models (LLMs) - Generative Pre-trained Transformer 4, Llama, and Gemini, each fine-tuned using specially engineered prompts to effectively diagnose, explain, and suggest therapeutic interventions for depression. These prompts are designed to guide the models in analyzing textual data from clinical interviews and online forums, ensuring nuanced and context-aware responses. The study utilizes a few-shot prompting methodology for the Diagnosis and Explanation component. This technique is optimized to provide DSM-5 based analysis and explanation, enhancing the models ability to identify and articulate depressive symptoms accurately. Models engage in empathetic dialogue management, guided by resources from Psychology Database and a Cognitive Behavioral Therapy guide, and fine-tuned using Chain of Thought and Tree of Thought prompting techniques. This facilitates meaningful interactions with individuals facing major depressive disorders, fostering a supportive and understanding environment. The research innovates in case conceptualization, treatment planning and therapeutic interventions by creating the Illuminate Database to guide the models in offering personalized therapy. The quantitative analysis of the study is demonstrated through metrics such as F1 scores, Precision, Recall, Cosine similarity, and ROUGE score across different test sets. This comprehensive approach offered through a mobile application prototype, with established psychological methodologies showcases the potential of LLMs in revolutionizing diagnosis and treatment strategies.",10.47611/jsrhs.v13i2.6718,['JournalArticle'],Computer Science; Psychology
AI-enhanced semantic feature norms for 786 concepts,2025,"Siddharth Suresh, Kushin Mukherjee, Tyler Giallanza, Xizheng Yu, Mia Patil, Jonathan D. Cohen, Timothy T. Rogers","Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.",10.48550/arXiv.2505.10718,['JournalArticle'],Computer Science
SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control,2025,"Adithya Chittem, Aishna Shrivastava, Sai Tarun Pendela, Jagat Sesh Challa, Dhruv Kumar","Large language models (LLMs) have gained significant traction across a wide range of fields in recent years. There is also a growing expectation for them to display human-like personalities during interactions. To meet this expectation, numerous studies have proposed methods for modelling LLM personalities through psychometric evaluations. However, most existing models face two major limitations: they rely on the Big Five (OCEAN) framework, which only provides coarse personality dimensions, and they lack mechanisms for controlling trait intensity. In this paper, we address this gap by extending the Machine Personality Inventory (MPI), which originally used the Big Five model, to incorporate the 16 Personality Factor (16PF) model, allowing expressive control over sixteen distinct traits. We also developed a structured framework known as Specific Attribute Control (SAC) for evaluating and dynamically inducing trait intensity in LLMs. Our method introduces adjective-based semantic anchoring to guide trait intensity expression and leverages behavioural questions across five intensity factors: \textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and \textit{Willingness}. Through experimentation, we find that modelling intensity as a continuous spectrum yields substantially more consistent and controllable personality expression compared to binary trait toggling. Moreover, we observe that changes in target trait intensity systematically influence closely related traits in psychologically coherent directions, suggesting that LLMs internalize multi-dimensional personality structures rather than treating traits in isolation. Our work opens new pathways for controlled and nuanced human-machine interactions in domains such as healthcare, education, and interviewing processes, bringing us one step closer to truly human-like social machines.",10.48550/arXiv.2506.20993,['JournalArticle'],Computer Science; Psychology
UQABench: Evaluating User Embedding for Prompting LLMs in Personalized Question Answering,2025,"Langming Liu, Shilei Liu, Yujin Yuan, Yizhen Zhang, Bencheng Yan, Zhiyuan Zeng, Zihao Wang, Jiaqi Liu, Di Wang, Wenbo Su, Pengjie Wang, Jian Xu, Bo Zheng","Large language models (LLMs) achieve remarkable success in natural language processing (NLP). In practical scenarios like recommendations, as users increasingly seek personalized experiences, it becomes crucial to incorporate user interaction history into the context of LLMs to enhance personalization. However, from a practical utility perspective, user interactions' extensive length and noise present challenges when used directly as text prompts. A promising solution is to compress and distill interactions into compact embeddings, serving as soft prompts to assist LLMs in generating personalized responses. Although this approach brings efficiency, a critical concern emerges: Can user embeddings adequately capture valuable information and prompt LLMs? To address this concern, we propose \name, a benchmark designed to evaluate the effectiveness of user embeddings in prompting LLMs for personalization. We establish a fair and standardized evaluation process, encompassing pre-training, fine-tuning, and evaluation stages. To thoroughly evaluate user embeddings, we design three dimensions of tasks: sequence understanding, action prediction, and interest perception. These evaluation tasks cover the industry's demands in traditional recommendation tasks, such as improving prediction accuracy, and its aspirations for LLM-based methods, such as accurately understanding user interests and enhancing the user experience. We conduct extensive experiments on various state-of-the-art methods for modeling user embeddings. Additionally, we reveal the scaling laws of leveraging user embeddings to prompt LLMs. The benchmark is available online.",10.48550/arXiv.2502.19178,"['JournalArticle', 'Conference']",Computer Science
Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation,2025,"Zilin Huang, Zihao Sheng, Zhengyang Wan, Yansong Qu, Yuhao Luo, Boyue Wang, Pei Li, Yen-Jung Chen, Jiancong Chen, Keke Long, Jiayi Meng, Yue Leng, Sikai Chen","Recent advances in autonomous system simulation platforms have significantly enhanced the safe and scalable testing of driving policies. However, existing simulators do not yet fully meet the needs of future transportation research-particularly in enabling effective human-AI collaboration and modeling socially-aware driving agents. This paper introduces Sky-Drive, a novel distributed multi-agent simulation platform that addresses these limitations through four key innovations: (a) a distributed architecture for synchronized simulation across multiple terminals; (b) a multi-modal human-in-the-loop framework integrating diverse sensors to collect rich behavioral data; (c) a human-AI collaboration mechanism supporting continuous and adaptive knowledge exchange; and (d) a digital twin framework for constructing high-fidelity virtual replicas of real-world transportation environments. Sky-Drive supports diverse applications such as autonomous vehicle-human road users interaction modeling, human-in-the-loop training, socially-aware reinforcement learning, personalized driving development, and customized scenario generation. Future extensions will incorporate foundation models for context-aware decision support and hardware-in-the-loop testing for real-world validation. By bridging scenario generation, data collection, algorithm training, and hardware integration, Sky-Drive has the potential to become a foundational platform for the next generation of human-centered and socially-aware autonomous transportation systems research. The demo video and code are available at:https://sky-lab-uw.github.io/Sky-Drive-website/",10.48550/arXiv.2504.18010,['JournalArticle'],Computer Science; Engineering
Chat GPT and Cyber Risks: Need of Custom Large Language Models,2024,,"Due to growing utilization of Artificial Intelligence (AI) for small, medium, and large business organizations cybersecurity risks are increasing. While AI provides enormous opportunities to generate great insight, it also enables risks on data privacy or network security. Organizations are integrating Chat GPT to enhance customer experience by providing personalization, product recommendations, and immediate responses of users queries. However, if underline AI models have not been trained appropriately or default ready to use models have been used to generate responses, the cybersecurity risk will be increased. The paper will discuss about the prompt injection attack in which an attacker can exploit the vulnerabilities of large language models (LLM) to get sensitive data or system specific information to incorporate data breach or other cyber-attacks. The study compares the response of cybersecurity related questions for Chat GPT and Ollama LLM models to discuss the possible cyber risks and suggest the need of custom LLM models for organizations. The paper also provides an approach how these risks can be quantified using concept of Cyber Value at Risk (CVaR).",10.1109/ICDDS62937.2024.10910783,['Conference'],Computer Science
Adaptive Elicitation of Latent Information Using Natural Language,2025,"Jimmy Wang, Thomas Zollo, Richard Zemel, Hongseok Namkoong","Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.",10.48550/arXiv.2504.04204,['JournalArticle'],Computer Science
Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning,2025,"Vikranth Udandarao, N. A. Tiju, Muthuraj Vairamuthu, Harsh Mistry, Dhruv Kumar","In this paper, we present Roamify, an Artificial Intelligence powered travel assistant that aims to ease the process of travel planning. We have tested and used multiple Large Language Models like Llama and T5 to generate personalised itineraries per user preferences. Results from user surveys highlight the preference for AI powered mediums over existing methods to help in travel planning across all user age groups. These results firmly validate the potential need of such a travel assistant. We highlight the two primary design considerations for travel assistance: D1) incorporating a web-scraping method to gather up-to-date news articles about destinations from various blog sources, which significantly improves our itinerary suggestions, and D2) utilising user preferences to create customised travel experiences along with a recommendation system which changes the itinerary according to the user needs. Our findings suggest that Roamify has the potential to improve and simplify how users across multiple age groups plan their travel experiences.",10.48550/arXiv.2504.10489,"['JournalArticle', 'Review']",Computer Science
"Cogniwealth: Revolutionizing Finance, Empowering Investors, and Shaping the Future of Wealth Management",2024,"R. Ramyadevi, G. Sasidharan",Predicting financial growth for future investment is the system that is used to take all kinds of finance data and store that data in the vector database and the Llama 2 model or the system is considered as a financial advisor and if we ask some investment question or any related question towards finance it will search on the data user provided before and the response will be formatted such a way that the response is coming from a human advisor because it uses Natural language processing(NLP) to understand the human nature and respond to there emotion and the system can be used by fund researchers and a laymen user who want to invest money in any finance related sector and this system can be feed with users wanted data and for front-end the react js is used and it is a frontend library and it is single page application and it is component based library and for backend Django is used and Llama 2 and pinecone db for storing vectors of the collected data.,10.1109/IC2PCT60090.2024.10486594,['Conference'],Business; Computer Science; Economics
Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions,2025,"Gauri Kambhatla, Sanjana Gautam, Angela Zhang, Alex Liu, Ravi Srinivasan, Junyi Jessy Li, Matthew Lease","The ability to accurately predict how different population groups would answer subjective questions would have great value. In this work, we show that use of relatively simple supervision can greatly improve language model alignment with diverse population groups, as measured over three datasets spanning various topics. Beyond evaluating average performance, we also report how alignment varies across specific groups. The simplicity and generality of our approach promotes easy adoption, while our broad findings provide useful guidance for when to use or not use our approach in practice. By conducting evaluation over many LLMs and prompting strategies, along with open-sourcing our work, we provide a useful benchmark to stimulate future research.",10.48550/arXiv.2507.00439,['JournalArticle'],Computer Science; Linguistics
The realization of tones in spontaneous spoken Taiwan Mandarin: a corpus-based survey and theory-driven computational modeling,2025,"Yuxin Lu, Yu-Ying Chuang, R. Baayen","A growing body of literature has demonstrated that semantics can co-determine fine phonetic detail. However, the complex interplay between phonetic realization and semantics remains understudied, particularly in pitch realization. The current study investigates the tonal realization of Mandarin disyllabic words with all 20 possible combinations of two tones, as found in a corpus of Taiwan Mandarin spontaneous speech. We made use of Generalized Additive Mixed Models (GAMs) to model f0 contours as a function of a series of predictors, including gender, tonal context, tone pattern, speech rate, word position, bigram probability, speaker and word. In the GAM analysis, word and sense emerged as crucial predictors of f0 contours, with effect sizes that exceed those of tone pattern. For each word token in our dataset, we then obtained a contextualized embedding by applying the GPT-2 large language model to the context of that token in the corpus. We show that the pitch contours of word tokens can be predicted to a considerable extent from these contextualized embeddings, which approximate token-specific meanings in contexts of use. The results of our corpus study show that meaning in context and phonetic realization are far more entangled than standard linguistic theory predicts.",10.48550/arXiv.2503.23163,"['JournalArticle', 'Review']",Computer Science; Linguistics
Application of Large Language Models in Stochastic Sampling Algorithms for Predictive Modeling of Population Behavior,2024,"Yongjian Xu, Akash Nandi, E. Markopoulos","Agent-based modeling of human behavior is often challenging due to restrictions associated with parametric models. Large language models (LLM) play a pivotal role in modeling human-based systems because of their capability to simulate a multitude of human behavior in contextualized environments; this makes them effective as a mappable natural language representation of human behavior. This paper proposes a Monte Carlo type stochastic simulation algorithm that leverages large language model agents in a population survey simulation (Monte-Carlo based LLM agent population simulation, MCLAPS). The proposed architecture is composed of a LLM-based demographic profile data generation model and an agent simulation model which theoretically enables complex modelling of a range of different complex social scenarios. An experiment is conducted with the algorithm in modeling quantitative pricing data, where 9 synthetic Van Westendorp Price Sensitivity Meter datasets are simulated across groups corresponding to pairings of 3 different demographics and 3 different product types. The 9 sub-experiments show the effectiveness of the architecture in capturing key expected behavior within a simulation scenario, while reflecting expected pricing values.",10.54941/ahfe1004637,['Review'],Computer Science
Flood Simulation: Integrating UAS Imagery and Ai-Generated Data With Diffusion Model,2024,"Xiyang Hu, Maryam Rahnemoonfar","The primary goal of early disaster impact assessments is to gather georeferenced data about affected areas. Floods, a major natural calamity, pose challenges in data collection and response coordination. The use of Unmanned Aerial Systems (UAS) has significantly improved data acquisition in flood-impacted regions, offering a cost-effective method for obtaining high-quality images. However, issues like accurate image description and data scarcity remain. Addressing these, we propose two novel methods: GPT models for precise image-to-text conversion and diffusion models for flood data simulation. GPT models enhance data quality with accurate descriptions, while diffusion models extend our dataset by simulating diverse flood scenarios, enriching our training resources. Further analysis of this augmented dataset for similarity demonstrates a wide-ranging variety in the data, highlighting a diverse array of elements within images.",10.1109/IGARSS53475.2024.10640764,['JournalArticle'],Computer Science; Engineering; Environmental Science
FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring,2025,"Yousef Emami, Hao Zhou, M. Gaitn, Kai Li, Luis Almeida","Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in wildfire monitoring, where early detection minimizes environmental impact. In UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor transmission scheduling and velocity is critical for minimizing Age of Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has been used for such optimization; however, its limitations such as low sampling efficiency, simulation-to-reality gaps, and complex training render it unsuitable for time-critical applications like wildfire monitoring. This paper introduces a new online Flight Resource Allocation scheme based on LLM-Enabled In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and data collection schedule along the trajectory in real time, thereby asymptotically minimizing the average AoI across ground sensors. In contrast to DRL, FRSICL generates data collection schedules and controls velocity using natural language task descriptions and feedback from the environment, enabling dynamic decision-making without extensive retraining. Simulation results confirm the effectiveness of the proposed FRSICL compared to Proximal Policy Optimization (PPO) and Nearest-Neighbor baselines.",10.48550/arXiv.2507.10134,['JournalArticle'],Computer Science; Engineering; Environmental Science
Calibrating LLM Confidence by Probing Perturbed Representation Stability,2025,"Reza Khanmohammadi, Erfan Miahi, Mehrsa Mardikoraem, Simerjot Kaur, Ivan Brugere, Charese H. Smiley, Kundan Thind, M.M. Ghassemi","Miscalibration in Large Language Models (LLMs) undermines their reliability, highlighting the need for accurate confidence estimation. We introduce CCPS (Calibrating LLM Confidence by Probing Perturbed Representation Stability), a novel method analyzing internal representational stability in LLMs. CCPS applies targeted adversarial perturbations to final hidden states, extracts features reflecting the model's response to these perturbations, and uses a lightweight classifier to predict answer correctness. CCPS was evaluated on LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and open-ended formats. Our results show that CCPS significantly outperforms current approaches. Across four LLMs and three MMLU variants, CCPS reduces Expected Calibration Error by approximately 55% and Brier score by 21%, while increasing accuracy by 5 percentage points, Area Under the Precision-Recall Curve by 4 percentage points, and Area Under the Receiver Operating Characteristic Curve by 6 percentage points, all relative to the strongest prior method. CCPS delivers an efficient, broadly applicable, and more accurate solution for estimating LLM confidence, thereby improving their trustworthiness.",10.48550/arXiv.2505.21772,['JournalArticle'],Computer Science
TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance,2025,"Syed Mekael Wasti, Shou-Yi Hung, Christopher Collins, En-Shiun Annie Lee","Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.",10.48550/arXiv.2506.18337,['JournalArticle'],Computer Science; Linguistics
CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration,2025,"Yizhe Yang, Palakorn Achananuparp, Heyan Huang, Jing Jiang, Kit Phey Leng, Nicholas Gabriel Lim, Cameron Tan Shi Ern, Ee-Peng Lim","Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client's state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI's performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client's state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",10.48550/arXiv.2502.02807,"['JournalArticle', 'Conference']",Computer Science; Psychology
Personalized Coaching for Lifestyle Behavior Change through Large Language Models: A Qualitative Study,2025,"Sophia Meywirth, Andreas Janson, Matthias Sllner","This paper investigates the impact of large language model (LLM)-based coaching interventions on enhancing physical activity and nutritional habits, following motivational interviewing guidelines. By exploring user perceptions through qualitative research involving eight interviews, five key themes emerged: the tension between the need for authenticity and reservations about AI humanization, the desire for personalized coaching and autonomy, the necessity of simplifying daily tasks, the aspiration for self-development, and the need for perceived privacy and trust. The findings reveal that perceptions of LLM-based coaching are multifaceted and cannot be easily classified as purely beneficial or concerning; they vary based on the specific implementation. This complexity indicates that certain aspects can simultaneously present both benefits and risks. The paper discusses theoretical implications and offers practical recommendations to enhance the advantages and mitigate the risks associated with LLM-based coaching interventions.",10.24251/hicss.2025.407,['JournalArticle'],Computer Science; Psychology
Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries,2025,"Koustuv Saha, Yoshee Jain, Munmun De Choudhury","The ubiquity and widespread use of digital and online technologies have transformed mental health support, with online mental health communities (OMHCs) providing safe spaces for peer support. More recently, generative AI and large language models (LLMs) have introduced new possibilities for scalable, around-the-clock mental health assistance that could potentially augment and supplement the capabilities of OMHCs. Although genAI shows promise in delivering immediate and personalized responses, their effectiveness in replicating the nuanced, experience-based support of human peers remains an open question. In this study, we harnessed 24,114 posts and 138,758 online community (OC) responses from 55 OMHCs on Reddit. We prompted several state-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts, and compared their (AI) responses to human-written (OC) responses based on a variety of linguistic measures across psycholinguistics and lexico-semantics. Our findings revealed that AI responses are more verbose, readable, and analytically structured, but lack linguistic diversity and personal narratives inherent in human-human interactions. Through a qualitative examination, we found validation as well as complementary insights into the nature of AI responses, such as its neutrality of stance and the absence of seeking back-and-forth clarifications. We discuss the ethical and practical implications of integrating generative AI into OMHCs, advocating for frameworks that balance AI's scalability and timeliness with the irreplaceable authenticity, social interactiveness, and expertise of human connections that form the ethos of online support communities.",10.48550/arXiv.2504.09271,['JournalArticle'],Computer Science; Linguistics; Psychology
The Ability of Large Language Models to Evaluate Constraint-satisfaction in Agent Responses to Open-ended Requests,2024,"Lior Madmoni, Amir Zait, Ilia Labzovsky, Danny Karmon","Generative AI agents are often expected to respond to complex user requests that have No One Right Answer (NORA), e.g.,""design a vegetarian meal plan below 1800 calories"". Such requests may entail a set of constraints that the agent should adhere to. To successfully develop agents for NORA scenarios, an accurate automatic evaluation framework is essential, and specifically - one capable of validating the satisfaction of constraints in the agent's response. Recently, large language models (LLMs) have been adopted as versatile evaluators for many NORA tasks, but their ability to evaluate constraint-satisfaction in generated text remains unclear. To study this, we develop and release a novel Arithmetic Constraint-Satisfaction (ACS) benchmarking dataset. The dataset consists of complex user requests with corresponding constraints, agent responses and human labels indicating each constraint's satisfaction level in the response. A unique property of this dataset is that validating many of its constraints requires reviewing the response as a whole (in contrast to many other benchmarks that require the validation of a single independent item). Moreover, it assesses LLMs in performing reasoning, in-context data extraction, arithmetic calculations, and counting. We then benchmark both open and proprietary LLMs on evaluating constraint-satisfaction, and show that most models still have a significant headroom for improvement, and that errors primarily stem from reasoning issues. In addition, most models exhibit a skewed constraint-satisfaction prediction pattern, with higher accuracy where the ground-truth label is""satisfied"". Lastly, few-shot prompting for our task proved to be rather challenging, since many of the studied models showed a degradation in performance when it was introduced.",10.48550/arXiv.2409.14371,"['JournalArticle', 'Review']",Computer Science
A Survey on Quality Evaluation of Instruction Fine-tuning Datasets for Large Language Models,2025,"Yitian Luo, Yu Liu, Lu Zhang, Feng Gao, Jinguang Gu","Instruction fine-tuning is a key method for adapting large language models (LLMs) to domain-specific tasks, and instruction quality significantly impacts model performance after fine-tuning. Hence, evaluating the quality of instruction and selecting high-quality instructions are essential steps in the process of LLM instruction fine-tuning. Although existing studies provide important theoretical foundations and techniques for this, there is still room for improvement in terms of generality, the relationship between methods and experimental verification. Current methods for evaluating instruction quality can be classified into four main categories: human evaluation, statistics-based evaluation, model-based evaluation, and LLMs-based evaluation. Among these methods, human evaluation relies on the subjective judgment and domain expertise of the evaluators, which offers interpretability and is suitable for scenarios involving small-scale data and sufficient budgets. Statistics-based evaluation estimates the quality of instructions using indicators such as stopwords and lexical diversity, providing high efficiency and a suitable evaluation for large-scale data. Model-based evaluation employs specific models to quantify indicators such as perplexity (PPL) and instruction following difficulty (IFD), which is flexible and suitable for specific tasks. The LLMs-based evaluation rates the quality of instructions through prompt-based interaction with LLMs, focusing on aspects such as accuracy and coherence, which is highly automated and customizable, simplifying the evaluation process. Finally, considering the limitations of current quality",10.3724/2096-7004.di.2025.0021,"['JournalArticle', 'Review']",Computer Science
PluralLLM: Pluralistic Alignment in LLMs via Federated Learning,2025,"Mahmoud Srewa, Tian Zhao, Salma Elmalaki","Ensuring Large Language Models (LLMs) align with diverse human preferences while preserving privacy and fairness remains a challenge. Existing methods, such as Reinforcement Learning from Human Feedback (RLHF), rely on centralized data collection, making them computationally expensive and privacy-invasive. We introduce PluralLLM1 a federated learning-based approach that enables multiple user groups to collaboratively train a transformer-based preference predictor without sharing sensitive data, which can also serve as a reward model for aligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate preference updates efficiently, achieving 46% faster convergence, a 4% improvement in alignment scores, and nearly the same group fairness measure as in centralized training. Evaluated on a Q/A preference alignment task, PluralLLM demonstrates that federated preference learning offers a scalable and privacy-preserving alternative for aligning LLMs with diverse human values.",10.1145/3722570.3726898,"['Book', 'JournalArticle']",Computer Science
Generating Unexpected yet Relevant User Dialog Acts,2024,"L. Galland, Catherine Pelachaud, Florian Pecune","The demand for mental health services has risen substantially in recent years, leading to challenges in meeting patient needs promptly. Virtual agents capable of emulating motivational interviews (MI) have emerged as a potential solution to address this issue, offering immediate support that is especially beneficial for therapy modalities requiring multiple sessions. However, developing effective patient simulation methods for training MI dialog systems poses challenges, particularly in generating syntactically and contextually correct, and diversified dialog acts while respecting existing patterns and trends in therapy data. This paper investigates data-driven approaches to simulate patients for training MI dialog systems. We propose a novel method that leverages time series models to generate diverse and contextually appropriate patient dialog acts, which are then transformed into utterances by a conditioned large language model. Additionally, we introduce evaluation measures tailored to assess the quality and coherence of simulated patient dialog. Our findings highlight the effectiveness of dialog act-conditioned approaches in improving patient simulation for MI, offering insights for developing virtual agents to support mental health therapy.",10.18653/v1/2024.sigdial-1.17,['JournalArticle'],Computer Science; Psychology
Counseling Responses for Mental Health Forum Questions with Early Maladaptive Schema Prediction,2024,"Sujatha Das Gollapalli, Mingzhe Du, See-Kiong Ng",". State-of-the-art Large Language Models (LLMs) have shown remarkable capabilities for general Question Answering (QA) tasks. However, their practical use for answering mental health questions has been limited due to the missing link between LLM-generated answer responses and well-established theories and guiding principles from Psychology and Counseling. We present a rst step in this direction with STeer , an AI-based method that supports S chema T herapy-e nabl e d r esponses for mental health questions on community QA forums. STeer uses Early Maladaptive Schemas (EMSs), a fundamental concept from Schema Therapy that characterizes self-defeating, unhealthy patterns of thought and behavior in individuals, to effectively prompt state-of-the-art LLMs to generate specic, theoretically-grounded, empathetic counseling responses to mental health questions. We present EMSRank , a novel method based on the Personalized PageRank algorithm, to automatically predict the EMSs from mental health forum question texts. We show that EMSRank is computationally scalable and can be further combined with textual entailment to obtain high precision, explain-able EMS",10.3233/FAIA240785,"['JournalArticle', 'Conference']",Computer Science; Psychology
ValueSim: Generating Backstories to Model Individual Value Systems,2025,"Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu","As Large Language Models (LLMs) continue to exhibit increasingly human-like capabilities, aligning them with human values has become critically important. Contemporary advanced techniques, such as prompt learning and reinforcement learning, are being deployed to better align LLMs with human values. However, while these approaches address broad ethical considerations and helpfulness, they rarely focus on simulating individualized human value systems. To address this gap, we present ValueSim, a framework that simulates individual values through the generation of personal backstories reflecting past experiences and demographic information. ValueSim converts structured individual data into narrative backstories and employs a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values based on these narratives. Testing ValueSim on a self-constructed benchmark derived from the World Values Survey demonstrates an improvement in top-1 accuracy by over 10% compared to retrieval-augmented generation methods. Further analysis reveals that performance enhances as additional user interaction history becomes available, indicating the model's ability to refine its persona simulation capabilities over time.",10.48550/arXiv.2505.23827,"['JournalArticle', 'Review']",Computer Science; Psychology
Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types,2024,"Ziming Guo, Chao Ma, Yinggang Sun, Tiancheng Zhao, Guangyao Wang, Hai Huang","Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q&A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries. Our dataset and code are publicly available at https://mcxiaoxiao.github.io/MMSQL.",10.48550/arXiv.2412.17867,['JournalArticle'],Computer Science
ALISE: Accelerating Large Language Model Serving with Speculative Scheduling,2024,"Youpeng Zhao, Jun Wang","Large Language Models (LLMs) represent a revolutionary advancement in the contemporary landscape of artificial general intelligence (AGI). As exemplified by ChatGPT, LLM-based applications necessitate minimal response latency and maximal throughput for inference serving. However, due to the unpredictability of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed by current LLM serving systems suffers from head-of-line (HoL) blocking issues and long job response times. In this paper, we propose a new efficient LLM inference serving framework, named ALISE. The key design paradigm of ALISE is to leverage a novel speculative scheduler by estimating the execution time for each job and exploiting such prior knowledge to assign appropriate job priority orders, thus minimizing potential queuing delays for heterogeneous workloads. Furthermore, to mitigate the memory overhead of the intermediate key-value (KV) cache, we employ a priority-based adaptive memory management protocol and quantization-based compression techniques. Evaluations demonstrate that in comparison to the state-of-the-art solution vLLM, ALISE improves the throughput of inference serving by up to 1.8 and 2.1 under the same latency constraint on the Alpaca and ShareGPT datasets, respectively.",10.1145/3676536.3676659,"['Book', 'JournalArticle', 'Conference']",Computer Science
Designing Domain-Specific Large Language Models: The Critical Role of Fine-Tuning in Public Opinion Simulation,2024,Haocheng Lin,"Large language models (LLMs) have transformed natural language processing, yet face challenges in specialized tasks such as simulating opinions on environmental policies. This paper introduces a novel fine-tuning approach that integrates socio-demographic data from the UK Household Longitudinal Study, uniquely using profiling factors, such as age, gender, income, education, and region. This method enhances the accuracy and representation of generated views. By emulating diverse synthetic profiles, the fine-tuned models significantly outperform pre-trained counterparts, achieving measurable improvements in capturing demographic nuances. Evaluation metrics, including Chi-Squared, Cosine Similarity, Jaccard Index, and KL-divergence, reveal a strong alignment between synthetic and real-world opinions. This work demonstrates the potential of fine-tuned LLMs tailored to societal contexts to enable more ethical and precise policy simulations. Its broader implications include deploying LLMs in domains like healthcare and education, fostering inclusive and data-driven decision-making in both research and practice.",10.48550/arXiv.2409.19308,['JournalArticle'],Computer Science; Environmental Science; Political Science
Predicting Entity Salience in Extremely Short Documents,2024,"Benjamin Bullough, Harrison Lundberg, Chen Hu, Weihang Xiao","A frequent challenge in applications that use entities extracted from text documents is selecting the most salient entities when only a small number can be used by the application (e.g., displayed to a user). Solving this challenge is particularly difficult in the setting of extremely short documents, such as the response from a digital assistant, where traditional signals of salience such as position and frequency are less likely to be useful. In this paper, we propose a lightweight and data-efficient approach for entity salience detection on short text documents. Our experiments show that our approach achieves competitive performance with respect to complex state-of-the-art models, such as GPT-4, at a significant advantage in latency and cost. In limited data settings, we show that a semi-supervised fine-tuning process can improve performance further. Furthermore, we introduce a novel human-labeled dataset for evaluating entity salience on short question-answer pair documents.",10.18653/v1/2024.emnlp-industry.5,"['JournalArticle', 'Conference']",Computer Science
Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding,2025,"Yun-Shiuan Chuang, Nikunj Harlalka, Sameer Narendran, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers","Guesstimation -- the task of making approximate quantitative estimates about objects or events -- is a common real-world skill, yet remains underexplored in large language model (LLM) research. We introduce three guesstimation datasets: MARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many marbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential election). Inspired by the social science concept of Wisdom of Crowds (WOC)- where the median of multiple estimates improves accuracy-we propose WOC decoding for LLMs. We replicate WOC effects in human participants and find that LLMs exhibit similar benefits: median aggregation across sampled responses consistently improves accuracy over greedy decoding, self-consistency decoding, and mean decoding. This suggests that LLMs encode a world model that supports approximate reasoning. Our results position guesstimation as a useful probe of LLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM guesstimation performance on real-world tasks.",10.48550/arXiv.2501.17310,['JournalArticle'],Computer Science
Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models,2024,"Li Sun, Liuan Wang, Jun Sun, Takayuki Okatani","Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.",10.48550/arXiv.2401.09861,['JournalArticle'],Computer Science
Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality,2025,"Naihe Feng, Yi Sui, Shiyi Hou, Jesse C. Cresswell, Ga Wu","Existing research on Retrieval-Augmented Generation (RAG) primarily focuses on improving overall question-answering accuracy, often overlooking the quality of sub-claims within generated responses. Recent methods that attempt to improve RAG trustworthiness, such as through auto-evaluation metrics, lack probabilistic guarantees or require ground truth answers. To address these limitations, we propose Conformal-RAG, a novel framework inspired by recent applications of conformal prediction (CP) on large language models (LLMs). Conformal-RAG leverages CP and internal information from the RAG mechanism to offer statistical guarantees on response quality. It ensures group-conditional coverage spanning multiple sub-domains without requiring manual labelling of conformal sets, making it suitable for complex RAG applications. Compared to existing RAG auto-evaluation methods, Conformal-RAG offers statistical guarantees on the quality of refined sub-claims, ensuring response reliability without the need for ground truth answers. Additionally, our experiments demonstrate that by leveraging information from the RAG system, Conformal-RAG retains up to 60\% more high-quality sub-claims from the response compared to direct applications of CP to LLMs, while maintaining the same reliability guarantee.",10.1145/3726302.3730244,"['JournalArticle', 'Conference']",Computer Science
LAURA: LLM-Assisted UAV Routing for AoI Minimization,2025,"Bisheng Wei, Ruichen Zhang, Ruihong Jiang, Mugen Peng, Dusit Niyato","With the rapid growth of the low-altitude economy, there is increasing demand for real-time data collection using UAV-assisted wireless sensor networks. This paper investigates the problem of minimizing the age of information (AoI) in UAV-assisted wireless sensor networks by optimizing the UAV flight routing. We formulate the AoI minimization task and propose a large language model (LLM)-assisted UAV routing algorithm (LAURA). LAURA employs an LLM as intelligent crossover operators within an evolutionary optimization framework to efficiently explore the solution space. Simulation results show that LAURA outperforms benchmark methods in reducing the maximum AoI, especially in scenarios with a large number of sensor nodes.",10.48550/arXiv.2503.23132,['JournalArticle'],Computer Science; Engineering; Mathematics
Real-Time Environment Monitoring and Response Through IoT and Retrieval-Augmented Generation,2024,"Seungtaek Oh, Seung-woo Kum, Jaewon Moon","This paper introduces an architecture that combines IoT-driven data collection with Retrieval-Augmented Generation (RAG) for real-time environmental monitoring, analysis, and prediction. Using edge computing for local processing and centralized LLMs for complex analysis, the system delivers scalable and timely insights into environmental issues like climate change and disaster prevention. Data from IoT sensors is preprocessed at edge devices and sent to a central server, where embeddings are generated, stored in a vector database, and analyzed by the LLM for fast query handling. This system provides real-time feedback through dashboards, focusing on overcoming challenges such as network delays, scalability, and security to deliver reliable environmental monitoring for non-expert users.",10.1109/ICTC62082.2024.10827595,"['JournalArticle', 'Conference']",Computer Science; Engineering; Environmental Science
Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis,2025,"Mithat Can Ozgun, Jiahuan Pei, Koen V. Hindriks, Lucia Donatelli, Qingzhi Liu, Junxiao Wang","LLM-based agents have emerged as transformative tools capable of executing complex tasks through iterative planning and action, achieving significant advancements in understanding and addressing user needs. Yet, their effectiveness remains limited in specialized domains such as mental health diagnosis, where they underperform compared to general applications. Current approaches to integrating diagnostic capabilities into LLMs rely on scarce, highly sensitive mental health datasets, which are challenging to acquire. These methods also fail to emulate clinicians'proactive inquiry skills, lack multi-turn conversational comprehension, and struggle to align outputs with expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1 diagnostic questionnaires. By simulating therapist-client dialogues with specific client profiles, the framework delivers transparent, step-by-step disorder predictions, producing explainable and trustworthy results. This workflow serves as a complementary tool for mental health diagnosis, ensuring adherence to ethical and legal standards. Through comprehensive experiments, we evaluate leading LLMs across three critical dimensions: conversational realism, diagnostic accuracy, and explainability. Our datasets and implementations are fully open-sourced.",10.48550/arXiv.2508.11398,['JournalArticle'],Computer Science; Psychology
SAGraph: A Large-Scale Social Graph Dataset with Comprehensive Context for Influencer Selection in Marketing,2024,"Xiaoqing Zhang, Xiuying Chen, Yuhan Liu, Jianzhou Wang, Zhenxing Hu, Rui Yan","Influencer marketing campaign success heavily depends on identifying key opinion leaders who can effectively leverage their credibility and reach to promote products or services. The selecting influencers process is vital for boosting brand visibility, fostering consumer trust, and driving sales. While traditional research often simplifies complex factors like user attitudes, interaction frequency, and advertising content, into simple numerical values. However, this reductionist approach fails to capture the dynamic nature of influencer marketing effectiveness. To bridge this gap, we present SAGraph, a novel comprehensive dataset from Weibo that captures multi-dimensional marketing campaign data across six product domains. The dataset encompasses 345,039 user profiles with their complete interaction histories, including 1.3M comments and 554K reposts across 44K posts, providing unprecedented granularity in influencer marketing dynamics. SAGraph uniquely integrates user profiles, content features, and temporal interaction patterns, enabling in-depth analysis of influencer marketing mechanisms. Experimental results using both traditional baselines and state-of-the-art large language models (LLMs) demonstrate the crucial role of content analysis in predicting advertising effectiveness. Our findings reveal that LLM-based approaches achieve superior performance in understanding and predicting campaign success, opening new avenues for data-driven influencer marketing strategies. We hope that this dataset will inspire further research https://github.com/xiaoqzhwhu/SAGraph/.",10.1145/3726302.3730334,"['JournalArticle', 'Conference']",Business; Computer Science
The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations,2025,"Yubo Zhu, Dongrui Liu, Zecheng Lin, Wei Tong, Sheng Zhong, Jing Shao","Estimating the difficulty of input questions as perceived by large language models (LLMs) is essential for accurate performance evaluation and adaptive inference. Existing methods typically rely on repeated response sampling, auxiliary models, or fine-tuning the target model itself, which may incur substantial computational costs or compromise generality. In this paper, we propose a novel approach for difficulty estimation that leverages only the hidden representations produced by the target LLM. We model the token-level generation process as a Markov chain and define a value function to estimate the expected output quality given any hidden state. This allows for efficient and accurate difficulty estimation based solely on the initial hidden state, without generating any output tokens. Extensive experiments across both textual and multimodal tasks demonstrate that our method consistently outperforms existing baselines in difficulty estimation. Moreover, we apply our difficulty estimates to guide adaptive reasoning strategies, including Self-Consistency, Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer generated tokens.",10.48550/arXiv.2509.12886,['JournalArticle'],Computer Science
Eternagram: Probing Player Attitudes in Alternate Climate Scenarios Through a ChatGPT-Driven Text Adventure,2024,"Suifang Zhou, Latisha Besariani Hendra, Qinshi Zhang, Jussi Holopainen, LC Ray","Conventional methods of assessing attitudes towards climate change are limited in capturing authentic opinions, primarily stemming from a lack of context-specific assessment strategies and an overreliance on simplistic surveys. Game-based Assessments (GBA) have demonstrated the ability to overcome these issues by immersing participants in engaging gameplay within carefully crafted, scenario-based environments. Concurrently, advancements in AI and Natural Language Processing (NLP) show promise in enhancing the gamified testing environment, achieving this by generating context-aware, human-like dialogues that contribute to a more natural and effective assessment. Our study introduces a new technique for probing climate change attitudes by actualizing a GPT-driven chatbot system in harmony with a game design depicting a futuristic climate scenario. The correlation analysis reveals an assimilation effect, where players' post-game climate awareness tends to align with their in-game perceptions. Key predictors of pro-climate attitudes are identified as traits like 'Openness' and 'Agreeableness', and a preference for democratic values.",10.48550/arXiv.2403.18160,"['JournalArticle', 'Review']",Computer Science; Environmental Science
Design of Library Management System Based on MVVM Framework and ZXing Scanning Code Technology,2023,"Yuqi Li, Hongyu Zhang, Jinhui Xu","The library is an important resource for university learning. Nowadays, the library has gradually become the place where students learn most in universities at home and abroad. Therefore, our team will design an interdisciplinary practice in the library of Huazhong University of Science and Technology. The survey found that the libraries of Huazhong University of Science and Technology and other universities achieve self-service management through self-service machines, public numbers, official websites, and other channels, but students generally think that the function is cumbersome and inconvenient to query, so we want to design a book self-service management system to bring the greatest convenience to users. This practice uses literature analysis, questionnaires, interviews, and experiments to deeply understand the pain points of library self-service machines, the design principles of relevant interfaces of libraries at home and abroad, and the needs of users to determine the information construction, interactive experience and technical needs of the system. This project is mainly an interdisciplinary practice of computer science and industrial design. In terms of technology, the system uses Android and MVVM architecture to display the front-end interface, realizes the Android network request through OkHttp and Retrofit, and uses ZXing open-source scanning technology to realize the function of borrowing and returning books on the palm; Go language and Echo Web framework is used for development, and Docker is used to deploying containers. In terms of design, the system combines ergonomics, design psychology, and so on, which not only realizes the basic functions of mobile phones, such as scanning code, borrowing, and returning books, map guide, and searching books but also adds special functions such as lost and found, recording reading time and so on, so as to enhance the user's personalized experience. After the usability test, the interviewees believe that the design can greatly improve the learning efficiency in the library.",10.54941/ahfe1003776,['Review'],Computer Science
Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,2025,"Georg Ahnert, Elena Wurth, Markus Strohmaier, Jutta Mata","Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction -- tailored to highly specific participant groups -- to then be tested in subsequent studies with human participants.",10.36190/2025.30,['JournalArticle'],Computer Science; Environmental Science
SPICE: Smart Projection Interface for Cooking Enhancement,2024,"Vera Prohaska, Eduardo Castell'o Ferrer","Tangible User Interfaces (TUI) for human--computer interaction (HCI) provide the user with physical representations of digital information with the aim to overcome the limitations of screen-based interfaces. Although many compelling demonstrations of TUIs exist in the literature, there is a lack of research on TUIs intended for daily two-handed tasks and processes, such as cooking. In response to this gap, we propose SPICE (Smart Projection Interface for Cooking Enhancement). SPICE investigates TUIs in a kitchen setting, aiming to transform the recipe following experience from simply text-based to tangibly interactive. SPICE uses a tracking system, an agent-based simulation software, and vision large language models to create and interpret a kitchen environment where recipe information is projected directly onto the cooking surface. We conducted comparative usability and a validation studies of SPICE, with 30 participants. The results show that participants using SPICE completed the recipe with far less stops and in a substantially shorter time. Despite this, participants self-reported negligible change in feelings of difficulty, which is a direction for future research. Overall, the SPICE project demonstrates the potential of using TUIs to improve everyday activities, paving the way for future research in HCI and new computing interfaces.",10.48550/arXiv.2412.03551,['JournalArticle'],Computer Science; Engineering
Exploring Generative AI as a Proxy User for Early Stage User Research - Preliminary Findings,2023,"Michael Jenkins, Elizabeth Thiry, Richard Stone, Caroline Kingsley, Calvn Leather","The potential of generative AI has exploded of late, largely due in part because of the improved accessibility that tools like ChatGPT afford for non-data-scientist / developer users. One potential area of application is for Generative AI models to serve as proxy users in early stage user research. User research is a crucial component of product development, helping to understand user needs, preferences, and behaviors. However, conducting user research can be time-consuming, resource-intensive, and may require access to a user population that is challenging to access (e.g., military users). Generative AI models have shown remarkable progress in generating human-like text and simulating user interactions based on a significant corpus of training materials that serves as the knowledge base for the AIs reasoning. This paper provides preliminary findings from explorations on the feasibility of leveraging generative AI as a proxy user to inform early stage user research. Using the GPT-4.0 architecture and the Open-AI ChatGPT user interface (chat.openai.com), we conducted preliminary research for six different candidate end user populations. This was accomplished by generating generic product descriptions, notional user personas each respective product, contextualizing ChatGPT to act as the user persona, and then asking a series of generic user experience research (UXR) questions of the GPT model. Responses from ChatGPT were then scored by three UXR / Human Factors subject-matter experts to evaluate the perceived utility of ChatGPTs responses in terms of supporting early stage product design as a proxy human user. By evaluating the effectiveness of generative AI as a proxy user, this research aims to shed light on its potential benefits and limitations in supporting early stage user research efforts. While additional research is still needed (e.g., comparing the results of ChatGPT to responses generated by actual end users, having SMEs evaluate the accuracy and completeness of ChatGPTs responses), preliminary findings are promising for the potential that generative AI models hold to serve as early stage proxy users to inform research and product design efforts in domains where significant corpuses of data already exist for model training, and where access to human end users may be restricted our otherwise prohibited.",10.54941/ahfe1004305,nan,Computer Science
A Study Towards Building Content Aware Models in NLP using Genetic Algorithms,2023,"Umesh Tank, Saranya Arirangan, A. Paduri, Narayana Darapaneni","INTRODUCTION: With the advancement in the large language models, often called LLMs, there has been increasing concerns around the usage of these models. As they can generate human-like text and can also perform a number of tasks such as generating code, question answering, essay writing and even generating text for research papers.OBJECTIVES: The generated text is subject to the usage of the original data (using which models are trained) which might be protected or may be personal/private data. The detailed description of such concerns and various potential solutions is discussed in Generative language models and automated influence operations: Emerging threats and potential mitigations. METHODS: Addressing these concerns becomes the paramount for LLMs usability. There are several directions explored by the researchers and one of the interesting works is around building content aware models. The idea is that the model is aware of the type of content it is learning from and aware what type of content should be used to generate a response to a specific query.RESULTS: In our work we explored direction by applying poisoning techniques to contaminate data and then applying genetic algorithms to extract the non-poisoned content from the poisoned content that can generate a good response when paraphrased.CONCLUSION: While we demonstrated the idea using poisoning techniques and tried to make the model aware of copyrighted content, the same can be extended to detect other types of contents or any other use cases where content awareness is required.",10.4108/airo.4078,['JournalArticle'],Computer Science
ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models,2025,"Dai Li, Linzhuo Li, Huilian Sophie Qiu","Large language models (LLMs) in the form of chatbots like ChatGPT and Llama are increasingly proposed as""silicon samples""for simulating human opinions. This study examines this notion, arguing that LLMs may misrepresent population-level opinions. We identify two fundamental challenges: a failure in structural consistency, where response accuracy doesn't hold across demographic aggregation levels, and homogenization, an underrepresentation of minority opinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama 3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized immigration from the American National Election Studies (ANES) 2020. Our findings reveal significant structural inconsistencies and severe homogenization in LLM responses compared to human data. We propose an""accuracy-optimization hypothesis,""suggesting homogenization stems from prioritizing modal responses. These issues challenge the validity of using LLMs, especially chatbots AI, as direct substitutes for human survey data, potentially reinforcing stereotypes and misinforming policy.",10.48550/arXiv.2507.02919,"['JournalArticle', 'Review']",Computer Science; Sociology
DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives,2025,"Sehwan Moon, Aram Lee, Jeong Eun Kim, Hee-Ju Kang, I. Shin, Sung-Wan Kim, Jae-Min Kim, Min Jhon, Ju-Wan Kim","Advances in large language models (LLMs) have enabled a wide range of applications. However, depression prediction is hindered by the lack of large-scale, high-quality, and rigorously annotated datasets. This study introduces DepressLLM, trained and evaluated on a novel corpus of 3,699 autobiographical narratives reflecting both happiness and distress. DepressLLM provides interpretable depression predictions and, via its Score-guided Token Probability Summation (SToPS) module, delivers both improved classification performance and reliable confidence estimates, achieving an AUC of 0.789, which rises to 0.904 on samples with confidence $\geq$ 0.95. To validate its robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets, including an Ecological Momentary Assessment (EMA) corpus of daily stress and mood recordings, and on public clinical interview data. Finally, a psychiatric review of high-confidence misclassifications highlighted key model and data limitations that suggest directions for future refinements. These findings demonstrate that interpretable AI can enable earlier diagnosis of depression and underscore the promise of medical AI in psychiatry.",10.48550/arXiv.2508.08591,"['JournalArticle', 'Review']",Computer Science; Psychology
Seeking Inspiration through Human-LLM Interaction,2025,"Xinrui Lin, Heyan Huang, Kaihuang Huang, Xin Shu, John Vines","Large language model (LLM) systems have been shown to stimulate creative thinking among creators, yet empirical research on whether users can seek inspiration in their everyday lives through these technologies is lacking. This paper explores which attributes of LLMs influence inspiration-seeking processes. Focusing on use cases of travel, cooking, and self-care, we interviewed 20 participants as they explored scenarios of these use cases using LLMs. Thematic analysis revealed that the vast data of LLMs inspires users with unexpected ideas, many of which were highly personalized, and inspired participants towards being motivated to act. Participants were also sensitive to the deficiencies of LLMs, and noted how ethical issues associated with these technologies could negatively impact them applying inspirational ideas into practice. We discuss the behavioral patterns of users actively seeking inspiration via LLMs, and provide design opportunities for LLMs that make the inspiration-seeking process more human-centric.",10.1145/3706598.3713259,"['Book', 'JournalArticle', 'Conference']",Computer Science; Linguistics
Exploiting Primacy Effect To Improve Large Language Models,2025,"Bianca Raimondi, Maurizio Gabbrielli","Large Language Models (LLMs) have become essential in many Natural Language Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to achieve high accuracy. However, like humans, LLMs exhibit biases, particularly positional biases such as primacy and recency effects, which can influence the accuracy of the answers. The primacy effect-where items presented first are more likely to be remembered or selected-plays a key role in Multiple Choice Question Answering (MCQA), where the order of answer options can affect prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We first show that fine-tuning amplifies this bias, probably due to exposure to human-like patterns. Hence, we strategically leverage this effect by reordering response options based on semantic similarity to the query, without requiring knowledge of the correct answer. Our experimental results show that this approach significantly improves performance in MCQA. More generally, our findings underscore the dual nature of biases as both challenges and opportunities, offering insights for bias-aware model design and NLP applications.",10.48550/arXiv.2507.13949,['JournalArticle'],Computer Science; Linguistics
Enhancing Player Experience in a First-Person Shooter with Dynamic Audio Cue Adjustment Based on Gaussian Progress Regression,2024,"Xiaoxu Li, Yi Xia, Mustafa Can Gursesli, Xiao You, Siyuan Chen, R. Thawonmas","This paper analyzes the experience of first-person shooter (FPS) players when game difficulty is adjusted by personalizing their audio cue settings, considering the balance between player performance, modeled using Gaussian process regression (GPR), and prior data serving as designer preference. In addition, we investigate why player experience changes according to in-game audio cues. Previous studies have proposed various dynamic difficulty adjustment (DDA) methods for FPS games. However, few studies have considered the role of audio cues in the player experience. This paper compares the player experience of personalized enemy audio cue volume settings (GPR-DDA) with that of predetermined settings in an FPS game. Two comprehensive experimental phases, involving 80 participants, are conducted to assess the efficacy of GPR-DDA. The experience of our players is measured using questions taken from the Game User Experience Satisfaction Scale (GUESS) questionnaire and a final survey asking for their open-ended feedback. A large language model (LLM) is used to analyze the natural language expressions of the players according to their native languages. To ensure the LLM effectively assists a limited number of qualified human evaluators in classifying player responses, we have developed an original procedure for this task. The GUESS results show that GPR-DDA can improve the player experience. In addition, the high consistency in the classification results over multiple runs of the selected LLM, as well as the similarity between its results and those of our human evaluators, reflects the reliability of the proposed LLM-assisted procedure.",10.3390/app142311146,"['JournalArticle', 'Review']",Computer Science
PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs,2025,Roberto Cerina,"This paper introduces PoSSUM, an open-source protocol for unobtrusive polling of social-media users via multimodal Large Language Models (LLMs). PoSSUM leverages users' real-time posts, images, and other digital traces to create silicon samples that capture information not present in the LLM's training data. To obtain representative estimates, PoSSUM employs Multilevel Regression and Post-Stratification (MrP) with structured priors to counteract the observable selection biases of social-media platforms. The protocol is validated during the 2024 U.S. Presidential Election, for which five PoSSUM polls were conducted and published on GitHub and X. In the final poll, fielded October 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately predicted the outcomes in 50 of 51 states and assigned the Republican candidate a win probability of 0.65. Notably, it also exhibited lower state-level bias than most established pollsters. These results demonstrate PoSSUM's potential as a fully automated, unobtrusive alternative to traditional survey methods.",10.48550/arXiv.2503.05529,"['JournalArticle', 'Review']",Computer Science; Mathematics; Sociology
Ontologies in Design: How Imagining a Tree Reveals Possibilities and Assumptions in Large Language Models,2025,"Nava Haghighi, Sunny Yu, James A. Landay, Daniela K. Rosner","Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologiesconcerning what we allow ourselves to think or talk aboutis a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.",10.1145/3706598.3713633,"['JournalArticle', 'Book', 'Conference']",Computer Science; Linguistics; Sociology
RAG for Question-Answering for Vocal Training Based on Domain Knowledge Base,2024,"Chun-hung Jonas Leung, Yicheng Yi, Le Kuai, Zongxi Li, Siu-Kei Au Yeung, Kwok-wah John Lee, Ka-him Kelvin Ho, Kevin Hung","Although Large language models (LLMs) are well-known due to their superior capacity for text generation and logical inference, they are found to be inaccurate in domain-specific question-answering tasks. The powerful generator still tends to generate content even when the LLM does not have sufficient knowledge at all, which is known as the hallucination problem. We find there is a research void in applying LLMs in the vocal training industry, which requires intensive expert knowledge in any chatbot or intelligent tutor services. This paper details employing Retrieval-Augmented Generation (RAG) technology to develop a domain-specific language model, addressing inherent challenges such as hallucination, where large models generate plausible but inaccurate content, and lack of domain specificity. By segmenting the knowledge base and establishing semantic similarities between user queries and knowledge data, the project lays a solid foundation for integrating RAG, significantly improving response accuracy and contextual relevance. The report highlights the successful implementation of RAG, enhancing system intelligence and personalization for user-specific needs, discusses challenges and solutions during the implementation process, and outlines future directions to expand RAG capabilities and improve user experiences.",10.1109/BESC64747.2024.10780718,"['JournalArticle', 'Conference']",Computer Science
Performance Evaluation,2022,J. L. Boudec,"Lectures that use probability or statistics often appear complex to students, sometimes because the underlying stochastic models are not explicited. Writing a stochastic simulation program is a common exercise in a performance evaluation course, and we can view a simulation program as an implementation of a stochastic model. Thus, students who took such a course are trained to fully specify stochastic models, and this can help them fully understand probabilistic statements. We give examples in the context of random graphs as well as in the interpretation of residual scores in machine learning. Last we observe that Palm calculus, which is at the heart of classical queuing theory, can be used to provide important insights into sampling problems encountered in data collection tasks.",10.1145/3543146.3543151,['JournalArticle'],Computer Science; Mathematics
Implicit and Explicit Research Quality Score Probabilities from ChatGPT,2025,"Mike Thelwall, Yunhan Yang","
 The large language model (LLM) ChatGPTs quality scores for journal articles correlate more strongly with human judgements than some citation-based indicators in most fields. Averaging multiple ChatGPT scores improves the results, apparently leveraging its internal probability model. To leverage these probabilities, this article tests two novel strategies: requesting percentage likelihoods for scores and extracting the probabilities of alternative tokens in the responses. The probability estimates were then used to calculate weighted average scores. Both strategies were evaluated with five iterations of ChatGPT 4o-mini on 96,800 articles submitted to the UK Research Excellence Framework (REF) 2021, using departmental average REF2021 quality scores as a proxy for article quality. The data was analysed separately for each of the 34 field-based REF Units of Assessment. For the first strategy, explicit requests for tables of score percentage likelihoods substantially decreased the value of the scores (lower correlation with the proxy quality indicator). In contrast, weighed averages of score token probabilities slightly increased the correlation with the quality proxy indicator and these probabilities reasonably accurately reflected ChatGPTs outputs. The token probability leveraging approach is therefore the most accurate method for ranking articles by research quality as well as being cheaper than comparable ChatGPT strategies.",10.48550/arXiv.2506.13525,['JournalArticle'],Computer Science
Chatting Up Attachment: Using LLMs to Predict Adult Bonds,2024,"Paulo Soares, Sean McCurdy, Andrew J. Gerber, Peter Fonagy","Obtaining data in the medical field is challenging, making the adoption of AI technology within the space slow and high-risk. We evaluate whether we can overcome this obstacle with synthetic data generated by large language models (LLMs). In particular, we use GPT-4 and Claude 3 Opus to create agents that simulate adults with varying profiles, childhood memories, and attachment styles. These agents participate in simulated Adult Attachment Interviews (AAI), and we use their responses to train models for predicting their underlying attachment styles. We evaluate our models using a transcript dataset from 9 humans who underwent the same interview protocol, analyzed and labeled by mental health professionals. Our findings indicate that training the models using only synthetic data achieves performance comparable to training the models on human data. Additionally, while the raw embeddings from synthetic answers occupy a distinct space compared to those from real human responses, the introduction of unlabeled human data and a simple standardization allows for a closer alignment of these representations. This adjustment is supported by qualitative analyses and is reflected in the enhanced predictive accuracy of the standardized embeddings.",10.48550/arXiv.2409.00347,['JournalArticle'],Computer Science; Psychology
"Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation",2025,"Bikash Saha, Nanda Rani, S. K. Shukla","Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compliance automation, GenAI is driving innovation across functions. However, this transformation comes with significant cybersecurity and ethical risks. We discuss emerging threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial attacks on AI systems, as well as concerns around bias, opacity, and data misuse. The evolving global regulatory landscape is explored in depth, including initiatives by major financial regulators and international efforts to develop risk-based AI governance. Finally, we propose best practices for secure and responsible adoption - including explainability techniques, adversarial testing, auditability, and human oversight. Drawing from academic literature, industry case studies, and policy frameworks, this chapter offers a perspective on how the financial sector can harness GenAI's transformative potential while navigating the complex risks it introduces.",10.48550/arXiv.2504.21574,"['JournalArticle', 'Review']",Business; Computer Science
LLMREI: Automating Requirements Elicitation Interviews with LLMs,2025,"Alexander Korn, Samuel Gorsch, Andreas Vogelsang","Requirements elicitation interviews are crucial for gathering system requirements but heavily depend on skilled analysts, making them resource-intensive, susceptible to human biases, and prone to miscommunication. Recent advancements in Large Language Models present new opportunities for automating parts of this process. This study introduces LLMREI, a chat bot designed to conduct requirements elicitation interviews with minimal human intervention, aiming to reduce common interviewer errors and improve the scalability of requirements elicitation. We explored two main approaches, zero-shot prompting and least-to-most prompting, to optimize LLMREI for requirements elicitation and evaluated its performance in 33 simulated stakeholder interviews. A third approach, fine-tuning, was initially considered but abandoned due to poor performance in preliminary trials. Our study assesses the chat bots effectiveness in three key areas: minimizing common interview errors, extracting relevant requirements, and adapting its questioning based on interview context and user responses. Our findings indicate that LLMREI makes a similar number of errors compared to human interviewers, is capable of extracting a large portion of requirements, and demonstrates a notable ability to generate highly context-dependent questions. We envision the greatest benefit of LLMREI in automating interviews with a large number of stakeholders.",10.1109/RE63999.2025.00013,"['JournalArticle', 'Conference']",Computer Science
Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning,2025,"Haijiang Liu, Qiyuan Li, Chao Gao, Yong Cao, Xiangyu Xu, Xun Wu, Daniel Hershcovich, Jinguang Gu","Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions.",10.48550/arXiv.2508.17855,"['JournalArticle', 'Review']",Computer Science; Psychology
"Enhancing Artistic Style Transfer: Integrating CycleGAN, Diffusion Models, and Neural Painting for Monet-Inspired Image Generation",2024,"Yuening Li, Runxuan Yu, Sparsh Amarnani, Sunny Bhatt, Yushan Xie","This project explores various approaches to style transfer, specifically focusing on transforming natural images into the iconic style of Claude Monets paintings. We explored three methods: CycleGAN-based, neural painting-based, and diffusion- based. CycleGAN enables style transfer between domains without paired training data, neural painting simulates the physical painting process, and diffusion models leverage a denoising process for high-quality results. To evaluate the effectiveness of these approaches, we conduct a survey assessing the aesthetic appeal, naturalness, and adherence to Monets style of the generated images. Our analysis provides insights into the strengths and limitations of each method and identifies areas for future improvement in reproducing Monets iconic style.",10.54254/2755-2721/107/2024bj0071,"['JournalArticle', 'Review']",Art; Computer Science
Unmasking camouflage: exploring the challenges of large language models in deciphering African American language & online performativity,2024,"Shana Kleiner, Jessica A. Grieser, Shug Miller, James Shepard, Javier Garcia-Perez, Nicholas Deas, D. Patton, Elsbeth Turcan, Kathleen McKeown","The growing accessibility of large language models (LLMs) has raised many questions about the reliability of probabilistically generated natural language responses. While researchers have documented how bias in the training data leads to biased and ethically problematic output, little attention has been paid to the problems which arise from the nature of the varieties of language on which these models are trained. In particular, certain kinds of expressive and performative language use are more common among African American social media users than they occur in the naturalistic speech of African Americans, a discrepancy which models may fail to take into account when they are training on easily-scraped data as being representative of African American speech. Because LLM training data is generally proprietary, in this work we simulate the training data using a collected dataset consisting of 274 posts from Twitter, Reddit, and Hip-Hop lyrics and analyze how LLMs interpreted their meaning. We highlight the difficulties LLMs, including GPT-3 and GPT-4, have in understanding performative AAL and examine how camouflaging and performativity are addressed (or not) by LLMs and demonstrate the harmful implications of misinterpreting online performance.",10.1007/s43681-024-00623-2,['JournalArticle'],Computer Science; Linguistics; Sociology
Simulating Task-Oriented Dialogues with State Transition Graphs and Large Language Models,2024,"Chris Samarinas, Pracha Promthaw, Atharva Nijasure, Hansi Zeng, Julian Killingback, Hamed Zamani","This paper explores SynTOD, a new synthetic data generation approach for developing end-to-end Task-Oriented Dialogue (TOD) Systems capable of handling complex tasks such as intent classification, slot filling, conversational question-answering, and retrieval-augmented response generation, without relying on crowdsourcing or real-world data. SynTOD utilizes a state transition graph to define the desired behavior of a TOD system and generates diverse, structured conversations through random walks and response simulation using large language models (LLMs). In our experiments, using graph-guided response simulations leads to significant improvements in intent classification, slot filling and response relevance compared to naive single-prompt simulated conversations. We also investigate the end-to-end TOD effectiveness of different base and instruction-tuned LLMs, with and without the constructed synthetic conversations. Finally, we explore how various LLMs can evaluate responses in a TOD system and how well they are correlated with human judgments. Our findings pave the path towards quick development and evaluation of domain-specific TOD systems. We release our datasets, models, and code for research purposes.",10.48550/arXiv.2404.14772,['JournalArticle'],Computer Science
Contextualizing Recommendation Explanations with LLMs: A User Study,2025,"Yuanjun Feng, Stefan Feuerriegel, Y. Shrestha","Large language models (LLMs) are increasingly prevalent in recommender systems, where LLMs can be used to generate personalized recommendations. Here, we examine how different LLM-generated explanations for movie recommendations affect users'perceptions of cognitive, affective, and utilitarian needs and consumption intentions. In a pre-registered, between-subject online experiment (N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic explanations, and (b) LLM-generated contextualized explanations. Our findings show that contextualized explanations (i.e., explanations that incorporate users'past behaviors) effectively meet users'cognitive needs while increasing users'intentions to watch recommended movies. However, adding explanations offers limited benefits in meeting users'utilitarian and affective needs, raising concerns about the proper design and implications of LLM-generated explanations. Qualitative insights from interviews reveal that referencing users'past preferences enhances trust and understanding but can feel excessive if overused. Furthermore, users with more active and positive engagement with the recommender system and movie-watching get substantial gains from contextualized explanations. Overall, our research clarifies how LLM-generated recommendations influence users'motivations and behaviors, providing valuable insights for the future development of user-centric recommender systems, a key element in social media platforms and online ecosystems.",10.48550/arXiv.2501.12152,['JournalArticle'],Computer Science; Psychology
The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its Own,2025,"Gokul Puthumanaillam, Melkior Ornik","This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a""minimal effort""protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AI's strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24\%), approaching but not exceeding the class average (84.99\%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: https://gradegpt.github.io.",10.48550/arXiv.2503.05760,['JournalArticle'],Computer Science; Engineering
"xRFM: Accurate, scalable, and interpretable feature learning models for tabular data",2025,"Daniel Beaglehole, David Holzmller, Adityanarayanan Radhakrishnan, Misha Belkin","Inference from tabular data, collections of continuous and categorical variables organized into matrices, is a foundation for modern technology and science. Yet, in contrast to the explosive changes in the rest of AI, the best practice for these predictive tasks has been relatively unchanged and is still primarily based on variations of Gradient Boosted Decision Trees (GBDTs). Very recently, there has been renewed interest in developing state-of-the-art methods for tabular data based on recent developments in neural networks and feature learning methods. In this work, we introduce xRFM, an algorithm that combines feature learning kernel machines with a tree structure to both adapt to the local structure of the data and scale to essentially unlimited amounts of training data. We show that compared to $31$ other methods, including recently introduced tabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performance across $100$ regression datasets and is competitive to the best methods across $200$ classification datasets outperforming GBDTs. Additionally, xRFM provides interpretability natively through the Average Gradient Outer Product.",10.48550/arXiv.2508.10053,['JournalArticle'],Computer Science; Mathematics
Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs,2025,"Manuel Pratelli, M. Petrocchi","Large language models (LLMs) make it possible to generate synthetic behavioural data at scale, offering an ethical and low-cost alternative to human experiments. Whether such data can faithfully capture psychological differences driven by personality traits, however, remains an open question. We evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to reproduce personality-based variation in susceptibility to misinformation, focusing on news discernment, the ability to judge true headlines as true and false headlines as false. Leveraging published datasets in which human participants with known personality profiles rated headline accuracy, we create matching LLM agents and compare their responses to the original human patterns. Certain trait-misinformation associations, notably those involving Agreeableness and Conscientiousness, are reliably replicated, whereas others diverge, revealing systematic biases in how LLMs internalize and express personality. The results underscore both the promise and the limits of personality-aligned LLMs for behavioral simulation, and offer new insight into modeling cognitive diversity in artificial agents.",10.48550/arXiv.2506.23610,['JournalArticle'],Computer Science; Psychology
Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation,2025,"Jianpeng Zhao, Chenyu Yuan, Weiming Luo, Haoling Xie, Guangwei Zhang, Steven Jige Quan, Zixuan Yuan, Pengyang Wang, Denghui Zhang","Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark",10.48550/arXiv.2509.06337,"['JournalArticle', 'Review']",Computer Science; Sociology
An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs,2025,"Gino Carmona-D'iaz, William Jim'enez-Leal, Mar'ia Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Berm'udez","Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.",10.48550/arXiv.2505.09724,"['JournalArticle', 'Review']",Computer Science
AI in Service of Software Quality: How ChatGPT and Personas Are Transforming Exploratory Testing,2024,"gatha de Almeida, Eliane Collins, A. Oran","Context: Exploratory testing is essential in the software validation process as a way to find unexpected and critical failures in a short time, complementing documented functional test cases. However, creating scenarios to explore the software (such as test charters) can be time-consuming, and depending on the teams experience, it may lack adequate coverage of functionalities and scenarios that target specific user profiles of the application. Objective: This article investigates how AI, through LLMs (Large Language Models), can assist in creating exploratory test charters that reflect the characteristics and needs of different user personas. Method: To achieve this, an experimental study was conducted where personas were used as input in ChatGPT 3.5 to generate exploratory test charters. The effectiveness of the approach was evaluated by Software Engineering students, who analyzed the performance and usefulness of the generated charters through a questionnaire based on the TAM model, supplemented by qualitative and quantitative analyses. Results: Data analysis indicated positive acceptance of ChatGPT 3.5 by the participants, highlighting its ease of use and perceived usefulness. Conclusion: This study contributes to the field of Software Engineering by demonstrating a practical application of artificial intelligence in the automated generation of test charters. ChatGPT 3.5 has proven to be a promising tool to support the creation of personalized exploratory test charters, contributing to software quality improvement. The integration of artificial intelligence techniques with user-centered design methods can significantly optimize the software testing process.",10.1145/3701625.3701657,"['JournalArticle', 'Book']",Computer Science
LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward,2025,"Yi Zhao, Siqi Wang, Jing Li","Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study, hence, focuses on producing precise, in-situ, step-by-step navigation instructions that are practically usable by VI users. Concretely, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate rewards guiding the Vision-Language Model (VLM) post-training. This enhances instruction usability while reducing costly real-world data needs. To facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced benchmark. It provides diverse navigation scenarios with accurate spatial coordinates, supporting detailed, open-ended in-situ instruction generation. Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and benchmark are available at \href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.",10.48550/arXiv.2506.04070,['JournalArticle'],Computer Science
ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool Capabilities,2024,"Zhenchao Jin, Mengchen Liu, Dongdong Chen, Lingting Zhu, Yunsheng Li, Lequan Yu","Through the integration of external tools, large language models (LLMs) such as GPT-4o and Llama 3.1 significantly expand their functional capabilities, evolving from elementary conversational agents to general-purpose assistants. We argue that the primary drivers of these advancements are the quality and diversity of the training data. However, the existing LLMs with external tool integration provide only limited transparency regarding their datasets and data collection methods, which has led to the initiation of this research. Specifically, in this paper, our objective is to elucidate the detailed process involved in constructing datasets that empower LLMs to effectively learn how to utilize external tools and make this information available to the public through the introduction of ToolBridge. ToolBridge proposes to employ a collection of general open-access datasets as its raw dataset pool and applies a series of strategies to identify appropriate data entries from the pool for external tool API insertions. By supervised fine-tuning on these curated data entries, LLMs can invoke external tools in appropriate contexts to boost their predictive accuracy, particularly for basic functions including data processing, numerical computation, and factual retrieval. Our experiments rigorously isolates model architectures and training configurations, focusing exclusively on the role of data. The experimental results indicate that LLMs trained on ToolBridge demonstrate consistent performance improvements on both standard benchmarks and custom evaluation datasets. All the associated code and data will be open-source at https://github.com/CharlesPikachu/ToolBridge, promoting transparency and facilitating the broader community to explore approaches for equipping LLMs with external tools capabilities.",10.48550/arXiv.2410.10872,['JournalArticle'],Computer Science
A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem,2024,"Haiyue Yuan, Ali Raza, Nikolay Matyunin, Jibesh Patra, Shujun Li","The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem.",10.1109/ITSC58415.2024.10919888,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering
A novel soft computing approach based on FIR to model and predict energy dynamic systems,2022,Sergio Jurado,"We are facing a global climate crisis that is demanding a change in the status quo of how we produce, distribute and consume energy. In the last decades, this is being redefined through Smart Grids(SG), an intelligent electrical network more observable, controllable, automated, fully integrated with energy services and the end-users. Most of the features and proposed SG scenarios are based on reliable, robust and fast energy predictions. For instance, for proper planning activities, such as generation, purchasing, maintenance and investment; for demand side management, like demand response programs; for energy trading, especially at local level, where productions and consumptions are more stochastics and dynamic; better forecasts also increase grid stability and thus supply security. A large variety of Artificial Intelligence(AI) techniques have been applied in the field of Short-term electricity Load Forecasting(SLF) at consumer level in low-voltage system, showing a better performance than classical techniques. Inaccuracy or failure in the SLF process may be translated not just in a non-optimal (low prediction accuracy) solution but also in frustration of end-users, especially in new services and functionalities that empower citizens. In this regard, some limitations have been observed in energy forecasting models based on AI such as robustness, reliability, accuracy and computation in the edge. 
This research proposes and develops a new version of Fuzzy Inductive Reasoning(FIR), called Flexible FIR, to model and predict the electricity consumption of an entity in the low-voltage grid with high uncertainties, and information missing, as well as the capacity to be deployed either in the cloud or locally in a new version of Smart Meters(SMs) based on Edge Computing(EC). FIR has been proved to be a powerful approach for model identification and system s prediction over dynamic and complex processes in different real world domains but not yet in the energy domain. Thus, the main goal of this thesis is to demonstrate that a new version of FIR, more robust, reliable and accurate can be a referent Soft Computing(SC) methodology to model and predict dynamic systems in the energy domain and that it is scalable to an EC integration. The core developments of Flexible FIR have been an algorithm that can cope with missing information in the input values, as well as learn from instances with Missing Values(MVs) in the knowledge-based, without compromising significantly the accuracy of the predictions. Moreover, Flexible FIR comes with new forecasting strategies that can cope better with loss of causality of a variable and dispersion of output classes than classical k nearest neighbours, making the FIR forecasting process more reliable and robust. Furthermore, Flexible FIR addresses another major challenge modelling with SC techniques, which is to select best model parameters. One of the most important parameters in FIR is the number k of nearest neighbours to be used in the forecast process. The challenge to select the optimal k, dynamically, is addressed through an algorithm, called KOS(K nearest neighbour Optimal Selection), which has been developed and tested also with real world data. It computes a membership aggregation function of all the neighbours with respect their belonging to the output classes.While with KOS the optimal parameter k is found online, with other approaches such as genetic algorithms or reinforcement learning is not, which increases the computational time.
 Ens trobem davant una crisis climtica global que exigeix un canvi al status quo de la manera que produm, distribum i consumim energia. En les darreres dcades, est sent redefinit grcies a les xarxa elctriques intelligents(SG: Smart Grid) amb millor observabilitat, control, automatitzaci, integrades amb nous serveis energtics i usuaris finals. La majoria de les funcionalitats i escenaris de les SG es basen en prediccions de la crrega elctrica confiables, robustes i rpides. Per les prediccions de crregues elctriques a curt termini(SLF: Short-term electricity Load Forecasting), a nivell de consumidors al baix voltatge, shan aplicat una gran varietat de tcniques intelligncia Artificial(IA) mostrant millor rendiment que tcniques estadstiques tradicionals. Un baix rendiment en SLF, pot traduir-se no noms en una soluci no-ptima (baixa precisi de predicci) sin tamb en la frustraci dels usuaris finals, especialment en nous serveis i funcionalitats que empoderarien als ciutadans. En el marc daquesta investigaci es proposa i desenvolupa una nova versi de la metodologia del Raonament Inductiu Difs(FIR: Fuzzy Inductive Reasoning), anomenat Flexible FIR, capa de modelar i predir el consum delectricitat duna entitat amb un grau dincertesa molt elevat, incls amb importants carncies dinformaci (missing values). A ms, Flexible FIR t la capacitat de desplegar-se al nvol, aix como localment, en el que podria ser una nova versi de Smart Meters (SM) basada en tecnologia dEdge Computing (EC). FIR ja ha demostrat ser una metodologia molt potent per la generaci de models i prediccions en processos dinmics en diferents mbits, per encara no en el de lenergia. Per tant, lobjectiu principal daquesta tesis s demostrar que una versi millorada de FIR, ms robusta, fiable i precisa pot consolidar-se com una metodologia Soft Computing SC) de referencia per modelar i predir sistemes dinmics en aplicacions per al sector de lenergia i que s escalable a una integraci dEC. Les principals millores de Flexible FIR han estat, en primer lloc, el desenvolupament i test dun algorisme capa de processar els valors dentrada dun model FIR tot i que continguin Missing Values (MV). Addicionalment, aquest algorisme tamb permet aprendre dinstncies amb MV en la matriu de coneixement dun model FIR, sense comprometre de manera significativa la precisi de les prediccions. En segon lloc, shan desenvolupat i testat noves estratgies per a la fase de predicci, comportant-se millor que els clssics k vens ms propers quan ens trobem amb prdua de causalitat duna variable i dispersi en les classes de sortida, aconseguint un procs daprenentatge i predicci ms confiable i robust. En tercer lloc, Flexible FIR aborda un repte molt com en tcniques de SC: lptima parametritzaci del model. En FIR, un dels parmetres ms determinants s el nmero k de vens ms propers que sutilitzaran durant la fase de predicci. La selecci del millor valor de k es planteja de manera dinmica a travs de lalgorisme KOS (K nearest neighbour Optimal Selection) que sha desenvolupat i testat tamb amb dades reals. Mentre que amb KOS el parmetre ptim de k es calcula online, altres enfocaments mitjanant algoritmes gentics o aprenentatge per refor el clcul s offline, incrementant significativament el temps de resposta, sent a ms a ms difcil la implantaci en escenaris dEC. Aquestes millores fan que Flexible FIR es pugui adaptar molt b en aplicacions dEC. En aquest sentit es proposa el concepte dun SM de segona generaci basat en EC, que integra Flexible FIR com mdul de predicci delectricitat executant-se en el propi dispositiu i un agent EC amb capacitat per el trading d'energia produda localment. 
Aquest agent executa un innovador mecanisme basat en incentius, anomenat NRG-X-Change que utilitza una nova moneda digital descentralitzada per lintercanvi denergia, que sanomena NRGcoin.
 Estamos ante una crisis climtica global que exige un cambio del status quo de la manera que producimos, distribuimos y consumimos energa. En las ltimas dcadas, este status quo est siendo redefinido debido a: la penetracin de las energas renovables y la generacin distribuida; nuevas tecnologas como bateras y paneles solares con altos rendimientos; y la forma en que se consume la energa, por ejemplo, a travs de vehculos elctricos o con la electrificacin de los hogares. Estas palancas requieren una red elctrica inteligente (SG: Smart Grid) con mayor observabilidad, control, automatizacin y que est totalmente integrada con nuevos servicios energticos, as como con sus usuarios finales. La mayora de las funcionalidades y escenarios de las redes elctricas inteligentes se basan en predicciones de la energa confiables, robustas y rpidas. Por ejemplo, para actividades de planificacin como la generacin, compra, mantenimiento e inversin; para la gestin de la demanda, como los programas de demand response; en el trading de electricidad, especialmente a nivel local, donde las producciones y los consumos son ms estocsticos y dinmicos; una mejor prediccin elctrica tambin aumenta la estabilidad de la red y, por lo tanto, mejora la seguridad. Para las predicciones elctricas a corto plazo (SLF: Short-term electricity Load Forecasting), a nivel de consumidores en el bajo voltaje, se han aplicado una gran variedad de tcnicas de Inteligencia Artificial (IA) mostrando mejor rendimiento que tcnicas estadsticas convencionales. Un bajo rendimiento en los modelos predictivos, puede traducirse no solamente en una solucin no-ptima (baja precisin de prediccin) sino tambin en frustracin de los usuarios finales, especialmente en nuevos servicios y funcionalidades que empoderan a los ciudadanos. En este sentido, se han identificado limitaciones en modelos de prediccin de energa basados en IA, como la robustez, fiabilidad, precisin i computacin en el borde. 
En el marco de esta investigacin se propone y desarrolla una nueva versin de la metodologa de Razonamiento Inductivo Difuso (FIR: Fuzzy Inductive Reasoning), que hemos llamado Flexible FIR, capaz de modelar y predecir el consumo de electricidad de una entidad con altos grados de incertidumbre e incluso con importantes carencias de informacin (missing values). Adems, Flexible FIR tiene la capacidad de desplegarse en la nube, as como localmente, en lo que podra ser una nueva ve",10.5821/dissertation-2117-375588,nan,Computer Science
MARK: Memory Augmented Refinement of Knowledge,2025,"Anish Ganguli, Prabal Deb, Debleena Banerjee","Large Language Models (LLMs) assist in specialized tasks but struggle to align with evolving domain knowledge without costly fine-tuning. Domain knowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid') and generally accepted principles (e.g., ethical standards); Refined Memory: Evolving insights shaped by business needs and real-world changes. However, a significant gap often exists between a domain expert's deep, nuanced understanding and the system's domain knowledge, which can hinder accurate information retrieval and application. Our Memory-Augmented Refinement of Knowledge (MARK) framework enables LLMs to continuously learn without retraining by leveraging structured refined memory, inspired by the Society of Mind. MARK operates through specialized agents, each serving a distinct role: Residual Refined Memory Agent: Stores and retrieves domain-specific insights to maintain context over time; User Question Refined Memory Agent: Captures user-provided facts, abbreviations, and terminology for better comprehension; LLM Response Refined Memory Agent: Extracts key elements from responses for refinement and personalization. These agents analyse stored refined memory, detect patterns, resolve contradictions, and improve response accuracy. Temporal factors like recency and frequency prioritize relevant information while discarding outdated insights. MARK enhances LLMs in multiple ways: Ground Truth Strategy: Reduces hallucinations by establishing a structured reference; Domain-Specific Adaptation: Essential for fields like healthcare, law, and manufacturing, where proprietary insights are absent from public datasets; Personalized AI Assistants: Improves virtual assistants by remembering user preferences, ensuring coherent responses over time.",10.48550/arXiv.2505.05177,['JournalArticle'],Computer Science
Selecting the Right Llm for Egov Explanations,2025,"Lior Limonad, Fabiana Fournier, Hadar Mulian, George Manias, Spiros A. Borotis, Danai Kyrkou","The perceived quality of the explanations accompanying e-government services is key to gaining trust in these institutions, consequently amplifying further usage of these services. Recent advances in generative AI, and concretely in Large Language Models (LLMs) allow the automation of such content articulations, eliciting explanations' interpretability and fidelity, and more generally, adapting content to various audiences. However, selecting the right LLM type for this has become a nontrivial task for e-government service providers. In this work, we adapted a previously developed scale to assist with this selection, providing a systematic approach for the comparative analysis of the perceived quality of explanations generated by various LLMs. We further demonstrated its applicability through the tax-return process, using it as an exemplar use case that could benefit from employing an LLM to generate explanations about tax refund decisions. This was attained through a user study with 128 survey respondents who were asked to rate different versions of LLMgenerated explanations about tax refund decisions, providing a methodological basis for selecting the most appropriate LLM. Recognizing the practical challenges of conducting such a survey, we also began exploring the automation of this process by attempting to replicate human feedback using a selection of cutting-edge predictive techniques.",10.1109/ICEDEG65568.2025.11081620,"['JournalArticle', 'Conference', 'Review']",Computer Science
Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems,2025,"Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu","Multi-agent systems powered by Large Language Models (LLM-MAS) have demonstrated remarkable capabilities in collaborative problem-solving. However, their deployment also introduces new security risks. Existing research on LLM-based agents has primarily examined single-agent scenarios, while the security of multi-agent systems remains largely unexplored. To address this gap, we present a systematic study of intention-hiding threats in LLM-MAS. We design four representative attack paradigms that subtly disrupt task completion while maintaining a high degree of stealth, and evaluate them under centralized, decentralized, and layered communication structures. Experimental results show that these attacks are highly disruptive and can easily evade existing defense mechanisms. To counter these threats, we propose AgentXposed, a psychology-inspired detection framework. AgentXposed draws on the HEXACO personality model, which characterizes agents through psychological trait dimensions, and the Reid interrogation technique, a structured method for eliciting concealed intentions. By combining progressive questionnaire probing with behavior-based inter-agent monitoring, the framework enables the proactive identification of malicious agents before harmful actions are carried out. Extensive experiments across six datasets against both our proposed attacks and two baseline threats demonstrate that AgentXposed effectively detects diverse forms of malicious behavior, achieving strong robustness across multiple communication settings.",10.48550/arXiv.2507.04724,['JournalArticle'],Computer Science
Are Large Language Models Chameleons?,2024,"Mingmeng Geng, Sihong He, Roberto Trotta",,10.48550/arXiv.2405.19323,"['JournalArticle', 'Review']",Computer Science; Linguistics; Psychology
Learning interactions to boost human creativity with bandits and GPT-4,2023,"Ara Vartanian, Xiaoxi Sun, Yun-Shiuan Chuang, Siddharth Suresh, Xiaojin Zhu, Timothy T. Rogers","This paper considers how interactions with AI algorithms can boost human creative thought. We employ a psychological task that demonstrates limits on human creativity, namely semantic feature generation: given a concept name, respondents must list as many of its features as possible. Human participants typically produce only a fraction of the features they know before getting""stuck.""In experiments with humans and with a language AI (GPT-4) we contrast behavior in the standard task versus a variant in which participants can ask for algorithmically-generated hints. Algorithm choice is administered by a multi-armed bandit whose reward indicates whether the hint helped generating more features. Humans and the AI show similar benefits from hints, and remarkably, bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior. The results suggest that strategies for boosting human creativity via computer interactions can be learned by bandits run on groups of simulated participants.",10.48550/arXiv.2311.10127,['JournalArticle'],Computer Science; Psychology
"I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations",2025,"Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah","This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts.",10.48550/arXiv.2508.04939,['JournalArticle'],Computer Science; Linguistics
GPT-4's One-Dimensional Mapping of Morality: How the Accuracy of Country-Estimates Depends on Moral Domain,2024,"P. Strimling, Joel Krueger, Simon Karlsson","Prior research demonstrates that Open AI's GPT models can predict variations in moral opinions between countries but that the accuracy tends to be substantially higher among high-income countries compared to low-income ones. This study aims to replicate previous findings and advance the research by examining how accuracy varies with different types of moral questions. Using responses from the World Value Survey and the European Value Study, covering 18 moral issues across 63 countries, we calculated country-level mean scores for each moral issue and compared them with GPT-4's predictions. Confirming previous findings, our results show that GPT-4 has greater predictive success in high-income than in low-income countries. However, our factor analysis reveals that GPT-4 bases its predictions primarily on a single dimension, presumably reflecting countries' degree of conservatism/liberalism. Conversely, the real-world moral landscape appears to be two-dimensional, differentiating between personal-sexual and violent-dishonest issues. When moral issues are categorized based on their moral domain, GPT-4's predictions are found to be remarkably accurate in the personal-sexual domain, across both high-income (r = .77) and low-income (r = .58) countries. Yet the predictive accuracy significantly drops in the violent-dishonest domain for both high-income (r = .30) and low-income (r = -.16) countries, indicating that GPT-4's one-dimensional world-view does not fully capture the complexity of the moral landscape. In sum, this study underscores the importance of not only considering country-specific characteristics to understand GPT-4's moral understanding, but also the characteristics of the moral issues at hand.",10.48550/arXiv.2407.16886,"['JournalArticle', 'Review']",Computer Science; Economics; Political Science
Bare-Metal Firmware Fuzzing: A Survey of Techniques and Approaches,2025,"Asmita Asmita, Ryan Tsang, Sujan Ghimire, Soheil Salehi, H. Homayoun","Firmware attacks are increasingly prevalent, often serving as low-hanging fruit for attackers due to the challenges of firmware security analysis. The complexity of hardware systems, platform diversity, peripheral interactions, and asynchronous events make thorough security analysis of embedded firmware particularly difficult. Despite these challenges, significant research has been dedicated to advancing dynamic analysis techniques, such as fuzzing, to improve firmware security. Existing research approaches these issues with varying methods and emphases. This survey paper examines the implementation of existing firmware fuzzing techniques, providing an overview of their emulation strategies and fuzzing methodologies. It also reviews several existing fuzzers and the application of large language models (LLMs) in fuzzing generic software. Our survey focuses specifically on frameworks for fuzzing embedded bare-metal/monolithic firmware. Our analysis highlights that most existing research has focused primarily on firmware emulation, rehosting, and back-end instrumentation to facilitate fuzzing, often relying on direct integration with existing fuzzers. However, the broader exploration of various fuzzing techniques, such as input generation, mutation, feedback, and scheduling strategies, widely used in generic software remains limited for embedded firmware. Recent efforts have started to address these aspects, with emerging work exploring fuzzing techniques beyond simple fuzzer integration. Furthermore, the application of LLMs presents a promising direction for further investigation. This survey provides a comprehensive overview of the past, present, and future landscape of bare-metal firmware fuzzing.",10.1109/ACCESS.2025.3575691,"['JournalArticle', 'Review']",Computer Science; Engineering
SCUNLP-1 at the NTCIR-17 FinArg-1 Task: Enhancing Classification Prediction through Feature Generation Based on ChatGPT,2023,"Ya-Mien Cheng, Jheng-Long Wu",,10.20736/0002001311,"['JournalArticle', 'Review']",Computer Science
DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs,2025,"Lake Yin, Fan Huang","As Large Language Models (LLMs) have risen in prominence over the past few years, there has been concern over the potential biases in LLMs inherited from the training data. Previous studies have examined how LLMs exhibit implicit bias, such as when response generation changes when different social contexts are introduced. We argue that this implicit bias is not only an ethical, but also a technical issue, as it reveals an inability of LLMs to accommodate extraneous information. However, unlike other measures of LLM intelligence, there are no standard methods to benchmark this specific subset of LLM bias. To bridge this gap, we developed a method for calculating an easily interpretable benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM logic and math problem datasets with sociodemographic personas. We demonstrate that this method can statistically validate the presence of implicit bias in LLM behavior and find an inverse trend between question answering accuracy and implicit bias, supporting our argument.",10.48550/arXiv.2505.10013,['JournalArticle'],Computer Science; Linguistics
Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars,2025,"Oliver Slumbers, Joel Z. Leibo, Marco A. Janssen","Collective risk social dilemmas (CRSD) highlight a trade-off between individual preferences and the need for all to contribute toward achieving a group objective. Problems such as climate change are in this category, and so it is critical to understand their social underpinnings. However, rigorous CRSD methodology often demands large-scale human experiments but it is difficult to guarantee sufficient power and heterogeneity over socio-demographic factors. Generative AI offers a potential complementary approach to address thisproblem. By replacing human participants with large language models (LLM), it allows for a scalable empirical framework. This paper focuses on the validity of this approach and whether it is feasible to represent a large-scale human-like experiment with sufficient diversity using LLM. In particular, where previous literature has focused on political surveys, virtual towns and classical game-theoretic examples, we focus on a complex CRSD used in the institutional economics and sustainability literature known as Port of Mars",10.48550/arXiv.2506.05555,"['JournalArticle', 'Review']",Computer Science; Environmental Science
When Large Language Models are Reliable for Judging Empathic Communication,2025,"Aakriti Kumar, Nalin Poungpeth, Diyi Yang, Erina Farrell, Bruce Lambert, Matthew Groh","Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? We investigate this question by comparing how experts, crowdworkers, and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing, and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess inter-rater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks'sub-components depending on their clarity, complexity, and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.",10.48550/arXiv.2506.10150,['JournalArticle'],Computer Science; Linguistics; Psychology
Leveraging Virtual Prototypes for Training Data Collection in LLM-Based Voice User Interface Development for Machines,2025,"Polina Hfner, Frithjof Eisenlohr, Abhijit Karande, Michael Grethler, Avik Mukherjee, Nhung Tran","Voice User Interfaces (VUIs) are becoming increasingly valuable in industrial applications, offering hands-free control in complex environments. However, developing and validating VUIs for such applications faces challenges, including limited access to physical prototypes and high testing costs. This paper presents a methodology that utilizes virtual reality (VR) prototypes to collect training data for large language model (LLM)-based VUIs, allowing early-stage voice control development before physical prototypes are accessible. Through an immersive Wizard-of-Oz (WoZ) method, participants interact with a virtual reality representation of a machine, generating realistic, scenario-based conversational data. This combined WoZ and VR approach enables high-quality data collection and iterative model training, offering an effective solution that can be applied across various types of machine. Preliminary findings demonstrate the viability of VR in generating diverse and robust data sets that closely simulate real-world dialogs for voice interactions in industrial settings.",10.1109/AIxVR63409.2025.00054,"['JournalArticle', 'Conference']",Computer Science; Engineering
Leveraging Large Language Models for Personalized Public Messaging,2025,"Amit Kumar Das, Cindy Xiong Bearfield, Klaus Mueller","We present a novel methodology for crafting effective public messages by combining large language models (LLMs) and conjoint analysis. Our approach personalizes messages for diverse personas  context-specific archetypes representing distinct attitudes and behaviors  while reducing the costs and time associated with traditional surveys. We tested this method in public health contexts (e.g., COVID-19 mandates) and civic engagement initiatives (e.g., voting). A total of 153 distinct messages were generated, each composed of components with varying levels, and evaluated across five personas tailored to each context. Conjoint analysis identified the most effective message components for each persona, validated through a study with 2,040 human participants. This research highlights LLMs potential to enhance public communication, providing a scalable, cost-effective alternative to surveys, and offers new directions for HCI, particularly for the design of adaptive, user-centered, persona-driven interfaces and systems.",10.1145/3706599.3720018,"['Book', 'JournalArticle', 'Review']",Computer Science; Political Science
Dialogue-Level Data Augmentation for Conversation Derailment Forecasting and Topic-Shift Detection,2024,"Nerses Yuzbashyan, Nikolay Banar, Walter Daelemans","Counteracting antisocial behavior on-line becomes an increasingly challenging task, as social media platforms continue to gain popularity. In addition, it often requires predicting in advance when an online conversation is heading towards derailment in order to take actions before any harm is done. Developing systems for such purposes requires a large amount of labeled data, which is difficult to collect and extremely expensive to annotate manually. In such conditions, data augmentation could be an attractive alternative. In this paper, we address the conversation derailment detection and forecasting tasks and conversation topic-shift detection task. These tasks require augmentation at the dialogue level, which presents unique challenges compared to other data augmentation approaches. We propose three methods for generating synthetic dialogues using large language models (LLMs) to augment training datasets without the need for additional data collection. Our results demonstrate that while the proposed methods yield improvements for scarce data, they cannot overcome the ceiling effect when data is abundant.",10.1145/3711542.3711568,"['Book', 'JournalArticle']",Computer Science
AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering,2024,"Mahiro Ukai, Shuhei Kurita, Atsushi Hashimoto, Yoshitaka Ushiku, Nakamasa Inoue","Visual question answering aims to provide responses to questions given visual input. Recently, visual programmatic models (VPMs), which generate programs to answer questions through large language models (LLMs), have attracted attention. However, they often require long input prompts to provide the LLM with sufficient API usage details to generate relevant code. To address this limitation, we propose AdaCoder, an adaptive prompt compression framework for VPMs. AdaCoder operates in two phases: a compression phase and an inference phase. In the compression phase, given a preprompt that describes all API definitions with example code snippets, a set of compressed preprompts is generated, each depending on a specific question type. In the inference phase, AdaCoder predicts the question type and chooses the appropriate corresponding compressed preprompt to generate code to answer the question. In experiments, we apply AdaCoder to ViperGPT and demonstrate that it reduces token length by 71.1%, while maintaining or even improving the performance of visual question answering.",10.1145/3664647.3681010,"['JournalArticle', 'Book']",Computer Science
Predicting ChatGPTs Ability to Solve Complex Programming Challenges,2024,"Nguyen Ho, James May, Bao Ngo, Jack Formato, Linh Ngo, Van Long Ho, Hoang Bui","The recent emergence of Large Language Model (LLM)-based tools such as OpenAIs ChatGPT and Googles Gemini has sparked excitement across the software development industry, and offered promises to transform the software development process. Despite the enthusiasm, it remains uncertain whether these tools are already good enough at coding to replace the role of software developers. Currently, no studies have provided insights into the performance of LLMs, such as understanding which characteristics of a programming task might affect an LLM's performance, or predicting how an LLM will handle new programming challenges. In this work, we address these challenges by first creating a data collection framework to gather 3,323 programming tasks from Kattis, a widely-used programming challenge platform. We then use OpenAI's ChatGPT to solve these programming tasks. The solutions obtained from ChatGPT are submitted back to Kattis to evaluate their correctness and effectiveness. Next, we use the collected data, including both problem and solution information, to analyze the task characteristics that significantly influence ChatGPT's performance. Building on the analysis, we develop predictive models that can forecast the efficacy of ChatGPT on new programming problems. Our analysis indicates that factors such as the difficulty level of a programming challenge, or the readability complexity of a problem description can significantly affect the efficacy of ChatGPT. Finally, the experimental results show that our predictive model can correctly predict ChatGPT performance with an accuracy of up to 90% for easy problems, and up to 79% for difficult problems.",10.1109/BigData62323.2024.10825139,"['JournalArticle', 'Conference']",Computer Science
AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation,2025,"Leah von der Heyde, Anna Haensch, Bernd Weiss, Jessika Daikeler","The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs'performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs'unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.",10.48550/arXiv.2506.14634,"['JournalArticle', 'Review']",Computer Science; Linguistics
Synthetic Social Media Influence Experimentation Via an Agentic Reinforcement Learning Large Language Model Bot,2024,"Bailu Jin, Weisi Guo","Understanding the dynamics of public opinion evolution on online social platforms is crucial for understanding influence mechanisms and the provenance of information. Traditional influence analysis is typically divided into qualitative assessments of personal attributes (e.g., psychology of influence) and quantitative evaluations of influence power mechanisms (e.g., social network analysis). One challenge faced by researchers is the ethics of real-world experimentation and the lack of social influence data. In this study, we provide a novel simulated environment that combines agentic intelligence with Large Language Models (LLMs) to test topic-specific influence mechanisms ethically. Our framework contains agents that generate posts, form opinions on specific topics, and socially follow/unfollow each other based on the outcome of discussions. This simulation allows researchers to observe the evolution of how opinions form and how influence leaders emerge. Using our own framework, we design an opinion leader that utilizes Reinforcement Learning (RL) to adapt its linguistic interaction with the community to maximize its influence and followers over time. Our current findings reveal that constraining the action space and incorporating self-observation are key factors for achieving stable and consistent opinion leader generation for topic-specific influence. This demonstrates the simulation framework's capacity to create agents that can adapt to complex and unpredictable social dynamics. The work is important in an age of increasing online influence on social attitudes and emerging technologies.",10.18564/jasss.5726,['JournalArticle'],Computer Science
Master of Deceit: Comparative Analysis of Human and Machine-Generated Deceptive Text,2025,"Quang Minh Trinh, Samiha Zarin, R. Rezapour","Deception, the intentional act of creating false impressions, has long been studied in human interactions. With the emergence of AI and large language models (LLMs), deception now extends to machine-generated content, raising concerns about distinguishing between human and AI-created content. In this study, we compare deceptive and truthful texts produced by humans and LLMs (GPT-3.5 and GPT-4o) using two datasets; a crowdsourced online Review Dataset and a transcribed interview dataset (MU3D). We replicate the data generation process with LLMs, introducing personas into prompts to examine linguistic differences and potential biases. Using LIWC, we analyze word choice, complexity, and cognitive patterns across human- and LLM-generated deception and truthful texts. Our findings show that LLM-generated deception differs significantly from human deception, exhibiting greater verbosity, formality, and lexical sophistication, while human deception is more socially driven, relying more on social references, interpersonal cues, and natural conversational patterns. Despite improvements in LLMs, context-dependent biases remain embedded in LLM-generated texts, emphasizing the need for stronger bias mitigation strategies and responsible AI deployment. Our study identifies key linguistic markers that differentiate LLM-generated from human deception and highlights the importance of assessing hidden biases and potential risks in AI-generated deceptive text and misinformation.",10.1145/3717867.3717914,"['JournalArticle', 'Book', 'Review']",Computer Science; Linguistics; Psychology
Toward Affective Empathy via Personalized Analogy Generation: A Case Study on Microaggression,2025,"Hyojin Ju, Jungeun Lee, Seungwon Yang, Jungseul Ok, Inseok Hwang","The importance of empathy cannot be overstated in modern societies where people of diverse backgrounds increasingly interact together. The HCI community has strived to foster affective empathy through immersive technologies. Many previous techniques are built upon a premise that presenting the same experience as-is may help evoke the same emotion, which however faces limitations in matters where the emotional responses largely differ across individuals. In this paper, we present a novel concept of generating a personalized experience based on a large language model (LLM) to facilitate affective empathy between individuals despite their differences. As a case study to showcase its effectiveness, we developed EmoSync, an LLM-based agent that generates personalized analogical microaggression situations, facilitating users to personally resonate with a specific microaggression situation of another person. EmoSync is designed and evaluated along a 3-phased user study with 100+ participants. We comprehensively discuss implications, limitations, and possible applications. Disclaimer: Readers may find content of a discriminative or stereotypical nature, which is inevitable given this works theme.",10.1145/3706598.3714122,"['Book', 'JournalArticle', 'Conference']",Computer Science; Psychology
AI-Driven Global Disaster Intelligence from News Media,2025,"Fahim K. Sufi, M. Alsulami","Open-source disaster intelligence (OSDI) is crucial for improving situational awareness, disaster preparedness, and real-time decision-making. Traditional OSDI frameworks often rely on social media data, which are susceptible to misinformation and credibility issues. This study proposes a novel AI-driven framework utilizing automated data collection from 444 large-scale online news portals, including CNN, BBC, CBS News, and The Guardian, to enhance data reliability. Over a 514-day period (27 September 2023 to 26 February 2025), 1.25 million news articles were collected, of which 17,884 were autonomously classified as disaster-related using Generative Pre-Trained Transformer (GPT) models. The analysis identified 185 distinct countries and 6068 unique locations, offering unprecedented geospatial and temporal intelligence. Advanced clustering and predictive analytics techniques, including K-means, DBSCAN, seasonal decomposition (STL), Fourier transform, and ARIMA, were employed to detect geographical hotspots, cyclical patterns, and temporal dependencies. The ARIMA (2, 1, 2) model achieved a mean squared error (MSE) of 823,761, demonstrating high predictive accuracy. Key findings highlight that the USA (6548 disasters), India (1393 disasters), and Australia (1260 disasters) are the most disaster-prone countries, while hurricanes/typhoons/cyclones (5227 occurrences), floods (3360 occurrences), and wildfires (2724 occurrences) are the most frequent disaster types. The framework establishes a comprehensive methodology for integrating geospatial clustering, temporal analysis, and multimodal data processing in OSDI. By leveraging AI automation and diverse news sources, this study provides a scalable, adaptable, and ethically robust solution for proactive disaster management, improving global resilience and preparedness.",10.3390/math13071083,['JournalArticle'],Computer Science; Environmental Science; Political Science
Next Token Is Enough: Realistic Image Quality and Aesthetic Scoring with Multimodal Large Language Model,2025,"Mingxing Li, Rui Wang, Lei Sun, Yancheng Bai, Xiangxiang Chu","The rapid expansion of mobile internet has resulted in a substantial increase in user-generated content (UGC) images, thereby making the thorough assessment of UGC images both urgent and essential. Recently, multimodal large language models (MLLMs) have shown great potential in image quality assessment (IQA) and image aesthetic assessment (IAA). Despite this progress, effectively scoring the quality and aesthetics of UGC images still faces two main challenges: 1) A single score is inadequate to capture the hierarchical human perception. 2) How to use MLLMs to output numerical scores, such as mean opinion scores (MOS), remains an open question. To address these challenges, we introduce a novel dataset, named Realistic image Quality and Aesthetic (RealQA), including 14,715 UGC images, each of which is annoted with 10 fine-grained attributes. These attributes span three levels: low level (e.g., image clarity), middle level (e.g., subject integrity) and high level (e.g., composition). Besides, we conduct a series of in-depth and comprehensive investigations into how to effectively predict numerical scores using MLLMs. Surprisingly, by predicting just two extra significant digits, the next token paradigm can achieve SOTA performance. Furthermore, with the help of chain of thought (CoT) combined with the learnt fine-grained attributes, the proposed method can outperform SOTA methods on five public datasets for IQA and IAA with superior interpretability and show strong zero-shot generalization for video quality assessment (VQA). The code and dataset will be released.",10.48550/arXiv.2503.06141,['JournalArticle'],Computer Science
Large Language Model Evaluation via Matrix Nuclear-Norm,2024,"Yahan Li, Tingyu Xia, Yi Chang, Yuan Wu","As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \( L_{1,2}\text{-norm} \) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.",10.48550/arXiv.2410.10672,['JournalArticle'],Computer Science
Understanding Privacy Norms through Web Forms,2024,"Hao Cui, Rahmadi Trimananda, A. Markopoulou","Web forms are one of the primary ways to collect personal information online, yet they are relatively under-studied. Unlike web tracking, data collection through web forms is explicit and contextualized. Users (i) are asked to input specific personal information types, and (ii) know the specific context (i.e., on which website and for what purpose). For web forms to be trusted by users, they must meet the common sense standards of appropriate data collection practices within a particular context (i.e., privacy norms). In this paper, we extract the privacy norms embedded within web forms through a measurement study. First, we build a specialized crawler to discover web forms on websites. We run it on 11,500 popular websites, and we create a dataset of 293K web forms. Second, to process data of this scale, we develop a cost-efficient way to annotate web forms with form types and personal information types, using text classifiers trained with assistance of large language models (LLMs). Third, by analyzing the annotated dataset, we reveal common patterns of data collection practices. We find that (i) these patterns are explained by functional necessities and legal obligations, thus reflecting privacy norms, and that (ii) deviations from the observed norms often signal unnecessary data collection. In addition, we analyze the privacy policies that accompany web forms. We show that, despite their wide adoption and use, there is a disconnect between privacy policy disclosures and the observed privacy norms.",10.48550/arXiv.2408.16304,['JournalArticle'],Computer Science
Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation,2025,"Michele Fiori, Gabriele Civitarese, Priyankar Choudhary, Claudio Bettini","Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning of machine learning models. In IoT systems, XAI improves the transparency of models processing sensor data from multiple heterogeneous devices, ensuring end-users understand and trust their outputs. Among the many applications, XAI has also been applied to sensor-based Activities of Daily Living (ADLs) recognition in smart homes. Existing approaches highlight which sensor events are most important for each predicted activity, using simple rules to convert these events into natural language explanations for non-expert users. However, these methods produce rigid explanations lacking natural language flexibility and are not scalable. With the recent rise of Large Language Models (LLMs), it is worth exploring whether they can enhance explanation generation, considering their proven knowledge of human activities. This paper investigates potential approaches to combine XAI and LLMs for sensor-based ADL recognition. We evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition models, avoiding costly labeled data collection, and b) to automate the generation of explanations for existing data-driven XAI approaches when training data is available and the goal is higher recognition rates. Our critical evaluation provides insights into the benefits and challenges of using LLMs for explainable ADL recognition.",10.48550/arXiv.2503.16622,['JournalArticle'],Computer Science; Engineering
Measuring and identifying factors of individuals' trust in Large Language Models,2025,"Edoardo Sebastiano De Duro, G. A. Veltri, Hudson F. Golino, Massimo Stella","Large Language Models (LLMs) can engage in human-looking conversational exchanges. Although conversations can elicit trust between users and LLMs, scarce empirical research has examined trust formation in human-LLM contexts, beyond LLMs' trustworthiness or human trust in AI in general. Here, we introduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure individuals' trust in LLMs, extending McAllister's cognitive and affective trust dimensions to LLM-human interactions. We developed TILLMI as a psychometric scale, prototyped with a novel protocol we called LLM-simulated validity. The LLM-based scale was then validated in a sample of 1,000 US respondents. Exploratory Factor Analysis identified a two-factor structure. Two items were then removed due to redundancy, yielding a final 6-item scale with a 2-factor structure. Confirmatory Factor Analysis on a separate subsample showed strong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2}>.05$). Convergent validity analysis revealed that trust in LLMs correlated positively with openness to experience, extraversion, and cognitive flexibility, but negatively with neuroticism. Based on these findings, we interpreted TILLMI's factors as""closeness with LLMs""(affective dimension) and""reliance on LLMs""(cognitive dimension). Younger males exhibited higher closeness with- and reliance on LLMs compared to older women. Individuals with no direct experience with LLMs exhibited lower levels of trust compared to LLMs' users. These findings offer a novel empirical foundation for measuring trust in AI-driven verbal communication, informing responsible design, and fostering balanced human-AI collaboration.",10.48550/arXiv.2502.21028,['JournalArticle'],Computer Science; Linguistics; Psychology
Educating Language Models as Promoters: Multi-Aspect Instruction Alignment With Self-Augmentation,2025,"Xueyao Sun, Kaize Shi, Haoran Tang, Dingxian Wang, Guandong Xu, Qing Li","E-commerce content generation necessitates creating engaging and customer-centric material to endorse products and enhance user satisfaction. Existing methods depend on task-specific feature design, which requires a fine-tailored model for each task with complex data collection and pre-processing, and their generation capabilities are limited. Meanwhile, large language models have demonstrated their capabilities in diverse natural language processing tasks, solving multiple tasks in a unified process. To address the concerns in e-commerce content generation, we leverage the impressive generation performance of large language models and propose a framework to educate them as proficient promoters in various e-commerce-related tasks. Our framework involves two modules: self-educating proliferates task instructions and data by instructing the unaligned model, and multi-aspect instruction alignment educates the language model by embedding all e-commerce tasks in a unified framework. The proposed model, Promoter, can perform a batch of prediction and generation tasks, working as a smart and creative promoter that only requires a quick view of the customer profile. Extensive experiments from automatic and human perspectives indicate that Promoter achieves state-of-the-art performances in various generation tasks, bringing the productivity of large language models to e-commerce in an integrated pipeline.",10.1109/TKDE.2025.3569585,['JournalArticle'],Business; Computer Science; Linguistics
ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM,2024,"Rithik Vir, Parsa Madinei","Cooking meals can be difficult, causing many to resort to cookbooks and online recipes. However, relying on these traditional methods of cooking often results in missing ingredients, nutritional hazards, and unsatisfactory meals. Using Augmented Reality (AR) can address these issues; however, current AR cooking applications have poor user interfaces and limited accessibility. This paper proposes a prototype of an iOS application that integrates AR and Computer Vision (CV) into the cooking process. We leverage Google's Gemini Large Language Model (LLM) to identify ingredients in the camera's field of vision and generate recipe choices with detailed nutritional information. Additionally, this application uses Apple's ARKit to create an AR user interface compatible with iOS devices. Users can personalize their meal suggestions by inputting their dietary preferences and rating each meal. The application's effectiveness is evaluated through three rounds of user experience surveys. This application advances the field of accessible cooking assistance technologies, aiming to reduce food wastage and improve the meal planning experience.",10.48550/arXiv.2412.00627,"['JournalArticle', 'Review']",Computer Science
Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models,2025,"Shuzhou Yuan, Ercong Nie, Mario Tawfelis, Helmut Schmid, Hinrich Schutze, Michael Farber","Hate speech detection is a socially sensitive and inherently subjective task, with judgments often varying based on personal traits. While prior work has examined how socio-demographic factors influence annotation, the impact of personality traits on Large Language Models (LLMs) remains largely unexplored. In this paper, we present the first comprehensive study on the role of persona prompts in hate speech classification, focusing on MBTI-based traits. A human annotation survey confirms that MBTI dimensions significantly affect labeling behavior. Extending this to LLMs, we prompt four open-source models with MBTI personas and evaluate their outputs across three hate speech datasets. Our analysis uncovers substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases. These findings highlight the need to carefully define persona prompts in LLM-based annotation workflows, with implications for fairness and alignment with human values.",10.48550/arXiv.2506.08593,"['JournalArticle', 'Review']",Computer Science; Psychology
Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction,2024,"Yu-Hsiang Lin, Huang-Ting Shieh, Chih-Yu Liu, Kuang-Ting Lee, Hsiao-Cheng Chang, Jing-Lun Yang, Yu-Sheng Lin","Extrapolation in Large language models (LLMs) for open-ended inquiry encounters two pivotal issues: (1) hallucination and (2) expensive training costs. These issues present challenges for LLMs in specialized domains and personalized data, requiring truthful responses and low fine-tuning costs. Existing works attempt to tackle the problem by augmenting the input of a smaller language model with information from a knowledge graph (KG). However, they have two limitations: (1) failing to extract relevant information from a large one-hop neighborhood in KG and (2) applying the same augmentation strategy for KGs with different characteristics that may result in low performance. Moreover, open-ended inquiry typically yields multiple responses, further complicating extrapolation. We propose a new task, the extreme multi-label KG link prediction task, to enable a model to perform extrapolation with multiple responses using structured real-world knowledge. Our retriever identifies relevant one-hop neighbors by considering entity, relation, and textual data together. Our experiments demonstrate that (1) KGs with different characteristics require different augmenting strategies, and (2) augmenting the language model's input with textual data improves task performance significantly. By incorporating the retrieval-augmented framework with KG, our framework, with a small parameter size, is able to extrapolate based on a given KG. The code can be obtained on GitHub: https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git",10.48550/arXiv.2405.12656,['JournalArticle'],Computer Science
Can Generative AI Solve Your In-Context Learning Problem? A Martingale Perspective,2024,"Andrew Jesson, Nicolas Beltran-Velez, David M. Blei","This work is about estimating when a conditional generative model (CGM) can solve an in-context learning (ICL) problem. An in-context learning (ICL) problem comprises a CGM, a dataset, and a prediction task. The CGM could be a multi-modal foundation model; the dataset, a collection of patient histories, test results, and recorded diagnoses; and the prediction task to communicate a diagnosis to a new patient. A Bayesian interpretation of ICL assumes that the CGM computes a posterior predictive distribution over an unknown Bayesian model defining a joint distribution over latent explanations and observable data. From this perspective, Bayesian model criticism is a reasonable approach to assess the suitability of a given CGM for an ICL problem. However, such approaches -- like posterior predictive checks (PPCs) -- often assume that we can sample from the likelihood and posterior defined by the Bayesian model, which are not explicitly given for contemporary CGMs. To address this, we show when ancestral sampling from the predictive distribution of a CGM is equivalent to sampling datasets from the posterior predictive of the assumed Bayesian model. Then we develop the generative predictive $p$-value, which enables PPCs and their cousins for contemporary CGMs. The generative predictive $p$-value can then be used in a statistical decision procedure to determine when the model is appropriate for an ICL problem. Our method only requires generating queries and responses from a CGM and evaluating its response log probability. We empirically evaluate our method on synthetic tabular, imaging, and natural language ICL tasks using large language models.",10.48550/arXiv.2412.06033,['JournalArticle'],Computer Science; Mathematics
Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,2024,"Xianzhe Fan, Qing Xiao, Xuhui Zhou, Yuran Su, Zhicong Lu, Maarten Sap, Hong Shen","AI companions based on large language models can role-play and converse very naturally. When value conflicts arise between the AI companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with AI companions, providing design implications for our study. Based on these, we created Minion, a technology probe to help users resolve human-AI value conflicts. Minion applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.",10.48550/arXiv.2411.07042,['JournalArticle'],Computer Science
Exploring Familiarity and Knowledgeability in Conversational Virtual Agents,2025,"F. Yang, Siqi Guo, Christos Mousas","
 In this study, we examined the impact of agent familiarity and knowledgeability on several variables spanning agent perceptions (i.e., perceived knowledge, familiarity, trust, anthropomorphism, uncanny valley effect, and likability), social and emotional experiences (i.e., co-presence, rapport, cognitive process expectations, and willingness for future interaction), and conversation dynamics (i.e., conversation transcript, participants response word count, and response time). We created two virtual agents for the study: a digital replica of a professor from our department (i.e., familiar agent) and an agent with similar demographic variables (i.e., age, gender, and ethnicity) but with a fabricated appearance and voice (i.e., unfamiliar agent). We implemented both agents to exhibit two levels of knowledgeability (i.e., low and high) in the domain of game development and course-specific information. We used large language models (LLMs) to provide the agents with persona information and domain knowledge through prompt engineering. For our user study, we followed a 2 (familiarity: unfamiliar vs. familiar agent)
 
 \(\times\)
 
 2 (knowledgeability: low vs. high knowledgeability) within-group study design and recruited 32 participants who engaged in a five-minute, conversation-based virtual reality (VR) interaction with all four experimental conditions: unfamiliar agent with low knowledgeability (ULK), unfamiliar agent with high knowledgeability (UHK), familiar agent with low knowledgeability (FLK), and familiar agent with high knowledgeability (FHK). The findings demonstrated a significant main effect of agent familiarity on perceived knowledge, suggesting that familiarity plays a crucial role in shaping users perception of the agent's knowledgeability level. Besides perceived knowledge, familiarity also affected all other variables, apart from co-presence. Conversely, agent knowledgeability affected perceived familiarity, trust, anthropomorphism, cognitive process expectations, willingness for future interaction, conversation content, and participants response word count. Finally, we found an interaction effect between agent familiarity and perceived knowledge, indicating that familiarity has a significant influence on users perceptions of the agent's knowledgeability. This study contributes to the field of conversational human-agent interaction in VR by providing empirical evidence on how adapting both familiarity and knowledgeability of virtual agents can significantly enhance user experience, offering valuable insights into designing more engaging, trustworthy, and effective embodied conversational agents.
",10.1145/3757062,['JournalArticle'],Computer Science; Psychology
Human-like conceptual representations emerge from language prediction,2025,"Ningyu Xu, Qi Zhang, Chao Du, Qiang Luo, Xipeng Qiu, Xuanjing Huang, Menghan Zhang","People acquire concepts through rich physical and social experiences and use them to understand the world. In contrast, large language models (LLMs), trained exclusively through next-token prediction over language data, exhibit remarkably human-like behaviors. Are these models developing concepts akin to humans, and if so, how are such concepts represented and organized? To address these questions, we reframed the classic reverse dictionary task to simulate human concept inference in context and investigated the emergence of human-like conceptual representations within LLMs. Our results demonstrate that LLMs can flexibly derive concepts from linguistic descriptions in relation to contextual cues about other concepts. The derived representations converged towards a shared, context-independent structure that effectively predicted human behavior across key psychological phenomena, including computation of similarities, categories and semantic scales. Moreover, these representations aligned well with neural activity patterns in the human brain, even in response to visual rather than linguistic stimuli, providing evidence for biological plausibility. These findings establish that structured, human-like conceptual representations can naturally emerge from language prediction without real-world grounding. More broadly, our work positions LLMs as promising computational tools for understanding complex human cognition and paves the way for better alignment between artificial and human intelligence.",10.48550/arXiv.2501.12547,['JournalArticle'],Computer Science; Linguistics; Psychology
The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations,2025,"S. Fulay, Deb Roy","Deliberation is essential to well-functioning democracies, yet physical, economic, and social barriers often exclude certain groups, reducing representativeness and contributing to issues like group polarization. In this work, we explore the use of large language model (LLM) personas to introduce missing perspectives in policy deliberations. We develop and evaluate a tool that transcribes conversations in real-time and simulates input from relevant but absent stakeholders. We deploy this tool in a 19-person student citizens' assembly on campus sustainability. Participants and facilitators found that the tool sparked new discussions and surfaced valuable perspectives they had not previously considered. However, they also noted that AI-generated responses were sometimes overly general. They raised concerns about overreliance on AI for perspective-taking. Our findings highlight both the promise and potential risks of using LLMs to raise missing points of view in group deliberation settings.",10.48550/arXiv.2503.13812,['JournalArticle'],Computer Science; Political Science
"ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making",2025,"Bharadwaj Ravichandran, David Joy, Paul Elliott, Brian Hu, Jadie Adams, Christopher Funk, Emily Veenhuis, A. Hoogs, Arslan Basharat","Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.",10.48550/arXiv.2507.09037,"['JournalArticle', 'Review']",Computer Science
Event Segmentation Applications in Large Language Model Enabled Automated Recall Assessments,2025,"Ryan A. Panela, Alex J. Barnett, Morgan D. Barense, Bjrn Herrmann","Understanding how individuals perceive and recall information in their natural environments is critical to understanding potential failures in perception (e.g., sensory loss) and memory (e.g., dementia). Event segmentation, the process of identifying distinct events within dynamic environments, is central to how we perceive, encode, and recall experiences. This cognitive process not only influences moment-to-moment comprehension but also shapes event specific memory. Despite the importance of event segmentation and event memory, current research methodologies rely heavily on human judgements for assessing segmentation patterns and recall ability, which are subjective and time-consuming. A few approaches have been introduced to automate event segmentation and recall scoring, but validity with human responses and ease of implementation require further advancements. To address these concerns, we leverage Large Language Models (LLMs) to automate event segmentation and assess recall, employing chat completion and text-embedding models, respectively. We validated these models against human annotations and determined that LLMs can accurately identify event boundaries, and that human event segmentation is more consistent with LLMs than among humans themselves. Using this framework, we advanced an automated approach for recall assessments which revealed semantic similarity between segmented narrative events and participant recall can estimate recall performance. Our findings demonstrate that LLMs can effectively simulate human segmentation patterns and provide recall evaluations that are a scalable alternative to manual scoring. This research opens novel avenues for studying the intersection between perception, memory, and cognitive impairment using methodologies driven by artificial intelligence.",10.48550/arXiv.2502.13349,['JournalArticle'],Computer Science; Psychology
Orchestration for Domain-specific Edge-Cloud Language Models,2025,"Prasoon Patidar, Alex Crown, Kevin Hsieh, Yifei Xu, Tusher Chakraborty, Ranveer Chandra, Yuvraj Agarwal","The remarkable performance of Large Language Models (LLMs) has inspired many applications, which often necessitate edge-cloud collaboration due to connectivity, privacy, and cost considerations. Traditional methods primarily focus on selecting the best LLM model for optimizing performance, while neglecting the critical interplay between the components of the LLM serving pipeline (context retrieval, query preprocessing, etc.) or the changing latency and cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs), a novel system that reframes this problem as a joint optimization challenge and solves it by systematically exploring component configurations and dynamically selecting optimal strategies at the query level. ECO-LLM consists of two components: (1) the ECO-LLM Emulator, which efficiently explores the vast configuration space utilizing query clustering and pareto-optimal path selection, gathering domain-specific performance metrics without exhaustive evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to dynamically select optimal resolution strategies for user queries while meeting user-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart home and a smart car assistant scenarios. With an exhaustive exploration of all possible configurations for seen queries, ECO-LLM outperforms cloud-based models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing costs by 90% and latency by 55%, demonstrating the value of its joint optimization at the query level. In practical deployment for previously unseen queries, ECO-LLM selects configurations that reduce costs by 62% or improve response times by 62% on average compared to state-of-the-art model routing approaches, while maintaining higher accuracy and consistently adhering to specified latency and cost constraints.",10.48550/arXiv.2507.09003,['JournalArticle'],Computer Science
Can LLMs Generate User Stories and Assess Their Quality?,2025,"G. Quattrocchi, Liliana Pasquale, P. Spoletini, L. Baresi","Requirements elicitation is still one of the most challenging activities of the requirements engineering process due to the difficulty requirements analysts face in understanding and translating complex needs into concrete requirements. In addition, specifying high-quality requirements is crucial, as it can directly impact the quality of the software to be developed. Although automated tools allow for assessing the syntactic quality of requirements, evaluating semantic metrics (e.g., language clarity, internal consistency) remains a manual and time-consuming activity. This paper explores how LLMs can help automate requirements elicitation within agile frameworks, where requirements are defined as user stories (US). We used 10 state-of-the-art LLMs to investigate their ability to generate US automatically by emulating customer interviews. We evaluated the quality of US generated by LLMs, comparing it with the quality of US generated by humans (domain experts and students). We also explored whether and how LLMs can be used to automatically evaluate the semantic quality of US. Our results indicate that LLMs can generate US similar to humans in terms of coverage and stylistic quality, but exhibit lower diversity and creativity. Although LLM-generated US are generally comparable in quality to those created by humans, they tend to meet the acceptance quality criteria less frequently, regardless of the scale of the LLM model. Finally, LLMs can reliably assess the semantic quality of US when provided with clear evaluation criteria and have the potential to reduce human effort in large-scale assessments.",10.48550/arXiv.2507.15157,['JournalArticle'],Computer Science
"Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS",2025,"Vignesh Ethiraj, Ashwath David, Sidhanth Menon, Divya Vijay","We introduce a low-latency telecom AI voice agent pipeline for real-time, interactive telecommunications use, enabling advanced voice AI for call center automation, intelligent IVR (Interactive Voice Response), and AI-driven customer support. The solution is built for telecom, combining four specialized models by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language Model (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific Automatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific Text-to-Speech (TTS) model. These models enable highly responsive, domain-adapted voice AI agents supporting knowledge-grounded spoken interactions with low latency. The pipeline integrates streaming ASR (TTE), conversational intelligence (TSLAM), retrieval augmented generation (RAG) over telecom documents, and real-time TTS (T-Synth), setting a new benchmark for telecom voice assistants. To evaluate the system, we built a dataset of 500 human-recorded telecom questions from RFCs, simulating real telecom agent queries. This framework allows analysis of latency, domain relevance, and real-time performance across the stack. Results show that TSLAM, TTE, and T-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise, low-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and T-Synth -- provide a foundation for next-generation telecom AI, enabling automated customer support, diagnostics, and more.",10.48550/arXiv.2508.04721,['JournalArticle'],Computer Science; Engineering
Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task,2025,"Mengyang Qiu, Zoe Brisebois, Siena Sun","Large language models (LLMs) are increasingly explored as substitutes for human participants in cognitive tasks, but their ability to simulate human behavioral variability remains unclear. This study examines whether LLMs can approximate individual differences in the phonemic fluency task, where participants generate words beginning with a target letter. We evaluated 34 model configurations, varying prompt specificity, sampling temperature, and model type, and compared outputs to responses from 106 human participants. While some configurations, especially Claude 3.7 Sonnet, matched human averages and lexical preferences, none reproduced the scope of human variability. LLM outputs were consistently less diverse and structurally rigid, and LLM ensembles failed to increase diversity. Network analyses further revealed fundamental differences in retrieval structure between humans and models. These results highlight key limitations in using LLMs to simulate human cognition and behavior.",10.48550/arXiv.2505.16164,['JournalArticle'],Computer Science; Linguistics; Psychology
Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics,2025,"Haotian Lan, Yao Gao, Yujun Cheng, Wei Yuan, Kun Wang","Social media's rise establishes user-generated content (UGC) as pivotal for travel decisions, yet analytical methods lack scalability. This study introduces a dual-method LLM framework: unsupervised expectation extraction from UGC paired with survey-informed supervised fine-tuning. Findings reveal leisure/social expectations drive engagement more than foundational natural/emotional factors. By establishing LLMs as precision tools for expectation quantification, we advance tourism analytics methodology and propose targeted strategies for experience personalization and social travel promotion. The framework's adaptability extends to consumer behavior research, demonstrating computational social science's transformative potential in marketing optimization.",10.48550/arXiv.2505.16118,"['JournalArticle', 'Review']",Business; Computer Science; Mathematics
Focusing on Needs: A Chatbot-Based Emotion Regulation Tool for Adolescents,2023,"Yeming Ni, Ruyi Ding, Yuqing Chen, Hanchao Hou, Shiguang Ni","Adolescents face much psychological stress in the current social environment, and effective emotional regulation is crucial to their mental health. This article introduces a paradigm of product-oriented psychological dialogue, that is, to study psychological problems first, determine user needs and the most effective way of action, and then develop tools based on this paradigm. We use the above paradigm to build an artificial intelligence-based adolescent emotion adjust the con-versational bot. Specifically, to explore adolescents' emotional regulation needs, this study collected the required data (n=317, 5,543 questionnaires) through the intensive tracking method. It revealed the mechanism of user needs and emotion regulation. Emotion regulation strategy weighting mechanism, and using the collected raw data and existing emotion support dialogue datasets (ESConv), a Chinese adolescent emotion regulation dia-logue dataset was constructed. After that, this paper fine-tunes the existing dialogue model (GPT-2 chitchat). Through these improvements, the dialogue model has dramatically improved its performance and can also provide more personalized and effective emotional regulation support according to the actual needs of adolescents. In summary, this study provides new ideas and methods for mental health support, and promotes the research and development of emotional regulation support for adolescents.",10.1109/SMC53992.2023.10394600,"['JournalArticle', 'Conference']",Computer Science; Psychology
An LLM-driven Transcription Task for Mobile Text Entry Studies,2024,"Andreas Komninos, A. Feit, Luis A Leiva, Florian Lehmann, Ioulia Simou, Dimosthenis Minas, Aggelos Fotopoulos, M. Xenos","We explore a novel transcription task in mobile text entry research, presenting stimuli within LLM-generated conversational contexts to improve participant engagement and phrase memorability. We conducted two studies: an eye-tracking study examining participants attention when presented with conversational contexts alongside stimuli, and an experiment comparing LLM-generated and human-generated prompt-response pairs in transcription tasks, involving both high and low memorability stimuli. Key findings reveal that presenting conversational contexts improves recall for low memorability phrases and results in fewer uncorrected errors during transcription. No significant effects were observed on other basic text entry metrics, or participant subjective appraisals of engagement with the novel task, suggesting it can be used safely as an alternative to the traditional transcription task. We discuss the potential of LLMs in improving text entry evaluation methods, including generating diverse linguistic styles, emotionally loaded contexts, and even simulating entire evaluation processes. Our study highlights the need for systematic approaches to generate and evaluate LLM outputs for research purposes, and for proposing new metrics and evaluation methods.",10.1145/3701571.3701586,"['Book', 'JournalArticle']",Computer Science
"Future Me, a Prospection-Based Chatbot to Promote Mental Well-Being in Youth: Two Exploratory User Experience Studies",2025,"M. Dechant, Eva Lash, Sarah Shokr, C. O'Driscoll","Abstract Background Digital interventions have been proposed as a solution to meet the growing demand for mental health support. Large language models (LLMs) have emerged as a promising technology for creating more personalized and adaptive mental health chatbots. While LLMs generate responses based on statistical patterns in training data rather than through conscious reasoning, they can be designed to support important psychological processes. Prospectionthe ability to envision and plan for future outcomesrepresents a transdiagnostic process altered across various mental health conditions that could be effectively targeted through such interventions. We designed Future Me, an LLM-powered chatbot designed to facilitate future-oriented thinking and promote goal pursuit using evidence-based interventions including visualization, implementation intentions, and values clarification. Objective This study aims to understand how users engage with Future Me, evaluate its effectiveness in supporting future-oriented thinking, and assess its acceptability across different populations, with particular attention to postgraduate students stress management needs. We also seek to identify design improvements that could enhance the chatbots ability to support users mental well-being. Methods In total, 2 complementary studies were conducted. Study 1 (n=20) examined how postgraduate students used Future Me during a single guided session, followed by semistructured interviews. Study 2 (n=14) investigated how postgraduate students interacted with Future Me over a 1-week period, with interviews before and after usage. Both studies analyzed conversation transcripts and interview data using thematic analysis to understand usage patterns, perceived benefits, and limitations. Results Across both studies, participants primarily engaged with Future Me to discuss career or education goals, personal obstacles, and relationship concerns. Users valued Future Mes ability to provide clarity around goal-setting (85% of participants), its nonjudgmental nature, and its 24/7 accessibility (58%). Future Me effectively facilitated self-reflection (80%) and offered new perspectives (70%), particularly for broader future-oriented concerns. However, both studies revealed limitations in the chatbots ability to provide personalized emotional support during high-stress situations, with participants noting that responses sometimes felt formulaic (50%) or lacked emotional depth. Postgraduate students specifically emphasized the need for greater context awareness during periods of academic stress (58%). Overall, 57% of requests occurred outside office hours, dropping from 40 on day 1 to 12 by day 7. Conclusions Future Me demonstrates promise as an accessible tool for promoting prospection skills and supporting mental well-being through future-oriented thinking. However, effectiveness appears context-dependent, with prospection techniques more suitable for broader life decisions than acute stress situations. Future development should focus on creating more adaptive systems that can adjust their approach based on the users emotional state and immediate needs. Rather than attempting to replicate human therapy entirely, chatbots like Future Me may be most effective when designed as complementary tools within broader support ecosystems, offering immediate guidance while facilitating connections to human support when needed.",10.2196/74411,['JournalArticle'],Computer Science; Psychology
"Rank It, Then Ask It: Input Reranking for Maximizing the Performance of LLMs on Symmetric Tasks",2024,"Mohsen Dehghankar, Abolfazl Asudeh","Large language models (LLMs) have quickly emerged as practical and versatile tools that provide new solutions for a wide range of domains. In this paper, we consider the application of LLMs on symmetric tasks where a query is asked on an (unordered) bag of elements. Examples of such tasks include answering aggregate queries on a database table. In general, when the bag contains a large number of elements, LLMs tend to overlook some elements, leading to challenges in generating accurate responses to the query. LLMs receive their inputs as ordered sequences. However, in this problem, we leverage the fact that the symmetric input is not ordered, and reordering should not affect the LLM's response. Observing that LLMs are less likely to miss elements at certain positions of the input, we introduce the problem of LLM input reranking: to find a ranking of the input that maximizes the LLM's accuracy for the given query without making explicit assumptions about the query. Finding the optimal ranking requires identifying (i) the relevance of each input element for answering the query and (ii) the importance of each rank position for the LLM's attention. We develop algorithms for estimating these values efficiently utilizing a helper LLM. We conduct comprehensive experiments on different synthetic and real datasets to validate our proposal and to evaluate the effectiveness of our proposed algorithms. Our experiments confirm that our reranking approach improves the accuracy of the LLMs on symmetric tasks by up to $99\%$ proximity to the optimum upper bound.",10.48550/arXiv.2412.00546,['JournalArticle'],Computer Science
Task-Aware Robotic Grasping by evaluating Quality Diversity Solutions through Foundation Models,2024,"Aurel X. Appius, miland Garrab, Franois Hlnon, Mahdi Khoramshahi, Stphane Doncieux","Task-aware robotic grasping is a challenging problem that requires the integration of semantic understanding and geometric reasoning. This paper proposes a novel framework that leverages Large Language Models (LLMs) and Quality Diversity (QD) algorithms to enable zero-shot task-conditioned grasp synthesis. The framework segments objects into meaningful subparts and labels each subpart semantically, creating structured representations that can be used to prompt an LLM. By coupling semantic and geometric representations of an object's structure, the LLM's knowledge about tasks and which parts to grasp can be applied in the physical world. The QD-generated grasp archive provides a diverse set of grasps, allowing us to select the most suitable grasp based on the task. We evaluated the proposed method on a subset of the YCB dataset with a Franka Emika robot. A consolidated ground truth for task-specific grasp regions is established through a survey. Our work achieves a weighted intersection over union (IoU) of 73.6% in predicting task-conditioned grasp regions in 65 task-object combinations. An end-to-end validation study on a smaller subset further confirms the effectiveness of our approach, with 88% of responses favoring the task-aware grasp over the control group. A binomial test shows that participants significantly prefer the task-aware grasp.",10.48550/arXiv.2411.14917,"['JournalArticle', 'Review']",Computer Science; Engineering
Simulating Macroeconomic Expectations using LLM Agents,2025,"Jianhao Lin, Lexuan Sun, Yixin Yan","We introduce a novel framework for simulating macroeconomic expectation formation using Large Language Model-Empowered Agents (LLM Agents). By constructing thousands of LLM Agents equipped with modules for personal characteristics, prior expectations, and knowledge, we replicate a survey experiment involving households and experts on inflation and unemployment. Our results show that although the expectations and thoughts generated by LLM Agents are more homogeneous than those of human participants, they still effectively capture key heterogeneity across agents and the underlying drivers of expectation formation. Furthermore, a module-ablation exercise highlights the critical role of prior expectations in simulating such heterogeneity. This approach complements traditional survey methods and offers new insights into AI behavioral science in macroeconomic research.",10.48550/arXiv.2505.17648,"['JournalArticle', 'Review']",Computer Science; Economics
"An LLM-Based Framework for Simulating, Classifying, and Correcting Students' Programming Knowledge with the SOLO Taxonomy",2025,"Shan Zhang, Pragati Shuddhodhan Meshram, Priyadharshini Ganapathy Prasad, Maya Israel, Suma Bhat","Novice programmers often face challenges in designing computational artifacts and fixing code errors, which can lead to task abandonment and over-reliance on external support. While research has explored effective meta-cognitive strategies to scaffold novice programmers' learning, it is essential to first understand and assess students' conceptual, procedural, and strategic/conditional programming knowledge at scale. To address this issue, we propose a three-model framework that leverages Large Language Models (LLMs) to simulate, classify, and correct student responses to programming questions based on the SOLO Taxonomy. The SOLO Taxonomy provides a structured approach for categorizing student understanding into four levels: Pre-structural, Uni-structural, Multi-structural, and Relational. Our results showed that GPT-4o achieved high accuracy in generating and classifying responses for the Relational category, with moderate accuracy in the Uni-structural and Pre-structural categories, but struggled with the Multi-structural category. The model successfully corrected responses to the Relational level. Although further refinement is needed, these findings suggest that LLMs hold significant potential for supporting computer science education by assessing programming knowledge and guiding students toward deeper cognitive engagement.",10.1145/3641555.3705125,"['JournalArticle', 'Book', 'Conference']",Computer Science
Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges,2025,"P. Taillandier, Jean-Daniel Zucker, A. Grignard, B. Gaudou, N. Huynh, A. Drogoul","This position paper examines the use of Large Language Models (LLMs) in social simulation, analyzing both their potential and their limitations from a computational social science perspective. The first part reviews recent findings on the ability of LLMs to replicate key aspects of human cognition, including Theory of Mind reasoning and social inference, while also highlighting significant limitations such as cognitive biases, lack of true understanding, and inconsistencies in behavior. The second part surveys emerging applications of LLMs in multi-agent simulation frameworks, focusing on system architectures, scale, and validation strategies. Notable projects such as Generative Agents (Smallville) and AgentSociety are discussed in terms of their design choices, empirical grounding, and methodological innovations. Particular attention is given to the challenges of behavioral fidelity, calibration, and reproducibility in large-scale LLM-driven simulations. The final section distinguishes between contexts where LLMs, like other black-box systems, offer direct value-such as interactive simulations and serious games-and those where their use is more problematic, notably in explanatory or predictive modeling. The paper concludes by advocating for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms (GAMA, Netlogo, etc), enabling modelers to combine the expressive flexibility of language-based reasoning with the transparency and analytical rigor of classical rule-based systems.",10.48550/arXiv.2507.19364,"['JournalArticle', 'Review']",Computer Science
Time-Reversal Provides Unsupervised Feedback to LLMs,2024,"Yerram Varun, Rahul Madhavan, Sravanti Addepalli, A. Suggala, Karthikeyan Shanmugam, Prateek Jain","Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.",10.48550/arXiv.2412.02626,['JournalArticle'],Computer Science
All Stories Are One Story: Emotional Arc Guided Procedural Game Level Generation,2025,"Yunge Wen, Chenliang Huang, Hangyu Zhou, Zhuo Zeng, Chun Ming Louis Po, Julian Togelius, Timothy Merino, Sam Earle","The emotional arc is a universal narrative structure underlying stories across cultures and media -- an idea central to structuralist narratology, often encapsulated in the phrase""all stories are one story.""We present a framework for procedural game narrative generation that incorporates emotional arcs as a structural backbone for both story progression and gameplay dynamics. Leveraging established narratological theories and large-scale empirical analyses, we focus on two core emotional patterns -- Rise and Fall -- to guide the generation of branching story graphs. Each story node is automatically populated with characters, items, and gameplay-relevant attributes (e.g., health, attack), with difficulty adjusted according to the emotional trajectory. Implemented in a prototype action role-playing game (ARPG), our system demonstrates how emotional arcs can be operationalized using large language models (LLMs) and adaptive entity generation. Evaluation through player ratings, interviews, and sentiment analysis shows that emotional arc integration significantly enhances engagement, narrative coherence, and emotional impact. These results highlight the potential of emotionally structured procedural generation for advancing interactive storytelling for games.",10.48550/arXiv.2508.02132,['JournalArticle'],Computer Science
Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models,2024,"Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier","This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We focus our analysis on the beginning of the COVID-19 pandemic that had a strong impact on public opinion and collective emotions. We validate our estimates against representative British survey data and find strong positive and significant correlations for several collective emotions. The estimates obtained are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. We demonstrate the flexibility of our method on questions of public opinion for which no pre-trained classifier is available. Our work extends the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. It enables flexible and new approaches to the longitudinal analysis of social media data.",10.1609/icwsm.v19i1.35801,"['JournalArticle', 'Review']",Computer Science; Political Science
"Utilizing Large Language Models (LLMs) in Data Analysis Pipeline for Digital Phenotyping: Description, Prediction, and Visualization",2024,"Derek Nissen, Tianyang Yu, Reyva Babtista, Yi Shang","Digital phenotyping is the ""moment-by-moment quantification of the individual-level human phenotype in situ using data from personal digital devices,"" according to Onnela and Rauch. Digital phenotyping research has historically contained many entry barriers due to high costs and complexity. However, the growing popularity of personal devices such as mobile phones has enabled researchers to collect participant data with more convenience and lower costs than ever before. This paper presents the Intelligent Phenotype Analysis Suite (IPAS), a new AI-powered tool for streamlined phenotype investigation. This tool provides researchers the ability to generate descriptions, predictions, and visualizations of their digital phenotyping data through a simple, novel chat-bot interface. IPAS combines an array of data science techniques with natural language processing capabilities of large language models to accelerate the data analysis process for researchers. IPAS extracts raw Beiwe data, an intuitive data collection platform which only requires participants to install an application on their mobile phone and permit data collection. Furthermore, we evaluate the accuracy of IPAS by using key LLM performance metrics: Precision, Recall, and F1 Score. While testing, IPAS sometimes struggled to generate code. However, in all test prompts, IPAS correctly identified the pre-written function needed to perform the requested task. Altogether, IPAS improves prior methods by enabling researchers of all levels of experience to analyze digital phenotyping data using natural language queries.",10.1109/BigData62323.2024.10825517,"['JournalArticle', 'Conference']",Computer Science
Zero-Shot Intent Classification Using a Semantic Similarity Aware Contrastive Loss and Large Language Model,2024,"Jaejin Cho, R. S. Srinivasa, Ching-Hua Lee, Yashas Malur Saidutta, Chouchang Yang, Yilin Shen, Hongxia Jin","Zero-shot systems can reduce the cost of collecting data and training in a new domain since they can work directly with the test data without further training. In this paper, we build zero-shot systems for intent classification, based on Semantic Similarity-aware Contrastive Loss (SSCL) that addresses an issue in the original CL which treats non-corresponding pairs indiscriminately. We confirm that SSCL outperforms CL through experiments. Then, we explore how including text or speech in-domain data during the SSCL training affects the out-of-domain intent classification.During the zero-shot classification, embeddings for a set of classes in the new domain are generated to calculate the similarities between each class embedding and an input utterance embedding, after which the most similar class is predicted for the utterances intent. Although manually-collected text sentences per class can be used to generate the class embedding, the data collection can be costly. Thus, we explore how to generate better class embeddings without human-collected text data in the target domain. The best proposed method employing an instruction-tuned Llama2, a public large language model, shows the performance comparable to the case where the human-collected text data was used, implying the importance of accurate class embedding generation.",10.1109/ICASSP48485.2024.10446276,"['JournalArticle', 'Conference']",Computer Science
Representing data in words,2025,"Amandine M. Caut, Amy Rouillard, Beimnet Zenebe, Matthias Green, gst Plmason Morthens, David J T Sumpter","An important part of data science is the use of visualisations to display data in a way that is easy to digest. Visualisations often rely on underlying statistical or machine learning models -- ranging from basic calculations like category means to advanced methods such as principal component analysis of multidimensional datasets -- to convey insights. We introduce an analogous concept for word descriptions of data, which we call wordalisations. Wordalisations describe data in easy to digest words, without necessarily reporting numerical values from the data. We show how to create wordalisations using large language models, through prompt templates engineered according to a task-agnostic structure which can be used to automatically generate prompts from data. We show how to produce reliable and engaging texts on three application areas: scouting football players, personality tests, and international survey data. Using the model cards framework, we emphasise the importance of clearly stating the model we are imposing on the data when creating the wordalisation, detailing how numerical values are translated into words, incorporating background information into prompts for the large language model, and documenting the limitations of the wordalisations. We argue that our model cards approach is a more appropriate framework for setting best practices in wordalisation of data than performance tests on benchmark datasets.",10.48550/arXiv.2503.15509,"['JournalArticle', 'Review']",Computer Science
Scaling Technology Acceptance Analysis with Large Language Model (LLM) Annotation Systems,2024,"P. Smolinski, Joseph Januszewicz, J. Winiarski","Technology acceptance models effectively predict how users will adopt new technology products. Traditional surveys, often expensive and cumbersome, are commonly used for this assessment. As an alternative to surveys, we explore the use of large language models for annotating online user-generated content, like digital reviews and comments. Our research involved designing an LLM annotation system that transform reviews into structured data based on the Unified Theory of Acceptance and Use of Technology model. We conducted two studies to validate the consistency and accuracy of the annotations. Results showed moderate-to-strong consistency of LLM annotation systems, improving further by lowering the model temperature. LLM annotations achieved close agreement with human expert annotations and outperformed the agreement between experts for UTAUT variables. These results suggest that LLMs can be an effective tool for analyzing user sentiment, offering a practical alternative to traditional survey methods and enabling deeper insights into technology design and adoption.",10.48550/arXiv.2407.00702,"['JournalArticle', 'Review']",Computer Science
Malware Behavior Detection System with RAG-Enhanced eBPF and Advanced Language Model,2024,"Sihang Yu, Zhaoxiang Li, Jinshan Chen, Xuhui Deng, Chen Hou","The article focuses on the malware threat within network security, especially the challenge of behavior detection on Linux systems. Traditional methods have limitations since static analysis is affected by various factors and dynamic analysis can be bypassed. To overcome these challenges, we propose an integrated approach using eBPF and language models. Its primary contributions are implementing malware behavior monitoring with eBPF for high-precision data collection, introducing a few-shot prompt-based log parsing and data structuring method to optimize inference, achieving integrated detection of malicious behavior using eBPF and LLMs to streamline log processing, and proposing an anomaly detection scheme with RAG-enhanced knowledge integration to improve the recognition of unknown behaviors and reduce training costs. Experiments involved identifying mining malware samples and behavior data from Virus Total and MalwareBazaar and executing them in a simulated Linux environment to analyze behavior. The results show that the accuracy reached 98.56%, and the introduction of RAG increased the recognition accuracy for unknown behavior logs by 15%.",10.1109/SGEE64306.2024.10865871,['Conference'],Computer Science
OnRL-RAG: Real-Time Personalized Mental Health Dialogue System,2025,"Ahsan Bilal, Beiyu Lin","Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.",10.48550/arXiv.2504.02894,"['JournalArticle', 'Review']",Computer Science; Psychology
Understanding Online Polarization Through Human-Agent Interaction in a Synthetic LLM-Based Social Network,2025,"Tim Donkers, Jrgen Ziegler","The rise of social media has fundamentally transformed how people engage in public discourse and form opinions. While these platforms offer unprecedented opportunities for democratic engagement, they have been implicated in increasing social polarization and the formation of ideological echo chambers. Previous research has primarily relied on observational studies of social media data or theoretical modeling approaches, leaving a significant gap in our understanding of how individuals respond to and are influenced by polarized online environments. Here we present a novel experimental framework for investigating polarization dynamics that allows human users to interact with LLM-based artificial agents in a controlled social network simulation. Through a user study with 122 participants, we demonstrate that this approach can successfully reproduce key characteristics of polarized online discourse while enabling precise manipulation of environmental factors. Our results provide empirical validation of theoretical predictions about online polarization, showing that polarized environments significantly increase perceived emotionality and group identity salience while reducing expressed uncertainty. These findings extend previous observational and theoretical work by providing causal evidence for how specific features of online environments influence user perceptions and behaviors. More broadly, this research introduces a powerful new methodology for studying social media dynamics, offering researchers unprecedented control over experimental conditions while maintaining ecological validity.",10.1609/icwsm.v19i1.35826,['JournalArticle'],Computer Science; Political Science; Sociology
The Future of Tourism: Examining the Potential Applications of Large Language Models,2024,Shengyu Gu,"Large language models such as the Generative Pre-trained Transformer (GPT) have recently gained attention for their impressive natural language processing capabilities. While their potential to revolutionize various industries is still being explored, the tourism industry stands to benefit significantly from their use. In this study, we conduct an early assessment of the impact potential of GPTs on the tourism industry using a mixed-methods approach.

We first analyze the existing literature on the use of GPTs in the tourism industry and identify several potential applications such as personalized travel recommendations, language translation, and chatbots. We then collect data from various stakeholders in the tourism industry through surveys and interviews to understand their current practices and their willingness to adopt GPT-based solutions.

Our results indicate that while there is a high level of awareness and interest in GPTs among tourism professionals, the adoption of these technologies is currently limited. The main barriers identified include a lack of technical expertise, concerns around data privacy and security, and the high cost of implementing GPT-based solutions. However, those who have adopted GPTs report significant benefits in terms of increased efficiency and improved customer satisfaction.

To further explore the potential of GPTs in the tourism industry, we conduct a pilot study to develop a GPT-based travel recommendation system. The system uses GPT to generate personalized travel itineraries based on user preferences and feedback. Our evaluation of the system indicates that it performs well in terms of accuracy and user satisfaction, demonstrating the potential for GPTs to provide personalized and tailored experiences to travellers.

Overall, our study provides an early look at the impact potential of GPTs on the tourism industry and identifies several avenues for future research. We recommend that tourism professionals and researchers collaborate to address the current barriers to adoption and explore the full range of applications for GPTs in the industry.
",10.32388/uyruwt,"['JournalArticle', 'Review']",Business; Computer Science
Intelligent FAQ Chatbot: A User-Centric Approach using Large Language Models,2025,"Sai Jyothi B., Vyshnavi U., Kavya Vasanthi Y., Sai Deepika R., Ramya Sindhu G.","The use of increasingly automated systems for tasks like customer support and information retrieval is considerably changing industries such as education, healthcare, and e-commerce. Typical FAQ chatbots often rely on rule-based or keyword-matching algorithms, which limit their ability to address complex questions, adapt to personalized contexts, and learn from interactions. This undertaking introduces a novel chatbot for frequently asked questions, which uses advanced LLMs to tackle these issues. Gemini is an advanced LLM, and the LangChain allows for advanced context management. This system improves comprehension of user intent and also generates natural, contextually relevant responses, progressing beyond static, database-dependent answers. It uses user-specific data, such as interaction history and preferences, to customize different interactions and generally increase user satisfaction. This framework directly addresses scalability challenges as well as delivers remarkably smart, user-centric automation. It does so for contemporary customer support along with information retrieval applications through thoughtfully combining contextual understanding, in addition to personalization. This approach illustrates the transition from prescriptive, rule-driven systems to more comprehensive, learning-oriented driving systems. It enhances the systems effectiveness and user experience while encouraging the growth of advanced, user-friendly automation help systems.",10.36548/jaicn.2025.1.006,['JournalArticle'],Computer Science
MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral Mental Health Question Answer,2025,"Qi Chen, Dexi Liu","The Mental Health Question Answer (MHQA) task requires the seeker and supporter to complete the support process in one-turn dialogue. Given the richness of help-seeker posts, supporters must thoroughly understand the content and provide logical, comprehensive, and well-structured responses. Previous works in MHQA mostly focus on single-agent approaches based on the cognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the interactions among various CBT elements, such as emotion and cognition. This limitation hinders the models' ability to thoroughly understand the distress of help-seekers. To address this, we propose a framework named Multi-Agent Deductive Planning (MADP), which is based on the interactions between the various psychological elements of CBT. This method guides Large Language Models (LLMs) to achieve a deeper understanding of the seeker's context and provide more personalized assistance based on individual circumstances. Furthermore, we construct a new dataset based on the MADP framework and use it to fine-tune LLMs, resulting in a specialized model named MADP-LLM. We conduct extensive experiments, including comparisons with multiple LLMs, human evaluations, and automatic evaluations, to validate the effectiveness of the MADP framework and MADP-LLM.",10.48550/arXiv.2501.15826,['JournalArticle'],Computer Science; Psychology
RumorSphere: A Framework for Million-scale Agent-based Dynamic Simulation of Rumor Propagation,2025,"Yijun Liu, Wu Liu, Xiaoyan Gu, Weiping Wang, Jiebo Luo, Yongdong Zhang","Rumor propagation modeling is critical for understanding the dynamics of misinformation spread. Previous models are either overly simplistic or static, making them ineffective for simulating real-world rumor dynamics. In this paper, leveraging the impressive human behavior imitation capabilities of large language models (LLMs), we present a novel dynamic and hierarchical social network simulation framework, which supports simulations with millions of agents. This simulator is used to explore the rumor dynamic in the real world. Experiments on real-world rumor propagation datasets reveal a strong alignment between simulated and real-world rumor dynamics, outperforming existing models with an average 64\% reduction in opinion bias. Our findings underscore the substantial potential of LLM-based multi-agent systems in social network simulations, offering critical insights for advancing social science research. Furthermore, our analysis reveals that the tightly connected local community structure within social networks is one of the key factors promoting the rapid spread of rumors. In these communities, as rumors propagate to a certain extent, some individuals, influenced by''social pressure'', are often compelled to conform, while holders of minority opinions are further silenced, resulting in a vicious cycle that accelerates rumor dissemination. Through counterfactual experiments, we evaluate various intervention strategies and demonstrate that early and sustained efforts to correct misinformation are more effective in mitigating the spread of rumors, while debunking rumors through opinion leaders proves to be the most effective strategy. These findings provide valuable insights for public opinion management and policymaking.",10.48550/arXiv.2509.02172,['JournalArticle'],Computer Science
Quantifying perturbation impacts for large language models,2024,"Paulius Rauba, Qiyao Wei, M. Schaar","We consider the problem of quantifying how an input perturbation impacts the outputs of large language models (LLMs), a fundamental task for model reliability and post-hoc interpretability. A key obstacle in this domain is disentangling the meaningful changes in model responses from the intrinsic stochasticity of LLM outputs. To overcome this, we introduce Distribution-Based Perturbation Analysis (DBPA), a framework that reformulates LLM perturbation analysis as a frequentist hypothesis testing problem. DBPA constructs empirical null and alternative output distributions within a low-dimensional semantic similarity space via Monte Carlo sampling. Comparisons of Monte Carlo estimates in the reduced dimensionality space enables tractable frequentist inference without relying on restrictive distributional assumptions. The framework is model-agnostic, supports the evaluation of arbitrary input perturbations on any black-box LLM, yields interpretable p-values, supports multiple perturbation testing via controlled error rates, and provides scalar effect sizes for any chosen similarity or distance metric. We demonstrate the effectiveness of DBPA in evaluating perturbation impacts, showing its versatility for perturbation analysis.",10.48550/arXiv.2412.00868,['JournalArticle'],Computer Science; Linguistics; Mathematics
HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning,2025,"Zhi Jing, Siyuan Yang, Jicong Ao, Ting Xiao, Yugang Jiang, Chenjia Bai","For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is https://openhumanoidgen.github.io.",10.48550/arXiv.2507.00833,['JournalArticle'],Computer Science; Engineering
Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs,2025,"Jakub Podolak, Rajeev Verma","We study the source of uncertainty in DeepSeek R1-32B by analyzing its self-reported verbal confidence on question answering (QA) tasks. In the default answer-then-confidence setting, the model is regularly over-confident, whereas semantic entropy - obtained by sampling many responses - remains reliable. We hypothesize that this is because of semantic entropy's larger test-time compute, which lets us explore the model's predictive distribution. We show that granting DeepSeek the budget to explore its distribution by forcing a long chain-of-thought before the final answer greatly improves its verbal score effectiveness, even on simple fact-retrieval questions that normally require no reasoning. Furthermore, a separate reader model that sees only the chain can reconstruct very similar confidences, indicating the verbal score might be merely a statistic of the alternatives surfaced during reasoning. Our analysis concludes that reliable uncertainty estimation requires explicit exploration of the generative space, and self-reported confidence is trustworthy only after such exploration.",10.48550/arXiv.2505.23845,['JournalArticle'],Computer Science
PROPS: Progressively Private Self-alignment of Large Language Models,2025,"N. Teku, Fengwei Tian, Payel Bhattacharjee, Souradip Chakraborty, A. S. Bedi, Ravi Tandon","Alignment is a key step in developing Large Language Models (LLMs) using human feedback to ensure adherence to human values and societal norms. Dependence on human feedback raises privacy concerns about how much a labeler's preferences may reveal about their personal values, beliefs, and personality traits. Existing approaches, such as Differentially Private SGD (DP-SGD), provide rigorous privacy guarantees by privatizing gradients during fine-tuning and alignment but can provide more privacy than necessary as human preferences are tied only to labels of (prompt, response) pairs and can degrade model utility. This work focuses on LLM alignment with preference-level privacy, which preserves the privacy of preference labels provided by humans. We propose PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving alignment framework where privately aligned models in previous stages can serve as labelers for supplementing training data in the subsequent stages of alignment. We present theoretical guarantees for PROPS as well as comprehensive validation using multiple models (Pythia and GPT) and datasets (AlpacaEval, Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over existing methods while still providing high privacy. For the same privacy budget, alignment via PROPS can achieve up to 3x higher win-rates compared to DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based alignment.",10.48550/arXiv.2508.06783,['JournalArticle'],Computer Science; Mathematics
Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin,2024,"Carolina Fortuna, Vid Hanel, Bla Bertalani","Domain specific digital twins, representing a digital replica of various segments of the smart grid, are foreseen as able to model, simulate, and control the respective segments. At the same time, knowledge-based digital twins, coupled with AI, may also empower humans to understand aspects of the system through natural language interaction in view of planning and policy making. This paper is the first to assess and report on the potential of Retrieval Augmented Generation (RAG) question answers related to household electrical energy measurement aspects leveraging a knowledge-based energy digital twin. Relying on the recently published electricity consumption knowledge graph that actually represents a knowledge-based digital twin, we study the capabilities of ChatGPT, Gemini and Llama in answering electricity related questions. Furthermore, we compare the answers with the ones generated through a RAG techniques that leverages an existing electricity knowledge-based digital twin. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.",10.1109/SmartGridComm60555.2024.10738062,"['JournalArticle', 'Conference']",Computer Science; Engineering; Environmental Science
Implementation of Artificial Neural Network in Predicting CPO Prices Using Backpropagation,2024,"Muhmmad Rofi Halmi, Raissa Amanda Putri","This research examines the development of a forecasting model for Crude Palm Oil (CPO) prices using artificial neural network algorithms, particularly the backpropagation algorithm. CPO, as Indonesia's main export commodity, has significant economic impacts and affects the income of oil palm farmers. Data on CPO prices taken from CIF Rotterdam from January 2019 to December 2023 were used in this study. The research method involved several stages, including data collection, pre-processing, model design, and model implementation using Python programming. The results of training the model using the backpropagation algorithm showed an error value of 0.537829578 after 1000 epochs, while evaluation using Mean Squared Error (MSE) showed an MSE value of 0.022709 during the training process and 0.017604 during the testing process. The model also produced predictions for CPO prices in the next three months: 932.578 for the first month, 949.568 for the second month, and 774.855 for the third month. These findings indicate that the developed model can predict future CPO prices with adequate accuracy, which can assist companies in making better financial decisions and managing risks associated with CPO price fluctuations.",10.25139/inform.v9i2.8434,['JournalArticle'],Computer Science; Economics
WIP: Engineering Class Students' Epistemic Cognition when Interacting with Generative AI,2024,"R. Y. Chan, Cecilia Ka Yuk Chan, M. S. Jong, Zihao Hu, Yuming Zhang","This work in progress belongs to the innovative practice category. Nowadays, generative AI (also known as GenAI) can produce novel data samples that closely resemble authentic datasets. The advent of large language models (LLMs), in particular, has caused a huge interest in utilizing GenAI within and beyond the realm of higher education. However, little about engineering students' views and behaviours related to knowing and knowledge when using GenAI, such as ChatGPT, is known. In this WIP, we have engaged a class of N = 37 engineering students taking a postgraduate course titled Social Media Analytics. They were required to write essays related to their course learning in the form of blog posts. They were required to use LLM tools, such as ChatGPT, to assist their writing processes. Their GenAI usage was guided by the cognitive-agent approach, Search Tree, Analyze and Repair, and Selection (STARS), while STARS was proposed by Kirk et al. in AAAI 2024 to extend and complement prompt engineering. In addition, the participants were invited to fill in the Epistemic Cognition Inventory (ECI) questionnaire to associate five aspects of epistemic cognition (EC) with their writing experience. It is confirmed in our results that students' EC, i.e., their beliefs related to knowledge and knowing, significantly predict their prompting engagement and academic performance. However, students' academic performance is found to be significantly and negatively associated with their preference for GenAI usage. Here, we have uncovered engineering students' EC when interacting with generative AI, an area where little has been known so far. Our findings also suggest that proper use of GenAI prompting might promote engineering students' EC and, therefore, engineering learning.",10.1109/FIE61694.2024.10893501,"['JournalArticle', 'Conference']",Computer Science; Engineering
AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models,2025,"Yuan Zhang, Chunkai Fan, Tao Huang, Ming Lu, Sicheng Yu, Junwen Pan, Kuan Cheng, Qi She, Shanghang Zhang","Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuristic visual prompts, such as overlaying a text-query-guided attention heatmap on the original input image. However, designing effective prompts manually is challenging and time-consuming, and it often fails to explore the benefits of different visual prompts, leading to sub-optimal performance. To this end, we propose \textbf{AutoV} that learns to automatically select the optimal visual prompt from various candidates based on given textual queries and the input image. To train AutoV, we developed an automatic data collection and labeling pipeline that evaluates various visual prompts with a pre-trained LVLM. We input a set of visual prompts into the LVLM and rank them according to the prediction losses generated by the model. Using the ranking as a supervision signal, we train AutoV to automatically choose the optimal visual prompt from various visual prompts for LVLMs. Experimental results indicate that AutoV enhances the performance of various LVLMs across multiple popular image understanding tasks. For instance, LLaVA-OV with AutoV achieves $\textbf{1.7}\%$ accuracy gain on LLaVA$^{\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\textbf{1.9}\%$ on MMMU, highlighting its potential as an optimal visual prompting method for LVLMs.",10.48550/arXiv.2506.16112,['JournalArticle'],Computer Science
Study on the Generation of Characteristic Tourism English Spoken Dialogues Based on the Large Language Model,2023,Huan Feng,"The English spoken dialogue generation system was studied in this research for characteristic tourism based on the large language model (LLM). To construct a corpus of characteristic tourism, the natural language processing (NLP) and sentiment analysis were used with multimodal fusion and multi-language supports. A English spoken dialogue system was designed to simulate a real tourism scenario, in which the user interacted with the system by asking questions and expressing interests, and the system generated personalized and practical spoken English responses to enhance the tourism experience during the travel process. The developed system was applied in Heilongjiang which has the characteristic tourism scenes to verify its effectiveness in providing some relevant information and guiding tourists.",10.1109/SSIM59263.2023.10468946,['Conference'],Computer Science; Linguistics
Recent Advances in Transformer and Large Language Models for UAV Applications,2025,"Hamza Kheddar, Yassine Habchi, Mohamed Chahine Ghanem, Mustapha Hemis, Dusit Niyato","The rapid advancement of Transformer-based models has reshaped the landscape of uncrewed aerial vehicle (UAV) systems by enhancing perception, decision-making, and autonomy. This review paper systematically categorizes and evaluates recent developments in Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and large language models (LLMs). Unlike previous surveys, this work presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications such as precision agriculture and autonomous navigation, and provides comparative analyses through structured tables and performance benchmarks. The paper also reviews key datasets, simulators, and evaluation metrics used in the field. Furthermore, it identifies existing gaps in the literature, outlines critical challenges in computational efficiency and real-time deployment, and offers future research directions. This comprehensive synthesis aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies.",10.48550/arXiv.2508.11834,"['JournalArticle', 'Review']",Computer Science; Engineering
AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data,2025,"Vu Dinh Xuan, Hao Vo, David Murphy, Hoang D. Nguyen","The scarcity of data depicting dangerous situations presents a major obstacle to training AI systems for safety-critical applications, such as construction safety, where ethical and logistical barriers hinder real-world data collection. This creates an urgent need for an end-to-end framework to generate synthetic data that can bridge this gap. While existing methods can produce synthetic scenes, they often lack the semantic depth required for scene simulations, limiting their effectiveness. To address this, we propose a novel multi-agent framework that employs an iterative, in-the-loop collaboration between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce semantic consistency and safety-specific constraints, and an Editor Agent, which generates and refines scenes based on this guidance. Powered by LLM's capabilities to reasoning and common-sense knowledge, this collaborative design produces synthetic images tailored to safety-critical scenarios. Our experiments suggest this design can generate useful scenes based on realistic specifications that address the shortcomings of prior approaches, balancing safety requirements with visual semantics. This iterative process holds promise for delivering robust, aesthetically sound simulations, offering a potential solution to the data scarcity challenge in multimedia safety applications.",10.48550/arXiv.2505.13466,['JournalArticle'],Computer Science; Engineering; Environmental Science
InstCache: A Predictive Cache for LLM Serving,2024,"Longwei Zou, Tingfeng Liu, Kai Chen, Jiangang Kong, Yangdong Deng","The revolutionary capabilities of Large Language Models (LLMs) are attracting rapidly growing popularity and leading to soaring user requests to inference serving systems. Caching techniques, which leverage data reuse to reduce computation, offer opportunities to optimize the performance of LLM inference engines. On the one hand, the low-level key-value (KV) cache working at the token level is widely adopted, albeit it incurs significant overhead as request volume grows. On the other hand, instruction-level caching, which stores full instruction-response pairs, is expected to play an increasingly crucial role. However, the high variability in the content and length of instructions make it rare for identical instructions to recur within a short time window, presenting challenges for effective caching instruction-response pairs. To address this challenge, we propose InstCache, a predictive caching mechanism for LLM serving systems. Leveraging the capability of LLMs, we can effectively reorder the representation space of instruction texts and develop a sufficient level of spatial locality. Such spatial locality enables us to predict potential instructions located in a compact region in the space, resulting in an effective caching system at runtime. Experimental results demonstrate that InstCache achieves a 2.3x higher hit rate compared to the upper bound of traditional caching mechanisms on WildChat dataset and reduces the time per output token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets, respectively.",10.48550/arXiv.2411.13820,['JournalArticle'],Computer Science
FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models,2024,"Jiaqi Wu, Simin Chen, Yuzhe Yang, Yijiang Li, Shiyue Hou, Rui Jing, Zehua Wang, Wei Chen, Zijian Tian","In recent years, large language models (LLMs) have significantly advanced the field of natural language processing (NLP). By fine-tuning LLMs with data from specific scenarios, these foundation models can better adapt to various downstream tasks. However, the fine-tuning process poses privacy leakage risks, particularly in centralized data processing scenarios. To address user privacy concerns, federated learning (FL) has been introduced to mitigate the risks associated with centralized data collection from multiple sources. Nevertheless, the privacy of LLMs themselves is equally critical, as potential malicious attacks challenge their security, an issue that has received limited attention in current research. Consequently, establishing a trusted multi-party model fine-tuning environment is essential. Additionally, the local deployment of large LLMs incurs significant storage costs and high computational demands. To address these challenges, we propose for the first time a federated discrete and transferable prompt tuning, namely FedDTPT, for black-box large language models. In the client optimization phase, we adopt a token-level discrete prompt optimization method that leverages a feedback loop based on prediction accuracy to drive gradient-free prompt optimization through the MLM API. For server optimization, we employ an attention mechanism based on semantic similarity to filter all local prompt tokens, along with an embedding distance elbow detection and DBSCAN clustering strategy to enhance the filtering process. Experimental results demonstrate that, compared to state-of-the-art methods, our approach achieves higher accuracy, reduced communication overhead, and robustness to non-iid data in a black-box setting. Moreover, the optimized prompts are transferable.",10.48550/arXiv.2411.00985,['JournalArticle'],Computer Science
Statistical Knowledge Assessment for Generative Language Models,2023,"Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Zhifang Sui, Lei Li",,10.48550/arXiv.2305.10519,['JournalArticle'],Computer Science
Mobile semiphysical experiment platform using ultracompact realtime simulator Pocket Bench for power electronics courses,2022,"Lei Shen, Xiuling Bian","The potential electrical hazard, the unreal experience, and the timespace inflexibility of the existing practical learning equipment are the barriers to the development of power electronics courses. To address these, this article introduces a mobile semiphysical experiment platform using Pocket Bench. The Pocket Bench is an ultracompact HardwareIntheLoop realtime simulator, which can emulate the power electronic circuit with a 1.25s time step, and provide realtime response signals to the physical controller. As the dangerous electrical power flow in the power electronic circuit is emulated in realtime, the electrical hazards are eliminated. In practical learning, the students can implement the semiphysical experiment system quickly by connecting the physical controller to the simulator. With the physical controller reserved, the students can get handson experiences with the real hardware. More importantly, the palmsize, lightweight simulator can be put into the pocket, and it can be directly powered by the universal serial bus port. Therefore, the semiphysical experiment platform using Pocket Bench can be deployed with high timespace flexibility. The distinctive features and the deployment method for the platform are illustrated. The application cases regarding the direct current (DC)DC converter, rectifier, and inverter are also given. A student survey concerning the feasibility, fidelity, and flexibility of the simulator was also conducted. The results show that the Pocket Bench grants the students new capabilities to safely carry out the practical learning of power electronics anytime, anywhere, with highfidelity sensory experience.",10.1002/cae.22563,"['JournalArticle', 'Review']",Computer Science; Engineering
Free Lunch for User Experience: Crowdsourcing Agents for Scalable User Studies,2025,"Siyang Liu, Sahand Sabour, Xiaoyang Wang, Rada Mihalcea","We demonstrate the potential of anthropomorphized language agents to generate budget-friendly, moderate-fidelity, yet sufficiently insightful user experiences at scale, supporting fast, early-stage prototyping. We explore this through the case of prototyping Large Language Model-driven non-player characters (NPCs). We present Agentic H-CI, a framework that mirrors traditional user research processes-surveying, screening, experiencing, and collecting feedback and insights-with simulated agents. Using this approach, we easily construct a team of 240 player agents with a balanced range of player types and personality traits, at extremely low cost (\$0.28/player) and minimal time commitment (6.9 minutes/player). Content analysis shows that agent-based players behave in ways aligned with their simulated backgrounds, achieving 82.5\% alignment with designated profiles. From their interactions, we distill 11 user insights and 6 design implications to guide further development. To evaluate practical value, we conduct parallel user studies with human participants recruited locally and via crowdsourcing. Ratings from three professional game developers show that the agentic player team offers a Pareto-optimal and well-balanced trade-off across fidelity, cost, time efficiency, and insight helpfulness.",10.48550/arXiv.2505.22981,"['JournalArticle', 'Review']",Computer Science
Exploring Designer-Generative AI Collaborative Personas: A Case Study on ChatGPT,2024,"Hsiao Chin Huang, Hong Lei, Chun-Cheng Hsu","Personas can assist designers in understanding user needs during the service design process. However, novice designers often encounter issues such as subjective assumptions and key information omissions when creating personas. This article aims to utilize AI tools to overcome the problems brought by traditional models. The study explores how novice designers use AI to co-create personas and what effects and impacts AI has during the co-creation process. We conducted workshops in both traditional and AI co-creation formats, collecting feedback and viewpoints through designer think-aloud protocols and focus group interviews, and used the KJ method to summarize and categorize the data. The study found that GPTs lack of counter-arguments to the designers viewpoints provided a discussion environment with less social pressure for designers, but also resulted in an inability to engage in deeper communication. GPT, through extensive background data analysis, can help designers familiarize themselves with character settings, but sometimes the responses are excessively lengthy, making it difficult to grasp the key points, which can lead to designers feeling irritable. Based on the above findings, collaborative suggestions and improvement directions for designers and generative AI tools in creating personas are proposed.",10.1145/3678698.3687172,"['JournalArticle', 'Book']",Computer Science
"Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges",2025,"Changyuan Zhao, Guangyuan Liu, Ruichen Zhang, Yinqiu Liu, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Zan Li, Xuemin Shen, Zhu Han, Sumei Sun, Chau Yuen, Dong In Kim","Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems.",10.48550/arXiv.2508.09561,"['JournalArticle', 'Review']",Computer Science; Engineering
Human Simulacra: A Step toward the Personification of Large Language Models,2024,"Qiuejie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Yuejie Zhang, Rui Feng, Shang Gao",,10.48550/arXiv.2402.18180,['JournalArticle'],Computer Science; Linguistics; Psychology
Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting,2025,"Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Lus Frazo, Nuno Costa, Antnio Pereira","This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.",10.48550/arXiv.2505.20521,['JournalArticle'],Computer Science
Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators,2025,"Sungjib Lim, Woojung Song, Eun-Ju Lee, Yohan Jo","As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs simulate human survey responses. We publicly release our dataset and code to support future work.",10.48550/arXiv.2507.05890,"['JournalArticle', 'Review']",Computer Science; Psychology
Integrating Graph Neural Networks and Large Language Models for Stance Detection via Heterogeneous Stance Networks,2025,"Xinyi Chen, Bo Liu, Huaping Hu, Yiqing Cai, Mengmeng Guo, Xingkong Ma","Stance detection, the task of identifying the stance expressed in a text toward a specific target, is essential for analyzing public opinion across diverse domains. The existing approaches primarily focus on modeling the semantic relationship between the text and target, but they often struggle when the target is implicit or indirectly referenced. In real-world scenarios, stance is frequently conveyed through references to related entities, events, or contextual implications, making stance detection particularly challenging. To tackle this challenge, we propose a novel framework that leverages large language models to construct a heterogeneous stance network from textual data. Based on this network, we develop two complementary methodologies tailored for distinct application scenarios: (1) In a supervised setting, we employ a graph neural network approach to learn stance representations from the heterogeneous stance network, enhancing stance prediction performance. (2) For zero-shot stance detection, we introduce an LLM-based method that leverages the heterogeneous stance network to infer stance without task-specific supervision. The experimental results on benchmark datasets demonstrate that our methods outperform the existing approaches, highlighting their effectiveness in both supervised and zero-shot scenarios.",10.3390/app15115809,['JournalArticle'],Computer Science
AI Trust Framework and Maturity Model: Improving Metrics for Evaluating Security & Trust in Autonomous Human Machine Teams & Systems,2023,"Michael Mylrea, Nikki Robinson","The following article develops an AI Trust Framework and Maturity Model (AI-TFMM) to improve trust in AI technologies used by Autonomous Human Machine Teams & Systems (A-HMT-S). The framework establishes a methodology to improve quantification of trust in AI technologies. Key areas of exploration include security, privacy, explainability, transparency and other requirements for AI technologies to be ethical in their development and application. A maturity model framework approach to measuring trust is applied to improve gaps in quantifying trust and associated metrics of evaluation. Finding the right balance between performance, governance and ethics also raises several critical questions on AI technology and trust. Research examines methods needed to develop an AI-TFMM. Validation tests of the framework are run and analyzed against the popular AI technology (Chat GPT). OpenAI's GPT, which stands for ""Generative Pre-training Transformer,"" is a deep learning language model that can generate human-like text by predicting the next word in a sequence based on a given prompt. ChatGPT is a version of GPT that is tailored for conversation and dialogue, and it has been trained on a dataset of human conversations to generate responses that are coherent and relevant to the context. The article concludes with results and conclusions from testing the AI Trust Framework and Maturity Model (AI-TFMM) applied to AI technology. Based on these findings, this paper highlights gaps that could be filled with future research to improve the accuracy, efficacy, application, and methodology of the AI-TFMM.",10.54941/ahfe1003760,['JournalArticle'],Computer Science; Engineering
Agentic AI for Cloud Troubleshooting: A Review of Multi Agent System for Automated Cloud Support,2025,"Kinjal A Patel, Eshan Chandra Pandey, Inshu Misra, Deepti Surve","One of the growing approaches in the field of artificial intelligence is known as agentic AI. This term describes autonomous systems that are meant to pursue complicated goals with minimum interaction from humans. Agentic artificial intelligence displays flexibility, advanced decision-making capabilities, and self-sufficiency, which enables it to work dynamically in contexts that are constantly changing. This is in contrast to traditional artificial intelligence, which is dependent on inflexible instructions and tight oversight. An important step forward in artificial intelligence and contemporary software systems is represented by the development of agentic systems. This development is being pushed by the desire for vertical intelligence that is adapted to a variety of different sectors. Through their capacity for learning, flexibility, and interaction with dynamic settings, these systems improve the results of corporate operations. Large Language Model (LLM) agents, who constitute the cognitive backbone of modern intelligent systems, are at the vanguard of this transformation. They are the agents that are revolutionizing intelligent systems. The aim of this research is to create domain-specific agents to address cloud and SaaS troubleshooting concerns. A particular agent will be created for a designated cloud platform. Manage Personally Identifiable Information to hide data and improve user privacy. This review aims to discuss Agentic AI, its core components, and applications across industries. It also surveys the literature, explores solutions provided by Agentic AI for challenges related to cloud platform failures, examines LLMs as agentic workflows, analyzes the accuracy issues of large language models (LLMs), and presents the proposed methodology along with associated challenges.",10.1109/ICICT64420.2025.11005005,"['Conference', 'Review']",Computer Science
ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media,2023,"Kung-Hsiang Huang, Hou Pong Chan, K. McKeown, Heng Ji","Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts and identify manipulated or inserted information. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective basic model that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and the domain and factuality of news articles, as well as revealing that manipulated sentences are more likely to encapsulate the main story or consequences of a news outlet.",10.48550/arXiv.2305.14225,"['JournalArticle', 'Conference']",Computer Science
AdQuestA: Knowledge-Guided Visual Question Answer Framework for Advertisements,2025,"Neha Choudhary, Poonam Goyal, Devashish Siwatch, Atharva Chandak, Harsh Mahajan, Varun Khurana, Yaman Kumar","In the rapidly evolving landscape of digital marketing, effective customer engagement through advertisements is crucial for brands. Thus, computational understanding of ads is pivotal for recommendation, authoring, and customer behaviour simulation. Despite advancements in knowledge-guided visual-question-answering (VQA) models, existing frameworks often lack domain-specific responses and suffer from a dearth of benchmark datasets for advertisements. To address this gap, we introduce ADVQA, the first dataset for ad-related VQA sourced from Facebook and X (twitter), which facilitates further research in ad comprehension. It comprises open-ended questions and detailed context obtained automatically from web articles. Moreover, we present AdQuestA, a novel multimodal framework for knowledge-guided open-ended question-answering tailored to advertisements. AdQuestA leverages a Retrieval Augmented Generation (RAG) to obtain question-aware ad context as explicit knowledge and image-grounded implicit knowledge, effectively exploiting inherent relationships for reasoning. Extensive experiments corroborate its efficacy, yielding state-of-the-art performance on the AD-VQA dataset, even surpassing 10X larger models such as GPT-4 on this task. Our framework not only enhances understanding of ad content but also advances the broader landscape of knowledge-guided VQA models.",10.1109/WACV61041.2025.00567,"['JournalArticle', 'Conference']",Business; Computer Science
Application of Startup Success Prediction Models and Business Document Extraction Using Large Language Models to Enhance Due Diligence Efficiency,2024,"Vito Christian Samudra, Dicky Prima Satya","Startups face extreme uncertainty and high failure rates, posing challenges for investors in identifying promising ventures. This research, based on a case study and interviews at a prominent Indonesian corporate venture capital firm, explores the due diligence process, typically taking 46 weeks depending on data completeness. Using Large Language Model (LLM) and Machine Learning (ML) technologies developed with the Team Data Science Process (TDSP) methodology, the research aims to enhance due diligence efficiency. Key development steps include data integration, ML model creation for startup success classification, and the integration of OpenAI's GPT -4 and Google Search APIs for comprehensive business analysis. The system's dashboard offers features such as pitch deck, financial, market trends, competitor, and founding team analyses, along with startup success prediction using the XGBoost model. This model, deployed via Flask, demonstrated consistent results through cross-validation. Customer acceptance testing, conducted with eight experienced startup investors, yielded a high satisfaction rate of 4.50 out of 5.00, indicating strong approval of the system's effectiveness.",10.1109/ICAICTA63815.2024.10763359,['Conference'],Business; Computer Science
Who Gets the Callback? Generative AI and Gender Bias,2025,"Sugat Chaturvedi, Rochana Chaturvedi","Generative artificial intelligence (AI), particularly large language models (LLMs), is being rapidly deployed in recruitment and for candidate shortlisting. We audit several mid-sized open-source LLMs for gender bias using a dataset of 332,044 real-world online job postings. For each posting, we prompt the model to recommend whether an equally qualified male or female candidate should receive an interview callback. We find that most models tend to favor men, especially for higher-wage roles. Mapping job descriptions to the Standard Occupational Classification system, we find lower callback rates for women in male-dominated occupations and higher rates in female-associated ones, indicating occupational segregation. A comprehensive analysis of linguistic features in job ads reveals strong alignment of model recommendations with traditional gender stereotypes. To examine the role of recruiter identity, we steer model behavior by infusing Big Five personality traits and simulating the perspectives of historical figures. We find that less agreeable personas reduce stereotyping, consistent with an agreeableness bias in LLMs. Our findings highlight how AI-driven hiring may perpetuate biases in the labor market and have implications for fairness and diversity within firms.",10.48550/arXiv.2504.21400,['JournalArticle'],Computer Science; Economics; Sociology
Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics,2025,"J. Kruijssen, Nicholas Emmons","Artificial intelligence (AI) systems powered by large language models have become increasingly prevalent in modern society, enabling a wide range of applications through natural language interaction. As AI agents proliferate in our daily lives, their generic and uniform expressiveness presents a significant limitation to their appeal and adoption. Personality expression represents a key prerequisite for creating more human-like and distinctive AI systems. We show that AI models can express deterministic and consistent personalities when instructed using established psychological frameworks, with varying degrees of accuracy depending on model capabilities. We find that more advanced models like GPT-4o and o1 demonstrate the highest accuracy in expressing specified personalities across both Big Five and Myers-Briggs assessments, and further analysis suggests that personality expression emerges from a combination of intelligence and reasoning capabilities. Our results reveal that personality expression operates through holistic reasoning rather than question-by-question optimization, with response-scale metrics showing higher variance than test-scale metrics. Furthermore, we find that model fine-tuning affects communication style independently of personality expression accuracy. These findings establish a foundation for creating AI agents with diverse and consistent personalities, which could significantly enhance human-AI interaction across applications from education to healthcare, while additionally enabling a broader range of more unique AI agents. The ability to quantitatively assess and implement personality expression in AI systems opens new avenues for research into more relatable, trustworthy, and ethically designed AI.",10.70235/allora.0x20015,['JournalArticle'],Computer Science; Psychology
A large language model for predicting T cell receptor-antigen binding specificity,2024,"Xing Fang, Chenpeng Yu, Shiye Tian, Hui Liu","The human immune response depends on the binding of T-cell receptors (TCRs) to antigens (pTCR), which elicits the T cells to eliminate viruses, tumor cells, and other pathogens. The ability of human immunity system responding to unknown viruses and bacteria stems from the TCR diversity. However, this vast diversity poses challenges on the TCR-antigen binding prediction methods. In this study, we propose a Masked Language Model (MLM), referred to as tcrLM, to overcome limitations in model generalization. Specifically, we randomly masked sequence segments and train tcrLM to infer the masked segment, thereby extract expressive feature from TCR sequences. Meanwhile, we introduced virtual adversarial training techniques to enhance the models robustness. We built the largest TCR CDR3 sequence dataset to date (comprising 2,277,773,840 residuals), and pre-trained tcrLM on this dataset. Our extensive experimental results demonstrate that tcrLM achieved AUC values of 0.937 and 0.933 on independent test sets and external validation sets, respectively, which remarkably outperformed four previously published prediction methods. On a large-scale COVID-19 pTCR binding test set, our method outperforms the current state-of-the-art method by at least 8%, highlighting the generalizability of our method. Furthermore, we validated that our approach effectively predicts immunotherapy response and clinical outcomes on a clinical cohorts. These findings clearly indicate that tcrLM exhibits significant potential in predicting antigenic immunogenicity. The source code and datasets used in this study are available at: https://github.com/hliulab/tcrLM.",10.1109/BIBM62325.2024.10822735,"['JournalArticle', 'Conference']",Computer Science
"ASSESSING A FINE-TUNED SCRUM AI AGENT: ACCURACY, UTILITY, AND EXPERT VALIDATION",2025,"Yama Kaya, Recep Suha Selcuk","Purpose: This study aims to address the inaccuracies present in default large language models (LLMs) concerning Scrum and Agile methodologies by developing and evaluating a specialized AI model fine-tuned for these domains.

Theoretical Framework: Grounded in Agile principles and Scrum frameworks, this research integrates systems thinking and pattern languages to enhance the AI model's understanding and application of Scrum practices.

Design/Methodology/Approach: The research commenced with a pre-trained LLM, which underwent targeted fine-tuning using a curated dataset comprising established Scrum patterns, systems thinking principles, and real-world scenarios from authoritative Scrum literature. Advanced prompt engineering techniques, including Chain of Thought (CoT) and Role-Playing, were employed to enhance the model's reasoning and context-specific response generation. The model's performance was assessed through quantitative metrics and qualitative evaluations by Scrum experts.

Findings: The fine-tuned model demonstrated superior performance over general-purpose LLMs in generating accurate, relevant, and actionable responses aligned with empirical Scrum practices. Quantitative analyses revealed significant improvements in accuracy and context relevance, while qualitative feedback from experts highlighted the model's enhanced understanding of nuanced Scrum scenarios.

Research, Practical & Social Implications: This study underscores the potential of domain-specific fine-tuning in mitigating misinformation propagated by general-purpose LLMs. Practically, the developed model serves as a reliable tool for Scrum practitioners seeking guidance, thereby improving Agile implementation outcomes. Socially, the research contributes to the broader discourse on responsible AI deployment, emphasizing the need for contextual awareness in AI applications.

Originality/Value: This research pioneers the fine-tuning of LLMs specifically for Scrum and Agile methodologies, offering a novel solution to the inaccuracies prevalent in default models. The integration of advanced prompt engineering techniques and the emphasis on empirical Scrum practices contribute uniquely to both AI and Agile communities.",10.26668/businessreview/2025.v10i4.5426,['JournalArticle'],Computer Science
Benchmarking LLM Privacy Recognition for Social Robot Decision Making,2025,"Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz","While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users'privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.",10.48550/arXiv.2507.16124,"['JournalArticle', 'Review']",Computer Science
"The Impact of Revealing Large Language Model Stochasticity on Trust, Reliability, and Anthropomorphization",2025,"Chelse Swoopes, Tyler Holloway, Elena L. Glassman","Interfaces for interacting with large language models (LLMs) are often designed to mimic human conversations, typically presenting a single response to user queries. This design choice can obscure the probabilistic and predictive nature of these models, potentially fostering undue trust and over-anthropomorphization of the underlying model. In this paper, we investigate (i) the effect of displaying multiple responses simultaneously as a countermeasure to these issues, and (ii) how a cognitive support mechanism-highlighting structural and semantic similarities across responses-helps users deal with the increased cognitive load of that intervention. We conducted a within-subjects study in which participants inspected responses generated by an LLM under three conditions: one response, ten responses with cognitive support, and ten responses without cognitive support. Participants then answered questions about workload, trust and reliance, and anthropomorphization. We conclude by reporting the results of these studies and discussing future work and design opportunities for future LLM interfaces.",10.48550/arXiv.2503.16114,['JournalArticle'],Computer Science
De-jargonizing Science for Journalists with GPT-4: A Pilot Study,2024,"Sachita Nishal, Eric Lee, Nicholas Diakopoulos","This study offers an initial evaluation of a human-in-the-loop system leveraging GPT-4 (a large language model or LLM), and Retrieval-Augmented Generation (RAG) to identify and define jargon terms in scientific abstracts, based on readers' self-reported knowledge. The system achieves fairly high recall in identifying jargon and preserves relative differences in readers' jargon identification, suggesting personalization as a feasible use-case for LLMs to support sense-making of complex information. Surprisingly, using only abstracts for context to generate definitions yields slightly more accurate and higher quality definitions than using RAG-based context from the fulltext of an article. The findings highlight the potential of generative AI for assisting science reporters, and can inform future work on developing tools to simplify dense documents.",10.48550/arXiv.2410.12069,['JournalArticle'],Computer Science
GermanPartiesQA: Benchmarking Commercial Large Language Models and AI Companions for Political Alignment and Sycophancy,2025,"Jan Batzner, Volker Stocker, Stefan Schmid, Gjergji Kasneci","Large language models (LLMs) are increasingly shaping citizens information ecosystems. Products incorporating LLMs, such as chatbots and AI Companions, are now widely used for decision support and information retrieval, including in sensitive domains, raising concerns about hidden biases and growing potential to shape individual decisions and public opinion. This paper introduces GermanPartiesQA, a benchmark of 418 political statements from German Voting Advice Applications across 11 elections to evaluate six commercial LLMs. We evaluate their political alignment based on role-playing experiments with political personas. Our evaluation reveals three specific findings: 
(1) Factual limitations: LLMs show limited ability to accurately generate factual party positions, particularly for centrist parties. 
(2) Model-specific ideological alignment: We identify consistent alignment patterns and degree of political steerability for each model across temperature settings and experiments. 
(3) Claim of sycophancy: While models adjust to political personas during role-play, we find this reflects persona-based steerability rather than the increasingly popular, yet contested concept of sycophancy. 
Our study contributes to evaluating the political alignment of closed-source LLMs that are increasingly embedded in electoral decision support tools and AI Companion chatbots.",10.1609/aies.v8i1.36552,"['JournalArticle', 'Conference']",Computer Science; Political Science
Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning,2025,"Bokeng Zheng, Bo Rao, Tianxiang Zhu, Chee-Wei Tan, Jingpu Duan, Zhi Zhou, Xu Chen, Xiaoxi Zhang","Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model fine-tuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning.To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.",10.48550/arXiv.2502.04399,['JournalArticle'],Computer Science; Engineering; Environmental Science
"GEMINI, a Novel Software System to Improve the Penetration Rate of a Tunnel Boring Machine",2024,"I. Aliguer, I. Oliver, C. de Santos, F. Vara, J. Gomez","
 
 This technical paper presents GEMINI, a software system designed to optimize the performance of tunnel boring machines (TBMs) using machine learning-based techniques. The system key features include an advanced architecture for querying and processing data from the TBM database, big data analysis and visualization in a web browser, a ground-machine interaction model to predict the TBM advance rate and operation anomalies detection via machine learning algorithms to enhance the efficiency and reliability of tunnel excavation operations. In collaboration with ACCIONA, the system has been validated in the Mularroya (Spain) hydraulic tunnel (small diameter) in fractured rock mass with faulted and sheared zones, and implemneted in a larger diameter tunnel in Sao Paulo (Brazil) in weathered granite materials. In both cases, the ground-machine excavation model was trained with excavation data from each tunnel and the model was deployed to provide real-time predictions of the penetration rate during the TBM operation.
 
 
 
 Generally., in tunnelling operations regardless the excavation method, penetration rate and advance rate are the most relevant indicators to deliver the project on time and within budget. In this context, an appropriate methodology to predict their values during operation is a challenge that ACCIONA as a contractor is willing to tackle for mechanized excavations with TBMs in collaboration with SAALG Geomechanics. Geotechnical back-analysis to characterize ground behaviour has received attention from both the academia and the industry since the 80s (Gioda and Maier, 1980) within the context of forensic geotechnics and the Observational Method. It consists in comparing a set of ground measurements that capture the actual ground response with a conceptual model that is able to predict such response by means of some ground-related parameters. The parameters that best represent the ground behaviour are those that minimize the difference between the measurements and model results, expressed as an objective function.
 SAALG Geomechanics develops and commercializes DAARWIN (Acosta et al. 2023), a cloud-based web application that implements the general workflow of geotechnical back-analysis at the same pace as construction progresses, also referred as real-time back-analysis  RTBA). Its inputs are: i) Finite Elements (FE) as a predictive model of ground behaviour, ii) data from sensors installed within the construction site and uses genetic algorithms as the optimization technique to minimize the objective function (de Santos, 2015).
",10.56952/arma-2024-1007,nan,Computer Science; Engineering
Detection and Prediction of Future Mental Disorder from Social Media Data Using Machine Learning Ensemble Learning and Large Language Models,2025,"Cheni Sruneethi, Kondarajula Sharmila","With the exponential growth of user-generated content on social media, researchers are exploring new ways to extract meaningful patterns to understand public health trendsparticularly mental health conditions. This paper investigates the detection and prediction of future mental disorders through social media analysis using a combination of machine learning, ensemble learning methods, and large language models (LLMs). The approach aims to identify behavioral and linguistic indicators of mental distress before clinical diagnosis or self-reporting. Ensemble methods such as Random Forests and Gradient Boosting are integrated with deep learning language models like BERT and RoBERTa to improve prediction accuracy. A diverse set of features including sentiment polarity, temporal posting patterns, and linguistic markers are extracted from social posts to train the models. The proposed system achieves high accuracy in predicting early warning signs of mental disorders, such as depression, anxiety, and PTSD, with explainability incorporated through SHAP values. This research offers a scalable, data-driven solution to assist clinicians and policymakers in early mental health intervention strategies.",10.32628/ijsrst2512338,['JournalArticle'],Computer Science; Psychology
Imitating Human Reasoning to Extract 5W1H in News,2025,"Carlos Muoz, M. Mendoza, Hans Lobel, Brian F. Keith","Extracting key information from news articles is crucial for advancing search systems. Historically, the 5W1H framework, which organises information based on 'Who', 'What', 'When', 'Where', 'Why', and 'How', has been a predominant method in digital journalism empowering search tools. The rise of Large Language Models (LLMs) has sparked new research into their potential for performing such information extraction tasks effectively. Our study examines a novel approach to employing LLMs in the 5W1H extraction process, particularly focusing on their capacity to mimic human reasoning. We introduce two innovative Chain-of-Thought (COT) prompting techniques to extract 5W1H in news: extractive reasoning and question-level reasoning. The former directs the LLM to pinpoint and highlight essential details from texts, while the latter encourages the model to emulate human-like reasoning at the question-response level. Our research methodology includes experiments with leading LLMs using prompting strategies to ascertain the most effective approach. The results indicate that COT prompting significantly outperforms other methods. In addition, we show that the effectiveness of LLMs in such tasks depends greatly on the nature of the questions posed.",10.1145/3701716.3715532,"['Book', 'JournalArticle', 'Conference']",Computer Science; Linguistics
PhDGPT: Introducing a psychometric and linguistic dataset about how large language models perceive graduate students and professors in psychology,2024,"Edoardo Sebastiano De Duro, Enrique Taietta, Riccardo Improta, Massimo Stella","Machine psychology aims to reconstruct the mindset of Large Language Models (LLMs), i.e. how these artificial intelligences perceive and associate ideas. This work introduces PhDGPT, a prompting framework and synthetic dataset that encapsulates the machine psychology of PhD researchers and professors as perceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints, counting 300 iterations repeated across 15 academic events, 2 biological genders, 2 career levels and 42 unique item responses of the Depression, Anxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric scores with their explanations in plain language. This synergy of scores and texts offers a dual, comprehensive perspective on the emotional well-being of simulated academics, e.g. male/female PhD students or professors. By combining network psychometrics and psycholinguistic dimensions, this study identifies several similarities and distinctions between human and LLM data. The psychometric networks of simulated male professors do not differ between physical and emotional anxiety subscales, unlike humans. Other LLMs' personification can reconstruct human DASS factors with a purity up to 80%. Furthemore, LLM-generated personifications across different scenarios are found to elicit explanations lower in concreteness and imageability in items coding for anxiety, in agreement with past studies about human psychology. Our findings indicate an advanced yet incomplete ability for LLMs to reproduce the complexity of human psychometric data, unveiling convenient advantages and limitations in using LLMs to replace human participants. PhDGPT also intriguingly capture the ability for LLMs to adapt and change language patterns according to prompted mental distress contextual features, opening new quantitative opportunities for assessing the machine psychology of these artificial intelligences.",10.48550/arXiv.2411.10473,['JournalArticle'],Computer Science; Linguistics; Psychology
"A Computational Approach to AI-Based Interview Simulations: Integrating Job Specific Question Generation, Speech Confidence Analysis, andNon-VerbalCues",2025,"Sumi S, Shaji B, Justin Jose","Traditional interview preparation techniques frequently fall short in giving individualized, real-time feedback due to the increased competitiveness in the job market. To improve candidates' readiness through real-time skill evaluation, this study offers an AI-powered mock interview platform that uses cutting-edge machine learning and natural language processing (NLP) techniques. The Meta Llama AI model, which is at the heart of the system, creates interview questions that are relevant to different professions and ensures that candidates are assessed on subjects that are pertinent to the sector. The platform uses speech recognition to examine hesitation patterns, tone, and voice modulation, providing information on a candidate's degree of confidence. To ensure that responses show clarity and depth of knowledge, NLP-based evaluation evaluates word fluency, sentence structure, and coherence. Additionally, facial recognition technology is used for real-time emotion analysis. This technology helps determine an interviewee's emotional stability and participation by detecting eye contact, facial expressions, and stress signs. The method uses several assessment characteristics, such as confidence level, response accuracy via keyword mapping, and overall involvement, to deliver a thorough performance review. An Employability Score, a measurable indicator that aids candidates in understanding their areas of strength and growth, is produced by combining these variables. This AI-driven method transforms interview preparation by providing structured feedback and tailored insights, making it more efficient, data-driven, and flexible to the changing demands of thelabormarket.",10.36948/ijfmr.2025.v07i02.39872,"['JournalArticle', 'Review']",Computer Science
A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges,2025,"Rahul Raja, Anshaj Vats, A. Vats, Anirban Majumder","Recommender systems have traditionally followed modular architectures comprising candidate generation, multi-stage ranking, and re-ranking, each trained separately with supervised objectives and hand-engineered features. While effective in many domains, such systems face persistent challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content. The recent emergence of Large Language Models (LLMs) offers a new paradigm for addressing these limitations through unified, language-native mechanisms that can generalize across tasks, domains, and modalities. In this paper, we present a comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. We examine the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation, illustrating how these approaches enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios by leveraging external knowledge and contextual cues. We categorize these emerging LLM-driven architectures and analyze their effectiveness in mitigating core bottlenecks of conventional pipelines. In doing so, we provide a structured framework for understanding the design space of LLM-enhanced recommenders, and outline the trade-offs between accuracy, scalability, and real-time performance. Our goal is to demonstrate that LLMs are not merely auxiliary components but foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems",10.48550/arXiv.2507.21117,"['JournalArticle', 'Review']",Computer Science; Linguistics
Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation,2025,"Jisu Shin, Juhyun Oh, Eunsu Kim, Hoyun Song, Alice Oh","Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",10.48550/arXiv.2506.19352,"['JournalArticle', 'Conference']",Computer Science
Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models,2025,"Ruiyang Zhang, Hu Zhang, Hao Fei, Zhedong Zheng","Large Multimodal Models (LMMs), harnessing the complementarity among diverse modalities, are often considered more robust than pure Language Large Models (LLMs); yet do LMMs know what they do not know? There are three key open questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to quantify uncertainty for downstream tasks. In an attempt to address these challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed to reveal uncertainty in LMMs regardless of their modalities, architectures, or capabilities, (2) an empirical exploration of multimodal prompt perturbations to uncover LMM uncertainty, offering insights and findings, and (3) derive the formulation of multimodal semantic uncertainty, which enables quantifying uncertainty from multimodal responses. Experiments across 18 benchmarks spanning various modalities and 10 LMMs (both open- and closed-source) demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM uncertainty, thereby enhancing downstream tasks such as hallucination detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought reasoning.",10.48550/arXiv.2506.07575,['JournalArticle'],Computer Science
A Survey on Spatio-Temporal Prediction: From Transformers to Foundation Models,2025,"Yingchi Mao, Hongliang Zhou, Ling Chen, Rongzhi Qi, Zhende Sun, Yi Rong, Xiaoming He, Mingkai Chen, Shahid Mumtaz, Valerio Frascolla, Mohsen Guizani, J. Rodrigues","
 Spatio-Temporal (ST) data is pervasive on the various aspects in our daily lives. By mining the ST information from the data, we are able to predict trends in numerous domains. The Transformer, and one of its more recent enhancements, foundation models, have achieved a remarkable success in such ST prediction. In this paper, we first survey the state of the art of Transformers-related work, then introduce the network architecture of the Transformer and summarize the improvements to adapt to the ST prediction Transformer and foundation models, including module enhancement and adjustment. Subsequently, we categorize the ST Transformer and foundation models in selected applications in some relevant domains, mainly urban transportation, climate monitoring, and motion prediction. Next, we propose an evaluation method in the ST prediction with Transformers and foundation models, list the most relevant open-source datasets, evaluation metrics and performance analysis. Finally, we discuss some future directions on the task of ST prediction with Transformer and foundation models.
 Relevant papers and open-source resources have been collated and are continuously updated at: https://github.com/cyhforlight/Spatio-Temporal-Prediction-Transformer-Review.
",10.1145/3766546,"['JournalArticle', 'Review']",Computer Science
A Survey of the State-of-the-Art in Conversational Question Answering Systems,2025,"Manoj Madushanka Perera, Adnan Mahmood, K. Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng","Conversational Question Answering (ConvQA) systems have emerged as a pivotal area within Natural Language Processing (NLP) by driving advancements that enable machines to engage in dynamic and context-aware conversations. These capabilities are increasingly being applied across various domains, i.e., customer support, education, legal, and healthcare where maintaining a coherent and relevant conversation is essential. Building on recent advancements, this survey provides a comprehensive analysis of the state-of-the-art in ConvQA. This survey begins by examining the core components of ConvQA systems, i.e., history selection, question understanding, and answer prediction, highlighting their interplay in ensuring coherence and relevance in multi-turn conversations. It further investigates the use of advanced machine learning techniques, including but not limited to, reinforcement learning, contrastive learning, and transfer learning to improve ConvQA accuracy and efficiency. The pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash, Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact through data scalability and architectural advancements. Additionally, this survey presents a comprehensive analysis of key ConvQA datasets and concludes by outlining open research directions. Overall, this work offers a comprehensive overview of the ConvQA landscape and provides valuable insights to guide future advancements in the field.",10.48550/arXiv.2509.05716,"['JournalArticle', 'Review']",Computer Science; Linguistics
ClimateQ&A: Bridging the gap between climate scientists and the general public,2024,"Natalia De La Calzada, Th'eo Alves Da Costa, A. Blangero, N. Chesneau","This research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&A platform. ClimateQ&A is a conversational agent that uses LLMs to respond to queries based on over 14,000 pages of scientific literature from the IPCC and IPBES reports. Launched online in March 2023, the tool has gathered over 30,000 questions, mainly from a French audience. Its chatbot interface allows for the free formulation of questions related to nature*. While its main goal is to make nature science more accessible, it also allows for the collection and analysis of questions and their themes. Unlike traditional surveys involving closed questions, this novel method offers a fresh perspective on individual interrogations about nature. Running NLP clustering algorithms on a sample of 3,425 questions, we find that a significant 25.8% inquire about how climate change and biodiversity loss will affect them personally (e.g., where they live or vacation, their consumption habits) and the specific impacts of their actions on nature (e.g., transportation or food choices). This suggests that traditional methods of surveying may not identify all existing knowledge gaps, and that relying solely on IPCC and IPBES reports may not address all individual inquiries about climate and biodiversity, potentially affecting public understanding and action on these issues. *we use 'nature' as an umbrella term for 'climate change' and 'biodiversity loss'",10.48550/arXiv.2403.14709,"['JournalArticle', 'Review']",Computer Science; Environmental Science
Do LLM Personas Dream of Bull Markets? Comparing Human and AI Investment Strategies Through the Lens of the Five-Factor Model,2024,"Harris Borman, A. Leontjeva, Luiz Pizzato, Max Kun Jiang, Dan Jermyn","Large Language Models (LLMs) have demonstrated the ability to adopt a personality and behave in a human-like manner. There is a large body of research that investigates the behavioural impacts of personality in less obvious areas such as investment attitudes or creative decision making. In this study, we investigated whether an LLM persona with a specific Big Five personality profile would perform an investment task similarly to a human with the same personality traits. We used a simulated investment task to determine if these results could be generalised into actual behaviours. In this simulated environment, our results show these personas produced meaningful behavioural differences in all assessed categories, with these behaviours generally being consistent with expectations derived from human research. We found that LLMs are able to generalise traits into expected behaviours in three areas: learning style, impulsivity and risk appetite while environmental attitudes could not be accurately represented. In addition, we showed that LLMs produce behaviour that is more reflective of human behaviour in a simulation environment compared to a survey environment.",10.48550/arXiv.2411.05801,"['JournalArticle', 'Review']",Business; Computer Science; Economics; Psychology
Generative ai and sociology: analyzing virtual respondent technology,2025,"Zhanna Puzanova, George Koshoridze, David Koshoridze","The article analyzes the use of virtual respondents and large language models (LLM) in sociological research, including a review of domestic and foreign examples. The technical aspects of survey simulation with the help of LLM, including the choice of models, as well as practical recommendations for their application are considered. The advantages of this technology, such as the possibility of rapid data collection and adaptability, as well as disadvantages, including ethical risks and limitations in representativeness, are analyzed. Based on two pilot studies, it is found that to improve the effectiveness of the virtual respondent, it is recommended to first conduct a pilot survey with real participants, upload the results to the LLM and create a generalized virtual respondent trained on real responses to generate more reliable responses to the questions of interest.",10.19181/4m.2025.34.1.6,"['JournalArticle', 'Review']",Computer Science; Sociology
ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution,2025,"Kanika Goswami, Puneet Mathur, Ryan A. Rossi, Franck Dernoncourt","Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.",10.48550/arXiv.2502.00989,['JournalArticle'],Computer Science
Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction,2025,"Sarah Ball, Simeon Allmendinger, Frauke Kreuter, Niklas Khl","Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.",10.48550/arXiv.2502.16280,"['JournalArticle', 'Review']",Computer Science; Political Science
On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction,2025,"Qingyu Liang, Jaime Banks","Shared understanding plays a key role in the effective communication in and performance of human-human interactions. With the increasingly common integration of AI into human contexts, the future of personal and workplace interactions will likely see human-AI interaction (HAII) in which the perception of shared understanding is important. Existing literature has addressed the processes and effects of PSU in human-human interactions, but the construal remains underexplored in HAII. To better understand PSU in HAII, we conducted an online survey to collect user reflections on interactions with a large language model when it sunderstanding of a situation was thought to be similar to or different from the participant's. Through inductive thematic analysis, we identified eight dimensions comprising PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.",10.48550/arXiv.2505.20068,"['JournalArticle', 'Review']",Computer Science; Psychology
Early Study of LLM Implementation in Survey Interviews,2025,"Lailatul Hasanah, Budi Yuniarto","Introduction/Main Objectives: This research aims to conduct a preliminary study into the use of LLMs for extracting information to fill out questionnaires in survey interviews. Background Problems: BPS-Statistics Indonesia used paper-based questionnaires for interviews and is recently utilizing the Computer Assisted Personal Interviewing (CAPI) method. However, the CAPI method has some drawbacks. Enumerators must input data into the device, which can be burdensome and prone to errors. Novelty: This study uses a large language model (LLM) to extract information from survey interviews. Research Methods: This study utilizes a text-to-speech application to translate interview results into text. Translation accuracy is measured by the Word Error Rate (WER). Then the text was extracted using the ChatGPT 3.5 Turbo model. GPT-3.5 Turbo is part of the GPT family of algorithms developed by OpenAI. Finding/Results: The extraction results are formatted into a JSON file, which is intended to be used for automatic filling into the database and then evaluated using precision, recall, and F1-score. Based on research conducted by utilizing the Speech Recognition API by Google and the ChatGPT 3.5 Turbo model, an average WER of 10% was obtained in speech recognition and an average accuracy of 76.16% in automatic data extraction.",10.34123/jurnalasks.v17i1.792,"['JournalArticle', 'Review']",Computer Science
SoK: On-Device Large Language Model Personalization for Meta Computing,2024,"Yanting Shi, Zhenge Jia, Zhaoyan Shen, Mengying Zhao","Meta computing has been developed to fully exploit all available computing resources, especially the computational capability of terminal IoT devices, to complete the targeting task. This computing paradigm aims to provide efficient, fault-tolerant, and personalized services while maintaining robust security and privacy for any computing task. Meanwhile, Large Language Models (LLMs), which are among the most widely used AI applications, have shown exceptional capabilities in tasks such as natural language understanding, language generation, and complex reasoning. LLMs have the potential to significantly impact our society. As LLMs become more prevalent as personalized intelligent assistants, deploying them on resource-constraint terminal devices and enabling on-device personalization (i.e., customizing models through fine-tuning) will be increasingly demanding due to the advantages of quick response and data privacy preservation. When the meta computing paradigm comes across the AI application, an urgent but open question is: How to fully utilize the capabilities of resource-constrained computing platforms to perform on-device LLM personalization? Current research on this topic is fragmented and lacks a comprehensive overview. This paper seeks to fill this gap by organizing and synthesizing previous studies in this area. We offer a thorough review of on-device LLM personalization research, categorizing the literature into a taxonomy that includes four main categories, each covering interconnected topics from data-focused, model-focused, optimizer-focused, and distribution-focused perspectives. Our goal is to serve as a valuable resource, helping researchers and practitioners gain a comprehensive understanding of the advancements in on-device LLM personalization and inspiring further contributions to this vital and dynamic field.",10.1109/ICMC60390.2024.00036,"['Conference', 'Review']",Computer Science; Engineering
AI Agents: Design and Evaluation of Gamified Conversational Agents,2025,"Christian Poglitsch, Maria Seiser, Markus Buchsteiner, Johanna Pirker","Effective social communication relies on understanding emotions, interpreting social cues, and maintaining meaningful conversations. To support the development of these skills, we propose a serious game featuring Conversational Artificial Intelligence agents that enable users to engage in questdriven conversations across various scenarios. Users can design custom scenarios and set specific conversation goals as quests to complete one-on-one conversations with a virtual agent who can express basic emotions. To provide structured guidance, the system offers predefined scenarios such as a date simulation, job interview, and Who Am I? game. In addition, it features Theory of Mind training to help people improve their ability to detect sarcasm, irony, and navigate everyday interactions more effectively. We design and evaluate our system to ensure a highly effective and engaging experience. Built on GPT-4o, our approach was evaluated using the Bot Usability Scale, with 20 users successfully recruited online. The results show that the agent's functionality was well received, showcasing high usability and effective communication. However, there is room for improvement in contextual relevance. Although LLM maintained coherent conversations, we faced challenges with long-term context retention. Furthermore, although the avatar's emotional expressions were generally accurate, further refinement is needed to better align them with the context.",10.1109/CoG64752.2025.11114224,"['JournalArticle', 'Conference']",Computer Science
Understanding Privacy Norms Around LLM-Based Chatbots: A Contextual Integrity Perspective,2025,"Sarah Tran, Hongfan Lu, Isaac Slaughter, Bernease Herman, Aayushi Dangol, Yue Fu, Lufei Chen, Biniyam Gebreyohannes, Bill Howe, Alexis Hiniker, Nicholas Weber, Robert Wolfe","LLM-driven chatbots like ChatGPT have created unprecedented volumes of conversational data, yet little is known about user privacy expectations for this information. We surveyed 300 US ChatGPT users to understand privacy norms around chatbot data sharing using the contextual integrity framework. Our findings reveal a stark disconnect between user concerns and behavior. While 82% of respondents rated chatbot conversations as sensitive or highly sensitivemore than email or social media postsnearly half reported discussing health topics and over one-third discussed personal finances with ChatGPT. Participants expressed strong privacy concerns (t(299) = 8.5, p",10.48550/arXiv.2508.06760,"['JournalArticle', 'Conference', 'Review']",Computer Science; Sociology
Understanding the Cognitive Complexity in Language Elicited by Product Images,2024,"Yan-Ying Chen, Shabnam Hakimi, Monica Van, Francine Chen, Matthew K. Hong, Matt Klenk, Charlene Wu","Product images (e.g., a phone) can be used to elicit a diverse set of consumer-reported features expressed through language, including surface-level perceptual attributes (e.g.,""white"") and more complex ones, like perceived utility (e.g.,""battery""). The cognitive complexity of elicited language reveals the nature of cognitive processes and the context required to understand them; cognitive complexity also predicts consumers' subsequent choices. This work offers an approach for measuring and validating the cognitive complexity of human language elicited by product images, providing a tool for understanding the cognitive processes of human as well as virtual respondents simulated by Large Language Models (LLMs). We also introduce a large dataset that includes diverse descriptive labels for product images, including human-rated complexity. We demonstrate that human-rated cognitive complexity can be approximated using a set of natural language models that, combined, roughly capture the complexity construct. Moreover, this approach is minimally supervised and scalable, even in use cases with limited human assessment of complexity.",10.48550/arXiv.2409.16521,['JournalArticle'],Business; Computer Science; Psychology
Exploring Political Factors in Clean Energy Transition Using Machine Learning Technique,2025,"Zining Yang, Ruiqian Li","A nationwide transition to clean energy still faces persistent political challenges in the past decade. It is because the public support for clean energy policies remains deeply polarized among partisan and ideological lines. While there is an established scientific consensus of climate change, the urgence to change the status quo fails to trigger existential insecurity of individual Americans. While extensive studies have examined the role of partisanship, regional economy, and media framing in shaping these divisions, scholars know very little about the emotional foundation that drive individual voters clean energy preferences and behaviors. A major challenge for the scholarship is due to the lack of longitudinal data that contains measurement of both individual emotions as well as clean energy ideologies over time. This study introduces a simulation-based methodology that combining machine learning with traditional survey analysis to examine how anxiety and fear, two emotions related to existential security, shape clean energy policy ideologies and behaviors in the net of social and political factors. Our analytical strategy proceeds in three stages. First, utilizing multiple waves of cross-sectional data collected by Chapman University since 2014, we train a semi-parametric model to estimate the relationship between commonly used apolitical demographic features and anxiety and fear. We will also run the robustness check to make the model is time invariant. Second, we apply the model to two of most recent probabilistic samples collected by Pew Research Center where it contains clean energy ideology and behavior items but lacks emotional measures. And finally, we analyze how these simulated emotional predispositions interact with a range of political and social factors to predict support for clean energy initiatives. Preliminary findings suggest that political and partisan preference may suppress the effective size of existential insecurity to the support of clean energy. And we find this impact is also varied across different clean energy behaviors. The study makes several key contributions. First and methodologically, the study demonstrates how machine learning can bridge gaps between datasets with different focuses and enables more comprehensive analysis of clean energy related studies with nationally representative data. Second and theoretically, it advances our understanding of how emotions related to existential insecurity shape clean energy policy behaviors. Future research could extend this framework to examine how different aspects of policy framing might differentially affect the suppression of climate-related fears. Additionally, longitudinal applications of this approach combining with large language modeling could help track how the relationship between political framing and negative emotions evolves in response to changing environmental conditions and policy-making.",10.54941/ahfe1006396,['Review'],Environmental Science; Political Science
Comparing Human Expertise and Large Language Models Embeddings in Content Validity Assessment of Personality Tests,2025,"Nicola Milano, M. Ponticorvo, Davide Marocco","In this article we explore the application of Large Language Models (LLMs) in assessing the content validity of psychometric instruments, focusing on the Big Five Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a cornerstone of test construction, ensures that psychological measures adequately cover their intended constructs. Using both human expert evaluations and advanced LLMs, we compared the accuracy of semantic item-construct alignment. Graduate psychology students employed the Content Validity Ratio (CVR) to rate test items, forming the human baseline. In parallel, state-of-the-art LLMs, including multilingual and fine-tuned models, analyzed item embeddings to predict construct mappings. The results reveal distinct strengths and limitations of human and AI approaches. Human validators excelled in aligning the behaviorally rich BFQ items, while LLMs performed better with the linguistically concise BFI items. Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs. Here we highlights the complementary potential of hybrid validation systems that integrate human expertise and AI precision. The findings underscore the transformative role of LLMs in psychological assessment, paving the way for scalable, objective, and robust test development methodologies.",10.48550/arXiv.2503.12080,['JournalArticle'],Computer Science; Psychology
PersonaAI: Leveraging Retrieval-Augmented Generation and Personalized Context for AI-Driven Digital Avatars,2025,"Elvis Kimara, K. S. Oguntoye, Jian Sun","This paper introduces PersonaAI, a cutting-edge application that leverages Retrieval-Augmented Generation (RAG) and the LLAMA model to create highly personalized digital avatars capable of accurately mimicking individual personalities. Designed as a cloud-based mobile application, PersonaAI captures user data seamlessly, storing it in a secure database for retrieval and analysis. The result is a system that provides context-aware, accurate responses to user queries, enhancing the potential of AI-driven personalization. Why should you care? PersonaAI combines the scalability of RAG with the efficiency of prompt-engineered LLAMA3, offering a lightweight, sustainable alternative to traditional large language model (LLM) training methods. The system's novel approach to data collection, utilizing real-time user interactions via a mobile app, ensures enhanced context relevance while maintaining user privacy. By open-sourcing our implementation, we aim to foster adaptability and community-driven development. PersonaAI demonstrates how AI can transform interactions by merging efficiency, scalability, and personalization, making it a significant step forward in the future of digital avatars and personalized AI.",10.48550/arXiv.2503.15489,['JournalArticle'],Computer Science
Anchoring Bias in Generative AI: A Comparative Analysis of Large Language Models in a Pricing Scenario,2025,"Rka Koteczki, Boglrka Eisingern Balassa",,10.1109/CogInfoCom66819.2025.11200816,['Conference'],Computer Science
A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow,2025,"George Paterakis, Andrea Castellani, George Papoutsoglou, Tobias Rodemann, Ioannis Tsamardinos","Artificial intelligence is reshaping science and industry, yet many users still regard its models as opaque""black boxes"". Conventional explainable artificial-intelligence methods clarify individual predictions but overlook the upstream decisions and downstream quality checks that determine whether insights can be trusted. In this work, we present Holistic Explainable Artificial Intelligence (HXAI), a user-centric framework that embeds explanation into every stage of the data-analysis workflow and tailors those explanations to users. HXAI unifies six components (data, analysis set-up, learning process, model output, model quality, communication channel) into a single taxonomy and aligns each component with the needs of domain experts, data analysts and data scientists. A 112-item question bank covers these needs; our survey of contemporary tools highlights critical coverage gaps. Grounded in theories of human explanation, principles from human-computer interaction and findings from empirical user studies, HXAI identifies the characteristics that make explanations clear, actionable and cognitively manageable. A comprehensive taxonomy operationalises these insights, reducing terminological ambiguity and enabling rigorous coverage analysis of existing toolchains. We further demonstrate how AI agents that embed large-language models can orchestrate diverse explanation techniques, translating technical artifacts into stakeholder-specific narratives that bridge the gap between AI developers and domain experts. Departing from traditional surveys or perspective articles, this work melds concepts from multiple disciplines, lessons from real-world projects and a critical synthesis of the literature to advance a novel, end-to-end viewpoint on transparency, trustworthiness and responsible AI deployment.",10.48550/arXiv.2508.11529,"['JournalArticle', 'Review']",Computer Science
"Neural Network-Enhanced Model Predictive Cruise Control for a Heavy
 Truck with LLM-Assisted Data Collection for Training",2025,"Xiao Han, Kang Song, Qing Fang Lv, Yi Zhang, Hui Xie","Predictive Cruise Control (PCC) is a promising approach for improving fuel
 efficiency and reducing operational costs in heavy trucks. However, its
 implementation using conventional Nonlinear Model Predictive Control (NMPC)
 methods is hindered by computational limitations, often restricting the use of
 long-horizon slope information. This paper addresses these challenges by
 proposing a neural network-enhanced slope-adaptive NMPC framework. A
 Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) architecture is
 employed to integrate long-horizon slope information and dynamically update
 control parameters, effectively overcoming computational constraints of
 traditional NMPC. To further enhance efficiency, an automated simulation
 scheduling system is developed, leveraging Large Language Models (LLMs) and
 expert knowledge to optimize parameter tuning and streamline data collection,
 significantly reducing training overhead. Validation on a high-fidelity
 simulation platform demonstrates that the proposed method achieves fuel savings
 of 0.53% on long downhill slopes and 0.88% during incline-to-flat transitions at
 a constant speed of 72 km/h, outperforming fixed-parameter PCC approaches. The
 automated simulation scheduling system reduces human involvement in data
 preparation by 60%, highlighting the potential of integrating LLMs into control
 systems. These results confirm the feasibility and advantages of the proposed
 method for real-world applications in fuel-efficient heavy truck operations.",10.4271/2025-01-7033,nan,Computer Science; Engineering
Facilitation of Kansei engineering design process with LLM multi-agent discussion,2024,"S. Ishihara, Taku Ishihara, K. Ishihara","Recent advances in Large Language Models and Generative AI continue to have a positive impact on the design process and Kansei engineering.At AHFE 2023, we presented the trials of product design using Generative AI based on Kansei engineering (Ishihara, Kuo & Ishihara, AI image generation boosts Kansei engineering design process, 2023).In this study, we explored the possibilities of multi-agent discussion in the design process and Kansei engineering. GPT-4 (OpenAI) was asked to create experts and discuss the given topic from each expert's point of view. In the example below, the topic was given as ""We would like to discuss the design of an innovative chair made with a 3D printer. We would like to hear opinions from as many different perspectives as possible.GPT-4 automatically creates 3 virtual experts as: a designer, an engineer, and an environmental specialist. These different experts were created without any specific direction.The following lines are the roles of the experts set up in GPT-4: Designer - Focuses the discussion from an aesthetic point of view. Emphasizes innovation and aesthetics in design. Passionate in tone and fond of creative ideas.Engineer - Focuses on practicality and technical feasibility. Discusses cost and material efficiency in detail. Analytical in tone and logical in approach.Environmentalist - Focuses on sustainability and environmental impact. Discusses the use of recyclable materials and reducing environmental impact. They are soft-spoken and have deep insight into environmental issues.Intermediate discussions are in the following lines;---Persona 1: DesignerInspired by the engineer's opinion, I'd like to think a little more about the balance between durability and design. Durability is very important because chairs are used on a daily basis, and the key to manufacturing with a 3D printer is ensuring structural strength while still maintaining an aesthetically pleasing design. This can be seen as a design challenge.Persona 3: Environmental SpecialistI agree with the designer's suggestion, but materials should be carefully chosen to minimize environmental impact. Environmentally friendly materials, such as bioplastics, can provide greater design freedom while ensuring sustainability. The energy efficiency of the 3D printer is also important.Persona 2: EngineerI agree with the points made by the environmental experts. However, when using environmentally friendly materials, one must also pay attention to the physical properties of these materials. It is important to choose materials that are sustainable while still meeting the basic performance requirements of the chair, such as strength and heat resistance.Persona 1: DesignerThe engineer's technical perspective is very important. As a designer, the challenge is how to turn these technical limitations into art. For example, if there is a limit to the strength of a material, we can incorporate that as part of the design. I think balancing the flexibility of the design with the technical requirements will be key to the success of this project.The final agenda is as follows;---Designer's PerspectiveDesign Innovation: 3D printing allows for unique shapes and patterns that were previously impossible.Aesthetics and Individuality: Aesthetically pleasing designs that match consumer tastes and contemporary interior trends are important.Appealing to consumer sensibilities: Unique and beautiful designs can serve as status symbols, making them feel worth the high price tag.Engineer's PerspectivePracticality and technical feasibility: Consider the balance between manufacturing efficiency and cost while maintaining durability and safety.Customization and Quality: Ensure long-term cost-effectiveness and satisfaction by allowing adjustments to accommodate each individual's body shape and preferences.Overcoming technical limitations: Consider the physical limitations of 3D printing technology as part of the design.Environmental ExpertiseSustainability: Consider the use of recyclable materials and the recyclability of the product.Environmental impact: Energy consumption and material choices during the manufacturing process are important.Appealing to eco-conscious consumers: Green products add value and justify a price premium.---This is a summary of the discussion on the design of 3D-printed chairs from each expert's perspective. A balance between design innovation, technical feasibility, and sustainability is important, and a product based on these factors will influence consumer purchase incentives and can be successful in the marketplace, even at a high price point.We are now trying to combine this with Generative Graphics AI to visualize ideas based on this discussion among virtual experts.",10.54941/ahfe1005135,nan,Computer Science; Engineering
Efficacy of Large Language Models in Predicting Hindi Movies' Attributes: A Comprehensive Survey and Content-Based Analysis,2024,"Prabir Mondal, Siddharth Singh, Kushum, Sriparna Saha, Jyoti Prakash Singh, Brijraj Singh, N. Pedanekar","This research explores the efficacy of four state-of-the-art Large Language Models (LLMs): GPT-3.5-turbo-0301, Vicuna, PaLM 2, and Dolly in predicting (i) movie genres using audio transcripts of movie trailers and (ii) meta-information such as director and cast details using movie name and its year-of-release (YoR) for Hindi movies. In the contemporary landscape, training models for movie meta-information prediction often demand extensive data and parameters, posing significant challenges. We aim to discern whether LLMs mitigate these challenges. Focusing on Hindi movies within the Flickscore dataset, our study concentrates on trailer data. Preliminary findings reveal that GPT-3.5 stands out as the most effective LLM in predicting movie meta-information. Despite the inherent complexities of predicting diverse aspects such as genres and user preferences, GPT-3.5 exhibits promising capabilities. This research not only contributes to advancing our understanding of LLMs in the context of movie-related tasks but also sheds light on their potential application in Recommendation Systems (RS), indicating a notable leap forward in user preference comprehension and personalized content recommendations.",10.1145/3589335.3651496,"['Book', 'JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Dialogagent: An Auto-Engagement Agent for Code Question Answering Data Production,2024,"Xiaoyun Liang, Jingyi Ren, Jiayi Qi, Chao Peng, Bo Jiang","Large Language Models (LLMs) have become increasingly integral to enhancing developer productivity, particularly in code generation, comprehension, and repair tasks. However, fine-tuning these models with high-quality, real-world data is challenging due to privacy concerns and the lack of accessible, labeled datasets. In this paper, we present DialogAgent, an automated tool for generating synthetic training data that closely mimics real developer interactions within Integrated Development Environments (IDEs). DialogAgent enables the production of diverse, high-fidelity query-response pairs by simulating multi-turn dialogues and contextual behaviors observed in real-world programming scenarios. The tool significantly reduces the reliance on manual data generation, increasing efficiency by 4.8 times compared to traditional methods. Our experiments and online deployment demonstrate substantial improvements in model performance for code-related question-answering tasks: the acceptance rate of responses generated by our in-house model is improved by 33%, after training on synthesized data generated by DialogAgent.",10.1109/ICSE-SEIP66354.2025.00012,"['JournalArticle', 'Conference']",Computer Science
Pioneers of Managerial Thought: Woodrow Wilson Case,2024,Y. Y. Petrunin,"The article focuses on the diversity of the struggle for priority in the history of managerial thought. The subject of the study is the role of the American political scientist and politician Woodrow Wilson in the development of the study of public administration. The article reveals the contradiction between historical facts and prevailing ideas about the priority of W. Wilson in the development of the public administration theory. This dichotomy is proven in particular through the use of generative artificial intelligence and large language models (ChatGPT). The article analyzes the struggle for W. Wilsons priority in the creation of the public administration science and compares it with the outwardly similar story of the discovery of genetics by Gregor Mendel. The article shows that the external similarity of the deferred priority in the creation of two sciences  genetics and public administration  differs significantly in the internal logic and consequences of the rediscovery of conceptual innovations. We can say that the deferred priority of the American scientist turns into a false priority, but fixed by public opinion as real. A conclusion is drawn about the relevance of W. Wilsons case for solving modern problems of economic cybernetics. The past forms the present and future and distorting the past we cant understand the present and predict and manage the better future.",10.55959/msu2070-1381-104-2024-27-36,['JournalArticle'],Political Science
An Imperial Analysis of Large Language Models for Automated Tweet Sentiment Prediction,2023,"Shivam Akhouri, Vidhi Ajbani, Ritika Lakshminarayanan, Trilok Nath Pandey, Meher Shrishti Nigam, S. Patra","Company' $s$ brand perception majorly depends on customer experience and the reviews which follow that. A customer is capable of influencing many more people just on the basis of reviews given by him/her. Google released Pathways Language Model (PaLM) which is a major advancement in Artificial Intelligence (AI). It has been trained with the Pathways System, which allows it to generalize tasks in various domains. In this work, a trustworthy platform is provided for the examination of millions of people's continually moving and changing perspectives. Twitter data is captured, and effective sentiment and data analysis is used to generate trustworthy and helpful info graphics reflecting public opinion. Product sales, stock returns, election outcomes, and other commercial and social events may all be predicted and explained using the information found in tweets. Brands, product manufacturers, and other companies may utilise the information derived from data analysis to better understand their brand image, expand their market share by targeting the relevant demographics at the right moments, and enhance their offerings in terms of both quality and customer service.",10.1109/ICSCNA58489.2023.10370253,"['Conference', 'Review']",Business; Computer Science
Utilizing Large Language Models for Social Simulations: Responding to the Degree of Freedom Questionnaire with ChatGPT,2024,"Jaeyoun You, Bongwon Suh",,10.17210/jhsk.2024.09.19.3.49,['JournalArticle'],Computer Science; Psychology
"Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech",2025,"Sanjeeevan Selvaganapathy, Mehwish Nasim","We investigate the efficacy of Large Language Models (LLMs) in detecting implicit and explicit hate speech, examining whether models with minimal safety alignment (uncensored) might provide more objective classification capabilities compared to their heavily-aligned (censored) counterparts. While uncensored models theoretically offer a less constrained perspective free from moral guardrails that could bias classification decisions, our results reveal a surprising trade-off: censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy. However, this enhanced performance comes with its own limitation -- the safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable to ideological framing. Furthermore, we identify critical failures across all models in understanding nuanced language such as irony. We also find alarming fairness disparities in performance across different targeted groups and systemic overconfidence that renders self-reported certainty unreliable. These findings challenge the notion of LLMs as objective arbiters and highlight the need for more sophisticated auditing frameworks that account for fairness, calibration, and ideological consistency.",10.48550/arXiv.2509.00673,['JournalArticle'],Computer Science; Linguistics
From Heuristics to Multi-Agent Learning: A Survey of Intelligent Scheduling Methods in Port Seaside Operations,2025,"Yaqiong Lv, Jingwen Wang, Zhongyuan Liu, Mingkai Zou","Port seaside scheduling, involving berth allocation, quay crane, and tugboat scheduling, is central to intelligent port operations. This survey reviews and statistically analyzes 152 academic publications from 2000 to 2025 that focus on optimization techniques for port seaside scheduling. The reviewed methods span mathematical modeling and exact algorithms, heuristic and simulation-based approaches, and agent-based and learning-driven techniques. Findings show deterministic models remain mainstream (77% of studies), with uncertainty-aware models accounting for 23%. Heuristic and simulation approaches are most commonly used (60.5%), followed by exact algorithms (21.7%) and agent-based methods (12.5%). While berth and quay crane scheduling have historically been the primary focus, there is growing research interest in tugboat operations, pilot assignment, and vessel routing under navigational constraints. The review traces a clear evolution from static, single-resource optimization to dynamic, multi-resource coordination enabled by intelligent modeling. Finally, emerging trends such as the integration of large language models, green scheduling strategies, and humanmachine collaboration are discussed, providing insights and directions for future research and practical implementations.",10.3390/math13172744,"['JournalArticle', 'Review']",Computer Science; Engineering
BrewBooze: Barista and Bartender Drinks Simulator Integrated with Business Cost Prediction Using Multiple Linear Regression,2024,"Lexter Louis T. Apuada, Marilou B. Mangrobang, Rimuel S. Caada, M. A. Mercado, C. Centeno, A. A. C. Sison","This research aimed to develop an innovative system that simulates the creation of barista and bartender beverages, enhances customer satisfaction through personalized coffee and cocktail flavor recommendations, and predicts startup costs for cafs. The system uses 3D models to guide users through beverage-making processes, and the GPT OpenAI API powers its flavor recommendation engine for accuracy and personalization. Additionally, multiple linear regression was employed to provide entrepreneurs with a detailed cost breakdown based on their capital and the year of establishment. BrewBooze was evaluated through a survey using Google Forms, having IT professionals, baristas, bartenders, and general consumers as their respondents. Half of the respondents drink their coffee/cocktail more than three times a week and only a quarter of them do not know the process of making it. Results have also shown that people are satisfied with the coffee/cocktail they are ordering and for the entrepreneurial side, most respondents prefer to have a capital of less than a million pesos, making it insufficient for starting a caf. The ISO 25010 software quality model assessed the system, focusing on functional suitability, reliability, performance efficiency, and usability. The results returned a very satisfactory score, indicating that BrewBooze effectively simulates beverages, provides accurate flavor recommendations, and predicts startup costs, making it a valuable tool for both consumers and aspiring caf owners.",10.1109/ICSES63760.2024.10910334,"['Conference', 'Review']",Business; Computer Science
A SMART INTERVIEW SIMULATOR USING AI AVATARS AND REAL-TIME FEEDBACK MECHANISMS (AI AVATAR FOR INTERVIEW PREPRATION),2025,"KM Kajal Sahani, Mohammad Sahil Khan, Sanchay Khatwani, Shubham Gupta, Amit Dubey","An example of such a tool is an application founded on a 3D avatar that provides a simulated interview. Such a platform provides users with the opportunity to engage with a virtual interviewer within a secure setting. Throughout the interview, the system gives them immediate feedback and grades their performance. React.js, which is a very popular and used JavaScript library, builds a responsive and smooth user interface. OpenAI GPT-3, which is a very advanced language model, assists in providing natural questions and answers, thus making the interview look genuine. Three.js is used to render the 3D animated avatar, providing a visual and interactive experience. With the combination of all these tools, the website provides a content-filled experience that can assist users in preparing for actual interviews better. The AI interview tool is particularly helpful for job applicants, students, and working professionals. Live feedback corrects them in the process of practicing improvement on responses, body language, and demeanour. Rather than practicing with friends or reading off guides, the users get the hands-on practice in a simulated environment.",10.29121/ijetmr.v12.i5.2025.1618,['JournalArticle'],Computer Science
LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation,2025,"Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacsu, Harry Mayne, Ryan Kearns, Andrew M. Bean, Adam Mahdi",,10.48550/arXiv.2503.02972,"['JournalArticle', 'Review']",Computer Science; Linguistics
Research on methods and applications of question answering system in the context of ChatGPT,2023,Xiangyu Li,"In the 21st century, there has been a growing importance placed on the ""body"" of artificial intelligence, particularly as it relates to language processing. Researchers have developed various machine learning models with a focus on language understanding, including Large Language Model (LLM), Bidirectional Encoder Representation from Transformers (BERT), and Natural Language Processing (NLP). These models have led to the development of numerous applications, such as ChatGPT-3.5, which has recently gained widespread attention. In addition to ChatGPT, other applications have also benefited from these language processing models, including Question Answering Systems (QAS). This paper will examine three QAS that have been enhanced by the context of ChatGPT, discuss the relevant applications, and analyze these different applications in order to predict future trends in this field. One notable QAS is OpenAI's GPT-3-powered AI that can answer questions about any topic. This application leverages the capabilities of GPT-3 to provide accurate and informative responses to a wide range of questions. Another QAS is IBM's Watson, which utilizes natural language processing and machine learning algorithms to understand and respond to user queries. Watson has been used in various industries, including healthcare, finance, and retail. A third QAS is Google's BERT-based system, which uses pre-trained language models to improve its responses to user queries. This system has been integrated into Google Search and other products, allowing users to receive more precise and relevant search results. Overall, the development of these QAS and other language processing applications marks an exciting period of progress in the field of artificial intelligence. As researchers continue to refine these models and explore new applications, we can expect to see even more advanced and sophisticated language processing systems emerge in the future.",10.54254/2755-2721/18/20230964,['JournalArticle'],Computer Science
"Literature Study on AI Mechanisms, Consciousness, and Emotion Integration in Chat GPT",2025,"Angelicha Putri Dewi Ivanka, Jenar Mahesa Ayu, Sarah Surya Rabbani, Muhammad Darwis","The development of artificial intelligence (AI), particularly ChatGPT, demonstrates the ability to generate responses that resemble human emotional understanding and raises questions about the integration of consciousness, emotions, and algorithms in the context of singularity. This study aims to analyze how AI builds the illusion of consciousness and emotional closeness through computational mechanisms and its impact on human-AI interactions across various sectors. The method used is a structured literature review, examining academic journals, official reports, and the latest technical documentation classified by technical domain, including model architecture, emotion simulation, ethical implications, and publication year to assess its developmental dynamics. The results show that ChatGPT is capable of simulating empathy through affective computing and language prediction patterns, but it does not possess subjective emotional experiences like humans. This illusion of emotional closeness has proven beneficial in enhancing the effectiveness of interactions in education, public services, and healthcare, although it also poses risks such as emotional manipulation, data bias, and unrealistic empathy standards. The discussion emphasizes that the term empathy in AI should be understood technically as a data-driven adaptive response, not authentic emotional experience, and thus must be distinguished from human empathy. Critical analysis also reveals contradictions between AI's effectiveness in mimicking human behavior and its limitations in achieving genuine emotional connection. The discussion emphasizes that the term empathy in AI should be understood technically as a data-driven adaptive response, not an authentic emotional experience, and therefore needs to be distinguished from human empathy. Critical analysis also reveals a contradiction between AI's effectiveness in mimicking emotional behavior and its limitations in understanding meaning and consciousness at a deeper level. Therefore, this research contributes to the field of Computer Science by presenting a conceptual synthesis that clarifies both the limitations and potential of AI, while offering a foundation for designing more ethical interaction systems and developing risk assessment models in vulnerable sectors.",10.52436/1.jutif.2025.6.4.4985,"['JournalArticle', 'Review']",Computer Science; Psychology
AdaFlowLite: Scalable and Non-Blocking Inference on Asynchronous Mobile Data,2025,"Sicong Liu, F. Wu, Yuan Gao, Bin Guo, Zimu Zhou, Hongkai Wen, Zhiwen Yu","The rise of mobile devices equipped with numerous sensors, such as LiDAR and cameras, has driven the adoption of multi-modal deep intelligence for distributed sensing tasks, such as smart cabins and driving assistance. However, the arrival time of mobile sensory data vary due to modality size and network dynamics, which can lead to delays (if waiting for slow data) or accuracy decline (if inference proceeds without waiting). Moreover, the diversity and dynamic nature of mobile systems exacerbate this challenge. In response, we present a shift to <italic>opportunistic</italic> inference for asynchronous distributed multi-modal data, enabling inference as soon as partial data arrives. While existing methods focus on optimizing modality consistency and complementarity, known as modal affinity, they lack a <italic>computational</italic> approach to control this affinity in open-world mobile environments. <inline-formula><tex-math notation=""LaTeX"">${\sf AdaFlowLite}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""sans-serif"">AdaFlowLite</mml:mi></mml:math><inline-graphic xlink:href=""guo-ieq1-3582060.gif""/></alternatives></inline-formula> pioneers the formulation of structured cross-modality affinity in mobile contexts using a hierarchical analysis-based normalized matrix. This approach accommodates the diversity and dynamics of modalities, generalizing across different types and numbers of inputs. Employing an multi-modal lightweight Swin Transformer (MMLST), <inline-formula><tex-math notation=""LaTeX"">${\sf AdaFlowLite}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""sans-serif"">AdaFlowLite</mml:mi></mml:math><inline-graphic xlink:href=""guo-ieq2-3582060.gif""/></alternatives></inline-formula> facilitates real-time and flexible data imputation, adapting to various modalities and downstream tasks without retraining. Experiments show that <inline-formula><tex-math notation=""LaTeX"">${\sf AdaFlowLite}$</tex-math><alternatives><mml:math><mml:mi mathvariant=""sans-serif"">AdaFlowLite</mml:mi></mml:math><inline-graphic xlink:href=""guo-ieq3-3582060.gif""/></alternatives></inline-formula> significantly reduces inference latency by up to 80.4% and enhances accuracy by up to 62.1%, while achieving nearly a 50% reduction in energy consumption, outperforming status quo approaches. Also, this method can enhance LLM performance to preprocess asynchronous data.",10.1109/TMC.2025.3582060,['JournalArticle'],Computer Science; Engineering
Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning,2025,"Runtao Ren, Jian Ma, Jianxi Luo","Retrieval-Augmented Generation (RAG) systems in the Intellectual Property (IP) field often struggle with diverse user queries, including colloquial expressions, spelling errors, and ambiguous terminology, leading to inaccurate retrieval and suboptimal responses. To address this challenge, we propose Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a novel framework that leverages large language models (LLMs) to simulate varied user inquiries and fine-tunes retrieval models to align semantically equivalent but linguistically diverse questions. Unlike complex architectural modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining prompt-engineered query generation with hard negative mining to enhance retrieval robustness without costly infrastructure changes. Experimental results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval accuracy on the Patent Consultation dataset and 262.26% improvement on the Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in generation quality over the baselines, respectively. By bridging the gap between user intent and system comprehension through semantic-aware retrieval optimization, MQG-RFM offers a practical, scalable approach for rapid, cost-effective deployment among small and medium-sized agencies seeking reliable patent intelligence solutions. Additionally, our proposed method has already been adopted by ScholarMate, the largest professional research social networking platform in China, to support real-world development and deployment. A demo version of the instantiated is available at https://github.com/renruntao/patent_rag.",10.48550/arXiv.2506.00527,['JournalArticle'],Computer Science; Law
DEQA: Descriptions Enhanced Question-Answering Framework for Multimodal Aspect-Based Sentiment Analysis,2025,"Zhixin Han, Mengting Hu, Yinhao Bai, Xunzhi Wang, Bitong Luo","Multimodal aspect-based sentiment analysis (MABSA) integrates text and images to perform fine-grained sentiment analysis on specific aspects, enhancing the understanding of user opinions in various applications. Existing methods use modality alignment for information interaction and fusion between images and text, but an inherent gap between these two modalities necessitates a more direct bridging mechanism to effectively connect image understanding with text content. For this, we propose the Descriptions Enhanced Question-Answering Framework (DEQA), which generates descriptions of images using GPT-4, leveraging the multimodal large language model to provide more direct semantic context of images. In DEQA, to help the model better understand the task's purpose, we frame MABSA as a multi-turn question-answering problem to add semantic guidance and hints. We input text, image, and description into separate experts in various combinations, allowing each expert to focus on different features and thereby improving the comprehensive utilization of input information. By integrating these expert outputs within a multi-turn question-answering format, we employ a multi-expert ensemble decision-making approach to produce the final prediction results. Experimental results on two widely-used datasets demonstrate that our method achieves state-of-the-art performance. Furthermore, our framework substantially outperforms GPT-4o and other multimodal large language models, showcasing its superior effectiveness in multimodal sentiment analysis.",10.1609/aaai.v39i22.34572,"['JournalArticle', 'Conference']",Computer Science
Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations,2025,"Eun Gyung Kong, Je Won Yeom, Yonghoon Jeon, Taesup Kim","Federated Learning (FL) aims to train models across decentralized clients or devices holding local data without the need for centralized data collection, thus enhancing data privacy and security. However, achieving both generalization and personalization in heterogeneous settings remains a significant challenge. To address this, we introduce FedOT, a novel approach that leverages black-box foundation models. FedOT shares only a global task-dependent classifier across clients while locally adapting features through orthogonal transformations. By enforcing orthogonality, FedOT mitigates gradient conflicts across diverse clients, preserves semantic integrity, and achieves robust performance even in the presence of substantial data heterogeneity. The strategy of combining global and local parameters enables a more balanced approach for both generalization and personalization, outperforming baseline FL methods across multiple benchmarks. Furthermore, our extensive analysis confirms that joint optimization of global classifiers and local orthogonal transformations yields superior performance and suggests broader applicability.",10.48550/arXiv.2505.19888,['JournalArticle'],Computer Science
New applications of artificial intelligence in elections,2025,V. Fedorov,"The purpose of this article is to study artificial intelligence and large language models as a tool 
for political communication in the electoral process. The research methods are the discourse analysis 
of scientific problems of academic research related to the risks and prospects of using large language models in the political sphere, comparative analysis and the method of predictive scripting. 
The theoretical significance of the work is to clarify and expand the understanding of the role 
of artificial intelligence and large language models in ensuring Russia's digital sovereignty. 
The practical significance of the research is to enrich the theory of the information society with new knowledge about the nature of the influence of Russian-language language models on modern political communications and electoral behavior. The author comes to the conclusion that: 
1) The creation of an AI observer's assistant will allow a person to get rid of the routine work with electoral legislation and electoral statistics, freeing up time for qualitative research through personal interviews with candidates, voters and public figures to formulate objective conclusions.; 
2) large language models can be used to explain political processes in order to involve citizens 
in conventional political participation, but the developers of Russian large language models need 
to reconsider their approaches to restrictions on political topics in order to ensure the protection 
of the information space for the independent democratic development of Russia. Summarizing 
the results of the study, the author identified six general principles of composing a high-quality text query, which contributes to the development of the theory of political marketing.",10.12737/2587-6295-2025-9-2-86-96,['JournalArticle'],Computer Science; Political Science
UrbanScore: A Real-Time Personalised Liveability Analytics Platform,2025,Vrinceanu Alin Vladut,"This paper introduces UrbanScore - a real-time web platform that computes a personalised liveability score for any urban address. The system fuses five data streams: (i) address geocoding via Nominatim, (ii) facility extraction from OpenStreetMap through Overpass QL, (iii) segment-level traffic metrics from TomTom Flow v10, (iv) hourly air-quality readings from OpenWeatherMap, and (v) user-declared preference profiles, all persisted in an Oracle 19c relational store. Six sub-scores (air, traffic, lifestyle, education, metro access, surface transport) are derived, adaptively weighted and combined; an OpenAI large-language model then converts the numeric results into concise, user-friendly explanations. A pilot deployment covering the 226 km2 metropolitan area of Bucharest evaluated 3,450 unique addresses over four weeks. Median end-to-end latency was 2.1 s (p95 = 2.9s), meeting the<3 non-functional requirement. Aggregate scores ranged from 34 to 92 (mean 68, SD 11), with high-scoring clusters along metro corridors that pair abundant green space with PM2.5 levels below 35 ug m-3. A detailed case study of the Tineretului district produced an overall score of 91/100 and demonstrated how the narrative layer guides users toward comparable neighbourhoods. Limitations include dependence on third-party API uptime, spatial bias toward well-mapped OSM regions and the absence of noise and crime layers, cited by 18% of survey participants as a desired enhancement. Overall, the results show that open geodata, commercial mobility feeds and conversational AI can be integrated into a performant, explainable decision-support tool that places""liveability analytics""in the hands of every house-hunter, commuter and city planner.",10.48550/arXiv.2508.00857,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
"Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text",2025,"Pia Sommerauer, Giulia Rambelli, Tommaso Caselli","Persona-prompting is a growing strategy to steer LLMs toward simulating particular perspectives or linguistic styles through the lens of a specified identity. While this method is often used to personalize outputs, its impact on how LLMs represent social groups remains underexplored. In this paper, we investigate whether persona-prompting leads to different levels of linguistic abstraction - an established marker of stereotyping - when generating short texts linking socio-demographic categories with stereotypical or non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias framework, we analyze outputs from six open-weight LLMs under three prompting conditions, comparing 11 persona-driven responses to those of a generic AI assistant. To support this analysis, we introduce Self-Stereo, a new dataset of self-reported stereotypes from Reddit. We measure abstraction through three metrics: concreteness, specificity, and negation. Our results highlight the limits of persona-prompting in modulating abstraction in language, confirming criticisms about the ecology of personas as representative of socio-demographic groups and raising concerns about the risk of propagating stereotypes even when seemingly evoking the voice of a marginalized group.",10.48550/arXiv.2509.08484,['JournalArticle'],Computer Science; Linguistics
Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents,2025,"Sarah Mercer, Daniel P. Martin, Phil Swatton","Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee,&Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents'responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.",10.48550/arXiv.2508.00742,"['JournalArticle', 'Review']",Computer Science; Psychology
Comparative Analysis of GenAI Models for Extensions and Validation of EV Battery Characterization Data,2024,"Sandip Sing, Rushikesh Pawar, Umesh N. Hivarkar","Rapid advancement of electric vehicle (EV) technology has propelled the need for reliable and efficient methods of battery data. This has vital importance  to ensure safety aspects and efficient design of EV system. Traditional data collection methods for battery characterization is a large subject for the design of experiments and is often experts skill intensive, time-consuming, and do not allow scalability. This study proposes an approach which bases on Generative Artificial Intelligence (GenAI) for two activities. First, to assist the DOE in characterizing cell/batteries at different C-rates and temperatures considering different degradation rates. Second, for manipulation of characterization data taking into account measurement and data recording errors. The study compares GenAI models like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based (Time-GPT) models in generating and validating EV battery characterization data. This is not a complete replacement for battery testing, as batteries must physically undergo cyclic aging and other tests. The paper explores ability of different GenAI models to accurately capture critical electro-chemical and thermal features. This enables better planning of next characterization experiments and assist in eliminating boundary and intermediate scenarios for cell characterization experiments by generating synthetic data from GenAI models. The model so tuned for cell characteristics can also be extended as data manipulator to re-establish battery characterization data generated from testing experiments, considering sensor issues, data logging issues, data transportation and synchronization issues, etc. The robustness of these models in handling diverse, heterogeneous, and asynchronous datasets sourced from different EV manufacturers, battery chemistries, and specifications are scrutinized. The performance of the models is compared across multiple attributes like execution times, computing resource requirements, accuracy and consistency of generated data, and volume of data required to optimize the models. This study contributes to improve the modelling, simulation, and optimization of EV batteries and enable rapid development of data-driven products specifically for battery health analysis.",10.4271/2024-28-0032,nan,Computer Science; Engineering; Environmental Science
Offline-to-Online Co-Evolutional User Simulator and Dialogue System,2022,"Dafeng Chi, Yuzheng Zhuang, Yao Mu, Bin Wang, Jianzhu Bao, Yasheng Wang, Yuhan Dong, Xin Jiang, Qun Liu, Jianye Hao","Reinforcement learning (RL) has emerged as a promising approach to fine-tune offline pretrained GPT-2 model in task-oriented dialogue (TOD) systems. In order to obtain human-like online interactions while extending the usage of RL, building pretrained user simulators (US) along with dialogue systems (DS) and facilitating jointly fine-tuning via RL becomes prevalent. However, joint training brings distributional shift problem caused by compounding exposure bias. Existing methods usually iterative update US and DS to ameliorate the ensued non-stationarity problem, which could lead to sub-optimal policy and less sample efficiency. To take a step further for tackling the problem, we introduce an Offline-to-oNline Co-Evolutional (ONCE) framework, which enables bias-aware concurrent joint update for RL-based fine-tuning whilst takes advantages from GPT-2 based end-to-end modeling on US and DS. Extensive experiments demonstrate that ONCE builds high-quality loops of policy learning and dialogues data collection, and achieves state-of-the-art online and offline evaluation results on MultiWOZ2.1 dataset. Opensourced code will be implemented with Mindspore (MS, 2022) and released on our homepage.",10.18653/v1/2022.seretod-1.11,nan,Computer Science
Guided Persona-based AI Surveys: Can we replicate personal mobility preferences at scale using LLMs?,2025,"Ioannis Tzachristas, Santhanakrishnan Narayanan, Constantinos Antoniou","This study explores the potential of Large Language Models (LLMs) to generate artificial surveys, with a focus on personal mobility preferences in Germany. By leveraging LLMs for synthetic data creation, we aim to address the limitations of traditional survey methods, such as high costs, inefficiency and scalability challenges. A novel approach incorporating""Personas""- combinations of demographic and behavioural attributes - is introduced and compared to five other synthetic survey methods, which vary in their use of real-world data and methodological complexity. The MiD 2017 dataset, a comprehensive mobility survey in Germany, serves as a benchmark to assess the alignment of synthetic data with real-world patterns. The results demonstrate that LLMs can effectively capture complex dependencies between demographic attributes and preferences while offering flexibility to explore hypothetical scenarios. This approach presents valuable opportunities for transportation planning and social science research, enabling scalable, cost-efficient and privacy-preserving data generation.",10.48550/arXiv.2501.13955,"['JournalArticle', 'Review']",Computer Science; Sociology
Trajectory State Model-based Reinforcement Learning for Truck-trailer Reverse Driving,2024,"Hao Yan, Mohamed A. Zohdy, E. Alhawsawi, Amr Mahmoud","Maneuver the truck-trailer wheeled robot (TTWR) system for reverse trajectory following is a challenging task even for experienced drivers, which requires advanced control strategies to ensure system stability, steering precision, and driving safety. In this paper, we present the integration of a small size network model with trajectory planning and reference state prediction to achieve remarkable sample efficiency compared with pure model-free reinforcement approaches. By employing the trajectory state prediction model as the foundation model to interact with the Proximal Policy Optimization (PPO) framework, we enable reversible access to the Markov Decision Process (MDP) dynamics and help the PPO to better estimate vehicle dynamics used for controller. A numerical simulation is conducted to demonstrate the trajectory following accuracy of the proposed methodology compared with popular industry control methods such as linear-quadratic regulator (LQR). Our results indicate that the proposed hybrid trajectory model based approach not only reduces the need for extensive data collection but also achieves similar control accuracy, suggesting a promising direction for future research in autonomous vehicle control, emphasizing the need for efficient, adaptable, and robust learning algorithms.",10.1109/ICRCA60878.2024.10649133,"['JournalArticle', 'Conference']",Computer Science; Engineering
Artificial intelligence for virtual reality: a review,2025,"Lili Wang, Weiwei Xu, Yebin Liu, Miao Wang, Beibei Wang, Xubo Yang, Lan Xu, Zhangyao Tan, Runze Fan, Zijun Wang, Chi Wang, Hongwen Zhang, Yijian Wen, Haozhong Yang, Jian Wu, Jiahui Fan, Hui Wang, Qixuan Zhang, Guoping Wang, Yongtian Wang, Qinping Zhao",,10.1007/s11432-024-4541-9,"['JournalArticle', 'Review']",Computer Science; Engineering
Generative Artificial Intelligence: Transforming the Future,2024,Kaviyaraj R,"We are at the dawn of a new area of AI: Generative Artificial Intelligence (AI), which holds the potential to automate and generate in ways never before possible and can be applied across countless disciplines from natural language processing (NLP) to image synthesis, interactive simulations, and more. Generative models like Generative Adversarial Networks and transformers, as well as cutting edge AI tools such as GitHub Copilot, DALL-E, Gemini, and Bing Copilot have made content generation a different game. From realizing intelligent code generation, realistic image synthesis, or personalized search, these tools are transforming industries to software development and digital art, to search engines. In this paper we present a comprehensive survey of generative AI technologies and the methodologies behind these technologies, including attention mechanisms, variational autoencoders, diffusion models, and transformer-based models such as GPT and BERT. The paper also discusses the implementation of generative AI across Augmented Reality and Virtual Reality, highlighting how the use of AI to generate content is revolutionizing the interactive experiences in gaming, education, and training. In the study we also perform performance evaluations of generative AI models on benchmark datasets using BLEU, ROUGE scores etc. We discuss some of the challenges facing generative AI on ethical, computational, and too-often opaque lines and predict that generative AI can transform industries by creating autonomous creativity and personalized learning.",10.1109/EmergIN63207.2024.10961802,"['Conference', 'Review']",Art; Computer Science
"Powerful tools for personalisation: Using large language model-based agents, knowledge graphs and customer signals to connect with users",2024,"Seth Earley, Sanjay Mehta","This paper discusses how large language models (LLMs) agentic workflows powering ChatGPT types of applications can use a combination of enterprise data sources to hyper-personalise information at scale for customers or employees. Typical use cases include marketing communications, customer support, content creation and digital assistants. The approaches described are at one level established in theory; however, practical adoption has been challenging and the combination of templated prompts with LLMs and agent call outs to external application programming interfaces and knowledge sources are new. The data sources using these approaches include knowledge, content and transactional data with near real time and real time customer signals. Customer signal data can include first, second or third party data that describes the characteristics of a customer or employee, as well as real time digital body language  click paths, searches, responses to campaigns and chatbot dialogues. Two use cases in two industries  automotive and industrial manufacturing  will be detailed to illustrate how the same principles and approaches can be applied in situations that are very different, and how a knowledge architecture combined with retrieval augmented generation (RAG) should be developed and applied. Analytics to monitor outcomes and enable manual and automated course corrections will be discussed. The outcomes are unified and contextualised experiences realising the sometimes ambitious designs of user experience developers. It is easier to storyboard a design than it is to make it a reality. Marketing organisations are more and more responsible for the end-to-end customer journey and experience. However, the customer journey is a knowledge journey. At each step of the process, they are asking questions about the company, product or service. What product and solutions do you offer? Which ones are right for me? How do I choose a particular offering? How do I purchase or procure the product or service? How can I maintain it, and get service or support? How do I get the most from my purchase? What are the options for upgrading or enhancing my solution? These are marketing communications that consist of educating the prospect rather than selling to them. Today's prospects are empowered with greater information and understanding of offerings and the competition than ever before. Marketing is therefore responsible for helping them make the decision based on information and references that are presented at each stage of the journey.",10.69554/nmce9908,['JournalArticle'],Computer Science
Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning,2025,"Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao","Big trajectory data hold great promise for human mobility analysis, but their utility is often constrained by the absence of critical traveler attributes, particularly sociodemographic information. While prior studies have explored predicting such attributes from mobility patterns, they often overlooked underlying cognitive mechanisms and exhibited low predictive accuracy. This study introduces SILIC, short for Sociodemographic Inference with LLM-guided Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a theoretically grounded framework that leverages LLMs to infer sociodemographic attributes from observed mobility patterns by capturing latent behavioral intentions and reasoning through psychological constructs. Particularly, our approach explicitly follows the Theory of Planned Behavior (TPB), a foundational behavioral framework in transportation research, to model individuals' latent cognitive processes underlying travel decision-making. The LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space. Evaluated in the 2017 Puget Sound Regional Council Household Travel Survey, our method substantially outperforms state-of-the-art baselines and shows great promise for enriching big trajectory data to support more behaviorally grounded applications in transportation planning and beyond.",10.48550/arXiv.2505.17249,"['JournalArticle', 'Review']",Computer Science; Sociology
Brief state of the art in social information mining: Practical application in analysis of trends in French legislative 2024,2024,Jose A. Garcia Gutierrez,"The analysis of social media information has undergone significant evolution in the last decade due to advancements in artificial intelligence (AI) and machine learning (ML). This paper provides an overview of the state-of-the-art techniques in social media mining, with a practical application in analyzing trends in the 2024 French legislative elections. We leverage natural language processing (NLP) tools to gauge public opinion by extracting and analyzing comments and reactions from the AgoraVox platform. The study reveals that the National Rally party, led by Marine Le Pen, maintains a high level of engagement on social media, outperforming traditional parties. This trend is corroborated by user interactions, indicating a strong digital presence. The results highlight the utility of advanced AI models, such as transformers and large language models (LLMs), in capturing nuanced public sentiments and predicting political leanings, demonstrating their potential in real-time reputation management and crisis response.",10.48550/arXiv.2408.01911,"['JournalArticle', 'Review']",Computer Science; Political Science
LLM-Powered CPI Prediction Inference with Online Text Time Series,2025,"Yingying Fan, Jinchi Lv, Ao Sun, Yurou Wang","Forecasting the Consumer Price Index (CPI) is an important yet challenging task in economics, where most existing approaches rely on low-frequency, survey-based data. With the recent advances of large language models (LLMs), there is growing potential to leverage high-frequency online text data for improved CPI prediction, an area still largely unexplored. This paper proposes LLM-CPI, an LLM-based approach for CPI prediction inference incorporating online text time series. We collect a large set of high-frequency online texts from a popularly used Chinese social network site and employ LLMs such as ChatGPT and the trained BERT models to construct continuous inflation labels for posts that are related to inflation. Online text embeddings are extracted via LDA and BERT. We develop a joint time series framework that combines monthly CPI data with LLM-generated daily CPI surrogates. The monthly model employs an ARX structure combining observed CPI data with text embeddings and macroeconomic variables, while the daily model uses a VARX structure built on LLM-generated CPI surrogates and text embeddings. We establish the asymptotic properties of the method and provide two forms of constructed prediction intervals. The finite-sample performance and practical advantages of LLM-CPI are demonstrated through both simulation and real data examples.",10.48550/arXiv.2506.09516,"['JournalArticle', 'Review']",Computer Science; Economics; Mathematics
Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future,2025,"Guoping Xu, J. Udupa, Yajun Yu, H. Shao, Songlin Zhao, Wei Liu, You Zhang","Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.",10.48550/arXiv.2507.22792,"['JournalArticle', 'Review']",Computer Science
Empowering Smart Retail: Leveraging Large Language Models for Intelligent Shopping Assistants,2025,Sohag Maitra,"The rapid evolution of smart retail has created a growing demand for intelligent, responsive, and personalized shopping experiences. This study explores the integration of Large Language Models (LLMs), particularly GPT-based architectures, into retail ecosystems to power intelligent shopping assistants. A domain-specific LLM was fine-tuned and deployed in a simulated retail environment, where it handled natural language queries, offered product recommendations, and maintained multi-turn conversations. The system was evaluated across five key dimensions: response accuracy, personalization, context retention, response time, and user satisfaction. Results showed an average response accuracy of 91.3%, with strong personalization alignment (87.1%) and over 96% context retention in multi-turn dialogues. User surveys indicated high satisfaction with interaction quality, ease of use, and recommendation relevance. Compared to traditional rule-based systems, the LLM assistant demonstrated superior performance in contextual understanding and user engagement, albeit with a marginal increase in response time. These findings highlight the viability of LLMs as a foundation for scalable, intelligent customer service in retail. The study concludes by emphasizing the importance of ethical deployment and future optimization to enhance accessibility and real-time performance.",10.52783/jisem.v10i4.9302,"['JournalArticle', 'Review']",Business; Computer Science
Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models,2025,"Yang Zhao, Pu Wang, Yibo Zhao, Hongru Du, Hao Yang","Predicting crash events is crucial for understanding crash distributions and their contributing factors, thereby enabling the design of proactive traffic safety policy interventions. However, existing methods struggle to interpret the complex interplay among various sources of traffic crash data, including numeric characteristics, textual reports, crash imagery, environmental conditions, and driver behavior records. As a result, they often fail to capture the rich semantic information and intricate interrelationships embedded in these diverse data sources, limiting their ability to identify critical crash risk factors. In this research, we propose TrafficSafe, a framework that adapts LLMs to reframe crash prediction and feature attribution as text-based reasoning. A multi-modal crash dataset including 58,903 real-world reports together with belonged infrastructure, environmental, driver, and vehicle information is collected and textualized into TrafficSafe Event Dataset. By customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves a 42% average improvement in F1-score over baselines. To interpret these predictions and uncover contributing factors, we introduce TrafficSafe Attribution, a sentence-level feature attribution framework enabling conditional risk analysis. Findings show that alcohol-impaired driving is the leading factor in severe crashes, with aggressive and impairment-related behaviors having nearly twice the contribution for severe crashes compared to other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal features during model training, guiding strategic crash data collection for iterative performance improvements. The proposed TrafficSafe offers a transformative leap in traffic safety research, providing a blueprint for translating advanced AI technologies into responsible, actionable, and life-saving outcomes.",10.48550/arXiv.2505.12545,['JournalArticle'],Computer Science; Engineering; Environmental Science
Exploring the Natural Language Generation: Current Trends and Research Challenges,2024,"Usama Azhar, Ambreen Nazir","Natural Language Generation (NLG) acts as a bridge between input data and human communication. NLG systems are meant to generate human-understandable output making it a useful technique mainly in report generation, automatic replies to queries like Chatbots, and other domains that involve Natural Language Processing (NLP). NLG continues to evolve; embracing the latest trends like neural networks, large language models, personalized features, cognitive architectures, and emotional intelligence, but it still faces some challenges with its implementations and operations for cohesive integration. Therefore, this survey presents a brief introduction to some of the latest trends and challenges faced by NLG which are playing a dynamic role in shaping the evolution of NLG.",10.1109/ICECT61618.2024.10581073,"['Conference', 'Review']",Computer Science; Linguistics
Designing Emotionally Adaptive Chatbots for Diverse Users: A User-Centered Human-AI Interface Framework,2025,"Priyanka Deshmukh, Bhavana Karmore, Mahendra Ingole, Kamal Upreti","Recent advancements in conversational AI have improved task efficiency but often neglect the emotional and cognitive diversity of users. This research introduces a novel, user-centered framework for emotionally adaptive chatbots that integrates ML-based emotion recognition with personalized responses that are ethically filtered  meaning they are designed to respect user privacy, fairness, and transparency principles. The Berlin Emotional Speech Database (EmoDB) was used to train and evaluate three machine learning models using MFCC features. Among them, the XGBoost model achieved the highest classification accuracy of 77.6%, outperforming Random Forest (75.0%) and SVM (68.2%). To evaluate user experience, a dataset of 385 participants was generated using a 15-item Likert-scale questionnaire adapted from the UTAUT model and extended with trust and emotional alignment measures. Statistical tests, including a t-test (p = 0.711) between neurodiverse and non-neurodiverse users and an ANOVA (p = 0.337) across domains, confirmed the consistency and inclusivity of perceived satisfaction. Visual analytics, including correlation heatmaps and radar charts, revealed that users with predicted emotions such as happiness and neutral reported the highest satisfaction scores (mean = 4.49, SD = 0.29 and mean = 4.26, SD = 0.31, respectively). A seven-layered modular architecture was proposed, supporting real-time emotional adaptivity, personalization, and ethical compliance. The framework is integration-ready with NLP engines like GPT and Dialogflow, offering a scalable solution for affective AI deployment across healthcare, education, and public service domains.",10.31127/tuje.1715271,['JournalArticle'],Computer Science; Psychology
Prediction of Public Opinion Event Types Combining Retrieval-Augmented Generation and Large Language Models,2025,"Qiang Huo, Long Zhang, Qiusheng Zheng","The prediction of public opinion events plays a crucial role in social governance, public safety, and enterprise management. Accurate prediction of event types helps identify potential crises, effectively guide public opinion, and provide scientific support for decision-makers. This study leverages large language models and introduces a Retrieval-Augmented Generation (RAG) system to propose a novel method for improving the accuracy of public opinion event prediction. Using a small amount of labeled data, the method retrieves relevant data slices to dynamically generate high-quality few-shot prompts, enabling the language model to classify events more accurately and enhance its predictive capabilities. The findings demonstrate that effective few-shot prompt generation significantly optimizes the application of RAG systems in public opinion analysis, offering a new perspective for event prediction and showcasing its potential for real-world applications.",10.1145/3729706.3729768,"['Book', 'Conference']",Computer Science; Political Science
Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models,2025,"Yi Feng, Jiaqi Wang, Wenxuan Zhang, Zhuang Chen, Yutong Shen, Xiyao Xiao, Minlie Huang, Liping Jing, Jian Yu","Recent progress in large language models (LLMs) has opened new possibilities for mental health support, yet current approaches lack realism in simulating specialized psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps individuals transform problematic life stories into empowering alternatives, remains underutilized due to limited access and social stigma. We address these limitations through a comprehensive framework with two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative therapists by planning therapeutic stages, guiding reflection levels, and generating contextually appropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-centric evaluation method that quantifies effectiveness by tracking""Innovative Moments""(IMs), critical narrative shifts in client speech signaling therapy progress. Experimental results on 260 simulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-quality support conversations to facilitate social applications.",10.48550/arXiv.2507.20241,['JournalArticle'],Computer Science; Psychology
A Survey on Memory-Efficient Transformer-Based Model Training in AI for Science,2025,"Kaiyuan Tian, Linbo Qiao, Baihui Liu, Gongqingjian Jiang, Shanshan Li, Dongsheng Li","Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews transformer-based LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. This survey systematically reviews and categorizes memory-efficient pre-training techniques for large-scale transformers, including algorithm-level, system-level, and hardware-software co-optimization. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. By bridging model efficiency and scientific application needs, we hope to provide insights for scalable and cost-effective LLM training in AI for science.",10.1007/s11704-025-50302-6,['Review'],Computer Science
Foundation Model Driven Robotics: A Comprehensive Review,2025,"Muhammad Tayyab Khan, Ammar Waheed","The rapid emergence of foundation models, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), has introduced a transformative paradigm in robotics. These models offer powerful capabilities in semantic understanding, high-level reasoning, and cross-modal generalization, enabling significant advances in perception, planning, control, and human-robot interaction. This critical review provides a structured synthesis of recent developments, categorizing applications across simulation-driven design, open-world execution, sim-to-real transfer, and adaptable robotics. Unlike existing surveys that emphasize isolated capabilities, this work highlights integrated, system-level strategies and evaluates their practical feasibility in real-world environments. Key enabling trends such as procedural scene generation, policy generalization, and multimodal reasoning are discussed alongside core bottlenecks, including limited embodiment, lack of multimodal data, safety risks, and computational constraints. Through this lens, this paper identifies both the architectural strengths and critical limitations of foundation model-based robotics, highlighting open challenges in real-time operation, grounding, resilience, and trust. The review concludes with a roadmap for future research aimed at bridging semantic reasoning and physical intelligence through more robust, interpretable, and embodied models.",10.48550/arXiv.2507.10087,"['JournalArticle', 'Review']",Computer Science; Engineering
Forecast-Then-Optimize Deep Learning Methods,2025,"Jinhang Jiang, Nan Wu, Ben Liu, Mei Feng, Xin Ji, Karthik Srinivasan","Time series forecasting underpins vital decision-making across various sectors, yet raw predictions from sophisticated models often harbor systematic errors and biases. We examine the Forecast-Then-Optimize (FTO) framework, pioneering its systematic synopsis. Unlike conventional Predict-Then-Optimize (PTO) methods, FTO explicitly refines forecasts through optimization techniques such as ensemble methods, meta-learners, and uncertainty adjustments. Furthermore, deep learning and large language models have established superiority over traditional parametric forecasting models for most enterprise applications. This paper surveys significant advancements from 2016 to 2025, analyzing mainstream deep learning FTO architectures. Focusing on real-world applications in operations management, we demonstrate FTO's crucial role in enhancing predictive accuracy, robustness, and decision efficacy. Our study establishes foundational guidelines for future forecasting methodologies, bridging theory and operational practicality.",10.48550/arXiv.2506.13036,"['JournalArticle', 'Review']",Business; Computer Science
Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval,2025,"Kirandeep Kaur, Preetam Prabhu Srikar Dammu, Hideo Joho, Chirag Shah","Personalized AI agents are becoming central to modern information retrieval, yet most evaluation methodologies remain static, relying on fixed benchmarks and one-off metrics that fail to reflect how users'needs evolve over time. These limitations hinder our ability to assess whether agents can meaningfully adapt to individuals across dynamic, longitudinal interactions. In this perspective paper, we propose a conceptual lens for rethinking evaluation in adaptive personalization, shifting the focus from static performance snapshots to interaction-aware, evolving assessments. We organize this lens around three core components: (1) persona-based user simulation with temporally evolving preference models; (2) structured elicitation protocols inspired by reference interviews to extract preferences in context; and (3) adaptation-aware evaluation mechanisms that measure how agent behavior improves across sessions and tasks. While recent works have embraced LLM-driven user simulation, we situate this practice within a broader paradigm for evaluating agents over time. To illustrate our ideas, we conduct a case study in e-commerce search using the PersonalWAB dataset. Beyond presenting a framework, our work lays a conceptual foundation for understanding and evaluating personalization as a continuous, user-centric endeavor.",10.1145/3767695.3769484,nan,Computer Science
ChatGPT TECHNOLOGY AS A STRATEGIC TOOL IN ECOTOURISM DESTINATION MARKETING: A ROMANIAN CASE STUDY,2024,"A. Candrea, I. Ivasciuc, A. Ispas","In the era of digital communication, the utilization of technology in tourism marketing strategies has gained paramount importance. As digital technologies continue to evolve, their impact on marketing strategies across various sectors has become increasingly significant. The practical potential of AI-generated content may be considered by destination marketing practitioners to create personalized, effective campaigns that resonate with diverse audiences, enhancing their connection with travelers. Therefore, this paper specifically examines how advanced artificial intelligence, such as OpenAIs GPT-3, is shaping the future of destination marketing in the ecotourism industry. The main purpose of this study is to ascertain how Destination Marketing Organizations (DMOs) can utilize Chat GPT-3 to produce engaging and targeted content on Facebook, thereby increasing visitor engagement. The methodology involves a mixed approach, starting with the analysis of existing content on selected ecotourism Facebook pages, followed by the creation and evaluation of new GPT-3 generated posts. Data was gathered through secondary research and surveys administered to administrators from seven ecotourism destinations in Romania. The study aims to determine how AIgenerated content can influence user interaction and preferences, offering insights into the practical applications of Chat GPT-3 in destination marketing. This research is significant as it provides empirical data on the integration and impact of Chat GPT-3 technology in the promotion of ecotourism, offering actionable insights for DMOs to enhance their digital marketing strategies.",10.35603/sws.iscss.2024/s16/91,"['Conference', 'Review']",Business; Computer Science; Environmental Science
"Human Behavior Analysis: A Comprehensive Survey on Techniques, Applications, Challenges, and Future Directions",2025,"Siham Essahraui, Ismail Lamaakal, Yassine Maleh, Khalid El Makkaoui, Mouncef Filali Bouami, Ibrahim Ouahbi, A. A. Abd El-Latif, May Almousa, J. Rodrigues",,10.1109/ACCESS.2025.3589938,"['JournalArticle', 'Review']",Computer Science; Psychology; Sociology
Remote Sensing for Building Energy Efficiency Assessment: A Multi-modal Multi-task Approach,2025,"Ziqi Gu, Zhaiyu Chen, Yilei Shi, Xiao Xiang Zhu","Buildings account for a substantial portion of global energy consumption, underscoring the importance of accurately assessing their energy efficiency. Traditional approaches for evaluating building energy efficiency typically rely on extensive tabular data gathered through labor-intensive on-site surveys, impeding their practicality for large-scale applications. Remote sensing data has emerged as a promising alternative due to its extensive spatial coverage and growing availability. However, existing approaches that utilize single-modality remote sensing data are limited in their capacity of predicting building energy efficiency. In this study, we propose a novel multi-modal, multitask neural network to model building energy efficiency using remote sensing sources. Our approach builds upon three key pillars: the integration of diverse remote sensing data, the implicit inclusion of tabular attributes during training and the application of foundation models for feature extraction. Extensive experiments on data from the city of Peterborough demonstrate the effectiveness of our method, achieving an F1 score of 68.14% for energy-efficiency classification. These results highlight our approachs potential to enhance building energy modeling, providing a scalable and efficient alternative to traditional energy assessment techniques.",10.1109/JURSE60372.2025.11076072,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
LLM-Based Community Surveys for Operational Decision Making in Interconnected Utility Infrastructures,2025,"Adaeze Okeukwu-Ogbonnaya, Rahul Amatapu, Jason Bergtold, George Amariucai","We represent interdependent infrastructure systems and communities alike with a hetero-functional graph (HFG) that encodes the dependencies between functionalities. This graph naturally imposes a partial order of functionalities that can inform the sequence of repair decisions to be made during a disaster across affected communities. However, using such technical criteria alone provides limited guidance at the point where the functionalities directly impact the communities, since these can be repaired in any order without violating the system constraints. To address this gap and improve resilience, we integrate community preferences to refine this partial order from the HFG into a total order. Our strategy involves getting the communities'opinions on their preferred sequence for repair crews to address infrastructure issues, considering potential constraints on resources. Due to the delay and cost associated with real-world survey data, we utilize a Large Language Model (LLM) as a proxy survey tool. We use the LLM to craft distinct personas representing individuals, each with varied disaster experiences. We construct diverse disaster scenarios, and each simulated persona provides input on prioritizing infrastructure repair needs across various communities. Finally, we apply learning algorithms to generate a global order based on the aggregated responses from these LLM-generated personas.",10.48550/arXiv.2507.13577,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM Reasoning,2025,"Yifei Xu, Jiaying Wu, Herun Wan, Yang Li, Zhen Hou, Min-Yen Kan","Hashtag trends ignite campaigns, shift public opinion, and steer millions of dollars in advertising spend, yet forecasting which tag goes viral is elusive. Classical regressors digest surface features but ignore context, while large language models (LLMs) excel at contextual reasoning but misestimate numbers. We present BuzzProphet, a reasoning-augmented hashtag popularity prediction framework that (1) instructs an LLM to articulate a hashtag's topical virality, audience reach, and timing advantage; (2) utilizes these popularity-oriented rationales to enrich the input features; and (3) regresses on these inputs. To facilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated from social media. Across diverse regressor-LLM combinations, BuzzProphet reduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while producing human-readable rationales. Results demonstrate that using LLMs as context reasoners rather than numeric predictors injects domain insight into tabular models, yielding an interpretable and deployable solution for social media trend forecasting.",10.1145/3746252.3760970,nan,Computer Science
The process of frame-building regarding climate change in Indonesia,2024,Mira Rochyadi-Reetz,"This study examines the frame-building process regarding climate change in Indonesia, an emerging country in the Global South that produces significant carbon emissions and is one of the nations most affected by the climate crisis. Through a quantitative content analysis of press releases from the Indonesian government and environmental nongovernmental organizations (NGOs) and media coverage of climate change, this study identifies three frames promoted by policy actors and five frames used by in-house journalists from print and online media organizations in Indonesia. A comparison of these frame sets shows that journalists often use frames on climate change that are not promoted by either political actors or NGOs. Nevertheless, the government is the most important source for journalists on climate change reporting. Next, a second step investigates how different contextual conditions influence the collaboration between journalists and their sources in the frame-creation process to examine it in more detail. Hence, this study relies on the hierarchy of influence (HI) and intereffication (IE) models. To this end, journalists, academics, and PR officers from Indonesias government organizations, environmental NGOs, and palm oil lobby organizations are interviewed. The results show that organizational structure, financial resources, and personal networks influence how successfully PR officers disseminate their frames, while journalists frame selection is influenced by their media affiliations, routines, professional experience, and interests. This study also discusses how macro conditions in Indonesia influence these micro- and meso-level processes, such as the social system and distinct cultural and historical contexts.",10.5771/2192-4007-2024-2-125,['JournalArticle'],Environmental Science; Political Science
Research on Conversational Recommender System Considering Consumer Types,2025,"Yaying Luo, Hui Fang, Zhu Sun","Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users'heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--dependent, efficient, cautious, and expert--derived from two dimensions: decision-making style (maximizers vs. satisficers) and knowledge level (high vs. low). CT-CRS employs interaction histories and fine-tunes the large language model to automatically infer user types in real time, avoiding reliance on static questionnaires. We incorporate user types into state representation and design a type-adaptive policy that dynamically adjusts recommendation granularity, diversity, and attribute query complexity. To further optimize the dialogue policy, we adopt Inverse Reinforcement Learning (IRL), enabling the agent to approximate expert-like strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book, and Yelp show that CTCRS improves recommendation success rate and reduces interaction turns compared to strong baselines. Ablation studies confirm that both consumer type modeling and IRL contribute significantly to performance gains. These results demonstrate that CT-CRS offers a scalable and interpretable solution for enhancing CRS personalization through the integration of psychological modeling and advanced policy optimization.",10.48550/arXiv.2508.13209,['JournalArticle'],Computer Science
Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation,2025,"Adib Bazgir, Amir Habibdoust, Yuwen Zhang, Xing Song","Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.",10.48550/arXiv.2509.00987,"['JournalArticle', 'Review']",Computer Science
Low-code AI enabling marketing innovation: The mediating role of marketing ambidexterity,2025,"Xiaoyi Wang, Yuzhou Wang, Nannan Li, Yuhan Zhao",,10.1007/s44362-025-00015-4,"['JournalArticle', 'Review']",Business; Computer Science
Foundation Models for Clean Energy Forecasting: A Comprehensive Review,2025,"M. Ferdaus, T. Dam, Md. Rasel Sarkar, M. Uddin, S. Anavatti","As global energy systems transit to clean energy, accurate renewable generation and renewable demand forecasting is imperative for effective grid management. Foundation Models (FMs) can help improve forecasting of renewable generation and demand because FMs can rapidly process complex, high-dimensional time-series data. This review paper focuses on FMs in the realm of renewable energy forecasting, primarily focusing on wind and solar. We present an overview of the architectures, pretraining strategies, finetuning methods, and types of data used in the context of renewable energy forecasting. We emphasize the role of models that are trained at a large scale, domain specific Transformer architectures, where attention is paid to spatial temporal correlations, the embedding of domain knowledge, and also the brief and intermittent nature of renewable generation. We assess recent FM based advancements in forecast accuracy such as reconciling predictions over multiple time scales and quantifying uncertainty in renewable energy forecasting. We also review existing challenges and areas of improvement in long-term and multivariate time series forecasting. In this survey, a distinction between theory and practice is established regarding the use of FMs in the clean energy forecasting domain. Additionally, it critically assesses the strengths and weaknesses of FMs while advancing future research direction in this new and exciting area of forecasting.",10.48550/arXiv.2507.23147,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing Open-Domain Dialogues in Several Languages With No Examples in Targets and No Machine Translation,2025,"Ahmed Njifenjou, Virgile Sucal, B. Jabaian, Fabrice Lefvre","The prevailing paradigm in the domain of Open-Domain Dialogue agents predominantly focuses on the English language, encompassing both models and datasets. Furthermore, the financial and temporal investments required for crowdsourcing such datasets for finetuning are substantial, particularly when multiple languages are involved. Fortunately, advancements in Large Language Models (LLMs) have unveiled a plethora of possibilities across diverse tasks. Specifically, instruction-tuning has enabled LLMs to execute tasks based on natural language instructions, occasionally surpassing the performance of human crowdworkers. Additionally, these models possess the capability to function in various languages within a single thread. Consequently, to generate new samples in different languages, we propose leveraging these capabilities to replicate the data collection process. We introduce a pipeline for generating Open-Domain Dialogue data in multiple Target Languages using LLMs, with demonstrations provided in a unique Source Language. By eschewing explicit Machine Translation in this approach, we enhance the adherence to language-specific nuances. We apply this methodology to the PersonaChat dataset. To enhance the openness of generated dialogues and mimic real life scenarii, we added the notion of speech events corresponding to the type of conversation the speakers are involved in and also that of common ground which represents the premises of a conversation.",10.48550/arXiv.2503.03462,['JournalArticle'],Computer Science; Linguistics
Audit Keamanan Sistem Informasi Euclid Menggunakan Framework Cobit 5 pada PT. XYZ,2023,"N. Arifin, Eki Saputra, Tengku Khairil Ahsyar, Fitriani Mutakkin"," PT. XYZ is a private company engaged in the planting and processing of oil palm fruit. Currently PT. XYZ has implemented a technology called the EuClid system which functions to assist the data collection process and reports for each staff. However, in its implementation, there are still several problems, such as missing submission reports, providers who often experience network interruptions, and data hacking or leaks that have occurred. For this reason, it is necessary to carry out a security audit of EuClids information system at PT. XYZ in order to measure, guarantee, and find the root of the problem also makes recommendations on the information system used. This study uses the COBIT 5 Framework with the DSS05 (Manage Security Service) domain to measure the EuClid Information System Capability Level. The results of achieving the value of each level in the DSS05 domain process are level 1 (performed process) of 62.5%, level 2 (Manage Process) of 65.36%, level 3 (Established Process) of 63.02%, level 4 (Predictable Process) is 62.6%, and level 5 (Optimizing Process) is 59.63%. Achievement at each level is in the range of 50% - 85%, so that the ranking in the ISO/IEC 15501-2-2003",10.35314/isi.v8i1.3229,['JournalArticle'],Computer Science
Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction,2025,"Yiming Xu, Junfeng Jiao","Accurately predicting travel mode choice is essential for effective transportation planning, yet traditional statistical and machine learning models are constrained by rigid assumptions, limited contextual reasoning, and reduced generalizability. This study explores the potential of Large Language Models (LLMs) as a more flexible and context-aware approach to travel mode choice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground predictions in empirical data. We develop a modular framework for integrating RAG into LLM-based travel mode choice prediction and evaluate four retrieval strategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder for re-ranking, and RAG with balanced retrieval and cross-encoder for re-ranking. These strategies are tested across three LLM architectures (OpenAI GPT-4o, o4-mini, and o3) to examine the interaction between model reasoning capabilities and retrieval methods. Using the 2023 Puget Sound Regional Household Travel Survey data, we conduct a series of experiments to evaluate model performance. The results demonstrate that RAG substantially enhances predictive accuracy across a range of models. Notably, the GPT-4o model combined with balanced retrieval and cross-encoder re-ranking achieves the highest accuracy of 80.8%, exceeding that of conventional statistical and machine learning baselines. Furthermore, LLM-based models exhibit superior generalization abilities relative to these baselines. Findings highlight the critical interplay between LLM reasoning capabilities and retrieval strategies, demonstrating the importance of aligning retrieval strategies with model capabilities to maximize the potential of LLM-based travel behavior modeling.",10.48550/arXiv.2508.17527,"['JournalArticle', 'Review']",Computer Science
Humanity in AI: Detecting the Personality of Large Language Models,2024,"Baohua Zhang, Yongyi Huang, Wenyao Cui, Huaping Zhang, Jianyun Shang","Questionnaires are a common method for detecting the personality of Large Language Models (LLMs). However, their reliability is often compromised by two main issues: hallucinations (where LLMs produce inaccurate or irrelevant responses) and the sensitivity of responses to the order of the presented options. To address these issues, we propose combining text mining with questionnaires method. Text mining can extract psychological features from the LLMs' responses without being affected by the order of options. Furthermore, because this method does not rely on specific answers, it reduces the influence of hallucinations. By normalizing the scores from both methods and calculating the root mean square error, our experiment results confirm the effectiveness of this approach. To further investigate the origins of personality traits in LLMs, we conduct experiments on both pre-trained language models (PLMs), such as BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT. The results show that LLMs do contain certain personalities, for example, ChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'. Additionally, we find that the personalities of LLMs are derived from their pre-trained data. The instruction data used to train ChatLLMs can enhance the generation of data containing personalities and expose their hidden personality. We compare the results with the human average personality score, and we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is more similar to that of a human, with score differences of 0.34 and 0.22, respectively.",10.48550/arXiv.2410.08545,['JournalArticle'],Computer Science; Psychology
Semiotics now,2024,"K. Kull, E. Velmezova","We present the results of a survey conducted among professional semioticians from around the world, who were asked to answer two questions: (A) what problems should semiotics solve in the near future? and (B) what are the most important publications on semiotics since 2000? The collection of 63 responses received provides a self-description of contemporary semiotics, as well as some material for scholars to learn from one another. In addition, we add responses from the latest versions of a large language simulator (GPT) as a characteristic feature of the current decade.",10.12697/sss.2024.52.3-4.11,"['JournalArticle', 'Review']",Linguistics
Eloqify: Intelligent Interview Companion,2025,"Dr. J. M. Patil, Eisha Nikam, Samruddhi Katole, Sanika Dose, Vaishnavi Avadhut","In todays highly competitive job market, securing a technical role requires rigorous preparation and real-time feedback. Eloqify is an AI-driven mock interview platform exclusively designed for technical interviews, helping candidates refine their core skills through interactive and unbiased assessments. Our platform leverages Google Gemini AI to dynamically generate technical interview questions based on resume parsing, ensuring that candidates practice with job-specific, industry-relevant questions. The resume parsing, powered by Python-based processing, extracts key skills and experiences, allowing Eloqify to simulate real-world technical interviews tailored to each candidate's expertise. Eloqify is built with Next.js for the backend, React.js for the frontend, and Clerk authentication, ensuring a seamless and secure user experience. The platform features an interactive user dashboard that provides real-time performance insights, including feedback on clarity, problem-solving approach, and technical depth. These insights help candidates identify strengths and areas for improvement, making their preparation structured and goal-oriented. By focusing exclusively on technical assessments, Eloqify ensures that candidates engage in precise, skill-driven mock interviews, bridging the gap between preparation and success. With AI-powered, resume-based question generation and actionable feedback, our platform transforms technical interview readinessmaking it efficient, structured, and highly personalized.",10.47392/irjaeh.2025.0110,['JournalArticle'],Computer Science
Leveraging Immersive Reality and LLMs for Social Reintegration: AI-Driven Skill Development in Correctional Rehabilitation,2025,"Elisa Ayumi Masasi de Oliveira, L. M. F. Cintra, Gustavo Higino Webster Barbosa, Rafael Teixeira Sousa, A. R. G. Filho","This paper explores the use of immersive technolo-gies and AI-driven virtual interactions to aid the social reintegration of incarcerated individuals. By integrating a ChatGPT-based AI within a virtual reality framework, the developed application offers interactive, realistic simulations aimed at fostering essential skills, such as effective communication and vocational abilities. Users interact with a lifelike avatar, designed to simulate scenarios like job interviews and interpersonal dialogues, promoting empathy, self-confidence, and behavioral adaptability. Initial assessments indicate that the application effectively supports engaging and empathetic interactions, which are crucial for preparing individuals for reintegration into society. The application will be open-access, allowing further refinement and adaptability by experts and developers, ultimately aiming to reduce recidivism and support a more humane approach to correctional rehabilitation through technological advancement.",10.1109/AIxVR63409.2025.00049,"['JournalArticle', 'Conference']",Computer Science; Psychology
IndoBERT-based Indonesian Cyberbullying Detection with Multi-stage Labeling,2024,"Yohanes Deny Novandian, Ardytha Luthfiarta, Dhiaka Shabrina Assyifa, Johanes Setiawan, Lailatul Cahyaningrum, Noval Althoff, Mufida Rahayu, Adhitya Nugraha, Rismiyati","The problem of cyberbullying on social media in Indonesia needs to be addressed to protect users from irresponsible behavior. An efficient and effective text classification model is required to detect indications of bullying. This study collected a dataset from Twitter by searching for keywords related to body shaming. The dataset was labeled by using Large Language Model (LLM) and Vader Lexicon approaches for more accurate labeling. Due to the imbalanced dataset labeling, SMOTE was applied for oversampling the data to get a balanced dataset. The detection model was conducted by using IndoBERT. IndoBERT resulted in 96.7% accuracy with fine-tuning. The proposed model was then examined on real data based on a survey related to bullying sentences conducted with 20 students from Dian Nuswantoro University. The results showed that our models accurately predicted that these sentences contained elements of bullying.",10.1109/iSemantic63362.2024.10762553,['Review'],Computer Science
Mapping and Predicting Crimes in Small Cities Using Web Scraping and Machine Learning,2025,"Pedro Ortiz, Leandro Freitas",": This paper presents an approach to municipal crime analysis and prediction through the integration of web scraping techniques and artificial intelligence. Focusing on Alvorada, Brazil, we address the challenge of limited crime data availability in small cities by developing an automated system that extracts and processes crime-related information from local news sources. Our methodology employs the Anthropic Claude AI API for structured data extraction and implements a machine learning model (Random Forest) for crime prediction. The research demonstrates the feasibility of creating crime prediction systems for small cities while identifying temporal and spatial patterns in criminal activity. Additionally, we provide a framework for future improvements through potential law enforcement partnerships and dataset expansion. This study contributes to the growing field of smart city development by offering a replicable methodology for municipalities lacking standardized crime data collection systems.",10.5220/0013421400003929,['JournalArticle'],Computer Science; Law
Automated Test Generation and Marking Using LLMs,2025,"Ioannis Papachristou, G. Dimitroulakos, C. Vassilakis","This paper presents an innovative exam-creation and grading system powered by advanced natural language processing and local large language models. The system automatically generates clear, grammatically accurate questions from both short passages and longer documents across different languages, supports multiple formats and difficulty levels, and ensures semantic diversity while minimizing redundancy, thus maximizing the percentage of the material that is covered in the generated exam paper. For grading, it employs a semantic-similarity model to evaluate essays and open-ended responses, awards partial credit, and mitigates bias from phrasing or syntax via named entity recognition. A major advantage of the proposed approach is its ability to run entirely on standard personal computers, without specialized artificial intelligence hardware, promoting privacy and exam security while maintaining low operational and maintenance costs. Moreover, its modular architecture allows the seamless swapping of models with minimal intervention, ensuring adaptability and the easy integration of future improvements. A requirementscompliance evaluation, combined with established performance metrics, was used to review and compare two popular multilingual LLMs and monolingual alternatives, demonstrating the systems effectiveness and flexibility. The experimental results show that the system achieves a grading accuracy within a 17% normalized error margin compared to that of human experts, with generated questions reaching up to 89.5% semantic similarity to source content. The full exam generation and grading pipeline runs efficiently on consumer-grade hardware, with average inference times under 30 s.",10.3390/electronics14142835,"['JournalArticle', 'Review']",Computer Science
Anomaly Detection in GeoLLM Using Variational Autoencoder,2025,"D. Maladhy, K. Kumari","Geospatial-based Large Language Models are an emerging technology based on the geospatial locations and the different applications based on the locations. From the survey, it is seen that the GeollM produces the misinformation for the requested input due to various factors. The accuracy of the response needs to be high and the LLM should not provide the misinformation regarding the location of the required output. Thus, this paper proposes the anomaly detection of factors that pay off the misinformation using the Variational Autoencoder in the GeoLLM system. The Variational Autoencoder provides the context-based information from the attention model, where the Linear Regression model is used to find the anomalies that are the reason for the misinformation in the GeollM system. The simulation carried out for the proposed work, the anomalies based on the parameters chosen are calculated, and the metrics for the proposed method.",10.1109/ICDICI66477.2025.11135151,"['Conference', 'Review']",Computer Science
"CodeEdu- Code, Compile, Collaborate",2025,Shivam Kashyap,"Abstract This research paper explores the development, functionality, and impact of CodeEdu, an AI-integrated, realtime collaborative platform for coding practice, online assessments, and technical interviews. Developed using the MERN stack with Socket.IO and WebRTC, it features a dynamic DSA problem-solving interface with support for C++, JavaScript, Python, and Java. CodeEdu integrates Google Gemini AI to offer error explanation, solution guidance, and intelligent hint generation. The platforms secure interview module includes real-time proctoring, ESC key tracking, email-based room access, and face-to-face interviews with video calling and live code sharing via sockets. Users can also upload custom problems for personalized practice. With responsive UI, real-time collaboration, and scalable architecture, CodeEdu benefits students, educators, and recruiters alike. Future enhancements include mobile support, code grading, and adaptive AI learning paths to meet evolving academic and industry demands.",10.55041/ijsrem44977,['JournalArticle'],Computer Science
Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation,2025,"G. Guerino, Luiz Rodrigues, B. Capeleti, R. Mello, Andr'e Freire, Luciana A. M. Zaina","Heuristic evaluation is a widely used method in Human-Computer Interaction (HCI) to inspect interfaces and identify issues based on heuristics. Recently, Large Language Models (LLMs), such as GPT-4o, have been applied in HCI to assist in persona creation, the ideation process, and the analysis of semi-structured interviews. However, considering the need to understand heuristics and the high degree of abstraction required to evaluate them, LLMs may have difficulty conducting heuristic evaluation. However, prior research has not investigated GPT-4o's performance in heuristic evaluation compared to HCI experts in web-based systems. In this context, this study aims to compare the results of a heuristic evaluation performed by GPT-4o and human experts. To this end, we selected a set of screenshots from a web system and asked GPT-4o to perform a heuristic evaluation based on Nielsen's Heuristics from a literature-grounded prompt. Our results indicate that only 21.2% of the issues identified by human experts were also identified by GPT-4o, despite it found 27 new issues. We also found that GPT-4o performed better for heuristics related to aesthetic and minimalist design and match between system and real world, whereas it has difficulty identifying issues in heuristics related to flexibility, control, and user efficiency. Additionally, we noticed that GPT-4o generated several false positives due to hallucinations and attempts to predict issues. Finally, we highlight five takeaways for the conscious use of GPT-4o in heuristic evaluations.",10.48550/arXiv.2506.16345,"['JournalArticle', 'Conference']",Computer Science
The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution,2025,"Elon Ezra, Ariel Weizman, A. Azaria","Large language models (LLMs) are commonly evaluated on tasks that test their knowledge or reasoning abilities. In this paper, we explore a different type of evaluation: whether an LLM can predict aspects of its own responses. Since LLMs lack the ability to execute themselves, we introduce the Self-Execution Benchmark, which measures a model's ability to anticipate properties of its output, such as whether a question will be difficult for it, whether it will refuse to answer, or what kinds of associations it is likely to produce. Our experiments show that models generally perform poorly on this benchmark, and that increased model size or capability does not consistently lead to better performance. These results suggest a fundamental limitation in how LLMs represent and reason about their own behavior.",10.48550/arXiv.2508.12277,['JournalArticle'],Computer Science
Towards Transparent Urban Perception: A Concept-Driven Framework with Visual Foundation Models,2025,"Yixin Yu, Zepeng Yu, Xuhua Shi, Ran Wan, Bowen Wang, Jiaxin Zhang","Understanding urban visual perception is crucial for modeling how individuals cognitively and emotionally interact with the built environment. However, traditional survey-based approaches are limited in scalability and often fail to generalize across diverse urban contexts. In this study, we introduce the UP-CBM, a transparent framework that leverages visual foundation models (VFMs) and concept-based reasoning to address these challenges. The UP-CBM automatically constructs a task-specific vocabulary of perceptual concepts using GPT-4o and processes urban scene images through a multi-scale visual prompting pipeline. This pipeline generates CLIP-based similarity maps that facilitate the learning of an interpretable bottleneck layer, effectively linking visual features with human perceptual judgments. Our framework not only achieves higher predictive accuracy but also offers enhanced interpretability, enabling transparent reasoning about urban perception. Experiments on two benchmark datasetsPlace Pulse 2.0 (achieving improvements of +0.041 in comparison accuracy and +0.029 in R2) and VRVWPR (+0.018 in classification accuracy)demonstrate the effectiveness and generalizability of our approach. These results underscore the potential of integrating VFMs with structured concept-driven pipelines for more explainable urban visual analytics.",10.3390/ijgi14080315,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents,2025,"Ian Steenstra, Timothy W. Bickmore","The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk ontology specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the ontology aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this ontology, detailing its grounding, and discuss potential use cases. We discuss four use cases in detail: monitoring real user interactions, evaluation with simulated patients, benchmarking and comparative analysis, and identifying unexpected outcomes. The proposed ontology offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.",10.1145/3717511.3749286,"['JournalArticle', 'Review']",Computer Science; Psychology
NexInterview - AI-Driven Mock Interview Preparation Platform,2025,"Dr. Prakash S, Siranjeevi. K, Vinoth Kumar. N, Pathmesh. G, Ponnarasu. A","NexInterview is an AI-powered interview preparation platform that addresses the limitations of traditional preparation methods by offering personalized mock interviews with voice interaction. Built on Next.js, React, and TypeScript, the system leverages Google Gemini AI for generating role-specific interview questions and feedback, while VAPI enables natural voice conversations. The application guides users through specifying job parameters, conducts realistic mock interviews via AI voice assistants, and provides comprehensive performance analysis. Our development journey involved overcoming challenges in question relevance, natural voice interactions, and meaningful feedback generation. NexInterview demonstrates how AI can create human-centered experiences that enhance career development through practical, personalized preparation in a pressure-free environment.",10.48175/ijarsct-26835,['JournalArticle'],Computer Science
A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems,2025,"DJordje Klisura, Astrid R Bernaga Torres, Anna Karen G'arate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, Anthony Rios","Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information.",10.48550/arXiv.2506.02998,"['JournalArticle', 'Conference']",Computer Science; Linguistics
What does AI consider praiseworthy?,2024,Andrew J. Peterson,"As large language models (LLMs) are increasingly used for work, personal, and therapeutic purposes, researchers have begun to investigate these models' implicit and explicit moral views. Previous work, however, focuses on asking LLMs to state opinions, or on other technical evaluations that do not reflect common user interactions. We propose a novel evaluation of LLM behavior that analyzes responses to user-stated intentions, such as""I'm thinking of campaigning for {candidate}.""LLMs frequently respond with critiques or praise, often beginning responses with phrases such as""That's great to hear!...""While this makes them friendly, these praise responses are not universal and thus reflect a normative stance by the LLM. We map out the moral landscape of LLMs in how they respond to user statements in different domains including politics and everyday ethical actions. In particular, although a na\""ive analysis might suggest LLMs are biased against right-leaning politics, our findings on news sources indicate that trustworthiness is a stronger driver of praise and critique than ideology. Second, we find strong alignment across models in response to ethically-relevant action statements, but that doing so requires them to engage in high levels of praise and critique of users, suggesting a reticence-alignment tradeoff. Finally, our experiment on statements about world leaders finds no evidence of bias favoring the country of origin of the models. We conclude that as AI systems become more integrated into society, their patterns of praise, critique, and neutrality must be carefully monitored to prevent unintended psychological and societal consequences.",10.1007/s43681-025-00682-z,['JournalArticle'],Computer Science
Influence Message Wargaming in the Metaverse: Towards LLM-Based Persuasion and Counter-Persuasion,2025,"Xinyi Liu, Dachun Sun, Tarek F. Abdelzaher","The future metaverse will not only offer immersive environments, but also populate them with interactive agentsvirtual characters that mirror real-world users or social groups. A key application use-case for such agents will be to test social persuasion techniques (for such purposes as marketing, advertisement, and calls to action) by first exploring their effects on simulated target populations in the metaverse environment. Inoculation techniques against undue persuasion can be studied using the same approach. Such applications call for designing believable agents that faithfully reproduce realistic and diverse social behaviors at scale, without requiring large observational data. In this work, we explore whether large language models (LLMs), trained on massive web-scale corpora that embed latent social patterns, can serve as social proxiesagents capable of simulating nuanced group behaviors in sociotechnical information systems with a focus on wargaming both persuasion and mitigation techniques. Our approach generates differentiated, socially plausible responses to persuasive content across different simulated population segments. We pose three central questions: (1) Can LLMs adapt their responses to persuasive content when conditioned on specific community profiles? (2) Can they predict impact of mitigation policies aimed at exposing undue persuasion attempts? (3) Do their predictions align with known social influence theories? While large-scale human validation is beyond the scope of this study, our experiments demonstrate that LLM-generated community-specific responses are both consistent and theory-congruent. These findings position LLMs as promising tools for building adaptive, socially grounded agents in next-generation digital ecosystems, advancing the realism and credibility of social simulations in the metaverse, and calling for research on technical and ethical implications.",10.1109/MetaCom65502.2025.00046,['Conference'],Computer Science; Psychology
A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts,2025,"Rose E. Guingrich, Michael S. A. Graziano","Relationships with social artificial intelligence (AI) agents are on the rise. Most notably, people are forming relationships with companion chatbots such as Replika, a type of social AI agent that people can converse with via text or voice chat. These Large Language Model (LLM) chatbots are designed for companionship, and people have reported forming friendships, mentorships, and romantic partnerships with them. Concerns that companion chatbots may harm or replace real human relationships have been raised, but whether and how these social consequences occur remains unclear. Prior research suggests that peoples state of social need and their anthropomorphism of AI may play a role in how human-AI interaction impacts human-human interaction. In this longitudinal study (N = 183), participants were randomly assigned to a chatbot condition (text chat with a companion chatbot) or to a control condition (engage in text-based word games) in which they engaged in their daily task at least 10 minutes a day for 21 consecutive days. Participants completed four surveys during the 21 days, and engaged in audio recorded interviews on day 1 and 21. We investigated three primary questions. First, do daily interactions with a companion chatbot impact peoples social health and human relationships? Second, do peoples social states such the desire for social connection or loneliness predict anthropomorphism of the chatbot? Third, does anthropomorphism of the chatbot predict human-human social impacts? We found that peoples social health and relationships were not significantly impacted by companion chatbot interactions across 21 days of use. However, people who had a higher desire for social connection, but not higher loneliness, anthropomorphized the chatbot more. Those who anthropomorphized the chatbot more indicated that the human-chatbot interaction had greater impacts on their social interactions and relationships with family and friends. Via a mediation analysis, our results suggest that the impact of human-AI interaction on human-human social outcomes is mediated by the extent to which people anthropomorphize the AI agent, which may be motivated by a desire for social connection.",10.1609/aies.v8i2.36618,"['JournalArticle', 'Conference', 'Review']",Computer Science; Psychology; Sociology
Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection,2025,"Steven Robinson, Antonio Carlos Rivera","The pervasive deployment of large language models (LLMs) in conversational AI systems has revolutionized information access, yet their propensity for generating factually unsupported or hallucinated responses remains a critical impediment to trustworthiness and widespread adoption. This paper introduces Reinforced Unanswerability Learning (RUL), a novel hybrid training paradigm designed to imbue LLMs with the intrinsic capability to accurately detect unanswerable questions and generate reliably appropriate responses. Unlike conventional approaches that rely on external classifiers or simple prompting, RUL integrates a discriminative unanswerability prediction head with the LLM's generative core, guided by a multi-stage learning strategy. This includes supervised fine-tuning on a novel, richly annotated dataset, Enhanced-CAsT-Answerability (ECA), which features hierarchical answerability labels and ground-truth refusal responses. Crucially, RUL incorporates a subsequent reinforcement learning with human feedback (RLHF) phase to refine the nuance, helpfulness, and informativeness of refusal responses. Extensive experiments demonstrate RUL's superior performance, achieving significantly higher accuracy in unanswerability detection across sentence, paragraph, and ranking levels, and substantially increasing the generation of appropriate refusals for unanswerable queries, alongside strong performance on answerable questions. Human evaluations further corroborate RUL's effectiveness, highlighting a marked improvement in perceived helpfulness and trustworthiness, ultimately paving the way for more reliable and user-centric conversational AI.",10.48550/arXiv.2506.01104,['JournalArticle'],Computer Science
Optimalisasi Artificial Intelligence untuk Promosi Produk Ekonomi Kreatif melalui Media Sosial,2025,"Septa Intiar, Ahmad Mansur, Umar Abdul Jabbar","This study aims to analyze the use of Artificial Intelligence (AI) in promoting creative economy products in Central Java's tourist areas, particularly through social media. In today's digital era, developments in information and communication technology, including artificial intelligence, have opened up new opportunities for more efficient, personalized, and data-driven marketing strategies. Creative economy products such as batik, handicrafts, and local culinary delights can gain broader exposure through innovative digital promotional approaches. However, Micro, Small, and Medium Enterprises (MSMEs) in Central Java's tourist areas, such as Jatirejo Village in Semarang, known for its palm fruit (kolang-kaling), still face various challenges in adopting this technology. This study used a descriptive qualitative method with a case study approach. Data were collected through in-depth interviews with MSMEs, field observations, and documentation related to their digital promotional activities. The results indicate that limited human resources, lack of digital literacy, minimal understanding of the concept and application of AI, and inadequate technological infrastructure are the main obstacles in implementing AI for promoting creative economy products. However, this study also identified significant opportunities for optimizing AI implementation. These opportunities include training and mentoring for MSMEs, collaboration between the government, academia, and the private sector, and the development of equitable digital infrastructure. This study provides strategic recommendations to enhance MSME competitiveness in the digital era and strengthen synergies between technology and the creative economy. These findings are expected to serve as an important reference for policymakers and businesses in formulating adaptive and sustainable AI-based promotional strategies.",10.55606/jupsim.v4i2.5106,['JournalArticle'],Business; Computer Science; Economics
WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis,2025,"Liangliang Chen, Huiru Xie, Jacqueline Rohde, Ying Zhang","This research-to-practice work-in-progress (WIP) paper presents an AI-enabled smart tutor designed to provide homework assessment and feedback for students in an undergraduate circuit analysis course. We detail the tutor's design philosophy and core components, including open-ended question answering and homework feedback generation. The prompts are carefully crafted to optimize responses across different problems. The smart tutor was deployed on the Microsoft Azure platform and is currently in use in an undergraduate circuit analysis course at the School of Electrical and Computer Engineering in a large, public, research-intensive institution in the Southeastern United States. Beyond offering personalized instruction and feedback, the tutor collects student interaction data, which is summarized and shared with the course instructor. To evaluate its effectiveness, we collected student feedback, with 90.9% of responses indicating satisfaction with the tutor. Additionally, we analyze a subset of collected data on preliminary circuit analysis topics to assess tutor usage frequency for each problem and identify frequently asked questions. These insights help instructors gain real-time awareness of student difficulties, enabling more targeted classroom instruction. In future work, we will release a full analysis once the complete dataset is available after the Spring 2025 semester. We also explore the potential applications of this smart tutor across a broader range of engineering disciplines by developing improved prompts, diagram-recognition methods, and database management strategies, which remain ongoing areas of research.",10.48550/arXiv.2506.08962,['JournalArticle'],Computer Science; Engineering
Banking Cybersecurity: Safeguarding Financial Information in the Digital Era,2025,"H. Zangana, Harman Salih Mohammed, Mamo M. Husain","This study explores the escalating cybersecurity challenges in the banking sector and the potential of large language models (LLMs) to enhance digital defense mechanisms. Employing a qualitative methodology that includes a systematic literature review, expert interviews, and case study evaluations, the research investigates the integration of LLMs in cybersecurity operations such as threat detection, automated incident response, and user authentication. The findings reveal that LLMs offer significant advantages in real-time anomaly detection, predictive analytics, and natural language-based security training. However, their adoption is hindered by concerns over algorithmic transparency, data privacy, and the need for specialized technical expertise within financial institutions. A key contribution of this work is the development of an integrated cybersecurity framework that combines AI-driven technologies, blockchain-based transaction security, digital forensic tools, and human-centered security practices. The proposed framework aims to guide financial institutions in implementing adaptive, intelligent cybersecurity strategies aligned with evolving global regulatory standards. This research offers both theoretical insights and practical recommendations for enhancing cyber resilience in digital banking environments. It emphasizes the importance of a multidimensional approach that addresses technical innovation, organizational preparedness, and regulatory compliance. Future studies are encouraged to validate the proposed framework through empirical testing across diverse banking infrastructures.",10.56427/jcbd.v4i2.751,"['JournalArticle', 'Review']",Business; Computer Science
A Survey on Open Dataset Search in the LLM Era: Retrospectives and Perspectives,2025,"Pengyue Li, Sheng Wang, Hua Dai, Zhiyu Chen, Zhifeng Bao, Brian D. Davison","High-quality datasets are typically required for accomplishing data-driven tasks, such as training medical diagnosis models, predicting real-time traffic conditions, or conducting experiments to validate research hypotheses. Consequently, open dataset search, which aims to ensure the efficient and accurate fulfillment of users'dataset requirements, has emerged as a critical research challenge and has attracted widespread interest. Recent studies have made notable progress in enhancing the flexibility and intelligence of open dataset search, and large language models (LLMs) have demonstrated strong potential in addressing long-standing challenges in this area. Therefore, a systematic and comprehensive review of the open dataset search problem is essential, detailing the current state of research and exploring future directions. In this survey, we focus on recent advances in open dataset search beyond traditional approaches that rely on metadata and keywords. From the perspective of dataset modalities, we place particular emphasis on example-based dataset search, advanced similarity measurement techniques based on dataset content, and efficient search acceleration techniques. In addition, we emphasize the mutually beneficial relationship between LLMs and open dataset search. On the one hand, LLMs help address complex challenges in query understanding, semantic modeling, and interactive guidance within open dataset search. In turn, advances in dataset search can support LLMs by enabling more effective integration into retrieval-augmented generation (RAG) frameworks and data selection processes, thereby enhancing downstream task performance. Finally, we summarize open research problems and outline promising directions for future work. This work aims to offer a structured reference for researchers and practitioners in the field of open dataset search.",10.48550/arXiv.2509.00728,"['JournalArticle', 'Review']",Computer Science
FRACTAL: Fine-Grained Scoring from Aggregate Text Labels,2024,"Yukti Makhija, Priyanka Agrawal, Rishi Saket, Aravindan Raghuveer","Large language models (LLMs) are being increasingly tuned to power complex generation tasks such as writing, fact-seeking, querying and reasoning. Traditionally, human or model feedback for evaluating and further tuning LLM performance has been provided at the response level, enabling faster and more cost-effective assessments. However, recent works (Amplayo et al. [2022], Wu et al. [2023]) indicate that sentence-level labels may provide more accurate and interpretable feedback for LLM optimization. In this work, we introduce methods to disaggregate response-level labels into sentence-level (pseudo-)labels. Our approach leverages multiple instance learning (MIL) and learning from label proportions (LLP) techniques in conjunction with prior information (e.g., document-sentence cosine similarity) to train a specialized model for sentence-level scoring. We also employ techniques which use model predictions to pseudo-label the train-set at the sentence-level for model training to further improve performance. We conduct extensive evaluations of our methods across six datasets and four tasks: retrieval, question answering, summarization, and math reasoning. Our results demonstrate improved performance compared to multiple baselines across most of these tasks. Our work is the first to develop response-level feedback to sentence-level scoring techniques, leveraging sentence-level prior information, along with comprehensive evaluations on multiple tasks as well as end-to-end finetuning evaluation showing performance comparable to a model trained on fine-grained human annotated labels.",10.48550/arXiv.2404.04817,"['JournalArticle', 'Conference']",Computer Science
Assessing the Predictive Power of Social Media Data-Fed Large Language Models on Vote Preference,2024,"Ehsan Barkhordar, kr Atszelti","This article investigates how large language models (LLMs) reflect human preferences and exhibit biases influenced by the diversity and nature of their input data. We used survey data related to Turkish presidential elections alongside tweets to assess the predictive performance and bias manifestations of LLMs under three different data inclusion strategies: (1) using only demographic information, (2) integrating demographic information with tweets, and (3) relying solely on tweets. Our findings reveal that prompts enriched with tweets typically achieve higher F1 Macro scores. However, this trend differs significantly when examining classes individually. While user-generated content significantly improves performance in predictions related to Recep Tayyip Erdoan, it does not show the same effect for Kemal Kldarolu. This study shows that different models and prompting styles result in varied biases for each candidate, leading to mixed outcomes. These results underscore the importance of exploring how biases vary across different scenarios, models, and prompting strategies for each case.",10.1145/3630744.3659831,"['JournalArticle', 'Book', 'Review']",Computer Science; Political Science
Integration of large language models into chatbot-based customer request processing systems.,2025,. . ,"The advancement of artificial intelligence has facilitated the emergence of a new generation of tools for automating customer inquiry processing, particularly through the widespread implementation of large language models (LLMs). These models have unlocked new possibilities in the development of dialog systems. The integration of LLMs into chatbots has become a key factor in transforming customer service, ensuring a high level of personalization, contextual relevance, and linguistic flexibility in user interactions. The aim of this article is to analyze the role of large language models in the development of intelligent customer service systems and to design architectural and algorithmic solutions for their effective integration into chatbot platforms. The study adopts a systemic approach to assess the functional potential of LLMs in multichannel communication, applies comparative analysis to explore current approaches to dialog agent implementation, and employs structural-functional modeling to build an integration architecture for LLMs in client services. It has been established that models such as GPT, LLaMA, Claude, and other transformer-based architectures enable not only the automation of routine inquiries but also the handling of complex requests that require contextual understanding, sentiment analysis, and user intent recognition. Key advantages include reducing the workload of contact center operators, increasing system response speed, adaptive learning based on inquiry history, and generating linguistically relevant replies. At the same time, the study identified a range of challenges associated with integrating LLMs into business processes. These include the complexity of customizing models to domain-specific contexts, risks of generating inaccurate or ethically questionable content, high computational costs of inference, and the necessity to ensure secure processing of personal data. A conceptual integration model is proposed, comprising three core levels: (1) the input collection layer that gathers customer inquiries from various channels (messaging apps, web interfaces, mobile applications); (2) the processing layer based on LLMs, including preliminary sentiment and thematic analysis; and (3) the decision-making and response generation layer, with escalation to a human operator in the case of complex or critical scenarios. In conclusion, the article outlines recommendations for the effective implementation of LLMs in customer service systems: (1) selecting a hybrid architecture combining rule-based and generative components; (2) applying fine-tuning or Retrieval-Augmented Generation (RAG) approaches for domain adaptation; (3) maintaining continuous monitoring of generated responses to ensure quality, ethical compliance, and contextual relevance; (4) leveraging API gateways and edge computing to reduce response latency. Future research perspectives include enhancing models for multilingual support, integration with CRM/ERP systems, developing self-learning dialog agents, and establishing metrics to evaluate the effectiveness of generative responses in real-world environments",10.36910/6775-2524-0560-2025-60-06,['JournalArticle'],Computer Science; Linguistics
An LLM-Based Behavior Agent with Natural Language Personality Control,2025,"Jos Timanta Tarigan, Brian Wijaya, Avin Chaili Salim, Sri Melvani Hardi","This study explores the use of Large Language Models (LLMs) for implementing personality-driven behavior in Non-Player Characters (NPCs) within games. A companion NPC leverages the OCEAN personality model to guide decision-making through natural language prompts, eliminating the need for traditional scripting or behavior trees. A stateless LLM combined with an automated prompt generator dynamically constructs context-aware prompts based on NPC traits, game states, and environmental factors. Implemented in the roguelike Rudantara RPG game, the companion NPC responds to gameplay conditions with behaviors aligned to its defined personality. The test results show that the system enables flexible and coherent decision-making and lowers the technical barrier to creating personalized behavior by allowing the player to interact using natural language instead of a complex behavior tree and scripting. Furthermore, to evaluate the decision-making process, participants with prior experience in RPG games were invited to play the prototype. Their responses indicated that the system was capable of simulating behavior aligned with the assigned personality traits.",10.48084/etasr.12631,['JournalArticle'],Computer Science
Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings,2025,"Harbin Hong, Sebastian Caldas, Liu Leqi","As Large Language Models (LLMs) increasingly appear in social science research (e.g., economics and marketing), it becomes crucial to assess how well these models replicate human behavior. In this work, using hypothesis testing, we present a quantitative framework to assess the misalignment between LLM-simulated and actual human behaviors in multiple-choice survey settings. This framework allows us to determine in a principled way whether a specific language model can effectively simulate human opinions, decision-making, and general behaviors represented through multiple-choice options. We applied this framework to a popular language model for simulating people's opinions in various public surveys and found that this model is ill-suited for simulating the tested sub-populations (e.g., across different races, ages, and incomes) for contentious questions. This raises questions about the alignment of this language model with the tested populations, highlighting the need for new practices in using LLMs for social science studies beyond naive simulations of human subjects.",10.48550/arXiv.2506.14997,"['JournalArticle', 'Review']",Computer Science; Psychology
Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection,2025,"Zhixiong Chen, Jiangzhou Wang, Hyundong Shin, Arumugam Nallanathan","The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited endurance and communication range of UAVs necessitate intelligent trajectory planning. While reinforcement learning (RL) has been extensively explored for UAV trajectory optimization, its interactive nature entails high costs and risks in real-world environments. Offline RL mitigates these issues but remains susceptible to unstable training and heavily rely on expert-quality datasets. To address these challenges, we formulate a joint UAV trajectory planning and resource allocation problem to maximize energy efficiency of data collection. The resource allocation subproblem is first transformed into an equivalent linear programming formulation and solved optimally with polynomial-time complexity. Then, we propose a large language model (LLM)-empowered critic-regularized decision transformer (DT) framework, termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we incorporate critic networks to regularize the DT model training, thereby integrating the sequence modeling capabilities of DT with critic-based value guidance to enable learning effective policies from suboptimal datasets. Furthermore, to mitigate the data-hungry nature of transformer models, we employ a pre-trained LLM as the transformer backbone of the DT model and adopt a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid adaptation to UAV control tasks with small-scale dataset and low computational overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark online and offline RL methods, achieving up to 36.7\% higher energy efficiency than the current state-of-the-art DT approaches.",10.48550/arXiv.2509.13934,['JournalArticle'],Computer Science; Engineering; Environmental Science
Generative AI and Large Language Model Assisted Causal Discovery and Inference for Driving Process Improvements,2024,"P. Rai, A. Jain, A. Anand","Data-driven process management coupled with machine learning have been successful in driving commercial value to oil and gas operators by offering insights into process disruptions and their root causes. One frequently used approach is to analyze causes of process disruptions exclusively from historical data. In general, specific insights in the form of high correlation between certain process performance indicators and a well-defined measure of production inefficiency is often confounded as responsible causal factors. While this may yield some insights, the complexity of processes, measured in terms of number of entities involved and their interrelationships, requires a more nuanced approach that must include the context of the specific process. Thus, data analysis must be augmented with significant inputs from experts. Causal Inference provides a conceptual framework and tools for doing such analysis.
 In causal analysis, we embed this specific knowledge of subject matter experts using causal graphs consisting of process features (nodes) and their dependency (directed edges). For complex processes however, constructing causal graphs could be non-trivial due to ambiguity over which nodes to include and the plausible direction of their relationships. With the advent of foundational Large Language Models (LLM), there is an opportunity to mitigate this problem by utilizing the enormous information it encodes. Tools and technologies now exist to customize the response of LLM using retrieval of information from a corpus of specific high-quality knowledge in the form of related literature and data. It can therefore be used to assist the domain expert in building and finetuning the causal graph, and in simpler cases, can completely automate this step.
 In this work, we propose a two-step approach to combine the power of LLMs and Causal Analysis for analyzing inefficiencies in production processes. In the first step, we implement a Retrieval Augmented Generation (RAG) enhanced LLM prompting on a curated dataset designed to answer specific questions on relationship between process performance indicators. The outcome of this step is a directed acyclic graph encoding dependency of process performance indicators. Domain experts can validate or potentially refine the LLM-generated causal graph based on their domain knowledge for eliminating spurious hallucinations. In the second step, we use an appropriate causal inference method on the refined causal diagram and historical production data to estimate the causal effect of process variable contributing to disruptions or inefficiencies. Thus, by combining human expertise with machine learning, this framework offers a comprehensive approach for optimizing production processes.",10.2118/221872-ms,nan,Computer Science; Engineering
Latent Feature Mining for Predictive Model Enhancement with Large Language Models,2024,"Bingxuan Li, Pengyi Shi, Amy Ward","Predictive modeling often faces challenges due to limited data availability and quality, especially in domains where collected features are weakly correlated with outcomes and where additional feature collection is constrained by ethical or practical difficulties. Traditional machine learning (ML) models struggle to incorporate unobserved yet critical factors. In this work, we introduce an effective approach to formulate latent feature mining as text-to-text propositional logical reasoning. We propose FLAME (Faithful Latent Feature Mining for Predictive Model Enhancement), a framework that leverages large language models (LLMs) to augment observed features with latent features and enhance the predictive power of ML models in downstream tasks. Our framework is generalizable across various domains with necessary domain-specific adaptation, as it is designed to incorporate contextual information unique to each area, ensuring effective transfer to different areas facing similar data availability challenges. We validate our framework with two case studies: (1) the criminal justice system, a domain characterized by limited and ethically challenging data collection; (2) the healthcare domain, where patient privacy concerns and the complexity of medical data limit comprehensive feature collection. Our results show that inferred latent features align well with ground truth labels and significantly enhance the downstream classifier.",10.48550/arXiv.2410.04347,['JournalArticle'],Computer Science
The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents,2025,Mohammad Rubyet Islam,"The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.",10.48550/arXiv.2505.14727,"['JournalArticle', 'Review']",Business; Computer Science
Measuring Morality in the 2020 US Presidential Election,2022,S. Atkins,"concepts in terms of concrete experiences, and how linguistic metaphors reveal these underlying concepts. For example, someone may refer to the abstract notion of understanding using the concrete experience of seeing, exemplified by metaphorical phrases such as I see what you mean, or My eyes have been opened to the truth. Following this line of reasoning, Lakoff (1996) proposed that liberals and conservatives 6 have distinctly different underlying conceptualizations of morals in politics. He drew on research that demonstrates a common conceptual metaphor of the United States as a family, with governmental entities and politicians taking parental roles. Lakoff (1996) proposed that the prototypical liberal understanding of this family is one led by a nurturant parent, while the prototypical conservative understanding is a family led by a strict parent, specifically a strict father. This informs how individuals understand political issues and evaluate their moral worth. Under a pure nurturant parent moral concept, individuals conceptualize a family in which children are inherently good, and the role of parents is to exhibit care towards them and help them develop into mature adults. These adults then care for other people and other aspects of society, such as the environment. Nurturant parent morality follows metaphorical concepts that depict morality as empathy, nurturance, and fair distribution. In contrast, the strict father moral concept is based less on care and more on justice, or the idea that people should receive what they deserve based on their own initiative and action. In this family concept, children are not necessarily inherently good; they must learn good from bad and right from wrong through a just system of rewards and punishments. Strict father morality follows metaphorical concepts that depict morality as purity, health, and self reliance. As an example of one conflict inherent between these moral concepts, Lakoff (1996) described the state of modern-day capitalism. Conservatives may point to free market capitalism to demonstrate the benefits of a just worldview. In this system, theoretically anyone can achieve wealth commensurate to the effort they put in towards this achievement. Competition in the free market begets innovation, and those who 7 innovate receive financial reward. Conversely, liberals may point out that this system creates a wealth gap that leaves many people behind the financial curve of elites. Those who are impoverished may not deserve their situation simply because they do not have access to the resources necessary to build wealth. Thus, the government should care for these individuals by eliminating wealth inequality in some way. Lakoff (1996) acknowledged that his model was just that: a model, rather than a fully elaborated theory. However, he believed that these family based moral evaluations would be evident in the language used to discuss issues from either a liberal or conservative viewpoint. Researchers have since used the family model to explore the moral content of political documents, speeches, and the like, as well as to test relationships between political affiliation and family conceptualizations. Cienki (2005) analyzed debates between Republican candidate George W. Bush and Democratic candidate Al Gore during the 2000 US Presidential election. He found that each candidate did use speech that would indicate entailments of their predicted family models, but at times both candidates appeared to draw from both family models. Deason and Gonzalez (2012) found a similar pattern when analyzing the Presidential and Vice-Presidential convention speeches from the 2008 election. The Democratic candidates Barack Obama and Joe Biden mainly drew from a nurturant parent model, but the Republican candidates John McCain and Sarah Palin drew from both a strict father and nurturant parent model. Deason and Gonzalez (2012) concluded that issue frames, rather than underlying moral concepts, appeared to be the salient factor that determined how candidates would discuss certain topics. For example, there was a significant global recession occurring at the time which warranted language from the 8 nurturant parent model regardless of political affiliation. It would not do to tell millions of people that they needed to pull themselves out of financial trouble, as a strict father moralist might; rather, candidates discussed how the government would care for citizens, in line with nurturant parent morality. As a measure of individual moral leanings, Feinberg et al. (2020) described the process of developing the Moral Political Scale. This scale asks participants to rate their agreement with various statements about a family, such as Id rather see my children play cooperatively than competitively or Obedience must be instilled in children (Feinberg et al., 2020, p. 784), with the intent of discovering a persons underlying family concept (strict or nurturing). During development, the authors administered this scale to participants from the Amazon Mechanical Turk website alongside other scales that determined political leanings. Fienberg et al. (2020) found significant correlations between strict parent scores and indicators of political conservatism, and likewise between nurturant parent scores and indicators of liberalism. Their results supported Lakoffs (1996) model, and, importantly, it connected this model with a newer model for morality in politics: Moral Foundations Theory (Haidt, 2013). Feinberg et al. (2020) found significant correlations between nurturant parent morality and ideals that emphasize care and fairness, as well as between strict father morality and ideals emphasizing loyalty, authority, and purity. These five categories make up the five moral foundations theorized by moral psychologist Jonathan Haidt and colleagues, a theory which I will build on for the remainder of this study. 9 Moral Foundations Theory Haidt (2013) and Graham et al. (2013) described the process of constructing a multidimensional theory of moral psychology by first reviewing past theories on morality that rest on only one foundation. For example, they cited Lawrence Kohlbergs promotion of a moral imperative of justice, wherein actions are deemed moral or immoral solely based on whether they promote justice, that is, whether good deeds are rewarded and bad deeds are punished. Similarly, they reviewed Carol Gilligans work, which argued that women view morality as based on whether actions promote care towards others while preventing harm. These moral foundations are not unlike Lakoffs strict and nurturant models. Graham et al. (2013) argued that such monistic moral theories do not capture the complexity of human moral intuition because they only provide one basis on which humans judge something as moral or immoraldoes an action promote justice or not? Does it promote care or not? Graham et al. (2013) described themselves as unabashed pluralists (p. 57), that is, their theory rests on the notion that humans have several foundations on which to draw when making moral judgments. Haidt and Joseph (2004) began constructing a multi-foundation model of moral psychology by exploring possible evolutionary challenges of human physical and social development. This became the Moral Foundations Theory, which is comprised of five pillars of morality, each based on an evolutionary challenge of human history. Haidt and Joseph (2004) argued that humanity needed to overcome these challenges to develop within a complex social structure. The solutions to these challenges originated as social goals common to members of a social group. Common goals became group values and persist today as moralsunderlying beliefs about what actions, attitudes, or behaviors are 10 fundamentally right or wrong (Haidt, 2013). A brief description of these foundations follows, adapted from Graham et al. (2013) and Haidt (2013): 1. The care/harm foundation, based on an evolutionary need to protect children. Current manifestations include compassion for victims of harm, both human and non-human (for example, compassion for nature or endangered species), as well as anger at the perpetrators of this harm. 2. The fairness/cheating foundation, based on an evolutionary need to form beneficial interpersonal relationships. Current manifestations include interpersonal issues, such as relational fidelity, along with non-interpersonal issues, such as equitable treatment towards all members of a society. 3. The loyalty/betrayal foundation, based on an evolutionary need to form social groups. Current manifestations include group pride ranging from school alumni and sports fans to political party and national pride. 4. The authority/subversion foundation, based on an evolutionary need to form hierarchies within social groups. Current manifestations include respect and obedience for those in authority in various contexts, such as teachers, bosses, or political leaders. 5. The sanctity/degradation foundation, based on an evolutionary need to avoid disease. Current manifestations include disgust at anything deemed a threat to personal or group sanctity, such as sexual deviancy, immigration, or physical maladies. Graham et al. (2013) described the process of developing Moral Foundations Theory as the co-development of theory and methodology. During the early stages of 11 development, the theory informed researchers on methods to test the theory; these methods then informed new directions and revisions for the theory, spurring new methods and so forth. Moral Foundations Theory has primarily been tested in four categories of methodology: word count analyses, self-report scales, implicit measurements, and physiological measurements. I describe some of the studies here with the exception of the word count analyses, to which I devote more attention in upcoming sections.",10.15760/etd.7764,['Review'],Political Science
Innovations in Recipe Generation and Ingredient Substitution: A Survey on RAG and Generative AI Approaches,2025,"Swati Jakkan, Rupali Susar Patil, Shantanu Ekad, Pranjali Shinde","The Recipe Generation and Ingredient Substitution System redefines the cooking experience by offering
personalized recipe suggestions and intelligent ingredient substitutions tailored to user preferences, dietary needs, and
available ingredients. Leveraging advanced Large Language Models (LLMs) integrated with Retrieval-Augmented
Generation (RAG),

the system processes natural language inputs to retrieve relevant recipes or generate customized recipes
with precision. Its robust ingredient substitution mechanism evaluates alternatives through a scoring framework that
considers flavor profiles, texture, functionality, and compatibility, ensuring that substitute ingredient maintain the dishs
integrity and quality. By incorporating flavor profile analysis, the system ensures substitutions preserve the harmony of
tastes, enhancing both the quality and enjoyment of meals. The system is designed to empower novice and experienced cooks,
the system transforms kitchen challenges into opportunities for exploration, enabling confident and innovative meal
preparation.",10.38124/ijisrt/25mar1460,"['JournalArticle', 'Review']",Computer Science
Generative AI Applied to the Design Thinking Process in Knowledge Engineering Projects,2025,"Ranieri Alves dos Santos, Fernando Alvaro Ostuni Gauthier, Marcelo Macedo, Vanessa Roberg","The need for user-centered and agile solutions has driven the adoption of methodologies such as Design Thinking in knowledge engineering projects. While the Design Thinking emphasizes empathy, iteration, and innovation focused on user experience, the knowledge engineering aims to build intelligent systems based on formalized expert knowledge. Despite the conceptual alignment between these approaches, the integration of Generative Artificial into this context remains underexplored. This study proposes a model that incorporates generative AI tools into the design thinking process to accelerate and enhance the development of knowledge-based systems. Based on the Design Science Research (DSR) methodology, a narrative literature review and exploratory research were conducted to identify generative AI techniques applicable to each design thinking phase: Empathize, Define, Ideate, Prototype, and Test. A total of 17 generative AI approaches were mapped and organized into a model that supports small and agile teams in knowledge engineering projects. The model was instantiated through the development of GPT-based agents customized for specific tasks, such as persona generation, empathy mapping, requirements analysis, and prototype creation. These agents leverage prompts containing transcripts, observations, or interview data to generate detailed and realistic outputs, streamlining processes that are traditionally manual and time-consuming. One example presented is the Persona Generation Agent, which creates structured user profiles and illustrative images from simple textual input. This integration contributes both theoretically and practically by demonstrating how generative AI can be used to enhance user experience (UX) focused design processes in knowledge engineering. The proposed model promotes more efficient workflows, while keeping the user at the center of development. It also supports interdisciplinary collaboration, faster iteration cycles, and the creation of intelligent systems that are more aligned with user needs. Future work includes empirical validation of the model in diverse application contexts, aiming to refine its use and encourage widespread adoption in knowledge engineering and related fields.",10.34190/eckm.26.1.3828,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering
GCE: An Audio-Visual Dataset for Group Cohesion and Emotion Analysis,2024,"Eunchae Lim, Ngoc-Huynh Ho, Sudarshan Pant, Young-Shin Kang, Seong-Eun Jeon, Seung-won Kim, Sooyoung Kim, Hyung-Jeong Yang","We present the Group Cohesion and Emotion (GCE) dataset, which comprises 1029 segmented films sourced from YouTube. These videos encompass a range of interactions, including interviews, meetings, informal discussions, and other similar contexts. In the annotation process, graduate psychology students were tasked with assigning coherence levels, ranging from 1 to 7, and affective states as negative, neutral, or positive for each 30 s film. We introduce a foundational model that utilizes advanced visual and audio embedding techniques to investigate the concepts of group cohesion and group emotion prediction. The application of Multi-Head Attention (MHA) fusion is utilized to enhance the process of cross-representation learning. The scope of our investigation includes both unimodal and multimodal techniques, which provide insights into the prediction of group cohesion and the detection of group emotion. The results emphasize the effectiveness of the GCE dataset in examining the level of group unity and emotional conditions.",10.3390/app14156742,['JournalArticle'],Computer Science; Psychology
Website Owner Identification through Multi-level Contrastive Representation Learning,2025,"Cheng Tu, Yunshan Ma, Yang Li, Min Zhang, Miao Hu, Fan Shi, Xiang Wang","Website owner identification aims to recognize the organization or individual who owns a given website that is served on the web. It is a crucial step for cyberspace surveying and mapping, playing a significant role in cyberspace administration and governance. Existing widely-employed solutions for website owner identification mainly fall into two paradigms: 1) querying the public information databases such as WHOIS, which store the Internet resources registered users or assignees; and 2) directly extracting the organization or individual name of the website owner from the webpage using the technique of named entity recognition. However, the former is less reliable due to the incomplete, encrypted, and outdated records in the public information databases. Meanwhile, the latter requires that the webpages explicitly and precisely present their owner names without ambiguity, which is often hard to guarantee in practice.
 
 To address these limitations, we propose to formulate website owner identification as a problem of webpage representation learning, thereby introducing a novel representation learning framework empowered by large language model-based text
 Re
 writing and
 M
 ulti-level c
 on
 trastive learning, named ReMon. First, we devise a prompt to rewrite the webpages using large language models, which effectively filters out noise from the original webpages. Second, we model website-website, website-owner, and owner-owner interactions through multi-level contrastive learning, fully utilizing the self-supervision signals on long-tail items to learn the multi-level constraints. Third, we design a retrieval-based prediction framework and a clustering-based framework to apply websites and owners representations for different scenarios of the website owner identification task. To evaluate ReMon under our formulation, we construct two datasets based on real-world data. Compared to existing approaches, our ReMon can address the challenging scenarios when valid information cannot be found in public information databases and the owners name does not appear on the webpage. Meanwhile, the experimental results show that ReMon outperforms all representation learning-based baselines and significantly enhances training efficiency. The code is available at
 https://github.com/tuchen9/ReMon
 .
",10.1145/3767155,"['JournalArticle', 'Review']",Computer Science
An Approach Based on Named Entity Recognition and Semantic Analysis for Recruitment Efficiency and Optimization,2025,"Ismail Ifakir, Noureddine Mohtaram, E. Nfaoui, Abderrahim Zannou, M. Hassouni","Modern recruitment requires smarter, faster, and more inclusive methods to manage the growing volume and diversity of job applications and candidate resumes. Manual screening is often ineffective and unreliable especially in low-resource or multilingual contexts. To address this challenge, we propose an approach that automates and optimizes key stages of the recruitment process. This three-stage approach includes: 1) extracting structured data from resumes using a robust Named Entity Recognition (NER) system, which comprises a NER annotator, a feature extractor, and a transition-based parser; 2) employing a fine-tuned transformer model to perform semantic matching between candidates and job descriptions; and 3) leveraging a large language model to generate interview questions tailored to specific job requirements, thereby improving the relevance and personalization of candidate assessments. The recruitment system was tested on a large-scale resume and job posting dataset across multiple domains. Our NER model reported an F1-score of 85.11% in entity extraction, and the matching component reported accuracy levels as high as 92% when using hierarchical job classes. The results prove the efficacy of combining deep learning techniques with semantic reasoning in enhancing automation, accuracy, and fairness in hiring.",10.14569/ijacsa.2025.0160889,['JournalArticle'],Computer Science
Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs,2025,"Sotiris Chatzimiltis, Mohammad Shojafar, Mahdi Boloursaz Mashhadi, Rahim Tafazolli","Next generation Radio Access Networks (RANs) introduce programmability, intelligence, and near real-time control through intelligent controllers, enabling enhanced security within the RAN and across broader 5G/6G infrastructures. This paper presents a comprehensive survey highlighting opportunities, challenges, and research gaps for Large Language Models (LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future RAN environments. Motivated by this, we propose an LLM interpretable anomaly-based detection system for distributed denial-of-service (DDoS) attacks using multivariate time series key performance measures (KPMs), extracted from E2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC). An LSTM-based model is trained to identify malicious User Equipment (UE) behavior based on these KPMs. To enhance transparency, we apply post-hoc local explainability methods such as LIME and SHAP to interpret individual predictions. Furthermore, LLMs are employed to convert technical explanations into natural-language insights accessible to non-expert users. Experimental results on real 5G network KPMs demonstrate that our framework achieves high detection accuracy (F1-score>0.96) while delivering actionable and interpretable outputs.",10.48550/arXiv.2507.21193,"['JournalArticle', 'Review']",Computer Science; Engineering
COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks,2025,"Dongping Li, Shaoting Peng, John Pohovey, K. Driggs-Campbell","Continuous advancements in robotics and AI are driving the integration of robots from industry into everyday environments. However, dynamic and unpredictable human activities in daily lives would directly or indirectly conflict with robot actions. Besides, due to the social attributes of such human-induced conflicts, solutions are not always unique and depend highly on the user's personal preferences. To address these challenges and facilitate the development of household robots, we propose COMMET, a system for human-induced COnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid detection approach, which begins with multi-modal retrieval and escalates to fine-tuned model inference for low-confidence cases. Based on collected user preferred options and settings, GPT-4o will be used to summarize user preferences from relevant cases. In preliminary studies, our detection module shows better accuracy and latency compared with GPT models. To facilitate future research, we also design a user-friendly interface for user data collection and demonstrate an effective workflow for real-world deployments.",10.48550/arXiv.2509.04836,['JournalArticle'],Computer Science; Engineering
Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models,2025,"Dongmin Choi, Woojung Song, Jongwook Han, Eun-Ju Lee, Yohan Jo","Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.",10.48550/arXiv.2509.10078,"['JournalArticle', 'Review']",Computer Science; Linguistics; Psychology
Who is Using AI in Romania? A Socio  Economic Profile of AI Users,2025,"Adina-Maria Vod, Doina Badea","Romania ranks last in the EU on AI usage. Whereas the AI knew an exponential growth worldwide over the last few years, especially after Chat GPT launch, Romania does not seem to have the capacity to catch up quickly. The present study aims to showcase the influence of socio-economic predictors, such as age, education level, income and professional experience, on AI usage in the workplace in Romania. We use ordinal regression model to measure what drove AI usage at the workplace in Romania over the last 12 months, which may later stimulate AI uptake by the labor market. We use for the first time the results of a survey targeting employees, applied online in Romania during December 2024  January 2025. We conclude that Romanian employees income and work experience are key predictors of AI usage, whereas age and professional experience do not have a significant impact.",10.47191/jefms/v8-i3-55,"['JournalArticle', 'Review']",Computer Science; Economics; Sociology
Enhanced Privacy Policy Comprehension via Pre-trained and Retrieval-Augmented Models,2024,"Xin Zhang, Bingbing Zhang, Chi Zhang, Lingbo Wei","Privacy policies are crucial for informing users about how their personal information is collected, stored, and used by organizations. However, privacy policies are lengthy, information-dense, and filled with legalese, making them difficult for users to comprehend. Although previous studies have attempted to improve the readability of privacy policies via traditional machine learning techniques, these methods overlook or oversimplify critical information or rely on predefined question-answers that cannot adapt to user personalized queries. Inspired by the fact that large language models perform well in terms of text comprehension and text question-answering, we propose a novel large language model-based privacy policy question-answering framework, which aims to help users understand privacy policies more intuitively and effectively. Specifically, our framework consists of two modules: the personal data practice disclosure module, which performs a pre-training and fine-tuning approach to extract structured information about data categories and corresponding data operations from privacy policies. The retrieval-augmented question-answering module integrates sparse and dense retrievers to find the most relevant evidence from the privacy policy and generate responses corresponding to user queries. The experimental results across two representative datasets demonstrate the superiority of our method over other prior methods.",10.1109/TrustCom63139.2024.00094,"['JournalArticle', 'Conference']",Computer Science
Advancing Realistic Non-Playable Characters Conversations in Juragan Fauna with GPT-3.5,2024,"Aldi Rusdi, Musthofa Galih Pradana, Nurul Afifah Arifuddin","This research explores the application of the GPT-3.5-based Natural Language Processing (NLP) model to enhance interactions between Non-Playable Characters (NPCs) and players in the simulation game Juragan Fauna. The study aims to evaluate the model's effectiveness in improving the accuracy and speed of NPC responses during player interactions. Using the Unity game engine, GPT-3.5 was integrated into various NPC roles, allowing for dynamic and contextually relevant responses to player inquiries. Performance evaluation focused on two key aspects: response accuracy and response speed. The results show that the model achieved an average response accuracy of 80.4%, demonstrating its ability to deliver relevant answers based on NPC roles and game context. Additionally, the average response time of 0.96 seconds ensured smooth, uninterrupted gameplay. Players experienced a more immersive and engaging environment, with NPCs responding to open-ended questions in real time. However, challenges such as the dependency on stable internet connections and the high operational costs associated with using API-based models were identified. The study provides several recommendations for developers, including optimizing NPC response lengths to balance response speed and cost-efficiency. This research highlights the potential of GPT-3.5 to revolutionize NPC interactions in simulation games, paving the way for more advanced and realistic gameplay experiences.",10.1109/ICIMCIS63449.2024.10956580,['Conference'],Computer Science
AI-Powered Financial Insights: Using Large Language Models to Improve Government Decision-Making and Policy Execution,2025,Luqing Ren,"Given the complexity of fiscal data types and the lengthy policy execution chain, this study constructs an application framework for language models supporting government decision-making. It systematically investigates task modules including decision-making question-answering identification, expenditure forecasting modeling, executive summary extraction, semantic matching, and conflict reasoning. The framework elucidates model architecture design methodologies and semantic fusion mechanisms, while introducing response capability simulation testing and performance evaluation systems. Using heterogeneous fiscal corpora and multi-task experimental data, demonstrates that the model exhibits strong performance in accuracy, generative consistency, and generalization capabilities, supporting intelligent applications across diverse fiscal scenarios.",10.70393/6a69656173.333139,['JournalArticle'],Computer Science; Economics; Political Science
Constrained C-Test Generation via Mixed-Integer Programming,2024,"Ji-Ung Lee, Marc E. Pfetsch, Iryna Gurevych","This work proposes a novel method to generate C-Tests; a deviated form of cloze tests (a gap filling exercise) where only the last part of a word is turned into a gap. In contrast to previous works that only consider varying the gap size or gap placement to achieve locally optimal solutions, we propose a mixed-integer programming (MIP) approach. This allows us to consider gap size and placement simultaneously, achieving globally optimal solutions, and to directly integrate state-of-the-art models for gap difficulty prediction into the optimization problem. A user study with 40 participants across four C-Test generation strategies (including GPT-4) shows that our approach (MIP) significantly outperforms two of the baseline strategies (based on gap placement and GPT-4); and performs on-par with the third (based on gap size). Our analysis shows that GPT-4 still struggles to fulfill explicit constraints during generation and that MIP produces C-Tests that correlate best with the perceived difficulty. We publish our code, model, and collected data consisting of 32 English C-Tests with 20 gaps each (totaling 3,200 individual gap responses) under an open source license.",10.48550/arXiv.2404.08821,['JournalArticle'],Computer Science
Learning from Elders: Making an LLM-powered Chatbot for Retirement Communities more Accessible through User-centered Design,2025,"Luna Xingyu Li, Ray-yuan Chung, Feng Chen, Wenyu Zeng, Yein Jeon, Oleg Zaslavsky","Low technology and eHealth literacy among older adults in retirement communities hinder engagement with digital tools. To address this, we designed an LLM-powered chatbot prototype using a human-centered approach for a local retirement community. Through interviews and persona development, we prioritized accessibility and dual functionality: simplifying internal information retrieval and improving technology and eHealth literacy. A pilot trial with residents demonstrated high satisfaction and ease of use, but also identified areas for further improvement. Based on the feedback, we refined the chatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt engineering to deliver concise responses. Accessible features like adjustable font size, interface theme and personalized follow-up responses were implemented. Future steps include enabling voice-to-text function and longitudinal intervention studies. Together, our results highlight the potential of LLM-driven chatbots to empower older adults through accessible, personalized interactions, bridging literacy gaps in retirement communities.",10.5281/zenodo.15292697,['JournalArticle'],Computer Science; Sociology
Analyzing Chatbot Architectures Utilising Deep Neural Networks,2024,"Kavita Patil, Rohit Patil, Vedanti Koyande, A. Thakur, Kshitij Kadam","This survey paper examines the advancements and challenges in chatbot technology, focusing on deep neural networks (DNNs) and their application in natural language processing (NLP). It discusses various chatbot models, including Elizabot, Alicebot, Mitsuku, and Cleverbot, highlighting their evolution from rule-based systems to sophisticated AI conversational agents. The study introduces a specialized chatbot for website integration, emphasizing the importance of swift, accurate, and personalized interactions to enhance customer engagement. Additionally, the paper explores the integration of Large Language Models (LLMs) such as Gemini, GPT and BERT, fine-tuning with deep learning techniques to improve chatbot performance, and their potential to revolutionize customer interactions and business growth.",10.1109/ICCCMLA63077.2024.10871275,"['Conference', 'Review']",Computer Science; Linguistics
Cultural Differences in Perception and Engagement of AI-generated Online Ads,2025,"Andreas Stckl, Daniel Diaz","AI-generated advertising media are fascinating for online advertising, as they can be used to achieve a high degree of personalization at a low cost. However, they also introduce unique challenges. The seamless integration of text and visuals, the ability to capture and retain audience attention, and the effectiveness of AI-generated content in diverse cultures are all areas that require in-depth understanding. This understanding is crucial as companies increasingly rely on AI to enhance their marketing efforts.In this study, we examine cultural differences in the perception of and interaction with Instagram advertising, which was created using generative AI. We used 75 people from Columbia and 41 from Austria to investigate how these two groups differ.For this purpose, an application was developed that generates ads for Instagram based on the GPT4 and DALLE-3 AI systems that can create Text and Images. To further define the intended demographic for the advertisements, a persona generator was developed to generate basic user profiles. Both target groups were then surveyed using a further application that presents a structured questionnaire. Six ads for five target groups, i.e., 30, were created and presented to the test persons, followed by the questionnaire.The questionnaire asks things such as the clarity of the message, the trustworthiness of the ad, whether it is visually appealing, whether it matches the interests, whether it attracts attention, whether it would be interacted with, etc., on a 5-point scale. In addition, free text questions are asked about which elements of the ad encourage interaction and which emotional responses there are. Specifically, the analysis aimed to investigate differences in engagement, visual appeal, relevance, and other factors that could influence the perception of the ads.To achieve this, a T-test was conducted to determine the significance of differences in the answers to each question in the questionnaire. Additionally, a separate analysis focused exclusively on ads related to Skiing and a Hotel in the Alps. This was done to see if filtering for elements culturally significant to Austrians would yield significant response differences.The cross-cultural analysis showed numerous significant results in how Colombians and Austrians evaluated AI-generated advertisements. For example, Colombians consistently determined that the ads were more visually appealing than Austrians. Colombians ranked culturally relevant advertising, such as those about skiing and the Alps, higher in visual appeal, catching attention more successfully. Colombians were more inclined to interact with the ads regarding engagement in the general comparison. However, this difference in interaction likelihood became less pronounced when only culturally specific ads were considered.Interestingly, while the general comparison showed no significant difference in overall quality ratings between the two groups, the filtered analysis for culturally specific ads revealed that Colombians rated the overall quality higher than Austrians. This suggests that Colombians found the culturally relevant ads more enjoyable overall. However, both groups' perceptions of clarity, credibility, and relevance remained similar, with no significant differences observed, indicating that these aspects were less influenced by cultural context. The capabilities of the AI models constrained the study used, specifically GPT 4 and DALL-E 3. These models, while advanced, still need to be improved in fully understanding and replicating human creativity, particularly in areas such as appropriate design, cultural references, and the integration of text and visuals. The lack of interaction between the text and image generation phases is a limitation that often results in inconsistencies between the content and visuals. Overusing certain words in the captions or over image text is a constraint of the model. It sticks to words like Enhance and Elevate regardless of the product or service, which deteriorates the quality of the final output. Integrating text and image generation models could significantly improve the coherence and quality of AI-generated ads. Using tools like ChatGPT-Vision to offer feedback on the generated DALL-E image to GPT could be a step forward in automating the whole process. The analysis was based on responses from participants from two cultural backgrounds (Austrian and Colombian). While this allowed for some cross-cultural insights, the sample size and cultural diversity were limited, which may affect the general result of the findings. Future studies with a more diverse participant pool could provide a broader understanding of how different cultures perceive AI-generated content.",10.54941/ahfe1005916,['Review'],Business; Computer Science; Sociology
Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales,2024,"Maor Reuben, Ortal Slobodin, Aviad Elyashar, Idan Cohen, O. Braun-Lewensohn, Odeya Cohen, Rami Puzis","Human-like personality traits have recently been discovered in large language models, raising the hypothesis that their (known and as yet undiscovered) biases conform with human latent psychological constructs. While large conversational models may be tricked into answering psychometric questionnaires, the latent psychological constructs of thousands of simpler transformers, trained for other tasks, cannot be assessed because appropriate psychometric methods are currently lacking. Here, we show how standard psychological questionnaires can be reformulated into natural language inference prompts, and we provide a code library to support the psychometric assessment of arbitrary models. We demonstrate, using a sample of 88 publicly available models, the existence of human-like mental health-related constructs (including anxiety, depression, and Sense of Coherence) which conform with standard theories in human psychology and show similar correlations and mitigation strategies. The ability to interpret and rectify the performance of language models by using psychological tools can boost the development of more explainable, controllable, and trustworthy models.",10.48550/arXiv.2409.19655,"['JournalArticle', 'Conference']",Computer Science; Psychology
MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models,2025,"Yifan Liu, Xishun Liao, Haoxuan Ma, Jonathan Liu, Rohan Jadhav, Jiaqi Ma","Understanding and modeling human mobility patterns is crucial for effective transportation planning and urban development. Despite significant advances in mobility research, there remains a critical gap in simulation platforms that allow for algorithm development, policy implementation, and comprehensive evaluation at scale. Traditional activity-based models require extensive data collection and manual calibration, machine learning approaches struggle with adaptation to dynamic conditions, and treding agent-based Large Language Models (LLMs) implementations face computational constraints with large-scale simulations. To address these challenges, we propose MobiVerse, a hybrid framework leverages the efficiency of lightweight domain-specific generator for generating base activity chains with the adaptability of LLMs for context-aware modifications. A case study was conducted in Westwood, Los Angeles, where we efficiently generated and dynamically adjusted schedules for the whole population of approximately 53,000 agents on a standard PC. Our experiments demonstrate that MobiVerse successfully enables agents to respond to environmental feedback, including road closures, large gathering events like football games, and congestion, through our hybrid framework. Its modular design facilitates testing various mobility algorithms at both transportation system and agent levels. Results show our approach maintains computational efficiency while enhancing behavioral realism. MobiVerse bridges the gap in mobility simulation by providing a customizable platform for mobility systems planning and operations with benchmark algorithms. Code and videos are available at https://github.com/ucla-mobility/MobiVerse.",10.48550/arXiv.2506.21784,['JournalArticle'],Computer Science; Engineering; Environmental Science
PEMANFAATAN CHAT GPT OPEN AI TERHADAP ANALISIS DAN STRATEGI DALAM MENINGKATKAN PERFORMA PENJUALAN PRODUK SECARA DIGITAL,2024,Julianto Julianto,"The rapid development of technology can produce several new innovations that are beneficial to society such as ChatGPT Open AI. ChatGPT is an artificial language model developed by OpenAI, an artificial intelligence technology company. GPT stands for ""Generative Pre-trained Transformer"", which refers to the model architecture used in the creation of ChatGPT. The method used in this research is a qualitative method. The results showed that the utilization of ChatGPT allows businesses to be able to conduct in-depth analysis of customer data and product reviews, help identify new problems and opportunities that have not been seen before, assist businesses in developing more targeted and personalized marketing strategies, enable businesses to provide fast and responsive responses to customer inquiries, businesses can quickly identify problems that may occur with the product, This allows for faster repairs and improvements, which in turn can increase customer satisfaction. By utilizing the analysis conducted with ChatGPT, businesses can enter a continuous cycle of improvement. Findings and feedback can be continuously processed to improve products and marketing strategies, ultimately supporting long-term sales growth. Although ChatGPT has a lot of potential, the software has limitations. Understanding the context and human interpretation remains critical to making the right decisions. In addition, ethics in data collection and use must also be prioritized. With awareness and understanding, utilizing ChatGPT in digital product sales analysis and strategy can significantly contribute to achieving business goals.",10.47024/js.v11i3.749,"['JournalArticle', 'Review']",Computer Science
VeriHire: The NextGen Interview Application,2025,"Aryan Singh, Vivek Yadav, Ayush Rai, Abhishek Yadav, Abhyuday Rai, Satendra Kumar","Abstract - Traditional automated interview systems often rely on static question banks, resulting in a generic and impersonal candidate experience. Such systems frequently fail to accurately assess an applicant's unique skill set. VeriHire addresses these challenges by introducing an adaptive, AI-powered interview platform designed to conduct dynamic and personalized interviews.

The system begins by analysing a candidates professional profile  including resumes, portfolios, and repositories  to understand their expertise. Using Natural Language Processing (NLP) and a Large Language Model (LLM), VeriHire generates tailored, real-time questionnaires and adapts subsequent questions based on the candidates live responses. This approach enables deeper exploration of technical knowledge, problem-solving skills, and project experience, closely simulating the process of a human interviewer.

MCP (Model Context Protocol) is a new standard that allows language models to securely communicate with external tools, databases, and APIs, making AI systems more extensible and capable. RAG (Retrieval-Augmented Generation) is a powerful technique that improves model accuracy by fetching relevant information from a knowledge source before generating a response. LangChain is a popular open-source framework that helps developers build applications using language models by chaining prompts, tools, and memory into a structured workflow

The system begins by analysing a candidates professional profile  including resumes, portfolios, and repositories  to understand their expertise. Using Natural Language Processing (NLP) and a Large Language Model (LLM), VeriHire generates tailored, real-time questionnaires and adapts subsequent questions based on the candidates live responses.

Key Words: MCP, RAG, LANG-CHAIN",10.55041/ijsrem53089,['JournalArticle'],Computer Science
"Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models",2025,"George Kour, Itay Nakash, Ateret Anaby-Tavor, Michal Shmueli-Scheuer","As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS",10.48550/arXiv.2505.19621,"['JournalArticle', 'Review']",Computer Science; Linguistics
Using Large Language Models to Analyze Interviews for Driver Psychological Assessment: A Performance Comparison of ChatGPT and Google-Gemini,2025,"Ruifen Sun, Xinni Hu, Yang Shao, Zhongbin Luo, Bin Liu, Yuzhu Cheng","This study examines the application of large language models (LLMs) in analyzing subjective driver perceptions during tunnel driving simulations, comparing the effectiveness of questionnaires and interviews. Building on previous research involving driver simulations, we recruited 29 new participants, collected their perceptions via questionnaires, and conducted follow-up interviews. The interview data were analyzed using three LLMs: GPT-3.5, GPT-4, and Google-Gemini. The results revealed that while GPT-4 provides more in-depth and accurate analysis, it is significantly slower than GPT-3.5. Conversely, Google-Gemini demonstrated a balance between analysis quality and speed, outperforming the other models overall. Despite the challenge of occasional misunderstandings, LLMs still have the potential to enhance the efficiency and accuracy of subjective data analysis in transportation research.",10.3390/sym17101713,['JournalArticle'],Computer Science; Psychology
Research and Analysis on Text Interaction Methods Based on Large Language Models,2025,Shangze Yu,"Large Language Models (LLMs), as a cutting-edge technology in the field of artificial intelligence, have revolutionized human-computer interaction paradigms by integrating natural language processing, Transformer architecture, and reinforcement learning techniques, thereby achieving in-depth understanding and generation of linguistic logic and emotions. Their technological core lies in the Transformer architecture trained on massive datasets, which can capture complex semantics and contextual associations, optimize generation quality through reinforcement learning, and enhance interpersonal-like interaction via affective computing. This paper systematically reviews the technical framework and application scenarios of LLMs: in fields such as intelligent customer service, educational assessment, and game narration, these models demonstrate application values such as multi-turn dialogue, personalized learning path planning, and dynamic plot generation. Meanwhile, it delves into the challenges faced by technological development, including ethical risks arising from biases in training data, deployment cost issues due to high computational power requirements, and deficiencies in the controllability of generated content. In response to these issues, collaborative solutions such as multimodal data fusion, lightweight model deployment, and interdisciplinary ethical governance are proposed. Research indicates that LLMs are at a critical stage of transitioning from technological breakthroughs to large-scale applications, and their sustainable development necessitates the construction of a technological governance framework to achieve social value balance, on top of algorithm optimization and computational power enhancement. Future research should focus on enhancing model interpretability, exploring green computing pathways, and promoting the virtuous development of technology through human-machine collaboration mechanisms.",10.62051/rs49my32,"['JournalArticle', 'Review']",Computer Science; Linguistics
A Driver Advisory System Based on Large Language Model for High-speed Train,2025,"Y. C. Luo, J. Xun, W. Wang, R. Zhang, Z. C. Zhao","With the rapid development of China high-speed railway, drivers face increasingly significant technical challenges during operations, such as fault handling. Currently, drivers depend on the onboard mechanic when facing technical issues, for instance, traction loss or sensor faults. This dependency can hinder effective operation, even lead to accidents, while waiting for faults to be addressed. To enhance the accuracy and explainability of actions during fault handling, an Intelligent Driver Advisory System (IDAS) framework based on a large language model (LLM) named IDAS-LLM, is introduced. Initially, domain-fine-tuning of the LLM is performed using a constructed railway knowledge question-and-answer dataset to improve answer accuracy in railway-related questions. Subsequently, integration of the Retrieval-augmented Generation (RAG) architecture is pursued for system design to enhance the explainability of generated responses. Comparative experiments are conducted using the constructed railway driving knowledge assessment dataset. Results indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by an average of 10%, outperforming some current mainstream LLMs. Additionally, the inclusion of the RAG framework increases the average recall rate of question-and-answer sessions by about 4%. Finally, the fault handling capability of IDAS-LLM is demonstrated through simulations of real operational scenarios, proving that the proposed framework has practical application prospects.",10.48550/arXiv.2501.07837,['JournalArticle'],Computer Science; Engineering
Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case,2025,"Basti'an Gonz'alez-Bustamante, Nando Verelst, Carla Cisternas","Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data. We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning models, as well as Llama and Qwen checkpoints. Three results stand out. First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy>0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59. Overall, LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.",10.5281/zenodo.17077752,"['JournalArticle', 'Review']",Computer Science; Political Science; Sociology
Integrating Text-Analysis to Explore the Application of Generative AI Tools on User Interface Design and Engineering,2024,"Yi-Ci Liao, Sheng-Ming Wang","This study investigates collaboration issues between UI designers and engineers and the application of Artificial Intelligence Generated Content tools, such as AI-assisted programming tools, through semi-structured interviews and interactions with LLMs like ChatGPT-4, Claude, and Gemini. Text analysis was conducted on the collected data. LLMs served as exploratory tools, supplementing in-person interviews and addressing participant scale limitations in traditional qualitative data collection by simulating different professional perspectives.The results indicate that AI tools can enhance development efficiency but require users to have basic programming knowledge for effective interaction. Additionally, designers learning front-end programming can improve cross-disciplinary collaboration and competitiveness, promoting better communication with engineers. This study aims to explore the potential of AIGC tools in assisting UI designers and front-end engineers in transitioning towards the UI engineering field, thereby enhancing their career flexibility and adaptability.",10.1109/GCCE62371.2024.10760914,"['JournalArticle', 'Conference']",Computer Science; Engineering
A Short Survey on Large Language Models' Application in Manufacturing Domain,2025,"T. Vafeiadis, Alexandros Nizamis, Thanasis Kotsiopoulos, D. Ioannidis, Dimitrios Tzovaras","Large Language Models (LLMs) are able to process and generate textual and structured data to support and optimize an extremely wide variety of applications in various domains. One of the domains where Artificial Intelligence (AI) and LLMs are constantly gaining interest is manufacturing. LLMs promote the transformation of traditional manufacturing processes, improve efficiency, and enable optimal data-driven decision-making. This short survey explores the diverse applications of LLMs in manufacturing, including key application areas such as enhanced decision-making, predictive maintenance, additive manufacturing, industrial robotics applications, etc. In addition, key aspects of LLMs' applications such as the models used, the training methods, the use of Retrieval-Augmented Generation (RAG) frameworks, etc. are extracted and discussed. By high-lighting LLM application areas in manufacturing and their key aspects, the study supports faster understanding and promotes future research and development from industry professionals, researchers, and policy makers on LLM application for the manufacturing domain.",10.1109/ICE/ITMC65658.2025.11106602,"['Conference', 'Review']",Computer Science; Engineering
A survey on the application and research progress of large language models in financial forecasting,2025,"Ruonan Wu, Hong Liu","Large language models (LLMs) are reshaping the technical paradigms of financial forecasting through their robust representation learning and reasoning capabilities. This paper systematically reviews the application pathways of architectures such as transformers and graph neural networks in scenarios like stock prediction and risk management, highlighting key technologies for enhancing prediction accuracy through knowledge injection and temporal modeling improvements. The study reveals that LLMs demonstrate significant advantages in unstructured data processing and cross-market correlation analysis but face challenges related to economic logic interpretability and data non-stationarity. Future research should focus on advancing causal reasoning augmentation and federated learning collaboration to achieve secure and trustworthy evolution of financial forecasting systems.",10.1063/5.0274031,"['JournalArticle', 'Review']",Business; Computer Science
Development of an Artificial Intelligence-Based Companion clone,2025,"Maroua Belkneni, Khaled Guedria, Skander Chamakhi, Wiem Hjiri, Nouha Ben Slimen","Virtual companionship is becoming increasingly common as artificial beings, such as robots or agents, establish social connections, develop emotional links, and provide emotional support. This study looks at the effectiveness and emotional impact of Gemini AI technology in creating a lifelike virtual companion that simulates conversations with family members by cloning their voice and dialect, addressing a critical need to improve the psychological well-being and social interaction of elderly people. The study found that the use of personalised voices greatly increases companionship emotions and decreases feelings of loneliness among older users, based on qualitative data acquired from user experiences and quantitative surveys evaluating emotional responses and levels of engagement. Participants reported significant gains in their emotional well-being, with many experiencing more happiness and a stronger sense of connectedness to their families, even from afar. These results highlight Gemini AIs potential as an effective healthcare intervention, indicating that these virtual companions could be a useful tool for enhancing senior citizens quality of life. Furthermore, this study has wider ramifications for the healthcare industry, as it emphasises how crucial it is to include cutting-edge technology into patient care plans to provide emotional support and meaningful social contacts for those who are isolated. The use of cutting-edge solutions like Oldies Mate presents a viable way to meet the emotional and social requirements of senior citizens as the population ages, underscoring the need for continued research and development in this crucial field of health technology.",10.1109/IWCMC65282.2025.11059604,"['JournalArticle', 'Review']",Computer Science; Psychology
Large Language Model - Based Conversational Recommender System for Personalizing Long-term Cryptocurrency Portfolio Recommendation,2025,"Alfara Nafi Dinara, Z. Baizal","The growth of cryptocurrencies in the last 10 years as an investment option has increased. However, the volatile prices often make investors panic and lose money. The cryptocurrency portfolio recommender system built in previous research is still limited to historical prices and trends and does not focus on user preferences and understanding in natural language conversation. Therefore, we propose the development of a Conversational Recommender System (CRS) using a Large Language Model (LLM) to personalize long-term cryptocurrency portfolio recommendations. We use the prompt engineering and fine-tuning approach on LLM to match user preferences and make the CRS more tailored to the cryptocurrency domain. This method can overcome the systems shortcomings in understanding user preferences, impacting recommendations personalization. Accuracy evaluation results show that our proposed system has an Item Coverage of 39.62%, an Expected Popularity Complement (EPC) value of 0.9405, an Average Recommendation Popularity (ARP) value of 0.0595, and a Fine-Tune Training Loss value of 0.4415. Meanwhile, our CRS has a positive response of 81% on satisfaction evaluation. The results show that our CRS has a deeper understanding of the conversations context and delivers more interactive recommendations for cryptocurrency portfolios, providing investors peace of mind when investing.",10.1109/ICoICT66265.2025.11193014,['Conference'],Business; Computer Science
"Exploring AI-Driven Social Media Content Creation Benefits, Barriers, And Good Practices: Experts Interviews",2025,"Marcin Palys, Jakub Krezel","The explosive growth of generative artificial intelligence (GenAI) is reshaping socialmedia workflows, yet its concrete contribution to professional content practice remains underexplored. To address this gap, we conducted semistructured online interviews in February2025 with 14 socialmedia specialists from event management, IT, marketing, graphic design, and gaming organisations and subjected the transcripts to thematic analysis. Practitioners reported integrating large language models such as ChatGPT and visual generators like CanvaMagic Design and AdobeFirefly to accelerate ideation, copywriting, image production, and scheduling, thereby shortening turnaround times, broadening idea pools, and enabling platformspecific personalisation, especially for teams with limited design resources. Respondents also highlighted persistent shortcomings, including formulaic tone, shallow emotional resonance, and inconsistent visual detail, alongside adoption barriers linked to subscription costs, data privacy, and the risk of brandidentity dilution. Effective use, they emphasised, depends on precise prompt engineering and rigorous human editing. By synthesising these crosssector insights, the study clarifies where GenAI currently amplifies efficiency and creativity in socialmedia content creation and where human oversight remains indispensable, providing actionable guidance for managers and setting an agenda for future research in fastevolving platform environments.",10.5171/2025.4542625,['JournalArticle'],Business; Computer Science
Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research,2025,Vincent E. Castillo,"Generative Agent-Based Models (GABMs) powered by large language models (LLMs) offer promising potential for empirical logistics and supply chain management (LSCM) research by enabling realistic simulation of complex human behaviors. Unlike traditional agent-based models, GABMs generate human-like responses through natural language reasoning, which creates potential for new perspectives on emergent LSCM phenomena. However, the validity of LLMs as proxies for human behavior in LSCM simulations is unknown. This study evaluates LLM equivalence of human behavior through a controlled experiment examining dyadic customer-worker engagements in food delivery scenarios. I test six state-of-the-art LLMs against 957 human participants (477 dyads) using a moderated mediation design. This study reveals a need to validate GABMs on two levels: (1) human equivalence testing, and (2) decision process validation. Results reveal GABMs can effectively simulate human behaviors in LSCM; however, an equivalence-versus-process paradox emerges. While a series of Two One-Sided Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level equivalence to humans, structural equation modeling (SEM) reveals artificial decision processes not present in human participants for some LLMs. These findings show GABMs as a potentially viable methodological instrument in LSCM with proper validation checks. The dual-validation framework also provides LSCM researchers with a guide to rigorous GABM development. For practitioners, this study offers evidence-based assessment for LLM selection for operational tasks.",10.48550/arXiv.2508.20234,['JournalArticle'],Business; Computer Science
Behind India's ChatGPT Conversations: A Retrospective Analysis of 238 Unedited User Prompts,2025,Kalyani Khona,"Understanding how users authentically interact with Large Language Models (LLMs) remains a significant challenge in human-computer interaction research. Most existing studies rely on self-reported usage patterns or controlled experimental conditions, potentially missing genuine behavioral adaptations. This study presents a behavioral analysis of the use of English-speaking urban professional ChatGPT in India based on 238 authentic, unedited user prompts from 40 participants in 15+ Indian cities, collected using retrospective survey methodology in August 2025. Using authentic retrospective prompt collection via anonymous social media survey to minimize real-time observer effects, we analyzed genuine usage patterns. Key findings include: (1) 85\% daily usage rate (34/40 users) indicating mature adoption beyond experimental use, (2) evidence of cross-domain integration spanning professional, personal, health and creative contexts among the majority of users, (3) 42.5\% (17/40) primarily use ChatGPT for professional workflows with evidence of real-time problem solving integration, and (4) cultural context navigation strategies with users incorporating Indian cultural specifications in their prompts. Users develop sophisticated adaptation techniques and the formation of advisory relationships for personal guidance. The study reveals the progression from experimental to essential workflow dependency, with users treating ChatGPT as an integrated life assistant rather than a specialized tool. However, the findings are limited to urban professionals in English recruited through social media networks and require a larger demographic validation. This work contributes a novel methodology to capture authentic AI usage patterns and provides evidence-based insights into cultural adaptation strategies among this specific demographic of users.",10.48550/arXiv.2509.13337,"['JournalArticle', 'Review']",Computer Science; Linguistics
Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students,2025,"T. Daryanto, Sophia Stil, Xiaohan Ding, Daniel Manesh, Sang Won Lee, Tim Lee, Stephanie Lunn, Sarah Rodriguez, Chris Brown, E. Rho","One challenge in technical interviews is the think-aloud process, where candidates verbalize their thought processes while solving coding tasks. Despite its importance, opportunities for structured practice remain limited. Conversational AI offers potential assistance, but limited research explores user perceptions of its role in think-aloud practice. To address this gap, we conducted a study with 17 participants using an LLM-based technical interview practice tool. Participants valued AI's role in simulation, feedback, and learning from generated examples. Key design recommendations include promoting social presence in conversational AI for technical interview simulation, providing feedback beyond verbal content analysis, and enabling crowdsourced think-aloud examples through human-AI collaboration. Beyond feature design, we examined broader considerations, including intersectional challenges and potential strategies to address them, how AI-driven interview preparation could promote equitable learning in computing careers, and the need to rethink AI's role in interview practice by suggesting a research direction that integrates human-AI collaboration.",10.48550/arXiv.2507.14418,['JournalArticle'],Computer Science
Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs,2025,"Manon Reusens, Bart Baesens, David Jurgens","Personalized Large Language Models (LLMs) are increasingly used in diverse applications, where they are assigned a specific persona - such as a happy high school teacher - to guide their responses. While prior research has examined how well LLMs adhere to predefined personas in writing style, a comprehensive analysis of consistency across different personas and task types is lacking. In this paper, we introduce a new standardized framework to analyze consistency in persona-assigned LLMs. We define consistency as the extent to which a model maintains coherent responses when assigned the same persona across different tasks and runs. Our framework evaluates personas across four different categories (happiness, occupation, personality, and political stance) spanning multiple task dimensions (survey writing, essay generation, social media post generation, single turn, and multi-turn conversations). Our findings reveal that consistency is influenced by multiple factors, including the assigned persona, stereotypes, and model design choices. Consistency also varies across tasks, increasing with more structured tasks and additional context. All code is available on GitHub.",10.48550/arXiv.2506.02659,"['JournalArticle', 'Review']",Computer Science; Economics
Evaluation of Large Language Model Responses for 32 Diverse Personality Types Using the Best Worst Method (BWM),2024,"Zhaleh Havaei, Morteza Saberi, O. Hussain","Today, Chat Generative Pre-trained Transformer (GPT), a generative language model tool created by OpenAI in November 2022, has greatly impacted the AI field. Large language models (LLMs) effectively gather useful information from interaction with humans in diverse fields, including tutoring, translation, customer support, etc. Certainly, ChatGPT is a chatbot designed to be used in conversation to generate human-like responses, and it can answer all kinds of questions in different fields. This study aims to analyze the performance of LLMs by examining how they respond to different prompts based on individual preferences. We applied different personalities and car features to the prompts and employed the Best-Worst Method (BWM). In This approach, ChatGPT generated responses regarding which car features were considered best and worst based on the 32 different personality types. We evaluated the LLM's responses and examined its performance by categorizing all ChatGPT outputs. We categorize them based on each feature chosen and by which type of 32 personality is the best or least feature. This approach enables us to observe how various personalities of humans influence the responses generated by the language models. It will also show us how ChatGPT's decision-making process makes sense to experts in the field of psychology.",10.1109/ICEBE62490.2024.00048,"['JournalArticle', 'Conference']",Computer Science; Psychology
THE ROLE OF AI APPLICATIONS IN ROMANIA'S BUSINESS AND ADMINISTRATION SECTORS,2025,"L. Boca, Carmen Boghean","This research examines the role of Artificial Intelligence (AI) applications, specifically those using Large Language Models (LLMs) with real-time feedback, decision support system and data analysis and predictive forecasting, across two different key sectors in Romania. Through interviews with a mayor and a company manager, the study investigates how AI can enhance decision-making, improve operational efficiency, and foster innovation in both sectors. The results reveal that AI has the potential to streamline public services, reduce administrative delays, and increase citizen engagement. In the business environment, AI facilitates cost reduction, operational optimization, and improved competitiveness. However, the study identifies several challenges to AI adoption, including infrastructure, a lack of digital skills, financial constraints, and resistance to change. These barriers hinder both public administrations and private companies' ability to fully leverage AI technologies. The findings underscore the importance of fostering collaboration between the public and private sectors to overcome these obstacles. In addition, the research raises some concerns about employee skills which underlines the need to invest in digital education and training to address the skills gap. The study concludes that the adoption of artificial intelligence (AI) technologies offers significant benefits while presenting notable challenges and both the public and private sectors acknowledge the importance of collaboration in developing applications that facilitate information exchange and support strategic decision-making.",10.4316/aepa.2024.24.2(40).220-229,['JournalArticle'],Business; Computer Science; Political Science
Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models,2025,"Paloma Piot, Patricia Mart'in-Rodilla, Javier Parapar","Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided.",10.48550/arXiv.2505.02252,['JournalArticle'],Computer Science
Leveraging Crowdsourced Activity Information for Transit Stations Flow Estimation,2024,"Piergiorgio Vitello, Richard D. Connors, Francesco Viti","Transit flow estimation and prediction requires capturing the complex urban mobility patterns and activity-travel behavior dynamics governing the travel demand. Most approaches rely on data from mobility providers such as smartcard data and travel surveys, which are seldom available for research purposes. Recently, emerging data-driven approaches based on crowdsourced data from mobile devices have gained great interest. These data can be a powerful, easy to collect and widespread source of information, and can be especially useful in areas where traditional transit data is not available or is characterized by low granularity. This work shows the opportunity for leveraging a special type of information, the Google Popular Times (GPT), to estimate passenger demand at stations. We build upon a previously developed data-driven framework, TransitCrowd, which estimates the number of passengers entering and exiting a station from the GPT data of the same station. We show that using GPT information of nearby activities improves the estimation and prediction results. We test and compare different Machine Learning approaches and identify the models that provide more robust results. Our methodology is applied to 185 stations from two different cities: New York and Washington D.C. and are validated using two months of transit count data showing transferability of the models.",10.1109/ACCESS.2024.3494012,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Federated Learning With Client Clustering Selection and Quality-Aware Model Aggregation (2024),2025,"Y. Peng, C. Wang, Hongjian Shi, Ruhui Ma, Haibing Guan, Hanbo Yang","The Industrial Internet of Things (IIoT) is revolutionizing industries through device interconnectivity, enabling real-time data collection and transmission for enhanced monitoring, control, and automation. This has led to improvements in predictive maintenance, production optimization, and supply chain management. The integration of machine learning (ML) and deep learning (DL) within IIoT has been accelerated by increased data availability, computational advancements, and algorithmic innovations, with applications ranging from image processing to autonomous vehicles. Foundation models, such as ChatGPT, are becoming prevalent in IIoT for their capabilities in natural language processing and computer vision. However, the growth of IIoT and foundation models presents challenges, including data volume, real-time processing requirements, computational costs, and security vulnerabilities. Federated learning (FL) addresses these issues by allowing distributed model training without raw data transfer, enhancing privacy and security. FL is particularly beneficial for IIoTs decentralized architecture and real-time decision-making needs. Despite the advantages, FL faces challenges, such as data heterogeneity and communication overhead. To overcome these, we propose a FL framework, Fed-CCSQMA, which includes client selection and model aggregation modules to mitigate data heterogeneitys impact. The client selection module uses Principal Component Analysis (PCA) and clustering to select clients with diverse yet representative data, while the model aggregation module assigns weights based on model accuracy to ensure faster global model convergence. We test our proposed framework on FMNIST and CIFAR-10 datasets, with accuracy surpassing baseline methods and faster convergence, demonstrating an improvement in overall generalizability and learning efficiency.",10.1109/JIOT.2025.3572901,['JournalArticle'],Computer Science; Engineering
"Superhuman AI Disclosure: Impacts on Toxicity, Fairness, and Trust Vary by Expertise and Persona Attributes",2025,"Jaymari Chua, Chen Wang, Lina Yao",,10.48550/arXiv.2503.15514,['JournalArticle'],Computer Science; Psychology
"Survey of Marketing Agents, Agent-Based Models, and Generative AI in Marketing",2025,"Meghana Puvvadi, Sai Kumar Arava, Adarsh Santoria, Sesha Sai Prasanna Chennupati, Harsha Vardhan Puvvadi",,10.1109/GINOTECH63460.2025.11076643,"['Conference', 'Review']",Business; Computer Science
Playing with AI Chat: Positioning Dangerous Language Model Futures through Interactive Fiction,2023,"John Murray, John Murray, Anastasia Salter","Large language models (LLMs) use statistical models to predict the next sequence of tokens and have capabilities previously considered unattainable outside of human intelligence. Communication design can benefit from a close examination of the ongoing conversations around the adoption and use of LLMs, both the public discourse and the specific language and rhetoric used in the initial set of application interfaces and prompts. Through a survey of existing practices and a case study of how AI is used within the interactive fiction community, where procedural content generation has played with expectations and personas, this paper offers a foundation for future critique of these models as they are embedded in the digital tools we rely upon for daily communication and work.",10.1145/3615335.3623015,"['Book', 'JournalArticle', 'Conference', 'Review']",Computer Science; Linguistics
Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics,2025,"Meysam Alizadeh, Fabrizio Gilardi, Zeynab Samei, Mohsen Mosleh","Large language models (LLMs) have traditionally relied on static training data, limiting their knowledge to fixed snapshots. Recent advancements, however, have equipped LLMs with web browsing capabilities, enabling real time information retrieval and multi step reasoning over live web content. While prior studies have demonstrated LLMs ability to access and analyze websites, their capacity to directly retrieve and analyze social media data remains unexplored. Here, we evaluate whether web browsing LLMs can infer demographic attributes of social media users given only their usernames. Using a synthetic dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international participants, we show that these models can access social media content and predict user demographics with reasonable accuracy. Analysis of the synthetic dataset further reveals how LLMs parse and interpret social media profiles, which may introduce gender and political biases against accounts with minimal activity. While this capability holds promise for computational social science in the post API era, it also raises risks of misuse particularly in information operations and targeted advertising underscoring the need for safeguards. We recommend that LLM providers restrict this capability in public facing applications, while preserving controlled access for verified research purposes.",10.48550/arXiv.2507.12372,"['JournalArticle', 'Review']",Computer Science
OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change,2025,"Pat Pataranutaporn, Alexander Doudkin, Pattie Maes","Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures -- specifically a beluga whale, a jellyfish, and a seahorse -- designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.",10.48550/arXiv.2502.02863,['JournalArticle'],Computer Science; Environmental Science
Nudge Evidence Briefing: A Proposal for Transferring Scientific Knowledge about Nudges,2024,"Vinicius Gomes, Jos Adson Cunha, Kssio Lima, H. Moura","A nudge is a concept from Behavioral Economics and Psychology that refers to any small change or intervention designed to influence people's behavior predictably, without restricting their options or significantly altering their incentives. The research follows the Design Science Research methodology, introducing Nudge Evidence Briefing (NEB) to facilitate the understanding, access, application of academic findings on nudges for non-academic professionals, considering the gap between academic research on nudges and their practical application. Leveraging insights from the Evidence-Based Medicine framework, NEBs distill key findings from primary research into concise, accessible documents. Through a systematic review of the literature on nudge integration into software privacy and security, 12 primary studies were selected and the data extracted from them was formatted into NEBs. Participants, specialists and non-specialists, were invited to evaluate the NEB through online questionnaires. Feedback highlighted the clarity and structured format of the NEB, with particular praise for its ability to communicate complex scientific evidence in an accessible way. Overall, the NEB demonstrates significant promise in making nudge-related research more accessible and feasible. Ongoing refinements based on participant feedback will be crucial to realizing its full potential, contributing to the advancement of Human-Computer Interaction (HCI) and the practical application of nudges in professional environments. Future work will focus on evaluating the practical applicability of the NEB with non-academic professionals, exploring more reliable alternatives for generating NEBs through LLM, and developing a comprehensive repository of NEBs.",10.1145/3702038.3702073,"['Book', 'JournalArticle', 'Review']",Computer Science; Economics; Psychology
Large Language Models for Telecom NLP: A Task-Oriented Survey,2025,"Amine Gonca Toprak, yk Berfin Mercan","Large language models (LLMs) have become an important tool for the automation of text-based tasks in the telecommunications domain due to their high success in natural language processing (NLP) in recent years. Tasks such as classification, summarization, question-answering, named entity recognition and sentiment analysis conducted on data sources such as technical documents, user complaints, call center records and standard documents specific to the telecom domain contribute to the improvement of service quality and operational efficiency. However, factors such as domain-specific data scarcity, linguistic diversity and contextual complexity limit the general-purpose use of models and increase the need for domain-adaptive solutions. This study comprehensively covers task-oriented NLP applications in the telecommunications domain and fine-tuned LLM-based methods with open-source datasets used in these applications. In addition, the role of LLMs in improving the quality of interaction is evaluated through applications such as sentiment analysis-based user satisfaction prediction and complaint tendency classification. The review also highlights key limitations and proposes future research directions for the advancement of LLM applications in telecom-specific NLP tasks.",10.1109/ACDSA65407.2025.11166249,"['Conference', 'Review']",Computer Science; Linguistics
TestAgent: An Adaptive and Intelligent Expert for Human Assessment,2025,"Junhao Yu, Zhuang Yan, Yuxuan Sun, Weibo Gao, Qi Liu, Mingyue Cheng, Zhenya Huang, Enhong Chen","Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers' responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions.",10.48550/arXiv.2506.03032,"['JournalArticle', 'Conference']",Computer Science; Psychology
Teaching Language Models To Gather Information Proactively,2025,"Tenghao Huang, Sihao Chen, Muhao Chen, Jonathan May, Longqi Yang, Mengting Wan, Pei Zhou","Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.",10.48550/arXiv.2507.21389,['JournalArticle'],Computer Science; Linguistics
Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events,2025,"Jingyi Gu, Xuan Zhang, Guiling Wang","Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.",10.48550/arXiv.2506.06380,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Cyber Crimes And Social Media Addictions: The Role Of Perceived Usage Of Conversational GPT-4 AI Model Among Residents In Nigeria,2024,"Caleb Onah, C. H. Ogwuche, Joy Oluwatoyin Oladejo","This study investigated cyber crimes and social media addictions: the role of perceived usage of conversational GPT-4 AI model among residents in Makurdi Nigeria. The cross-sectional survey design was used among 200 participants using the latest conversational GPT-4 AI model in their phones using the convenience sampling techniques. Hypothesis one results revealed there is a statistically significant between the usage of the conversational GPT-4 AI model and the incidence of cyber crime [R = .255, R2 = .065, [F (1,189) = 13.223, P< .01]. Hypothesis two showed that the baseline usage of the conversational GPT-4 AI model significantly predicts increase in social media addictions [R = .434, R2 = .188, [F (5,195) = 24.147, P<.01]. Hypothesis three revealed that the usage of conversational GPT-4 AI model is significantly associated with an increase in both cyber crime and social media addictions [R = .474, R2 = .226, [F (2,185) = 5.991; P<.01]. Based on this current study, there is need to develop and deliver educational programs aimed at informing residents and users about the responsible use of AI technologies, including those built on GPT-4 model.",10.22515/ajpc.v5i2.9067,"['JournalArticle', 'Review']",Computer Science; Sociology
ARTIFICIAL INTELLIGENCE IN PERSONNEL SELECTION: A LITERATURE REVIEW OF ORGANIZATIONAL PSYCHOLOGY,2025,Rostyslav Chayka,"Introduction. Artificial Intelligence (AI) has become integral to modern HR processes, particularly in personnel selection. Its integration enables the automation and optimization of traditional hiring approaches, reducing costs and enhancing process efficiency. Technologies such as machine learning, natural language processing (NLP), computer vision, and large language models (LLMs) facilitate resume analysis, candidate evaluation, job-fit prediction, and interview automation. 
Objective. The study aims to systematize existing knowledge about the use of AI in personnel selection, identify its advantages and challenges, and highlight gaps in the scientific literature for future research. 
Methods. A systematic literature review was conducted, analyzing 61 scientific articles from leading databases (PsycINFO, WoS, Scopus, Google Scholar). 
Results. Key AI applications include automated resume screening, chatbot communication, predictive analytics, and behavioral analysis during interviews. AI systems analyze psychological aspects such as personality traits, cognitive abilities, emotional intelligence, social skills, motivation, and cultural fit. Advantages include cost reduction, shorter hiring times, and improved candidate experience. However, challenges such as algorithmic bias and ethical concerns persist. 
Conclusions. AI in personnel selection has the potential to enhance the efficiency and quality of hiring processes significantly. However, its implementation requires a balanced approach considering ethical, legal, and social factors. Future research should focus on developing transparent and fair algorithms, assessing the long-term impact of AI on organizations and candidates, and exploring its adaptation to the specific needs of various industries and cultures.",10.48020/mppj.2025.01.09,"['JournalArticle', 'Review']",Business; Computer Science; Psychology
LLM Generative AI in Design Research - Focused on In-Depth Interview and Its Simulation -,2024,"Chanjong Kim, Ken Nah",,10.18208/ksdc.2024.30.2.65,['JournalArticle'],Computer Science; Engineering
A survey of multi-agent geosimulation methodologies: from ABM to LLM,2025,"Virginia Padilla Sifontes, Jacinto D'avila","We provide a comprehensive examination of agent-based approaches that codify the principles and linkages underlying multi-agent systems, simulations, and information systems. Based on two decades of study, this paper confirms a framework intended as a formal specification for geosimulation platforms. Our findings show that large language models (LLMs) can be effectively incorporated as agent components if they follow a structured architecture specific to fundamental agent activities such as perception, memory, planning, and action. This integration is precisely consistent with the architecture that we formalize, providing a solid platform for next-generation geosimulation systems.",10.48550/arXiv.2507.23694,"['JournalArticle', 'Review']",Computer Science; Environmental Science
"""Amazing, They All Lean Left"" - Analyzing the Political Temperaments of Current LLMs",2025,"W. R. Neuman, Chad Coleman, Ali Dasdan, Safinah Ali, Manan Shah, Kund Meghani","Recent studies have revealed a consistent liberal orientation in the ethical and political responses generated by most commercial large language models (LLMs), yet the underlying causes and resulting implications remain unclear. This paper systematically investigates the political temperament of seven prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity (Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes Moral Foundations Theory, a dozen established political ideology scales and a new index of current political controversies. We find strong and consistent prioritization of liberal-leaning values, particularly care and fairness, across most models. Further analysis attributes this trend to four overlapping factors: Liberal-leaning training corpora, reinforcement learning from human feedback (RLHF), the dominance of liberal frameworks in academic ethical discourse and safety-driven fine-tuning practices. We also distinguish between political""bias""and legitimate epistemic differences, cautioning against conflating the two. A comparison of base and fine-tuned model pairs reveals that fine-tuning generally increases liberal lean, an effect confirmed through both self-report and empirical testing. We argue that this""liberal tilt""is not a programming error or the personal preference of programmers but an emergent property of training on democratic rights-focused discourse. Finally, we propose that LLMs may indirectly echo John Rawls'famous veil-of ignorance philosophical aspiration, reflecting a moral stance unanchored to personal identity or interest. Rather than undermining democratic discourse, this pattern may offer a new lens through which to examine collective reasoning.",10.48550/arXiv.2507.08027,['JournalArticle'],Computer Science; Linguistics; Political Science
Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification,2025,"Jaime E. Cuellar, Oscar Moreno-Martinez, Paula Sofia Torres-Rodriguez, J. A. Pavlich-Mariscal, Andres Felipe Mican-Castiblanco, Juan Guillermo Torres Hurtado","One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT? This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini. Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time. The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications. The results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications. In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish. Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar. These findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions. Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations. The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use.",10.48550/arXiv.2504.12180,['JournalArticle'],Computer Science; Political Science
"AI-Powered Flat Finder: A Real Estate Search Assistant using Gemini, React, and Firebase",2025,Ashutosh Shukla,"ABSTRACT  The real estate industry is increasingly leveraging artificial intelligence to enhance property discovery and decision-making. This paper presents a web-based application that serves as an AI-powered flat-finding assistant. Built using ReactJS for the frontend, Gemini (Googles generative AI) for conversational intelligence, and Firebase for backend and database management, the application enables users to interact with a chatbot to find flats matching their preferences. The system allows users to ask natural language questions, which are interpreted by Gemini AI, and matched with property listings stored in Firebase. A custom training layer is added to Gemini to ensure relevant and consistent answers based on predefined intents. The paper details the architecture, data flow, and interaction design of the platform, emphasizing real-time communication and personalized responses. This work illustrates how AI-driven interfaces can modernize property searches, improve client engagement, and streamline real estate operations.

Keywords  Real Estate, Artificial Intelligence, Gemini AI, Firebase, ReactJS, Flat Finder, Property Recommendation, Chatbot.",10.55041/ijsrem48549,['JournalArticle'],Computer Science; Engineering
SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums,2025,"Mingyang Su, Chao Liu, Jingling Zhang, Shuang Wu, Mingming Fan","Offering diverse perspectives on a museum artifact can deepen visitors'understanding and help avoid the cognitive limitations of a single narrative, ultimately enhancing their overall experience. Physical museums promote diversity through visitor interactions. However, it remains a challenge to present multiple voices appropriately while attracting and sustaining a visitor's attention in the virtual museum. Inspired by recent studies that show the effectiveness of LLM-powered multi-agents in presenting different opinions about an event, we propose SimViews, an interactive multi-agent system that simulates visitor-to-visitor conversational patterns to promote the presentation of diverse perspectives. The system employs LLM-powered multi-agents that simulate virtual visitors with different professional identities, providing diverse interpretations of artifacts. Additionally, we constructed 4 conversational patterns between users and agents to simulate visitor interactions. We conducted a within-subject study with 20 participants, comparing SimViews to a traditional single-agent condition. Our results show that SimViews effectively facilitates the presentation of diverse perspectives through conversations, enhancing participants'understanding of viewpoints and engagement within the virtual museum.",10.1145/3746027.3754593,['JournalArticle'],Art; Computer Science
Predicting Uncertainty of Generative LLMs with MARS: Meaning-Aware Response Scoring,2024,"Y. Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, A. Avestimehr, Chenyang Tao, Dimitrios Dimitriadis","Generative Large Language Models (LLMs) have recently been widely utilized for their unprecedented capabil-ities across many tasks. Considering their use in high-stakes environments and for mission-critical applications, the fact that LLMs often can generate inaccurate or misleading results can be potentially harmful, which motivates us to study the correctness of generative LLM outputs. Uncertainty Estimation (UE) in generative LLMs is a developing area, with state-of-the-art probability-based techniques frequently using length-normalized scoring. As an alternative to length-normalized scoring in UE, in this work, we propose Meaning-Aware Response Scoring (MARS). The key idea of MARS is to consider the semantic contribution of each token of the generated sequence to the context of the question during UE. Through extensive experiments on three question-answering datasets across five pretrained LLMs, we show that utilizing MARS during UE results in a universal and significant improvement in UE performance.",10.1109/ISIT57864.2024.10619136,['JournalArticle'],Computer Science
Modelling Columnists with Large Language Models,2025,"Grkem Afin, Baturalp Arslan Kabaday, Eren zdil, Kamer Kaya, Onur Varol",,10.1109/SIU66497.2025.11112306,"['Conference', 'Review']",Computer Science; Political Science
A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs,2025,"Guoxi Zhang, Jiawei Chen, Tianzhuo Yang, Jiaming Ji, Yaodong Yang, Juntao Dai","The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.",10.48550/arXiv.2506.13245,"['JournalArticle', 'Review']",Computer Science; Linguistics
What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text,2025,"Aswathy Velutharambath, Kai Sassenberg, Roman Klinger","Can deception be detected solely from written text? Cues of deceptive communication are inherently subtle, even more so in text-only communication. Yet, prior studies have reported considerable success in automatic deception detection. We hypothesize that such findings are largely driven by artifacts introduced during data collection and do not generalize beyond specific datasets. We revisit this assumption by introducing a belief-based deception framework, which defines deception as a misalignment between an author's claims and true beliefs, irrespective of factual accuracy, allowing deception cues to be studied in isolation. Based on this framework, we construct three corpora, collectively referred to as DeFaBel, including a German-language corpus of deceptive and non-deceptive arguments and a multilingual version in German and English, each collected under varying conditions to account for belief change and enable cross-linguistic analysis. Using these corpora, we evaluate commonly reported linguistic cues of deception. Across all three DeFaBel variants, these cues show negligible, statistically insignificant correlations with deception labels, contrary to prior work that treats such cues as reliable indicators. We further benchmark against other English deception datasets following similar data collection protocols. While some show statistically significant correlations, effect sizes remain low and, critically, the set of predictive cues is inconsistent across datasets. We also evaluate deception detection using feature-based models, pretrained language models, and instruction-tuned large language models. While some models perform well on established deception datasets, they consistently perform near chance on DeFaBel. Our findings challenge the assumption that deception can be reliably inferred from linguistic cues and call for rethinking how deception is studied and modeled in NLP.",10.48550/arXiv.2505.13147,['JournalArticle'],Computer Science; Linguistics
Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis,2025,"H. Hamada, Ippei Fujisawa, Genji Kawakita, Yuki Yamada","Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities in producing human-like text. However, it is unclear how accurately these models internalize concepts that shape human thought and behavior. Here, we developed a quantitative framework to assess concept alignment between LLMs and human psychological dimensions using 43 standardized psychological questionnaires, selected for their established validity in measuring distinct psychological constructs. Our method evaluates how accurately language models reconstruct and classify questionnaire items through pairwise similarity analysis. We compared resulting cluster structures with the original categorical labels using hierarchical clustering. A GPT-4 model achieved superior classification accuracy (66.2\%), significantly outperforming GPT-3.5 (55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%). We also demonstrated that the estimated semantic similarity from GPT-4 is associated with Pearson's correlation coefficients of human responses in multiple psychological questionnaires. This framework provides a novel approach to evaluate the alignment of the human-LLM concept and identify potential representational biases. Our findings demonstrate that modern LLMs can approximate human psychological constructs with measurable accuracy, offering insights for developing more interpretable AI systems.",10.48550/arXiv.2506.23055,['JournalArticle'],Computer Science; Psychology
Generative Artificial Intelligence (AI) in News: A case study of selected digital-native news outlets in Zimbabwe,2025,"Douglas Ncube, Christina Ncube","The excitement and widespread publicity of OpenAIs Chat Generative Pre-Trained Transformer (ChatGPT) in 2022 further allowed news outlets and journalists to reimagine their interactions and engagements with audience members. Newsrooms, particularly digital-native, have increasingly woven generative artificial intelligence (AI) tools such as, among others, Google Gemini, ChatGPT, DALL-E 2, Canva, Grammarly, Headline Hero, StoryMap, Chartbeat, GitHub Copilot and Microsoft Copilot into their news production processes. Such AI tools are harnessed for their potential to automate text, image, video and audio creation, streamlining workflows and personalising content. Arguably, these are profound capabilities that are swiftly becoming essential for digital-native news outlets in a competitive environment. This study investigates how four digital-native news outlets, namely Matabeleland Pulse, Community Podium, Centre for Innovation and Technology (CITE), and Eco Climate Network, are adopting generative AI, their motivation in deploying such tools, and how newsroom leaders perceive the influence of these tools on journalistic practice. The study is anchored on the Social Construction of Technology (SCOT) framework, which emphasises that social, economic and cultural forces shape the adoption of technologies as much as technical efficiency does. This study analyses this phenomenon along the three axes of the journalistic field. These are, (i) techno-economic means, that is, financial and technical resources that enable or constrain an adoption, (ii) subject of journalism, that is, journalists perspectives on AI in their professional practice and (iii) object of journalism, that is, audience reception, trust and engagement with AI-generated content from the newsroom editors perspective. The study employs a qualitative design methodology using online surveys and brief follow-up interviews with newsroom leaders.",10.64645/nust.sajcis.3.1.268,"['JournalArticle', 'Review']",Computer Science; Linguistics
A Survey on Current Trends and Recent Advances in Text Anonymization,2025,"Tobias Deuer, Lorenz Sparrenberg, Armin Berger, Max Hahnbck, Christian Bauckhage, R. Sifa","The proliferation of textual data containing sensitive personal information across various domains requires robust anonymization techniques to protect privacy and comply with regulations, while preserving data usability for diverse and crucial downstream tasks. This survey provides a comprehensive overview of current trends and recent advances in text anonymization techniques. We begin by discussing foundational approaches, primarily centered on Named Entity Recognition, before examining the transformative impact of Large Language Models, detailing their dual role as sophisticated anonymizers and potent de-anonymization threats. The survey further explores domain-specific challenges and tailored solutions in critical sectors such as healthcare, law, finance, and education. We investigate advanced methodologies incorporating formal privacy models and risk-aware frameworks, and address the specialized subfield of authorship anonymization. Additionally, we review evaluation frameworks, comprehensive metrics, benchmarks, and practical toolkits for real-world deployment of anonymization solutions. This review consolidates current knowledge, identifies emerging trends and persistent challenges, including the evolving privacy-utility trade-off, the need to address quasi-identifiers, and the implications of LLM capabilities, and aims to guide future research directions for both academics and practitioners in this field.",10.48550/arXiv.2508.21587,"['JournalArticle', 'Review']",Computer Science
KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors,2025,"Zhiyang Qi, Takumasa Kaneko, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba","Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.",10.48550/arXiv.2506.01357,"['JournalArticle', 'Conference']",Computer Science; Psychology
Automated Startup Analysis Using Large Language Models and API Integration,2025,Purushotham Vadde,"Identifying and analyzing startups is a crucial yet time-intensive task for investors, researchers, and venture capitalists. Traditional methods rely on manual searches, conferences, and personal networks, which often lead to inefficiencies. This paper presents an AI-driven approach that leverages Large Language Models (LLMs) in conjunction with API integrations to streamline the process of startup discovery and evaluation. By combining OpenAIs LLM agents with Bing and Crunchbase APIs, the proposed system automates data collection, web scraping, and information synthesis. The methodology involves generating precise search queries, extracting company details, and retrieving structured insights on startups, including their founders, funding details, and industry presence. Experimental results demonstrate the effectiveness of this approach in improving the speed and accuracy of startup analysis. The findings highlight the potential of LLM-powered sourcing agents in transforming startup research, making it more scalable and data-driven for decision-makers in the investment and business ecosystem.",10.1109/IC2E365635.2025.11166754,['Conference'],Business; Computer Science
Can Large Language Models Predict Associations Among Human Attitudes?,2025,"Ana Ma, Derek Powell","Prior work has shown that large language models (LLMs) can predict human attitudes based on other attitudes, but this work has largely focused on predictions from highly similar and interrelated attitudes. In contrast, human attitudes are often strongly associated even across disparate and dissimilar topics. Using a novel dataset of human responses toward diverse attitude statements, we found that a frontier language model (GPT-4o) was able to recreate the pairwise correlations among individual attitudes and to predict individuals' attitudes from one another. Crucially, in an advance over prior work, we tested GPT-4o's ability to predict in the absence of surface-similarity between attitudes, finding that while surface similarity improves prediction accuracy, the model was still highly-capable of generating meaningful social inferences between dissimilar attitudes. Altogether, our findings indicate that LLMs capture crucial aspects of the deeper, latent structure of human belief systems.",10.48550/arXiv.2503.21011,['JournalArticle'],Computer Science; Linguistics; Psychology
Beyond ChatGMP: Improving LLM generation through user preferences,2025,"Fiammetta Caccavale, Carina L. Gargalo, K. Gernaey, Ulrich Krhne, Alessandra Russo","Prompt engineering  improving the command given to a large language model (LLM)  is becoming increasingly useful in order to maximize the performance of the model and therefore the quality of the output. However, in certain instances, the user is not able to enrich the prompt with additional and personalized details, such as the preferred tone and length of generated response. Therefore, it is useful to create models that learn these preferences and implement them directly in the prompt. Current state-of-the-art inductive logic programming (ILP) systems can play an important role in the development and advancement of digitalization strategies. For example, they can be used to learn personal preferences of users without sacrificing human interpretability of the learned outcomes. These systems have recently witnessed the development of data efficient, robust, and human interpretable algorithms and systems for learning predictive models from data and background knowledge. In this paper, one of these systems, ILASP (inductive learning of answer set programs), is used to develop a proof of concept of how personal preferences of groups of students participating in an interview exercise can be learned to tailor and improve the generated response of a LLM used in an educational context.",10.69997/sct.144855,nan,Computer Science
Interpretable Video based Stress Detection with Self-Refine Chain Reasoning,2025,"Yi Dai, Yang Ding, Lei Cao, Kaisheng Zeng, Junrui Tian, Zexi Lin, Ling Feng","Stress detection is critical for mental and physical well-being, yet traditional methods such as self-reports and physiological sensors face limitations in efficiency and scalability. Video-based stress detection, leveraging visual cues learned from an annotated video database, offers a non-invasive, cost-effective alternative. However, most models function as black boxes, lacking transparency in their decision-making process, which hinders their trustworthiness. To address this, we propose an interpretable video-based stress detection model that incorporates Chain-of-Thought (CoT) reasoning of large foundation models. Our model follows a structured reasoning chain Describes Assess-e-Highlight, mimicking the decision process of psychology experts. To further enhance model reliability, we integrate a self-refinement mechanism that allows the model to reflect on and improve its predictions using Direct Preference Optimization (DPO) to ensure accuracy and faithfulness. Experimental results on two video-based stress detection datasets demonstrate that our approach outperforms state-of-the-art models in both accuracy and interpretability. We release our code at https://github.com/debby1103/stressdetection.git.",10.1109/ICDE65448.2025.00206,"['JournalArticle', 'Conference']",Computer Science; Psychology
Human-AI Alignment of Multimodal Large Language Models with Speech-Language Pathologists in Parent-Child Interactions,2025,"Weiyan Shi, K. Choo","Joint attention is a critical marker of early social-communicative development, yet remains difficult for caregivers to assess without expert guidance. In this work, we explore how multimodal large language models (MLLMs) can be aligned with the reasoning processes of speech-language pathologists (SLPs) to support the interpretation of everyday parent-child interactions. We conducted in-depth interviews and video annotation studies with three experienced SLPs to uncover how they evaluate joint attention based on three core behavioural cues: gaze, action, and vocalisation. Using these insights, we developed a two-stage MLLM-based system that first extracts fine-grained behavioural descriptions from video segments and then judge joint attention quality using expert-aligned prompts. Our evaluation across 26 parent-child interaction videos shows that MLLMs can achieve up to 85% accuracy in perceptual cue extraction and over 75% average precision in simulating expert judgement. We further propose design guidelines for building MLLM-based behaviour observation-judgement systems that align with SLPs, emphasising the structuring of behavioural cues, the construction of exemplar libraries grounded in expert annotations, and the need to personalise system responses based on developmental stage and neurotypical or atypical presentation. This work provides structured behavioural cues derived from SLP expertise, demonstrates the feasibility of aligning SLPs observation and judgement using MLLMs, and offers practical design guidelines for building aligned systems to support parent-child interaction analysis.",10.48550/arXiv.2506.05879,['JournalArticle'],Computer Science; Linguistics; Psychology
Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision,2025,"Rosiana Natalie, Wenqian Xu, Ruei-Che Chang, Rada Mihalcea, Anhong Guo","Advances in vision language models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. However, prior research has not investigated such simulation capabilities in the accessibility domain. In this paper, we evaluate the extent to which VLMs can simulate the vision perception of low vision individuals when interpreting images. We first compile a benchmark dataset through a survey study with 40 low vision participants, collecting their brief and detailed vision information and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we construct prompts for VLMs (GPT-4o) to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluate the agreement between VLM-generated responses and participants'original answers. Our results indicate that VLMs tend to infer beyond the specified vision ability when given minimal prompts, resulting in low agreement (0.59). The agreement between the agent'and participants'responses remains low when only either the vision information (0.59) or example image responses (0.59) are provided, whereas a combination of both significantly increase the agreement (0.70, p<0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offers significant performance improvements over either alone (p<0.0001), while additional examples provided minimal benefits (p>0.05).",10.48550/arXiv.2508.10972,"['JournalArticle', 'Review']",Computer Science; Psychology
AffectLLM: A Large Language Model for Rating the Affective Quality of Representations,2025,"Abdallah Chehade, Areen Alsaid, Caleb Siefert, Barry Dauphin","Affective computing presents significant challenges, particularly in the context of human-machine interactions. Traditional approaches often involve developing data-driven models that rely on physiological and image-based data to infer the emotional states of individuals during interactions with machines. However, these models frequently lack contextual relevance and perform poorly in complex scenarios involving conflicting cues. Additionally, they suffer from weak generalization due to the absence of structured methodologies that disentangle interconnected external factors. To address these limitations, we propose leveraging the Affective Quality of Representations (AFF) score, a standardized metric for assessing human affective responses across varied contexts. The AFF score is derived from expert ratings of participant narratives elicited by questions or visual cards, following established SCORS-G guidelines. The primary challenge of this approach is its reliance on trained psychology experts for scoring. To mitigate this, we developed the AffectLLM, a large language model designed to predict AFF scores efficiently and support the analysis of human-machine interactions. A strong contribution of AffectLLM is providing a structured metric that can guide the design of humanmachine interaction environments that promote more positive affect quality outcomes.",10.1109/ICHMS65439.2025.11154254,"['JournalArticle', 'Conference']",Computer Science
From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models,2025,"M. Kamruzzaman, Abdullah Al Monsur, Gene Louis Kim, Anshuman Chhabra","Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.",10.48550/arXiv.2506.02431,['JournalArticle'],Computer Science; Linguistics; Psychology; Sociology
Assessing the Correlation Between News Sentiment and Stock Price Movements: A Case Study of WeWork Using Advanced NLP Techniques,2024,"Hamda Saeed Binhamdan Al Nahyan, Ahmed Shuhaiber","This research explores the intricate relationship between news article sentiment and stock price movements, with the company WeWork serving as a case study. Leveraging advanced Natural Language Processing (NLP) techniques and Large Language Models (LLMs) such as Open AI, this study aims to transform unstructured textual data into actionable financial insights. Through rigorous data collection and preprocessing, sentiment scores were derived from a wide range of news sources and correlated with historical stock prices. Statistical analyses, including linear regression and correlation metrics, revealed a weak positive correlation of 16.47% between sentiment and stock prices. Although the correlation suggests that sentiment analysis can offer valuable insights into market trends, it is insufficient as a standalone predictor for investment decisions. The findings underscore the importance of integrating sentiment analysis with traditional financial metrics, and call for the development of more robust models incorporating diverse data sources in future research.",10.1109/ICCA62237.2024.10927897,"['JournalArticle', 'Conference']",Business; Computer Science; Economics
Prevalence and prevention of large language model use in crowd work,2023,"Veniamin Veselovsky, Manoel Horta Ribeiro, Philip J. Cozzolino, Andrew Gordon, David M. Rothschild, Robert West","We show that the use of large language models (LLMs) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use. On a text summarization task where workers were not directed in any way regarding their LLM use, the estimated prevalence of LLM use was around 30%, but was reduced by about half by asking workers to not use LLMs and by raising the cost of using them, e.g., by disabling copy-pasting. Secondary analyses give further insight into LLM use and its prevention: LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information. Our estimates will likely change as LLMs increase in popularity or capabilities, and as norms around their usage change. Yet, understanding the co-evolution of LLM-based tools and users is key to maintaining the validity of research done using crowdsourcing, and we provide a critical baseline before widespread adoption ensues.",10.48550/arXiv.2310.15683,['JournalArticle'],Computer Science; Psychology
An AI-Powered Platform for Personalized Mock Interviews Using LLMs and Modern Full-Stack Web Technologies,2025,"Mr. K Kiran Babu, CH. Srikaran, G. Chandrashekar, D. Akshara, K. A. Reddy",,10.55248/gengpi.6.0625.2380,['JournalArticle'],Computer Science
ASSESSING OCCUPATIONS THROUGH ARTIFICIAL INTELLIGENCE: A COMPARISON OF HUMANS AND GPT-4,2025,"Christoph Lutz, Pawe Gmyrek, G. Newlands","Large language models (LLMs) such as GPT-4 have raised questions about the changing nature of work. Research has started to investigate how this technology affects labor markets and might replace or augment different types of jobs. Beyond their economic implications in the world of work, there are important sociological questions about how LLMs connect to subjective evaluations of work, such as the prestige and perceived social value of different occupations, and how the widespread use of LLMs perpetuate often biased views on the labor markets reflected in their training datasets. Despite initial research on LLMs world model, their inherent biases, attitudes and personalities, we lack evidence on how LLMs themselves evaluate occupations as well as how well they emulate the occupational evaluations of human evaluators. We present a systematic comparison of GPT-4 occupational evaluations with those from an in-depth, high-quality survey in the UK context. Our findings indicate that GPT-4 and human scores are highly correlated across all ISCO-08 major groups for prestige and social value. At the same time, GPT-4 substantially under- or overestimates the occupational prestige and social value of many occupations, particularly emerging occupations as well as stigmatized or contextual ones. In absolute terms, GPT-4 scores are more generous than those of the human respondents. Our analyses show both the potentials and risks of using LLM-generated data for occupational research.",10.5210/spir.v2024i0.13993,"['JournalArticle', 'Review']",Computer Science; Sociology
Great Power Brings Great Responsibility: Personalizing Conversational AI for Diverse Problem-Solvers,2025,"talo Santos, K. Felizardo, Igor Steinmacher, M. Gerosa","Newcomers onboarding to Open Source Software (OSS) projects face many challenges. Large Language Models (LLMs), like ChatGPT, have emerged as potential resources for answering questions and providing guidance, with many developers now turning to ChatGPT over traditional Q&A sites like Stack Overflow. Nonetheless, LLMs may carry biases in presenting information, which can be especially impactful for newcomers whose problem-solving styles may not be broadly represented. This raises important questions about the accessibility of AI-driven support for newcomers to OSS projects. This vision paper outlines the potential of adapting AI responses to various problem-solving styles to avoid privileging a particular subgroup. We discuss the potential of AI persona-based prompt engineering as a strategy for interacting with AI. This study invites further research to refine AI-based tools to better support contributions to OSS projects.",10.1109/CHASE66643.2025.00019,"['JournalArticle', 'Conference']",Computer Science
AuPPLE: Augmented Physical Priors through Language Enhancement using Self-Supervised Learning,2023,"Annie Dong, Zhuo En Chua, Anirudh Mazumder, Eveer Soriano, Mustafa Efe Gzel, Priyanshu Sethi, Badis Labbedi, M. Lutz","In recent years, a contentious debate has emerged surrounding the degree to which Large Language Models (LLMs) can truly achieve grounding in the physical world. Grounding, in this context, refers to the models ability to establish a meaningful connection between their language-based understanding and a concrete comprehension of real-world phenomena. Our research aims to explore the latent capability of LLMs to develop physical intuition: a prerequisite for embodied agents to effectively perform tasks in real-world environments. In this paper, we release a novel dataset of physical scenarios that serve as a benchmark for an LLMs physical intuition. Our benchmark AuPPLE (Augmented Physical Priors through Language Enhancement) for Language Models includes scenarios regarding free-fall and projectile motion, including various question-answer formulations: MultiQA, binary classification, and continuous number prediction to comprehend linguistic nuances and apply their understanding within a physical context. By meticulously fine-tuning LLMs on this specialized dataset, we assess their performance in providing responses that showcase an ability to draw upon underlying physical knowledge. With our fine-tuned LLMs achieving over 87%more than 3 times its base modelon free-fall evaluation dataset, our results shed light on the intrinsic grounding capabilities of LLMs, offering insights into their potential to bridge the gap between language and the physical world. This paper contributes to the ongoing discourse on the true nature of LLMs comprehension and its relationship with real-world context, underscoring the strides made in enhancing their intuitive understanding through targeted fine-tuning techniques.",10.1109/ICTC58733.2023.10392629,"['JournalArticle', 'Conference']",Computer Science; Engineering
Research on the Challenges and Countermeasures of Online Public Opinion under Generative Artificial Intelligence,2024,Zhang Qiang,"With the widespread application and sustained impact of AIGC (Artificial Intelligence Generated Content) technology, exemplified by Chat GPT, the ecology of online public opinion in the field of news communication is undergoing profound changes. The new characteristics of online public opinion, such as massive data, multimodal information, precise dissemination, content confusion, and efficient rate, present challenges including more complex information, emotional dissemination, intricate fields, and difficult judgment and analysis. To address these, responses can be formulated from macro, meso, and micro levels, including accelerating legislative standardization, identifying responsible entities, and enhancing personal literacy, thereby improving the governance capacity of public opinion.",10.54097/gjdm0630,['JournalArticle'],Computer Science; Political Science
A Novel Multi-Modal Framework for Sentiment Driven Depression Intensity Assessment,2025,"K. Jhansi, Lakshmi Bai, Y. Radhika","
 
 
 
Introduction: Depression is a mental illness that can cause a low mood, loss of interest in doing things, and sudden behavioral changes. Major depression may lead to suicide. It is the life threatening problem to be addressed immediately. According to the World Health Organization (WHO), about 280 million people worldwide have depression, including 5% of the worlds adults and 5.7% of adults above age 60. Once a person diagnosed with depression treatment continue by attending therapy sessions. All these traditional approaches will work on those who already diagnosed with the depression. Through the social media platforms vast data is generating, from the content taken from theses platforms by applying Large Language Models, which works on contest based rather than key based would give better results. So a preventive mechanism of finding depression severity from the text is a novel idea for earlier prediction. 
Objectives: The main objective of the paper is to identify the depression intensity from the social media posts by applying the LLMs to find the behavioral and emotional patterns of posts. Identification of negative sentiments from the posts by using LLMs. PHQ-9 Questionnaire is used to find the t The LLM based models identifies the negative sentiments based on polarity. From these posts the seed terms were identified based PHQ-9 questionnaire. The next objective is to identify the intensity based the terms used in the posts. 
Methods: This study proposes a multi-modal framework for depression intensity using NLP techniques. The data is extracted from twitter posted based on depression related hashtags. Preprocessing and feature extraction is done. Once data is ready after cleaning the transformer models like BERT, ALBERT techniques are applied fro depression severity estimation. Model is evaluated using deep learning models (BiLSTM, GRU) with accuracy, F1-score and loss estimation was evaluated. 
Results: The bLSTM model achieved 92% accuracy, outperforming traditional methods. Transformer-based models (BERT, ALBERT) demonstrated improved classification performance. 
Conclusions: The multi modal approach proved to be more accurate for earlier prediction purpose. The Same approach can be applied on various questionnaire based approaches for model evaluation. This approach can be used for self-assessment purpose. 
 
 
 
 
",10.52783/jisem.v10i30s.4779,['JournalArticle'],Computer Science; Psychology
Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity,2025,"Qi Zhou, Jie Zhang, Dongxia Wang, Qiang Liu, Tianlin Li, Jin Song Dong, Wenhai Wang, Qing Guo","Human preference plays a crucial role in the refinement of large language models (LLMs). However, collecting human preference feedback is costly and most existing datasets neglect the correlation between personalization and preferences. To address this issue, we introduce Fair-PP, a synthetic dataset of personalized preferences targeting social equity, derived from real-world social survey data, which includes 28 social groups, 98 equity topics, and 5 personal preference dimensions. Leveraging GPT-4o-mini, we engage in role-playing based on seven representative persona portrayals guided by existing social survey data, yielding a total of 238,623 preference records. Through Fair-PP, we also contribute (i) An automated framework for generating preference data, along with a more fine-grained dataset of personalized preferences; (ii) analysis of the positioning of the existing mainstream LLMs across five major global regions within the personalized preference space; and (iii) a sample reweighting method for personalized preference alignment, enabling alignment with a target persona while maximizing the divergence from other personas. Empirical experiments show our method outperforms the baselines.",10.48550/arXiv.2505.11861,"['JournalArticle', 'Review']",Computer Science
Reliability of Distribution Predictions by LLMs: Insights from Counterintuitive Pseudo-Distributions,2025,"Toma Suzuki, Ayuki Katayama, Seiji Gobara, Ryo Tsujimoto, Hibiki Nakatani, Kazuki Hayashi, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe, A. Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Hyung Paul Barham, Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, M. Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, S. Ghemawat, Sunipa Dev, H. Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, L. Fedus, Denny Zhou, Daphne Ippolito, D. Luan, Hyeontaek Lim, Bar-ret Zoph, A. Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana, Marie Pellat, Aitor Lewkowycz, Erica Moreira, R. Child, Oleksandr Polozov, Kather-ine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Daz, Orhan Firat, M. Catasta, Jason Wei, K. Meier-Hellstern, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, A. Hartshorn, Aobo Yang, Archi Mitra, A. Sravankumar, A. Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozire, Bethany Biron, Binh Tang, Bobbie Chern, C. Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, C. Touret, Chunyang Wu, Corinne Wong, Cris-tian Cantn Ferrer, Cyrus Nikolaidis, Damien Al-lonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mi-alon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Han-nah Korevaar, Hu Xu, Hugo Touvron, Imanol Iliyan Zarov, Arrieta Ibarra, Is-han Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, J. V. D. Linde, Jennifer Billock, Jenny Hong, Jenya Lee, J. Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, J. Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Kenneth Heafield, Kevin R. Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen-ley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Ma-hesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, M. Kambadur, Mike Lewis, Mitesh Min Si, Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bash-lykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasi, Peter Weng, Prajjwal Bhargava, P. Dubal, Praveen Krishnan Punit, S. Koura, Puxin Xu, Qing He, Qingxiao, Ragavan Dong, Raj Srinivasan, Ramon Ganapathy, Ricardo Silveira Calderer, Robert Cabral, Stojnic Roberta, R. Raileanu, Rohit Girdhar, Rohit Patel, R. Sauvestre, Roshan Polidoro, Sumbaly Ross, Ruan Taylor, Rui Silva, Rui Hou, Saghar Wang, Sahana Hosseini, S. Chennabasappa, Singh Sean, Seohyun Sonia Bell, Sergey Kim, Edunov Shaoliang, Sharan Nie, Sharath Narang, Raparthy Sheng, Sheng Shen, S.Y. Wan, Shun Bhosale, Simon Zhang, Soumya Vandenhende, Spencer Batra, Sten Whitman, Stephane Sootla, Suchin Collot, Sydney Gu-rurangan, Tamar Borodinsky, Tara Herman, Tarek Fowler, Thomas Sheasha, Thomas Georgiou, Tobias Scialom, Todor Speckbacher, Tong Mihaylov, Ujjwal Xiao, Vedanuj Karn, Vibhor Goswami, Vignesh Gupta, Viktor Ramanathan, Vincent Kerkez, Virginie Gonguet, Vish Do, Vladan Vogeti, Weiwei Petro-vic, Wenhan Chu, Wenyin Xiong, Whit-ney Fu, Xavier Meers, Xiaodong Martinet, Xiao-qing Wang, Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiwen Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, A. Menon, Ajay Sharma, Alex Boesen-berg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, An-drei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Apara-jita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yaz-dan, Beau James, Ben Maurer, B. Leonhardi, Bernie Huang, Beth Loyd, Beto de Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, B. Ni, Braden Han-cock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Tes-tuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Hol-land, Edward Dowling, Eissa Jamil, Elaine Mont-gomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmn, Frank J. Kanayet, Frank Seide, Gabriela Medina, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, G. Sizov, Guangyi, Guna Zhang, Lakshminarayanan Hamid, Han Shojanazeri, Han Zou, Hanwen Wang, Haroun Zha, Harrison Habeeb, Helen Rudolph, Henry Suk, Hunter Aspegren, Ibrahim Goldman, Igor Damlaj, Igor Molybog, Irina-Elena Tufanov, Itai Veliche, Jake Gat, James Weissman, Geboski James, Japhet Kohli, Jean-Baptiste Asher, Gaya Jeff, Jeff Marcus, Jennifer Tang, Jenny Chan, Zhen Jeremy, J. Reizenstein, Jessica Teboul, Zhong Jian, Jingyi Jin, Joe Yang, Jon Cummings, Carvill Jon, Jon Shepard, J. McPhie, Torres Josh, Junjie Ginsburg, Kai Wang, Kam Wu, Hou Karan, Karthik Saxena, Kartikay Prasad, Katayoun Khan-delwal, Kathy Zand, Kaushik Matosich, Kelly Veeraraghavan, Keqian Michelena, Kun Li, Kunal Huang, Kushal Chawla, Kyle Lakhotia, Lailin Huang, Lakshya Chen, Lavender A Leandro Garg, Lee Silva, Lei Bell, Liangpeng Zhang, Licheng Guo, Liron Yu, L. Moshkovich, Madian Wehrstedt, Manav Khabsa, Manish Avalani, Maria Bhatt, Martynas Tsim-poukelli, Matan Mankus, Matthew Hasson, Matthias Lennie, M. Reso, Maxim Groshev, Maya Naumov, Meghan Lathi, Michael L Keneally, Michal Seltzer, M. Valko, Mihir Restrepo, Mik Patel, Mik Vyatskov, Mike Samvelyan, Mike Clark, Mike Macey, Miquel Wang, Jubert Mo, Mo Metanat, Mun-ish Rastegari, Nandhini Bansal, Natascha Santhanam, Parks Natasha, Navyata White, Nayan Bawa, Nick Singhal, Nicolas Egebo, Usunier, Nikolay Pavlovich, Laptev Ning, Ning Dong, Norman Zhang, Oleg Cheng, Olivia Chernoguz, Omkar Hart, Ozlem Salpekar, Parkin Kalinli, Parth Kent, Paul Parekh, Pa-van Saab","The proportion of responses to a question and its options, known as the response distribution, enables detailed analysis of human society. Recent studies highlight the use of Large Language Models (LLMs) for predicting response distributions as a cost-effective survey method. However, the reliability of these predictions remains unclear. LLMs often generate answers by blindly following instructions rather than applying rational reasoning based on pretraining-acquired knowledge. This study investigates whether LLMs can rationally estimate distributions when presented with explanations of artificially generated distributions that are against commonsense. Specifically, we assess whether LLMs recognize counterintuitive explanations and adjust their predictions or simply follow these inconsistent explanations. Results indicate that smaller or less human-optimized LLMs tend to follow explanations uncritically, while larger or more optimized models are better at resisting counterintuitive explanations by leveraging their pretraining-acquired knowledge. These findings shed light on factors influencing distribution prediction performance in LLMs and are crucial for developing reliable distribution predictions using language models.",10.18653/v1/2025.naacl-srw.52,"['JournalArticle', 'Conference', 'Review']",Computer Science; Psychology
Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults,2025,"Lei Han, Mingnan Wei, Qiongyan Chen, Anqi Wang, Rong Pang, Kefei Liu, Rongrong Chen, David Yip","Photo-based reminiscence has the potential to have a positive impact on older adults'reconnection with their personal history and improve their well-being. Supporting reminiscence in older adults through technological implementations is becoming an increasingly important area of research in the fields of HCI and CSCW. However, the impact of integrating gaze and speech as mixed-initiative interactions in LLM-powered reminiscence conversations remains under-explored. To address this, we conducted expert interviews to understand the challenges that older adults face with LLM-powered, photo-based reminiscence experiences. Based on these design considerations, we developed Eye2Recall, a system that integrates eye tracking for detecting visual interest with natural language interaction to create a mixed-initiative reminiscence experience. We evaluated its effectiveness through a user study involving ten older adults. The results have important implications for the future design of more accessible and empowering reminiscence technologies that better align with older adults'natural interaction patterns and enhance their positive aging.",10.48550/arXiv.2508.02232,['JournalArticle'],Computer Science
Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study,2025,"Bing Song, Jianing Liu, Sisi Jian, Chenyang Wu, Vinayak Dixit","Large language models (LLMs) have made significant strides, extending their applications to dialogue systems, automated content creation, and domain-specific advisory tasks. However, as their use grows, concerns have emerged regarding their reliability in simulating complex decision-making behavior, such as risky decision-making, where a single choice can lead to multiple outcomes. This study investigates the ability of LLMs to simulate risky decision-making scenarios. We compare model-generated decisions with actual human responses in a series of lottery-based tasks, using transportation stated preference survey data from participants in Sydney, Dhaka, Hong Kong, and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk preferences were analyzed using the Constant Relative Risk Aversion (CRRA) framework. Results show that both models exhibit more risk-averse behavior than human participants, with o1-mini aligning more closely with observed human decisions. Further analysis of multilingual data from Nanjing and Hong Kong indicates that model predictions in Chinese deviate more from actual responses compared to English, suggesting that prompt language may influence simulation performance. These findings highlight both the promise and the current limitations of LLMs in replicating human-like risk behavior, particularly in linguistic and cultural settings.",10.48550/arXiv.2506.23107,"['JournalArticle', 'Review']",Computer Science; Psychology; Sociology
Scalable Pre-Training of Compact Urban Spatio-Temporal Predictive Models on Large-Scale Multi-Domain Data,2025,"Jindong Han, Hao Wang, Hui Xiong, Hao Liu","
 Spatio-Temporal Prediction (STP) is crucial for various smart city applications, such as traffic management and resource allocation. However, training samples can be scarce in data-constrained scenarios, which often degrades the predictive capability of existing deep STP models. Although recent STP foundation models excel in few-shot and zero-shot learning through extensive pre-training on large-scale, multi-domain spatio-temporal data, they often rely on large parameter scale to achieve enhanced performance, resulting in high computational demands that hinder practical deployment. In response, we develop CompactST, an efficient, compact, and versatile pre-trained model for STP in data-scarce settings. Recognizing the complexities posed by large-scale, heterogeneous pre-training datasets, CompactST integrates three specialized components: (1) a mixture-of-normalizers module to address domain and spatial heterogeneity, (2) a multi-scale spatio-temporal mixer that captures diverse patterns from datasets with varying spatio-temporal resolutions, and (3) an adaptive dataset-oriented tuning module that transfers the handling of dataset-specific parameters from pre-training to fine-tuning stage. These tailored designs enable CompactST to maximize generalizability across diverse datasets while maintaining a compact model size (
 i.e.
 , only 300K parameters). To validate its effectiveness, we pre-train CompactST on a substantial corpus of public spatio-temporal datasets spanning over 10 domains and encompassing 300 million data points. Extensive experimental results on ten real-world datasets demonstrate CompactST's significantly improved prediction accuracy and efficiency in data-scarce scenarios.
",10.14778/3734839.3734851,['JournalArticle'],Computer Science; Engineering; Environmental Science
AI-Buddies in MMORPGs: Player Perceptions of Conceptual LLM-Driven NPCs in World of Warcraft,2025,"Rasmus Ploug, Marco Scirea","This study investigates player perceptions of conceptual Large Language Model (LLM)-driven Non-Player Characters (NPCs) in World of Warcraft (WoW), referred to as AIBuddies. Building on prior work on Conversational Artificial Autonomous Agents (CA-bots), it explores how such companions could enhance gameplay and player experience. A mixed-method survey of 273 WoW players was conducted, using visual mock-ups to present the AI-Buddy concept rather than a working prototype. Participants evaluated interaction features, customization options, and potential gameplay impact. Results show general support, particularly among Casual players, for using AI-Buddies to improve solo content, personalization, and immersion. However, concerns were raised about balance, multiplayer exploitation, and ethical implications. Thematic analysis also revealed worries about reduced social interaction and technical feasibility. This short study contributes to ongoing discussions on CA-bots and LLM-driven NPCs in MMORPGs and offers early insights into how such features are perceived by the WoW playerbase.",10.1109/CoG64752.2025.11114149,"['JournalArticle', 'Conference', 'Review']",Computer Science; Sociology
Exploring persuasive interactions with generative social robots: An experimental framework,2025,"Stephan Vonschallen, Larissa Julia Corina Finsler, Theresa Schmiedel, Friederike Eyssel","Integrating generative AI such as Large Language Models into social robots has improved their ability to engage in natural, human-like communication. This study presents a method to examine their persuasive capabilities. We designed an experimental framework focused on decision making and tested it in a pilot that varied robot appearance and self-knowledge. Using qualitative analysis, we evaluated interaction quality, persuasion effectiveness, and the robot's communicative strategies. Participants generally experienced the interaction positively, describing the robot as competent, friendly, and supportive, while noting practical limits such as delayed responses and occasional speech-recognition errors. Persuasiveness was highly context dependent and shaped by robot behavior: Participants responded well to polite, reasoned suggestions and expressive gestures, but emphasized the need for more personalized, context-aware arguments and clearer social roles. These findings suggest that generative social robots can influence user decisions, but their effectiveness depends on communicative nuance and contextual relevance. We propose refinements to the framework to further study persuasive dynamics between robots and human users.",10.48550/arXiv.2509.03231,['JournalArticle'],Computer Science; Psychology
Research on the Application System of Big Models Based on Deep Learning in the Field of Financial Customer Service,2025,"Chaoyong Zhu, Ma Jian, Lingfeng Jiao","This paper proposes research on the application system of big models based on deep learning in the field of financial customer service. In view of the problems of insufficient understanding of complex financial contexts and inaccurate responses of existing financial customer service systems, this paper designs an adaptive financial dialogue generation algorithm. The article uses the pre-trained model qwen-2.5 instead of GPT-3, and the comparison models are changed to GPT-3.5 and other mainstream training models. By introducing special vocabulary in the financial field for adjustment, combined with adaptive attention mechanism and financial entity embedding, it enhances the semantic understanding ability of financial scenarios and the dynamic capture ability of customer intentions. The system simulation experiment uses a real dialogue dataset of a financial enterprise, which contains 100,000 annotated dialogue data. The results show that compared with traditional dialogue generation models (the comparison models are changed to GPT-3.5 and other mainstream training models), the model proposed in this paper improves the response accuracy by 12.3 % and reduces the response time by 18.6 %. In addition, in the user satisfaction survey, the customer satisfaction of using this system increased by 22 %. Data experiments also show that this system can handle complex financial problems more accurately, such as multiaccount transaction record query and personalized investment advice generation. This study proves the practical application value of large models based on deep learning in the field of financial customer service, which not only improves customer service efficiency, but also enhances user experience, and provides new ideas for the development of intelligent customer service systems.",10.1109/ICIPCA65645.2025.11138965,"['Conference', 'Review']",Business; Computer Science; Economics
A Study on Improving the Generation Quality of Large Language Model in Question-Answering Systems Using Multi-Population Genetic Algorithm,2025,"Cheng-Han Wu, Ren-Shiou Liu","Existing Retrieval-Augmented Generation (RAG) methods remain insufficient in handling hallucination due to several underlying limitations. First, semantic vector-based retrieval may lead to the loss of critical information from the original text. Second, ambiguous queries can result in the retrieval of unhelpful content. Third, the single-round retrieval strategy cannot simulate the human reasoning process. Therefore, we propose a novel answer reasoning framework that integrates Multi-Population Genetic Algorithms (MPGA) to enhance RAG-based QA systems. The framework first leverages a multi-head self-attention mechanism to capture diverse semantic aspects of the input text. Subsequently, relevant passages retrieved from multiple semantic perspectives are iteratively refined through the optimization process, making them more suitable as contexts for the model. Experimental results demonstrate that our approach effectively exploits semantic diversity across attention heads, leading to responses with improved diversity, completeness, and factual accuracy.",10.1109/ICoAILO66760.2025.11156060,['Conference'],Computer Science
Haircutting Robots: From Theory to Practice,2025,Shuai Li,"The field of haircutting robots is poised for a significant transformation, driven by advancements in artificial intelligence, mechatronics, and humanoid robotics. This perspective paper examines the emerging market for haircutting robots, propelled by decreasing hardware costs and a growing demand for automated grooming services. We review foundational technologies, including advanced hair modeling, real-time motion planning, and haptic feedback, and analyze their application in both teleoperated and fully autonomous systems. Key technical requirements and challenges in safety certification are discussed in detail. Furthermore, we explore how cutting-edge technologies like direct-drive systems, large language models, virtual reality, and big data collection can empower these robots to offer a human-like, personalized, and efficient experience. We propose a business model centered on supervised autonomy, which enables early commercialization and sets a path toward future scalability. This perspective paper provides a theoretical and technical framework for the future deployment and commercialization of haircutting robots, highlighting their potential to create a new sector in the automation industry.",10.3390/automation6030047,"['JournalArticle', 'Review']",Computer Science; Engineering
Multi-Party Conversational Agents: A Survey,2025,"Sagar Sapkota, Mohammad Saqib Hasan, Mubarak Shah, Santu Karmaker","Multi-party Conversational Agents (MPCAs) are systems designed to engage in dialogue with more than two participants simultaneously. Unlike traditional two-party agents, designing MPCAs faces additional challenges due to the need to interpret both utterance semantics and social dynamics. This survey explores recent progress in MPCAs by addressing three key questions: 1) Can agents model each participants' mental states? (State of Mind Modeling); 2) Can they properly understand the dialogue content? (Semantic Understanding); and 3) Can they reason about and predict future conversation flow? (Agent Action Modeling). We review methods ranging from classical machine learning to Large Language Models (LLMs) and multi-modal systems. Our analysis underscores Theory of Mind (ToM) as essential for building intelligent MPCAs and highlights multi-modal understanding as a promising yet underexplored direction. Finally, this survey offers guidance to future researchers on developing more capable MPCAs.",10.48550/arXiv.2505.18845,"['JournalArticle', 'Review']",Computer Science
Deep Learning and Transformers Accuracy in Rumor Detection on Social Media,2025,"Long Yu, Jiarui Dai, Jiaqi Dai, Yanan Wang","The increasing popularity of social media platforms has revolutionized how news and information are shared. While these social platforms facilitate rapid dissemination, they also provide fertile ground for the proliferation of rumors and unverified information. False information spreads as quickly as accurate news, often influencing public opinion and decision-making processes. Identifying rumors early is critical to limiting their potential harm and mitigating negative consequences. This study evaluates the practical application and scalability of transformer-based models, specifically GPT-2, in detecting rumors on social media platforms alongside traditional deep learning (DL) models. We explore various deep learning models such as Long Short-Term Memory (LSTM), Convolutional Neural Networks (CNN), ALBERT, and GPT-2. Performance was assessed using standard evaluation metrics, including accuracy, precision, recall, F1-score, and analysis of Receiver Operating Characteristic (ROC) curves. The comparative results reveal that transformer-based approaches significantly outperform traditional DL models in detecting rumors with higher accuracy and reliability. Among the evaluated models, GPT-2 achieved the highest scores across all performance metrics, demonstrating exceptional capability in identifying and predicting rumor-laden content. This study introduces key innovations, including a direct comparative analysis of transformer-based and traditional DL models, highlighting GPT-2s advanced attention mechanisms that capture nuanced linguistic and contextual features. Additionally, it underscores GPT-2s scalability for real-world misinformation mitigation and critically examines dataset biases and adaptability challenges. Future advancements, such as multimodal approaches integrating text, images, and videos, as well as hybrid models combining transformers with traditional DL techniques, are proposed to enhance detection accuracy and efficiency. These findings underline the transformative potential of advanced AI techniques in combating misinformation on social media platforms. The research emphasizes the potential for scalable and practical implementation of GPT-2-based tools in mitigating false information dissemination, contributing to a more reliable and resilient digital ecosystem. This work advances the understanding of AI's role in mitigating the spread of false information.",10.33166/aetic.2025.03.003,['JournalArticle'],Computer Science
Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks,2025,"Yi-Long Lu, Chunhui Zhang, Wei Wang","Large Language Models (LLMs) are increasingly used in tasks such as psychological text analysis and decision-making in automated workflows. However, their reliability remains a concern due to potential biases inherited from their training process. In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments. In a value statement judgments task and a text sentiment analysis task, we prompted LLMs to simulate human responses and tested both formats across several models, including both open-source and commercial models. Our findings revealed a consistent negative bias: LLMs were more likely to deliver""negative""judgments in binary formats compared to continuous ones. Control experiments further revealed that this pattern holds across both tasks. Our results highlight the importance of considering response format when applying LLMs to decision tasks, as small changes in task design can introduce systematic biases.",10.48550/arXiv.2504.19445,['JournalArticle'],Computer Science; Linguistics; Psychology
Can LLMs and humans be friends? Uncovering factors affecting human-AI intimacy formation,2025,"Yeseon Hong, Junhyuk Choi, Minju Kim, Bugeun Kim","Large language models (LLMs) are increasingly being used in conversational roles, yet little is known about how intimacy emerges in human-LLM interactions. Although previous work emphasized the importance of self-disclosure in human-chatbot interaction, it is questionable whether gradual and reciprocal self-disclosure is also helpful in human-LLM interaction. Thus, this study examined three possible aspects contributing to intimacy formation: gradual self-disclosure, reciprocity, and naturalness. Study 1 explored the impact of mutual, gradual self-disclosure with 29 users and a vanilla LLM. Study 2 adopted self-criticism methods for more natural responses and conducted a similar experiment with 53 users. Results indicate that gradual self-disclosure significantly enhances perceived social intimacy, regardless of persona reciprocity. Moreover, participants perceived utterances generated with self-criticism as more natural compared to those of vanilla LLMs; self-criticism fostered higher intimacy in early stages. Also, we observed that excessive empathetic expressions occasionally disrupted immersion, pointing to the importance of response calibration during intimacy formation.",10.48550/arXiv.2505.24658,['JournalArticle'],Computer Science; Psychology
SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs,2025,"Vahid Rahimzadeh, Erfan Moosavi Monazzah, Mohammad Taher Pilehvar, Yadollah Yaghoobzadeh","Persona-driven LLMs have emerged as powerful tools in computational social science, yet existing approaches fall at opposite extremes, either relying on costly human-curated data or producing synthetic personas that lack consistency and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from 10,000 real social media users from BlueSky open platform across three time windows, bridging this spectrum by grounding synthetic generation in authentic user activity. Our evaluation demonstrates that SYNTHIA achieves competitive performance with state-of-the-art methods in demographic diversity and social survey alignment while significantly outperforming them in narrative consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and provides rich social interaction metadata from the underlying network, enabling new research directions in computational social science and persona-driven language modeling.",10.48550/arXiv.2507.14922,"['JournalArticle', 'Review']",Computer Science; Sociology
AI Interviews with Facial Emotion Recognition for Real-Time Feedback and Career Recommendations,2025,"R.P.N.M. Herath, K. T. Hansana, D.S.U. Arachchi, Sanika K. Wijayasekara, M.H.B.P.T. Gunaratne, Dulani Jayasinghe","The hiring process is complex, requiring evaluation of candidates across multiple dimensions, including technical proficiency, behavioral traits, and credibility. Traditional interviews often suffer from biases and inefficiencies. This research presents an AI-driven Interview System integrating Machine Learning (ML), Natural Language Processing (NLP), and Computer Vision to automate and enhance recruitment. The system generates contextual interview questions, evaluates candidate responses using LLM-based scoring models, and provides real-time feedback for engagement. It includes speech-to-text transcription and offensive word detection to ensure professionalism. The behavioral analysis module leverages facial emotion recognition and computer vision to assess non-verbal cues such as confidence and attentiveness. Additionally, Curriculum Vitae (CV) parsing and LinkedIn data extraction use NLP-based entity recognition to extract educational background, work experience, and key skills, enabling personalized interviews. The technical assessment module administers real-time coding challenges, evaluating solutions for correctness, efficiency, and best practices while providing AI-generated feedback. By automating these key hiring aspects, this system enhances objectivity, efficiency, and decision-making, ensuring a data-driven, unbiased, and scalable selection process while improving the candidates experience and employer insights",10.1109/ICoICT66265.2025.11192958,['Conference'],Computer Science
Large Language Models in the Task of Automatic Validation of Text Classifier Predictions,2025,Aleksandr Tsymbalov,"Machine learning models for text classification are trained to predict a class for a given text. To do this, training and validation samples must be prepared: a set of texts is collected, and each text is assigned a class. These classes are usually assigned by human annotators with different expertise levels, depending on the specific classification task. Collecting such samples from scratch is labor-intensive because it requires finding specialists and compensating them for their work; moreover, the number of available specialists is limited, and their productivity is constrained by human factors. While it may not be too resource-intensive to collect samples once, the ongoing need to retrain models (especially in incremental learning pipelines) to address data drift (also called model drift) makes the data collection process crucial and costly over the model's entire lifecycle. This paper proposes several approaches to replace human annotators with Large Language Models (LLMs) to test classifier predictions for correctness, helping ensure model quality and support high-quality incremental learning.",10.48550/arXiv.2505.18688,['JournalArticle'],Computer Science
Challenges and Opportunities for Visual Analytics in Jurisprudence,2024,"Daniel Furst, Mennatallah El-Assady, Daniel A. Keim, M. T. Fischer","Legal exploration, analysis, and interpretation remain complex and demanding tasks, even for experienced legal scholars, due to the domain-specific language, tacit legal concepts, and intentional ambiguities embedded in legal texts. In related, text-based domains, Visual Analytics (VA) and Large Language Models (LLMs) have become indispensable tools for navigating documents, representing knowledge, and supporting analytical reasoning. However, legal scholarship presents distinct challenges: it requires managing formal legal structure, drawing on tacit domain knowledge, and documenting intricate and accurate reasoning processes - needs that current VA systems designs and LLMs fail to address adequately. We identify previously unexamined key challenges and underexplored opportunities in applying VA to jurisprudence to explore how these technologies might better serve the legal domain. Based on semi-structured interviews with nine legal experts, we find a significant gap in tools and means that can externalize tacit legal knowledge in a form that is both explicit and machine-interpretable. Hence, we propose leveraging interactive visualization for this articulation, teaching the machine relevant semantic relationships between legal documents that inform the predictions of LLMs, facilitating the enhanced navigation between hierarchies of legal collections. This work introduces a user-centered VA workflow to the jurisprudential context, recognizing tacit legal knowledge and expert experience as vital components in deriving legal insight, comparing it with established practices in other text-based domains, and outlining a research agenda that offers future guidance for researchers in Visual Analytics for law and beyond.",10.48550/arXiv.2412.06543,['JournalArticle'],Computer Science; Law
AISStream-MCP: A Real-Time Memory-Augmented Question-Answering System for Maritime Operations,2025,"Sien Chen, Ruoxian Zhao, Jian-Bo Yang, Yinghua Huang","Ports and maritime operations generate massive real-time data streams, particularly from Automatic Identification System (AIS) signals, which are challenging to query effectively using natural language. This study proposes a prototype AISStream-MCP, a memory-augmented real-time maritime question-answering (QA) system that integrates live AIS data streaming with a Model Context Protocol (MCP) toolchain to support port operations decision-making. The system combines a large language model (LLM) with four MCP-enabled modules: persistent dialogue memory, live AIS data query, knowledge graph lookup, and result evaluation. We hypothesize that augmenting an LLM with domain-specific tools significantly improves QA performance compared to systems without memory or live data access. To test this hypothesis, we developed two prototype systems (with and without MCP framework) and evaluated them on 30 queries across three task categories: ETA prediction, anomaly detection, and multi-turn route queries. Experimental results demonstrate that AISStream-MCP achieves 88% answer accuracy (vs. 75% baseline), 85% multi-turn coherence (vs. 60%), and 38.7% faster response times (4.6 s vs. 7.5 s), with user satisfaction scores of 4.6/5 (vs. 3.5/5). The improvements are statistically significant (p < 0.01), confirming that memory augmentation and real-time tool integration effectively enhance maritime QA capabilities. Specifically, AISStream-MCP improved ETA prediction accuracy from 80% to 90%, anomaly detection from 70% to 85%, and multi-turn query accuracy from 65% to 88%. This approach shows significant potential for improving maritime situational awareness and operational efficiency.",10.3390/jmse13091754,['JournalArticle'],Computer Science; Engineering
AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities,2024,"Fabrizio Davide, Pietro Torre, Andrea Gaggioli","We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts. This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.",10.48550/arXiv.2412.09385,"['JournalArticle', 'Review']",Computer Science; Political Science
CELL your Model: Contrastive Explanation Methods for Large Language Models,2024,"Ronny Luss, Erik Miehling, Amit Dhurandhar","The advent of black-box deep neural network classification models has sparked the need to explain their decisions. However, in the case of generative AI, such as large language models (LLMs), there is no class prediction to explain. Rather, one can ask why an LLM output a particular response to a given prompt. In this paper, we answer this question by proposing a contrastive explanation method requiring simply black-box/query access. Our explanations suggest that an LLM outputs a reply to a given prompt because if the prompt was slightly modified, the LLM would have given a different response that is either less preferable or contradicts the original response. The key insight is that contrastive explanations simply require a scoring function that has meaning to the user and not necessarily a specific real valued quantity (viz. class label). To this end, we offer a novel budgeted algorithm, our main algorithmic contribution, which intelligently creates contrasts based on such a scoring function while adhering to a query budget, necessary for longer contexts. We show the efficacy of our method on important natural language tasks such as open-text generation and chatbot conversations.",10.48550/arXiv.2406.11785,['JournalArticle'],Computer Science
Using large language models to predict relationships among survey scales and items,2025,Matti Vuorre,"How are the thousands of existing, and yet to be created, psychological measurement instruments related, and how reliable are they? Here, Hommel and Arslan (2025) trained a language modelSurveyBot3000to provide answers to these questions efficiently and without human intervention. In their Stage 1 submission, the authors described the training and pilot validation of a statistical model whose inputs are psychological measurement items or scales",10.24072/pci.rr.100990,"['JournalArticle', 'Review']",Computer Science; Psychology
Artificial Intelligence based Virtual Interviewing Platform,2025,Indira Joshi,"This project aims to develop a virtual voice-based interviewer powered by advanced natural language processing (NLP) technologies to address the inefficiencies and biases in traditional hiring methods. By automating the interview process and simulating human interactions, the system will assess candidates in real-time, evaluating the content of their communication. This approach ensures consistent and fair evaluations, reducing biases while providing hiring managers with data-driven insights to make more informed decisions. Ultimately, the virtual interviewer seeks to streamline the hiring process, improve talent acquisition outcomes, and enhance the overall candidate experience

Keywords Artificial Intelligence, Machine Learning, Virtual Interview, Gemini, Django, NLP, API.",10.55041/ijsrem44665,['JournalArticle'],Business; Computer Science
Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models,2024,"Yi Ren, Tianyi Zhang, Weibin Li, DuoMu Zhou, Chenhao Qin, FangCheng Dong","With the rapid advancement of large language models, several models, such as GPT-4o, have demonstrated extraordinary capabilities, surpassing human performance in various language tasks. This study proposes a novel approach using large language models for public opinion event heat level prediction. We preprocessed and classified 62,836 Chinese hot event data collected between July 2022 and December 2023. Based on each event s online dissemination heat index, we used the MiniBatchKMeans algorithm to cluster these events into four heat levels (from low heat to very high heat). We then randomly selected 250 events from each level, totaling 1,000 events, to form the evaluation dataset. In testing, we assessed the accuracy of various language models in predicting event heat levels in two scenarios: without reference cases and with similar case references. The results showed that GPT-4o and DeepseekV2 performed best in the referenced case scenario, achieving prediction accuracies of 41.4% and 41.5%, respectively. Although overall prediction accuracy remains relatively low, for low-heat (Level 1) events, GPT-4o and DeepseekV2 achieved accuracies of 73.6% and 70.4%, respectively. Additionally, prediction accuracy showed a downward trend from Level 1 to Level 4, corresponding to the uneven data distribution across heat levels. This suggests that with more robust datasets, public opinion event heat level prediction using large language models holds significant research potential for the future.",10.1109/CCAT64370.2024.00013,"['JournalArticle', 'Conference']",Computer Science
Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations,2025,"Jinyuan Luo, Zhen Fang, Yixuan Li, Seongheon Park, Ling Chen","Hallucination remains a key obstacle to the reliable deployment of large language models (LLMs) in real-world question answering tasks. A widely adopted strategy to detect hallucination, known as self-assessment, relies on the model's own output confidence to estimate the factual accuracy of its answers. However, this strategy assumes that the model's output distribution closely reflects the true data distribution, which may not always hold in practice. As bias accumulates through the model's layers, the final output can diverge from the underlying reasoning process, making output-level confidence an unreliable signal for hallucination detection. In this work, we propose Sample-Specific Prompting (SSP), a new framework that improves self-assessment by analyzing perturbation sensitivity at intermediate representations. These representations, being less influenced by model bias, offer a more faithful view of the model's latent reasoning process. Specifically, SSP dynamically generates noise prompts for each input and employs a lightweight encoder to amplify the changes in representations caused by the perturbation. A contrastive distance metric is then used to quantify these differences and separate truthful from hallucinated responses. By leveraging the dynamic behavior of intermediate representations under perturbation, SSP enables more reliable self-assessment. Extensive experiments demonstrate that SSP significantly outperforms prior methods across a range of hallucination detection benchmarks.",10.48550/arXiv.2506.02696,['JournalArticle'],Computer Science
Development and Evaluation of an Immersive Metaverse-Based Meditation System for Psychological Well-Being Using LLM-Driven Scenario Generation,2025,"Aihe Yu, Gyuhyuk Lee, Yu Liu, Mingfeng Zhang, Seunga Jung, Jisun Park, Jongtae Rhee, Kyungeun Cho","The increasing prevalence of mental health disorders highlights the need for innovative and accessible interventions. Although existing digital meditation applications offer valuable basic guidance, they often lack interactivity, real-time personalized feedback, and dynamic simulation of real-life scenarios necessary for comprehensive experiential training applicable to daily stressors. To address these limitations, this study developed a novel immersive meditation system specifically designed for deployment within a metaverse environment. The system provides mindfulness practice through two distinct modules within the virtual world. The experience-based module delivers AI-driven social interactions within simulated everyday scenarios, with narrative content dynamically generated by large language models (LLMs), followed by guided inner reflection, thereby forming a scenarioexperiencereflection cycle. The breathing-focused module provides real-time feedback through a breath-synchronization interface to enhance respiratory awareness. The feasibility and preliminary effects of this metaverse-based system were explored in a two-week, single-group, pre-test/post-test study involving 31 participants. The participants completed a battery of validated psychological questionnaires assessing psychological distress, mindfulness, acceptance, self-compassion, and self-esteem before and after engaging in the intervention. This study provides exploratory evidence supporting the feasibility and potential of immersive metaverse environments and LLM-based scenario generation for structured mental health interventions, providing initial insights into their psychological impact and user experience.",10.3390/systems13090798,['JournalArticle'],Computer Science; Psychology
Enhancing Sentiment Analysis with Multimodal Large Language Models,2025,"Thresa Jeniffer J, Swetha M, Raghuvaran E, D. R, S. R","Sentiment analysis is crucial in understanding user opinions, emotions, and attitudes across various domains. Traditional sentiment analysis methods rely primarily on textual data, limiting their ability to capture the full context of human expression, often including multimodal elements such as images, audio, and videos. Existing approaches struggle with ambiguity, sarcasm, and lack of contextual awareness, reducing accuracy and effectiveness. To address these limitations, we propose a novel framework called Sentiment Analysis using Machine Learning with Multimodal Models (SA-ML-MM), which integrates text, images, and audio inputs using large language models (LLMs) enhanced with multimodal capabilities. Our approach leverages ML-based feature extraction and fusion techniques to improve sentiment classification accuracy. The proposed framework is applied in social media analysis, customer feedback interpretation, and emotion recognition. Experimental results demonstrate that SA-ML-MM significantly outperforms traditional text-based models, achieving higher accuracy and robustness in sentiment prediction. By incorporating multiple data modalities, our approach effectively captures nuanced emotional expressions, making sentiment analysis more precise and reliable.",10.1109/INCET64471.2025.11140145,['Conference'],Computer Science; Linguistics
Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods,2025,"Tai-Quan Peng, Xuzhen Yang","This paper examines how large language models (LLMs) are transforming core quantitative methods in communication research in particular, and in the social sciences more broadly-namely, content analysis, survey research, and experimental studies. Rather than replacing classical approaches, LLMs introduce new possibilities for coding and interpreting text, simulating dynamic respondents, and generating personalized and interactive stimuli. Drawing on recent interdisciplinary work, the paper highlights both the potential and limitations of LLMs as research tools, including issues of validity, bias, and interpretability. To situate these developments theoretically, the paper revisits Lasswell's foundational framework --""Who says what, in which channel, to whom, with what effect?""-- and demonstrates how LLMs reconfigure message studies, audience analysis, and effects research by enabling interpretive variation, audience trajectory modeling, and counterfactual experimentation. Revisiting the metaphor of the methodological compass, the paper argues that classical research logics remain essential as the field integrates LLMs and generative AI. By treating LLMs not only as technical instruments but also as epistemic and cultural tools, the paper calls for thoughtful, rigorous, and imaginative use of LLMs in future communication and social science research.",10.48550/arXiv.2505.19402,"['JournalArticle', 'Review']",Computer Science; Linguistics; Sociology
Recent Advances in Artificial Intelligence for Management and Financial Technology,2025,"Renwei Yang, Yun Wang, Yongcan Luo, Zhengjie Yang, Zhimin Zong, Dapeng Wu","In this survey, we examine contemporary advancements in Artificial Intelligence (AI) applications for Financial Technology (FinTech), with a specific focus on three rapidly evolving domains: recommendation systems, risk analysis, and AI-generated commercial content (AIGC). For recommendation systems, self-supervised learning and graph neural network methodologies facilitate real-time, hyper-personalized financial product suggestions, optimizing the balance between conversion efficacy and regulatory adherence. For risk analysis, large language models, including GPT-4 and Llama 3, enhanced through sophisticated prompt engineering techniques, have significantly transformed credit assessment and stress testing processes for small and medium-sized enterprises, reducing analytical cycles from weeks to minutes. Concurrently, multimodal generative models, such as DALL-E 3, are revolutionizing advertising through the automated generation of compliant and engaging content across textual, visual, and video formats, markedly compressing production timelines. The survey further critically addresses persistent challenges, encompassing data privacy, algorithmic transparency, and cultural bias within AIGC, while delineating future research trajectories for developing trustworthy and scalable AI solutions in FinTech.",10.53941/tai.2025.100009,"['JournalArticle', 'Review']",Business; Computer Science
"Empathy, bias, and data responsibility: evaluating AI chatbots for gender-based violence support",2025,"Borja Sanz Urquijo, Mara Lpez Belloso, Ainhoa Izaguirre-Choperena","Artificial Intelligence (AI) chatbots are increasingly deployed as support tools in sensitive domains such as gender-based violence (GBV). This study evaluates the performance of three conversational AI modelsincluding a general-purpose Large Language Model (ChatGPT), an open-source model (LLaMA), and a specialized chatbot (AinoAid)in providing first-line assistance to women affected by GBV. Drawing on findings from the European IMPROVE project, the research uses a mixed-methods design combining qualitative narrative interviews with 30 survivors in Spain and quantitative natural language processing metrics. Chatbots were assessed through scenario-based simulations across the GBV cycle, with prompts designed via the Systematic Context Construction and Behavior Specification method to ensure ethical and empathetic alignment. Results reveal significant differences in emotional resonance, response quality, and gender bias handling, with ChatGPT showing the most empathetic engagement and AinoAid offering contextually precise guidance. However, all models lacked intersectional sensitivity and proactive attention to privacy. These findings highlight the importance of trauma-informed design and qualitative grounding in developing responsible AI for GBV support.",10.3389/fpos.2025.1631881,['JournalArticle'],Computer Science; Sociology
The (in)efficacy of AI personas in deception detection experiments,2025,"David M. Markowitz, Timothy R. Levine","
 Artificial intelligence (AI) has recently been used to aid in deception detection and to simulate human data in social scientific research. Thus, it is important to consider how well these tools can inform both enterprises. We report 12 studies, accessed through the Viewpoints.ai research platform, where AI (gemini-1.5-flash) made veracity judgments of humans. We systematically varied the nature and duration of the communication, modality, truth-lie base rate, and AI persona. AI performed best (57.7%) when detecting truths and lies involving feelings about friends, although it was notably truth-biased (71.7%). However, in assessing cheating interrogations, AI was lie-biased by judging more than three-quarters of interviewees as cheating liars. In assessing interviews where humans perform at rates over 70%, accuracy plummeted to 15.9% with an ecological base-rate. AI yielded results different from prior human studies and therefore, we caution using certain large language models for lie detection.",10.1093/joc/jqaf034,['JournalArticle'],Computer Science; Psychology
Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation,2024,"Yunsheng Wang, Songhao Chen, Kevin Jin","Knowledge graphs (KGs) are essential in applications such as network alignment, question-answering, and recommender systems (RSs) since they offer structured relational data that facilitate the inference of indirect relationships. However, the development of KG-based RSs capable of processing user inputs in natural language faces significant challenges. Firstly, natural language processing units must effectively handle the ambiguity and variability in human language to interpret user intents accurately. Secondly, the system must precisely identify and link entities, like product names, to their corresponding nodes in KGs. To overcome these challenges, supported by Lenovo, we developed a novel chatbot called""Prometheus,""which integrates a KG with a large language model (LLM), specifically designed for recommending computer components. This chatbot can accurately decode user requests and deliver personalized recommendations derived from KGs, ensuring precise comprehension and response to their computer setup needs.",10.48550/arXiv.2407.19643,['JournalArticle'],Computer Science
AI-Driven NPC Dialogues for Immersive Gameplay: Integrating OpenAI's NLP Technology in Unity-Based Games,2025,"Davinder Singh, Joideep Banerjee, Jatin Pandey",,10.1109/ICVARS66454.2025.11198669,"['Conference', 'Review']",Computer Science
Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models,2025,"Sebastian Grtl, Gloria Schimetta, David Kerschbaumer, Michael Liut, Alexander Steinmaurer","UML and ER diagrams are foundational in computer science education but come with challenges for learners due to the need for abstract thinking, contextual understanding, and mastery of both syntax and semantics. These complexities are difficult to address through traditional teaching methods, which often struggle to provide scalable, personalized feedback, especially in large classes. We introduce DUET (Diagrammatic UML&ER Tutor), a prototype of an LLM-based tool, which converts a reference diagram and a student-submitted diagram into a textual representation and provides structured feedback based on the differences. It uses a multi-stage LLM pipeline to compare diagrams and generate reflective feedback. Furthermore, the tool enables analytical insights for educators, aiming to foster self-directed learning and inform instructional strategies. We evaluated DUET through semi-structured interviews with six participants, including two educators and four teaching assistants. They identified strengths such as accessibility, scalability, and learning support alongside limitations, including reliability and potential misuse. Participants also suggested potential improvements, such as bulk upload functionality and interactive clarification features. DUET presents a promising direction for integrating LLMs into modeling education and offers a foundation for future classroom integration and empirical evaluation.",10.48550/arXiv.2507.23470,['JournalArticle'],Computer Science
Snippet-based Conversational Recommender System,2024,"Haibo Sun, Naoki Otani, Hannah Kim, Dan Zhang, Nikita Bhutani","Conversational Recommender Systems (CRS) engage users in interactive dialogues to gather preferences and provide personalized recommendations. While existing studies have advanced conversational strategies, they often rely on predefined attributes or expensive, domain-specific annotated datasets, which limits their flexibility in handling diverse user preferences and adaptability across domains. We propose SnipRec, a novel resource-efficient approach that leverages user-generated content, such as customer reviews, to capture a broader range of user expressions. By employing large language models to map reviews and user responses into concise snippets, SnipRec represents user preferences and retrieves relevant items without the need for intensive manual data collection or fine-tuning. Experiments across the restaurant, book, and clothing domains show that snippet-based representations outperform document- and sentence-based representations, achieving Hits@10 of 0.25-0.55 with 3,000 to 10,000 candidate items while successfully handling free-form user responses.",10.48550/arXiv.2411.06064,"['JournalArticle', 'Review']",Computer Science
Big language modeling in financial markets: present and future: One study overview,2025,Ke Deng,"With the breakthrough of large language models (LLMs) represented by GPT and BERT in the field of natural language processing, they provide a new paradigm for financial prediction and analysis by processing unstructured textual data (e.g., news, financial reports, and social media opinions). Studies have shown that LLMs have demonstrated significant advantages in areas such as customer service, risk management, and investment decision. Some models (e.g., BloombergGPT) have achieved a balance between specialized capabilities and general performance through hybrid data training and vertical fine-tuning, which provides a paradigm for the development of large language models in current and future verticals. However, LLMs still face challenges such as insufficient understanding of specialized terminology, data privacy risk, lack of model interpretability and high-frequency transaction latency in financial scenarios. Through the research review, this paper further proposes to deal with the above problems through the technical paths of vertical model development, federated learning, and interpretability enhancement, and emphasizes the need for continuous exploration in the future in the areas of large model multimodalization , regulatory synergy , and financial inclusion deepening, in order to promote the scaled landing and value release of LLMs in the financial domain. By analyzing and discussing the history of LLMs, its core technology, current applications and problems in the financial industry, this paper shows that big language models have a broad development potential in the financial field, and are one of the core driving forces for innovation of the future the industry.",10.30574/wjarr.2025.25.3.0754,"['JournalArticle', 'Review']",Business; Computer Science
Group Causal Policy Optimization for Post-Training Large Language Models,2025,"Ziyin Gu, Jingyao Wang, Ran Zuo, Chuxiong Sun, Zeen Song, Changwen Zheng, Wenwen Qiang","Recent advances in large language models (LLMs) have broadened their applicability across diverse tasks, yet specialized domains still require targeted post training. Among existing methods, Group Relative Policy Optimization (GRPO) stands out for its efficiency, leveraging groupwise relative rewards while avoiding costly value function learning. However, GRPO treats candidate responses as independent, overlooking semantic interactions such as complementarity and contradiction. To address this challenge, we first introduce a Structural Causal Model (SCM) that reveals hidden dependencies among candidate responses induced by conditioning on a final integrated output forming a collider structure. Then, our causal analysis leads to two insights: (1) projecting responses onto a causally informed subspace improves prediction quality, and (2) this projection yields a better baseline than query only conditioning. Building on these insights, we propose Group Causal Policy Optimization (GCPO), which integrates causal structure into optimization through two key components: a causally informed reward adjustment and a novel KL regularization term that aligns the policy with a causally projected reference distribution. Comprehensive experimental evaluations demonstrate that GCPO consistently surpasses existing methods, including GRPO across multiple reasoning benchmarks.",10.48550/arXiv.2508.05428,['JournalArticle'],Computer Science
Safeguarding Crowdsourcing Surveys from ChatGPT through Prompt Injection,2025,"Chaofan Wang, Samuel Kernan Freire, Mo Zhang, Jing Wei, Jorge Goncalves, V. Kostakos, Alessandro Bozzon, E. Niforatos","ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses ''prompt injection,'' such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 98% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.",10.1145/3757503,"['JournalArticle', 'Review']",Computer Science
Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey,2025,"Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj","The proliferation of IoT in cities, combined with Digital Twins, creates a rich data foundation for Smart Cities aimed at improving urban life and operations. Generative AI (GenAI) significantly enhances this potential, moving beyond traditional AI analytics and predictions by processing multimodal content and generating novel outputs like text and simulations. Using specialized or foundational models, GenAI's natural language abilities such as Natural Language Understanding (NLU) and Natural Language Generation (NLG) can power tailored applications and unified interfaces, dramatically lowering barriers for users interacting with complex smart city systems. In this paper, we focus on GenAI applications based on conversational interfaces within the context of three critical user archetypes in a Smart City - Citizens, Operators and Planners. We identify and review GenAI models and techniques that have been proposed or deployed for various urban subsystems in the contexts of these user archetypes. We also consider how GenAI can be built on the existing data foundation of official city records, IoT data streams and Urban Digital Twins. We believe this work represents the first comprehensive summarization of GenAI techniques for Smart Cities from the lens of the critical users in a Smart City.",10.1109/COINS65080.2025.11125750,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering; Environmental Science
PersonalAI: Towards digital twins in the graph form,2025,"Menschikov Mikhail, Dmitry Evseev, R. Kostoev, Ilya Perepechkin, Ilnaz Salimov, Victoria Dochkina, Petr Anokhin, Evgeny Burnaev, Nikita Semenov",,10.48550/arXiv.2506.17001,['JournalArticle'],Computer Science
Survey Paper On AI Based Sports Highlight Generation For Social Media,2025,"Sameena Banu, Mahin kauser","The rapid growth of social media platforms has transformed how sports content is consumed, with short, engaging highlight clips becoming a cornerstone of fan interaction. Traditional manual editing of highlights is time-consuming and inefficient, prompting the adoption of artificial intelligence (AI) to automate this process. This survey paper explores the state-of-the-art in AI-based sports highlight generation tailored for social media, focusing on techniques, applications, and performance evaluation. We review key advancements in video analysis (e.g., object detection, event segmentation), audio processing (e.g., crowd noise analysis), and multimodal approaches that integrate visual, auditory, and textual data to identify impactful moments. The paper examines prominent systems like WSC Sports and SPNet, highlighting their contributions to real-time and post-processed highlight creation. To assess these technologies, we conducted experiments using Google Colab, testing models such as 3D-CNN and pretrained Video-LLaMA on a sample dataset (SoccerNet). Results, visualized through tables and graphs, reveal high precision (up to 87%) and recall (up to 82%) in detecting key events, though challenges like real-time processing and subtle moment recognition persist. Social media demandsshort duration, high engagement, and personalizationare analyzed, alongside practical applications such as fan engagement and monetization. However, technical limitations (e.g., dataset bias) and ethical concerns (e.g., privacy in crowd footage) remain hurdles. This survey underscores AIs transformative potential in sports media, offering insights into current capabilities and future directions, including generative AI and immersive technologies like AR/VR. By synthesizing literature, experimental findings, and practical implications, this paper provides a roadmap for researchers and practitioners aiming to enhance sports highlight generation for the dynamic landscape of social media.",10.61808/jsrt194,"['JournalArticle', 'Review']",Computer Science
Generative AI Increases Cybersecurity Risks for Seniors,2024,Zahm Siyed,"We evaluate how generative AI exacerbates the cyber risks faced by senior citizens. We assess the risk that powerful LLMs can easily be misconfigured to serve a malicious purpose, and that platforms such as HackGPT or WormGPT can facilitate low-skilled script kiddies to replicate the effectiveness of high-skilled threat actors. We surveyed 85 seniors and found that the combination of loneliness and low cyber literacy places 87% of them at high risk of being hacked. Our survey further revealed that 67% of seniors have already been exposed to potentially exploitable digital intrusions and only 22% of seniors have sufficient awareness of risks to ask techno-literate for remedial assistance. Our risk analysis suggests that existing attack vectors can be augmented with AI to create highly personalized and believable digital exploits that are extremely difficult for seniors to distinguish from legitimate interactions. Technological advances allow for the replication of familiar voices, live digital reconstruction of faces, personalized targeting, and falsification of records. Once an attack vector is identified, certain generative polymorphic capabilities allow rapid mutation and obfuscation to deliver unique payloads. Both inbound and outbound risks exist. In addition to inbound attempts by individual threat actors, seniors are vulnerable to outbound attacks through poisoned LLMs, such as Threat GPT or PoisonGPT. Generative AI can maliciously alter databases to provide incorrect information or compromised instructions to gullible seniors seeking outbound digital guidance. By analyzing the extent to which senior citizens are at risk of exploitation through new developments in AI, the paper will contribute to the development of effective strategies to safeguard this vulnerable population.",10.5539/cis.v17n2p39,"['JournalArticle', 'Review']",Computer Science
Deep Learning Based Optimization of Immunoblotting Reaction Conditions,2025,Vivek Nithika,"Immunoblotting is a biochemical technique used to quantify proteins in a sample. It is present in 9% of all protein-related publications and is one of the most used biochemical techniques. However, its methods have remained stagnant since its emergence in the 1980s, with 41% of scientists reporting their blot as unsuccessful more than 25% of the time. In this research, we propose a method for assisting scientists with an optimal primary antibody dilution for their blot. Our research is five-fold: (1) perform a background survey to determine important independent variables, (2) use Large Language Models to scrape open-source articles to obtain variables, (3) extract key structural details, (4) train a multi-modal deep learning model to take protein structure image input and compiled tabular inputs to predict an antibody dilution, and (5) test model using cross validation, heatmaps, and prediction on out-of-sample data. The model achieved a validation accuracy of 94% and an out-of-sample testing accuracy of 89.09%, indicating its ability to reduce iterative processes a substantial amount of the time. Model reliability was assessed by backing regions of importance with scientific findings through saliency maps. Receiver Operating Characteristic (ROC) curves with an average AUC of 0.9317 indicate that the incorporation of multi-modal data increases overall accuracy and reduces both the false positive and false negative rate. The resulting model could reduce the time and resources associated with immunoblotting, allowing scientists to channel their resources into discovering new biological interactions that could contribute to cures and diagnostic tests for debilitating diseases.",10.1109/ICCSP64183.2025.11088640,"['Conference', 'Review']",Computer Science
Automating Video Thumbnails Selection and Generation with Multimodal and Multistage Analysis,2024,Elia Fantini,"This thesis presents an innovative approach to automate video thumbnail selection for traditional broadcast content. Our methodology establishes stringent criteria for diverse, representative, and aesthetically pleasing thumbnails, considering factors like logo placement space, incorporation of vertical aspect ratios, and accurate recognition of facial identities and emotions. We introduce a sophisticated multistage pipeline that can select candidate frames or generate novel images by blending video elements or using diffusion models. The pipeline incorporates state-of-the-art models for various tasks, including downsampling, redundancy reduction, automated cropping, face recognition, closed-eye and emotion detection, shot scale and aesthetic prediction, segmentation, matting, and harmonization. It also leverages large language models and visual transformers for semantic consistency. A GUI tool facilitates rapid navigation of the pipeline's output. To evaluate our method, we conducted comprehensive experiments. In a study of 69 videos, 53.6% of our proposed sets included thumbnails chosen by professional designers, with 73.9% containing similar images. A survey of 82 participants showed a 45.77% preference for our method, compared to 37.99% for manually chosen thumbnails and 16.36% for an alternative method. Professional designers reported a 3.57-fold increase in valid candidates compared to the alternative method, confirming that our approach meets established criteria. In conclusion, our findings affirm that the proposed method accelerates thumbnail creation while maintaining high-quality standards and fostering greater user engagement.",10.48550/arXiv.2410.19825,"['JournalArticle', 'Review']",Computer Science
CAPE: Context-Aware Personality Evaluation Framework for Large Language Models,2025,"Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki","Psychometric tests, traditionally used to assess humans, are now being applied to Large Language Models (LLMs) to evaluate their behavioral traits. However, existing studies follow a context-free approach, answering each question in isolation to avoid contextual influence. We term this the Disney World test, an artificial setting that ignores real-world applications, where conversational history shapes responses. To bridge this gap, we propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. To thoroughly analyze the influence of context, we introduce novel metrics to quantify the consistency of LLM responses, a fundamental trait in human behavior. Our exhaustive experiments on 7 LLMs reveal that conversational history enhances response consistency via in-context learning but also induces personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash and Llama-8B display significant sensitivity. Moreover, GPT models response stem from their intrinsic personality traits as well as prior interactions, whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions. Finally, applying our framework to Role Playing Agents (RPAs) shows context-dependent personality shifts improve response consistency and better align with human judgments. Our code and datasets are publicly available at: https://github.com/jivnesh/CAPE",10.48550/arXiv.2508.20385,['JournalArticle'],Computer Science; Psychology
Adaptive Modeling of User Preferences for Self-Driving Behaviors Using Verbal Interaction and AI,2025,"Hyowon Lee, Ocktaeck Lim, Siby Samuel","This study proposes an adaptive framework for understanding and modeling user preferences in self-driving behaviors through natural language interaction. A user survey conducted in North America revealed strong demand for customizable autonomous vehicle (AV) features, motivating the need for dynamic preference modeling. To capture diverse and context-specific verbal expressions of user intent, we leverage speech recognition and fine-tune a lightweight T5-base language model to classify preferences across predefined AV behavior categories. Given the computational constraints of in-vehicle environments, we adopt the T5-base model due to its efficiency and suitability for embedded deployment, in contrast to larger-scale LLMs. To overcome data scarcity, we applied a data augmentation strategy using a teacher model, increasing classification accuracy from 25% to 97%. The framework can integrate vision-language models (e.g., BLIP-2, CLIP, etc.) and multimodal sensor fusion (camera, LiDAR, radar) to represent traffic situations and support context-aware interpretation of user input. This approach enables the system to generalize user preferences across similar traffic conditions through similarity-based propagation. By supporting condition-specific behavioral expressions, the system can interpret and adapt user preferences accordingly. The proposed framework facilitates scalable, context-aware, and user-centered adaptation of autonomous vehicle behaviors, contributing to improved personalization and may improve system usability.",10.1177/10711813251366295,"['JournalArticle', 'Review']",Computer Science
Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models,2025,"Injae Na, Keonwoong Noh, Woohwan Jung","LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.",10.48550/arXiv.2505.20921,"['JournalArticle', 'Conference']",Computer Science
Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey,2025,"Wei Dai, Shengen Wu, Wei Wu, Zhenhao Wang, Sisuo Lyu, Haicheng Liao, Limin Yu, Weiping Ding, Runwei Guan, Yutao Yue","Trajectory prediction serves as a critical functionality in autonomous driving, enabling the anticipation of future motion paths for traffic participants such as vehicles and pedestrians, which is essential for driving safety. Although conventional deep learning methods have improved accuracy, they remain hindered by inherent limitations, including lack of interpretability, heavy reliance on large-scale annotated data, and weak generalization in long-tail scenarios. The rise of Large Foundation Models (LFMs) is transforming the research paradigm of trajectory prediction. This survey offers a systematic review of recent advances in LFMs, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for trajectory prediction. By integrating linguistic and scene semantics, LFMs facilitate interpretable contextual reasoning, significantly enhancing prediction safety and generalization in complex environments. The article highlights three core methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning. It covers prediction tasks for both vehicles and pedestrians, evaluation metrics, and dataset analyses. Key challenges such as computational latency, data scarcity, and real-world robustness are discussed, along with future research directions including low-latency inference, causality-aware modeling, and motion foundation models.",10.48550/arXiv.2509.10570,"['JournalArticle', 'Review']",Computer Science; Engineering
Scan-GPT: Bridging Visual Input and Textual Output for Question Solution,2024,"Sumit Raj Jha, Mukesh Kumar Bhardwaj, Shubham Bhugra, Gourav Kumar","The ScanGPT project represents an innovative approach to leveraging the power of advanced language models, specifically OpenAI's GPT-3.5, in conjunction with OCR technology, to provide users with a comprehensive platform for obtaining information. In a rapidly advancing landscape of artificial intelligence, ScanGPT serves as a bridge between textual and non-textual content, allowing users to interact with multiple sources and receive clear and accurate responses. Traditional search methods often fall short when dealing with non-textual content such as images containing valuable information. ScanGPT addresses these limitations by combining OCR technology with advanced language models to deliver precise answers based on both text and image inputs. This paper presents the architecture, functionality, and methodology of ScanGPT, highlighting its role in meeting the diverse needs of users seeking information. The proposed system architecture seamlessly integrates text and image processing capabilities, leveraging existing technologies such as ChatGPT, Microsoft Azure OCR, and the OpenAI API. Through a modular design and rigorous security and privacy measures, ScanGPT ensures scalability, flexibility, and user confidentiality. The role of HTML, CSS, and JavaScript in the user interface design is explored, emphasizing the importance of intuitive interfaces and dynamic capabilities in enhancing user experience. Additionally, existing solutions and challenges in conversational AI are reviewed, providing insights into the evolving landscape of AI-powered interactions. The proposed system architecture of ScanGPT offers a robust, scalable, and flexible solution for conversational AI, enabling users to interact with AI systems using both text and image inputs. By seamlessly integrating text and image processing capabilities, ScanGPT aims to redefine the boundaries of conversational AI platforms, providing users with a comprehensive and user-friendly experience. Future scope and potential advancements in conversational AI are also discussed, highlighting opportunities for integrating additional sensory inputs, personalization, and scalability. Through ongoing improvements in ethical AI considerations and linguistic capabilities, ScanGPT aims to remain a trustworthy and globally accessible technology, fostering wider adoption and cultural inclusivity in AI-driven interactions. Overall, ScanGPT represents a significant step forward in harnessing the power of advanced language models and OCR technology to provide users with accurate, contextually relevant information from diverse sources, paving the way for innovative solutions to everyday problems in the era of artificial intelligence.",10.48001/joitc.2024.1223-28,"['JournalArticle', 'Review']",Computer Science
Romanian Media Landscape in 7 Journalists Facebook Posts: A Chatgpt Sentiment Analysis,2024,Andrei Stipiuc,"Abstract This article used OpenAI Chat GPT 3.5s sentiment analysis capabilities to evaluate 595 Facebook posts written by seven renowned Romanian journalists. Our main objectives were to: 1) Assess the journalists use of Facebook regarding journalism, including its effectiveness in promoting quality articles, combating disinformation, and facilitating explanatory journalistic procedures. 2) Allow an unbiased AI system to analyze the tone of the journalists social media posts, as well as their attitudes and opinions on the primary issues affecting the media environment in Romania. 3) Determine the framework that embodies the Romanian societys 2023 zeitgeist from recurrent themes, categorized and ranked, gleaned from journalists posts. We extracted data from public Facebook posts using Apify. Our sample included posts uploaded within a timespan ranging from one month old to one year old. It is worth noting that the owners of the public accounts analyzed were consistently involved and active. We also had to be able to accommodate with time and financial constraints, as well as adhere to the limitations imposed on the use of the specified free tools. Therefore, the sample set consisted of an initial group of 12 journalists with varied socio-demographic origins. Out of this sample, some journalists were removed following subsequent considerations, resulting in a final sample group of 7 Romanian journalists. Outcomes The journalists engage in discussions or endorsements of their own work or the work of others, strive to provide reliable and accurate information to the public before others do, as well as analyze and expose false information and manipulative messaging. Aside from informative content, they share images featuring their personal lives, such as their children, feline companions, or acquaintances, just like the majority of the general users (although such posts are confined to the realm of personal space, as they rightfully should be). Even after accounting for false flagging, which ChatGPT rarely commits, the majority of the individual journalists posts were negative. The journalists have a broader perspective and a better understanding of societal and judicial issues, they express themselves accordingly, and the artificial intelligence entity is capable of parsing such sentiments manifested in social media. Hopefully, the time will come when also more political, judicial and civil participants fully understand the main issues in Romania, as journalistic texts also reflect them on social media.",10.2478/saec-2024-0002,['JournalArticle'],Computer Science
AI Advertisement Generator,2025,"Dushyanth Nd, P. S. Ahamed, Mohd Momin, Eshrath Aziz, S. Ks","The use of artificial intelligence has drastically revolutionized the way we do different things. Marketing,
advertising, and entertainment that people have been doing can be quickly transformed with AI interaction. A major
drawback of the current marketing environment is that the space for creation and problem-solving has mostly been captured
by the traditional and ready-made content formats with them as the main channels for advertisement, such as the social
media and search engines. Even though, it can be said that these methods solely bring desired results but more often than
not, they do not possess the creative and unique features that can stand them out in a fiercely competitive market. To tackle
this issue, we are introducing a system to automatically create product and UGC videos through an AI. It creates a script
through GPT 40, moreover, in the presence of product points, it merges them with the AI avatars, adds words in the
multilingual voices, and is finally subjected to the video editing powered by AI. The resulting videos are easily shared and
downloaded thus enabling fast, scalable and personalized content creation among other benefits. This survey delves into the
present capabilities, benefits, and limitations of AI-driven ad generators in the context of contemporary digital marketing.",10.38124/ijisrt/25may1578,"['JournalArticle', 'Review']",Business; Computer Science
An Ensemble Approach to Personalized Real Time Predictive Writing for Experts,2023,"Sourav Prosad, Viswa Datha Polavarapu, Shrutendra Harsola","Completing a sentence, phrase or word after typing few words / characters is very helpful for Intuit financial experts, while taking notes or having a live chat with users, since they need to write complex financial concepts more efficiently and accurately many times in a day. In this paper, we tie together different approaches like large language models, traditional Markov Models and char level models to create an end-to-end system to provide personalised sentence/word auto-complete suggestions to experts, under strict latency constraints. Proposed system can auto-complete sentences, phrases or words while writing with personalisation and can be trained with very less data and resources with good efficiency. Our proposed system is not only efficient and personalized but also robust as it leverages multiple machine learning techniques along with transfer learning approach to fine tune large language model with Intuit specific data. This ensures that even in cases of rare or unusual phrases, the system can provide relevant auto-complete suggestions in near real time. Survey has showed that this system saves expert note-taking time and boosts expert confidence in their communication with teammates and clients. Since enabling this predictive writing feature for QBLive experts, more than a million keystrokes have been saved based on these suggestions. We have done comparative study for our ensemble choice. Moreover this feature can be integrated with any product which has writing facility within a very short period of time.",10.48550/arXiv.2308.13576,"['JournalArticle', 'Review']",Computer Science
On Fact and Frequency: LLM Responses to Misinformation Expressed with Uncertainty,2025,"Y. V. D. Sande, Gunes Acar, Thabo van Woudenberg, Martha Larson","We study LLM judgments of misinformation expressed with uncertainty. Our experiments study the response of three widely used LLMs (GPT-4o, LlaMA3, DeepSeek-v2) to misinformation propositions that have been verified false and then are transformed into uncertain statements according to an uncertainty typology. Our results show that after transformation, LLMs change their factchecking classification from false to not-false in 25% of the cases. Analysis reveals that the change cannot be explained by predictors to which humans are expected to be sensitive, i.e., modality, linguistic cues, or argumentation strategy. The exception is doxastic transformations, which use linguistic cue phrases such as""It is believed ..."".To gain further insight, we prompt the LLM to make another judgment about the transformed misinformation statements that is not related to truth value. Specifically, we study LLM estimates of the frequency with which people make the uncertain statement. We find a small but significant correlation between judgment of fact and estimation of frequency.",10.48550/arXiv.2503.04271,['JournalArticle'],Computer Science
Hungary and AI: efforts and opportunities in comparison with Singapore,2025,Andr'as Ferenczy,"The study assesses Hungary's National AI Strategy and its implementation through the analysis of strategic documents, publicly available financial records, and expert interviews with the Hungarian AI Coalition President and Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from Hungary's strategy were evaluated through conceptual, governance, temporal, and financial dimensions before being benchmarked against Singapore's National AI Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of EUR 4.65 billion in AI-related public investment in Hungary. Openly available financial data was found for only half of the evaluated goals, and just three projects made up 98\% of all documented funding. The research also reveals Hungary's implementation challenges, including fragmented execution following ministerial reorganizations and the absence of designated biennial reviews since 2020. Furthermore, the paper provides targeted recommendations for Hungary's forthcoming AI strategy, drawing on Singapore's framework as a reference point. These include adapting to the era of large language models, restructuring the existing triple helix network to foster more effective dialogue and advocacy, and positioning the country as an East-West bridge for automotive AI experimentation.",10.48550/arXiv.2507.05280,"['JournalArticle', 'Review']",Computer Science; Political Science
Predicting Language Models' Success at Zero-Shot Probabilistic Prediction,2025,"Kevin Ren, Santiago Cortes-Gomez, C. Patio, Ananya Joshi, Ruiqi Lyu, Jingjing Tang, Alistair Turcan, Khurram Yamin, Steven Wu, Bryan Wilder","Recent work has investigated the capabilities of large language models (LLMs) as zero-shot models for generating individual-level characteristics (e.g., to serve as risk models or augment survey datasets). However, when should a user have confidence that an LLM will provide high-quality predictions for their particular task? To address this question, we conduct a large-scale empirical study of LLMs'zero-shot predictive capabilities across a wide range of tabular prediction tasks. We find that LLMs'performance is highly variable, both on tasks within the same dataset and across different datasets. However, when the LLM performs well on the base prediction task, its predicted probabilities become a stronger signal for individual-level accuracy. Then, we construct metrics to predict LLMs'performance at the task level, aiming to distinguish between tasks where LLMs may perform well and where they are likely unsuitable. We find that some of these metrics, each of which are assessed without labeled data, yield strong signals of LLMs'predictive performance on new tasks.",10.48550/arXiv.2509.15356,"['JournalArticle', 'Review']",Computer Science
Research on Intelligent Dispatching Method of Power System Based on Chain of Thought and Knowledge Graph of Large Language Model,2025,"Z. Meng, Xinlei Cai, Binye Ni, Zhijun Shen, Sibo Feng, Tao Yu","With the rapid expansion of new energy installations and power grid scale, traditional manual dispatching methods can no longer meet increasingly complex system requirements. This paper presents an intelligent dispatching methodology that integrates large language model chain-of-thought reasoning with knowledge graph technology. We propose a framework that simulates professional regulators' decision-making processes through chain-of-thought reasoning, enabling comprehensive understanding of complex dispatching scenarios. A formalized knowledge representation is constructed using knowledge graph technology to provide reliable support for the reasoning process. Additionally, we introduce an innovative dispatching strategy generation method that transforms professional expertise into executable dispatching schemes through prompt engineering and context management mechanisms. Experimental results demonstrate that this method achieves superior performance in complex dispatching scenarios, maintaining high accuracy while significantly reducing response time. The methodology exhibits robust stability and scalability, presenting a novel technical approach for power system intelligent dispatching by transitioning from experience-driven to intelligence-driven control. This advancement offers significant theoretical value and practical application potential.",10.1109/ICPST65050.2025.11089361,['Conference'],Computer Science; Engineering
Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability,2025,"Chirag Shah, Hideo Joho, Kirandeep Kaur, Preetam Prabhu Srikar Dammu","Recent advancements in generative AI have significantly increased interest in personalized agents. With increased personalization, there is also a greater need for being able to trust decision-making and action taking capabilities of these agents. However, the evaluation methods for these agents remain outdated and inadequate, often failing to capture the dynamic and evolving nature of user interactions. In this conceptual article, we argue for a paradigm shift in evaluating personalized and adaptive agents. We propose a comprehensive novel framework that models user personas with unique attributes and preferences. In this framework, agents interact with these simulated users through structured interviews to gather their preferences and offer customized recommendations. These recommendations are then assessed dynamically using simulations driven by Large Language Models (LLMs), enabling an adaptive and iterative evaluation process. Our flexible framework is designed to support a variety of agents and applications, ensuring a comprehensive and versatile evaluation of recommendation strategies that focus on proactive, personalized, and trustworthy aspects.",10.48550/arXiv.2504.06277,['JournalArticle'],Computer Science
Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software,2025,"Andreas Baumann, Peter Eberhard","Large Language Models (LLMs) are increasingly helpful in text generation, even writing code in programming languages based on user prompts written in natural language. They are even applied to generate simulation models for multibody systems from natural language. Research results suggest that LLMs surpass the mere replication of existing code examples, where some LLMs have been trained on an open-source multibody simulation code. However, for closed-source simulation software, such results are not to be expected as their ideas and concepts might differ from other publicly available ones. LLMs can hallucinate for knowledge-intensive tasks, such as model creation, which can lead to wrong responses. This is especially the case for the LLM unknown closed-source simulation software. The same applies to other internal knowledge kept private to protect intellectual property or data privacy. The Retrieval-Augmented Generation (RAG) approach might yield a solution for these knowledge-intensive tasks. This paper explores the application of RAG to closed-source simulation software and presents first experiments. After a brief introduction to LLMs, the RAG approach, and the simulation method applied by the close-source simulation software, several examples are provided to test LLMs' knowledge of the simulation software and the creation of simulation models using two RAG systems. The examples show promising results indicating the benefits of applying RAG systems to closed-source simulation software, helping to access their knowledge. Nevertheless, they also reveal gaps in the applied information and open questions for further research.",10.48550/arXiv.2502.03916,['JournalArticle'],Computer Science
AIGenerated PriceSetting Insights from Firms' Earnings Calls*,2025,"C. Windsor, Max Zang","We introduce new firmlevel indices covering input costs, demand and final prices based on firms' earnings calls. These are constructed using a powerful large language model (LLM). We show that our new LLMbased indices are superior to those based on detailed lists of keywords. The new indices have a leading relationship with conceptually similar indices from popular business surveys, as well as related official statistics, such as consumer price inflation, demonstrating that they are relevant for assessing current economic conditions. Next, we use our indices to examine various aspects of firms' pricesetting behaviour. The reducedform associations we estimate indicate that pricesetting behaviour depends on the source of the shocks firms face (demand or costdriven), the direction of the shock (with firms reacting more to cost increases relative to decreases) and which industries are most affected. This underscores the importance of continuing to develop rich multisector models of the economy to better understand firms' reactions to different types of shocks.",10.1111/1475-4932.70004,"['JournalArticle', 'Review']",Business; Computer Science; Economics
Phishing Attack Simulation and Detection Strategies: A Case Study on NTNU Moodle Platform,2025,"Yao Yu Lee, Albert Guan","With the development of the digital society, phishing attacks have become an increasingly serious cybersecurity threat, posing risks not only to general users but also serving as a common initial intrusion method in Advanced Persistent Threat (APT) attacks. In this study, we simulated a phishing attack targeting the Moodle system of National Taiwan Normal University and collected 104 valid survey responses to investigate phishing website recognition behaviors. The results indicate that checking the URL is one of the most effective methods for users to identify phishing websites. In the future, we plan to develop a browser extension integrated with Large Language Models (LLMs) to automatically detect high-risk phishing websites and provide real-time warnings to users, thereby enhancing overall protection capabilities.",10.29007/58hg,"['JournalArticle', 'Review']",Computer Science
Biometrics Employing Neural Network,2024,Sajjad Bhuiyan,"Biometrics involves using unique human traits, both physical and behavioral, for the digital identification of individuals to provide access to systems, devices, or information. Within the field of computer science, it acts as a method for identifying and verifying individuals and controlling access. While the conventional method for personal authentication involves passwords, the vulnerability arises when passwords are compromised, allowing unauthorized access to sensitive actions. Biometric authentication presents a viable answer to this problem and is the most secure and user-friendly authentication method. Today, fingerprints, iris and retina patterns, facial recognition, hand shapes, palm prints, and voice recognition are frequently used forms of biometrics. Despite the diverse nature of these biometric identifiers, the core objective remains consistent ensuring security, recognizing authorized users, and rejecting impostors. Hence, it is crucial to determine accurately whether the characteristics belong to the rightful person. For systems to be effective and widely accepted, the error rate in recognition and verification must approach zero. It is acknowledged that current biometric techniques, while advanced, are not infallible and require continuous improvement. A more refined classifier is deemed necessary to classify patterns accurately. Artificial Neural Networks, which simulate the human brain's operations, present themselves as a promising approach. The survey presented herein explores various biometric techniques based on neural networks, emphasizing the ongoing quest for enhanced accuracy and reliability. It concludes that The utilization of neural networks along with biometric features not only enhances accuracy but also contributes to overall better security.",10.48550/arXiv.2404.16840,"['JournalArticle', 'Review']",Computer Science
Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?,2025,"Ratna Kandala, Katie Hoemann","Understanding the nuances in everyday language is pivotal for advancements in computational linguistics&emotions research. Traditional lexicon-based tools such as LIWC and Pattern have long served as foundational instruments in this domain. LIWC is the most extensively validated word count based text analysis tool in the social sciences and Pattern is an open source Python library offering functionalities for NLP. However, everyday language is inherently spontaneous, richly expressive,&deeply context dependent. To explore the capabilities of LLMs in capturing the valences of daily narratives in Flemish, we first conducted a study involving approximately 25,000 textual responses from 102 Dutch-speaking participants. Each participant provided narratives prompted by the question,""What is happening right now and how do you feel about it?"", accompanied by self-assessed valence ratings on a continuous scale from -50 to +50. We then assessed the performance of three Dutch-specific LLMs in predicting these valence scores, and compared their outputs to those generated by LIWC and Pattern. Our findings indicate that, despite advancements in LLM architectures, these Dutch tuned models currently fall short in accurately capturing the emotional valence present in spontaneous, real-world narratives. This study underscores the imperative for developing culturally and linguistically tailored models/tools that can adeptly handle the complexities of natural language use. Enhancing automated valence analysis is not only pivotal for advancing computational methodologies but also holds significant promise for psychological research with ecologically valid insights into human daily experiences. We advocate for increased efforts in creating comprehensive datasets&finetuning LLMs for low-resource languages like Flemish, aiming to bridge the gap between computational linguistics&emotion research.",10.48550/arXiv.2506.04139,['JournalArticle'],Computer Science; Linguistics
ANSPRE: Improving Question-Answering in Large Language Models with Answer-Prefix Generation,2024,"Nguyen-Khang Le, Dieu-Hien Nguyen, Le-Minh Nguyen",". Large language models (LLMs) and Retrieval-Augmented-Generation (RAG) show remarkable capabilities in Open-domain question-answering (ODQA). Despite the advancements, LLMs tend to generate verbose responses, of which only a small part is the answer phrase. Although the ability to produce the condence score for the answer is essential when deploying LLMs in high-risk domains, sequence probabilities obtained from LLMs do not correlate well with the probabilities of correctness and thus fail to represent condence scores. This study introduces Answer-prex Generation (A NSPRE ) to improve generation quality, allowing the LLMs to output answer phrases and produce highly reliable condence scores. We guide the model in predicting the answer phrase using an answer prex and design a ranking score that integrates parametric and non-parametric knowledge. The answer phrases and their corresponding scores enable A NSPRE to aggregate results from different documents and samplings to boost performance and produce condence scores highly correlated with correctness. We show that A NSPRE can be applied to any LLM and present an approach called S ELF -A NSPRE to combine A NSPRE with Self-reective RAG, a state-of-the-art framework based on reection tokens. Empirical evaluation on popular ODQA benchmarks shows that A NSPRE and S ELF -A NSPRE signicantly improve state-of-the-art",10.3233/FAIA240778,"['JournalArticle', 'Conference']",Computer Science
GBC: GameScript Generation Using BERT Classification: Enhancing Interactive Narratives with LLM,2025,Jiahua Zhang,"Generative Artificial Intelligence (GAI) and Large Language Models (LLMs) have been applied in text-based games, but its potential in generating executable game scripts is unexplored. By establishing a mapping from text responses to game scripts, this research explored the application of LLMs in practical game production. This study utilizes a fine-tuned Bidrectional Encoder Representations from Transformers (BERT) model to transform dialogue generated by LLMs into practical game scripts. The proposed method achieves state-of-the-art performance of transformation with an F1 score of 89.71% and an accuracy of 89.74% on the validation set, while maintaining low training and computational costs. The results highlight the potential of LLMs in game development, paving the way for immersive and personalized gaming experiences. Future research could delve deeper into complex and diverse datasets, exploring the integration of multiple labels and numerical values to enhance game interaction and immersion experiences. This research may lay the groundwork for integrating generative AI into game engines in the future, simplifying the production process of adaptive game content.",10.1109/ACCTCS66275.2025.00028,['Conference'],Computer Science
Towards Client Engagement Using RAG System with Pattern Prediction Framework,2025,"Hanmin Jung, Athiruj Poositaporn",": Client engagement refers to the process of companies and customers building and maintaining relationships through communication, personalized marketing, and value-added services. This often results in analysis reports, consulting services, and strategic planning documents. Tools like GPT-4o have significant potential to support these interactions in sectors such as meteorological organizations. However, standalone generative models like GPT-4o face challenges in accessing external datasets and often produce generic outputs. To overcome these limitations, this study introduces a chat-based Retrieval-Augmented Generation (RAG) system integrated with a pattern prediction framework. We demonstrate our RAG system in analyzing air pollution pattern prediction results from our prior study and compare its generated answers with a standalone GPT-4o model. Experimental results show that the RAG system delivers actionable recommendations and contextually enriched outputs grounded in domain-specific data. In future work, we aim to explore the potential of RAG in real-world applications, such as improving client engagement by generating client-focused reports.",10.5220/0013474400003944,['JournalArticle'],Business; Computer Science
"""What's my model inside of?"": Exploring the role of environments for grounded natural language understanding",2024,Ronen Tamari,"In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of large language models on challenging commonsense reasoning tasks. We leveraged the richer supervision provided by text-based game environments to develop Breakpoint Transformers, a novel approach to modeling intermediate semantic information in long narrative or procedural texts. Finally, we integrated theories on the role of environments in collective human intelligence to propose a design for AI-augmented""social thinking environments""for knowledge workers like scientists.",10.48550/arXiv.2402.02548,['JournalArticle'],Computer Science; Environmental Science; Linguistics
Simulated Interactive Debugging,2025,"Yannic Noller, Erick Chandra, HC Srinidhi, K. Choo, Cyrille Jgourel, Oka Kurniawan, Christopher M. Poskitt","Debugging software, i.e., the localization of faults and their repair, is a key activity in software engineering. Therefore, effective and efficient debugging is one of the core skills a software engineer must develop. However, the teaching of debugging techniques is usually very limited or only taught in indirect ways, e.g., during software projects. As a result, most Computer Science (CS) students learn debugging only in an ad-hoc and unstructured way. In this work, we present our approach called Simulated Interactive Debugging that interactively guides students along the debugging process. The guidance aims to empower the students to repair their solutions and have a proper learning experience. We envision that such guided debugging techniques can be integrated into programming courses early in the CS education curriculum. We developed a prototypical implementation using traditional fault localization techniques and large language models. Students can use features like the automated setting of breakpoints or an interactive chatbot. We designed and executed a small-scale, controlled experiment with eight undergraduate CS students. Based on the responses, we conclude that the participants liked the systematic guidance. They rated the automated setting of breakpoints as most effective, followed by the interactive debugging and chatting, and the breakpoint explanations. In future, we will extend our concept and implementation, and perform more intensive user studies.",10.48550/arXiv.2501.09694,['JournalArticle'],Computer Science
Integrated Survey Classification and Trend Analysis via LLMs: An Ensemble Approach for Robust Literature Synthesis,2025,"Eleonora Bernasconi, Domenico Redavid, Stefano Ferilli","This study proposes a novel, scalable framework for the automated classification and synthesis of survey literature by integrating state-of-the-art Large Language Models (LLMs) with robust ensemble voting techniques. The framework consolidates predictions from three independent modelsGPT-4, LLaMA 3.3, and Claude 3to generate consensus-based classifications, thereby enhancing reliability and mitigating individual model biases. We demonstrate the generalizability of our approach through comprehensive evaluation on two distinct domains: Question Answering (QA) systems and Computer Vision (CV) survey literature, using a dataset of 1154 real papers extracted from arXiv. Comprehensive visual evaluation tools, including distribution charts, heatmaps, confusion matrices, and statistical validation metrics, are employed to rigorously assess model performance and inter-model agreement. The framework incorporates advanced statistical measures, including k-fold cross-validation, Fleiss kappa for inter-rater reliability, and chi-square tests for independence to validate classification robustness. Extensive experimental evaluations demonstrate that this ensemble approach achieves superior performance compared to individual models, with accuracy improvements of 10.0% over the best single model on QA literature and 10.9% on CV literature. Furthermore, comprehensive costbenefit analysis reveals that our automated approach reduces manual literature synthesis time by 95% while maintaining high classification accuracy (F1-score: 0.89 for QA, 0.87 for CV), making it a practical solution for large-scale literature analysis. The methodology effectively uncovers emerging research trends and persistent challenges across domains, providing researchers with powerful tools for continuous literature monitoring and informed decision-making in rapidly evolving scientific fields.",10.3390/electronics14173404,"['JournalArticle', 'Review']",Computer Science
Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs,2025,"Edward Phillips, Sean Wu, Soheila Molaei, Danielle Belgrave, Anshul Thakur, David Clifton","Large language models demonstrate impressive results across diverse tasks but are still known to hallucinate, generating linguistically plausible but incorrect answers to questions. Uncertainty quantification has been proposed as a strategy for hallucination detection, but no existing black-box approach provides estimates for both global and local uncertainty. The former attributes uncertainty to a batch of responses, while the latter attributes uncertainty to individual responses. Current local methods typically rely on white-box access to internal model states, whilst black-box methods only provide global uncertainty estimates. We introduce a geometric framework to address this, based on archetypal analysis of batches of responses sampled with only black-box model access. At the global level, we propose Geometric Volume, which measures the convex hull volume of archetypes derived from response embeddings. At the local level, we propose Geometric Suspicion, which ranks responses by reliability and enables hallucination reduction through preferential response selection. Unlike prior dispersion methods which yield only a single global score, our approach provides semantic boundary points which have utility for attributing reliability to individual responses. Experiments show that our framework performs comparably to or better than prior methods on short form question-answering datasets, and achieves superior results on medical datasets where hallucinations carry particularly critical risks. We also provide theoretical justification by proving a link between convex hull volume and entropy.",10.48550/arXiv.2509.13813,['JournalArticle'],Computer Science; Linguistics
Evaluation of Faithfulness Using the Longest Supported Subsequence,2023,"Anirudh Mittal, Timo Schick, Mikel Artetxe, Jane Dwivedi-Yu","As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarization dataset across six different models. Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric. We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness.",10.48550/arXiv.2308.12157,['JournalArticle'],Computer Science
AI Mock Interview Platform,2025,Vitthal Patil,"Abstract - The demand for effective interview preparation solutions has surged as job seekers seek comprehensive and interactive ways to enhance their readiness. The Full Stack AI Mock Interview App leverages cutting-edge technologies, including Next.js, Drizzle ORM, and AI integration, to provide a robust platform for dynamic interview practice. By integrating GPT-3 for question and answer generation, users receive personalized and adaptive interview experiences tailored to various job roles and industries.


The app's ability to allow users to record their answers via webcam and microphone simulates a realistic interview environment, enhancing the learning experience and engagement. Coupled with AI-generated feedback, users gain valuable insights into their performance, helping them refine their communication skills and confidence.


A well-defined database schema managed by Drizzle ORM ensures seamless data handling and retrieval, while the app's deployment on Vercel allows for efficient scalability and quick updates. This combination of modern web technologies and AI creates an innovative solution for aspiring professionals, setting a new standard in the realm of mock interview preparation",10.55041/ijsrem49510,['JournalArticle'],Computer Science
From Insight to Impact: How AI Tools Transform Managerial Practice in Czech Companies,2025,"Adam Pawliczek, Tereza lk, Aneta Matula","This study investigates the practical applications of artificial intelligence (AI) in managerial work, with a focus on how tools such as large language models (LLMs) support the personal and professional development of managers. It explores how AI is used for decision-making, information analysis, and communication, and assesses its perceived impact on managerial knowledge, skills, and productivity. The research combines a comprehensive literature review with empirical data from a structured survey of 42 managers in Moravian companies, primarily in the Czech Republic. The study evaluates the frequency, purpose, and tools used in AI-assisted management practice. AI tools are frequently used by managers, especially for information summarization, meeting preparation, and text generation. ChatGPT (GPT-4) is the dominant tool, with most respondents reporting weekly or daily use. The majority perceive AI as beneficial, with an average Likert score of 7.86 regarding its positive impact on performance. Limitations include the localized sample, rapid research timeframe, and fast-paced AI evolution, which may affect long-term generalizability. The study offers practical insights into how AI enhances managerial routines and leadership development. This paper provides rare, localized empirical evidence on AI adoption in management, contributing to a fast-growing global research area.",10.18690/um.epf.5.2025.59,"['Conference', 'Review']",Business; Computer Science
Synthetic Personas: Enhancing Demographic Response Simulation through Large Language Models and Genetic Algorithms,2025,"Morten Grundetjern, Per-Arne Andersen, Morten Goodwin","Understanding diverse demographic groups presents a significant challenge in market research. In this paper, we introduce a novel system that integrates large language models with genetic algorithms to create synthetic personas capable of generating feedback that approximates real-world human responses. Our experimental evaluation demonstrates that synthetic personas not only exhibit age-differentiated technology usage patterns consistent with documented trends but also benefit from genetic algorithm optimization, which improves response accuracy from 60.4% to 78.5% on training questions and from 62.6% to 68.8% on hidden questionsoutperforming human estimators. Moreover, the optimized personas achieve a 51.1% better correspondence with actual income distributions compared to random profiles. This approach makes it possible to rapidly generate feedback without requiring participants, facilitates iterative follow-ups, and systematically enhances demographic representativeness.",10.5121/ijci.2025.140202,['JournalArticle'],Computer Science
"DietQA: A Comprehensive Framework for Personalized Multi-Diet Recipe Retrieval Using Knowledge Graphs, Retrieval-Augmented Generation, and Large Language Models",2025,"Ioannis Tsampos, Emmanouil Marakakis","Recipes available on the web often lack nutritional transparency and clear indicators of dietary suitability. While searching by title is straightforward, exploring recipes that meet combined dietary needs, nutritional goals, and ingredient-level preferences remains challenging. Most existing recipe search systems do not effectively support flexible multi-dietary reasoning in combination with user preferences and restrictions. For example, users may seek gluten-free and dairy-free dinners with suitable substitutions, or compound goals such as vegan and low-fat desserts. Recent systematic reviews report that most food recommender systems are content-based and often non-personalized, with limited support for dietary restrictions, ingredient-level exclusions, and multi-criteria nutrition goals. This paper introduces DietQA, an end-to-end, language-adaptable chatbot system that integrates a Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large Language Model (LLM) to support personalized, dietary-aware recipe search and question answering. DietQA crawls Greek-language recipe websites to extract structured information such as titles, ingredients, and quantities. Nutritional values are calculated using validated food composition databases, and dietary tags are inferred automatically based on ingredient composition. All information is stored in a Neo4j-based knowledge graph, enabling flexible querying via Cypher. Users interact with the system through a natural language chatbot friendly interface, where they can express preferences for ingredients, nutrients, dishes, and diets, and filter recipes based on multiple factors such as ingredient availability, exclusions, and nutritional goals. DietQA supports multi-diet recipe search by retrieving both compliant recipes and those adaptable via ingredient substitutions, explaining how each result aligns with user preferences and constraints. An LLM extracts intents and entities from user queries to support rule-based Cypher retrieval, while the RAG pipeline generates contextualized responses using the user query and preferences, retrieved recipes, statistical summaries, and substitution logic. The system integrates real-time updates of recipe and nutritional data, supporting up-to-date, relevant, and personalized recommendations. It is designed for language-adaptable deployment and has been developed and evaluated using Greek-language content. DietQA provides a scalable framework for transparent and adaptive dietary recommendation systems powered by conversational AI.",10.3390/computers14100412,"['JournalArticle', 'Review']",Computer Science
"Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading",2025,"Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao","Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses. We introduce Pensieve (https://www.pensieve.co), an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work, providing instructors with rubric-aligned scores, transcriptions, and confidence ratings. Unlike prior tools that focus narrowly on specific tasks like transcription or rubric generation, Pensieve supports the entire grading pipeline-from scanned student submissions to final feedback-within a human-in-the-loop interface. Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. We present system details and empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions.",10.48550/arXiv.2507.01431,['JournalArticle'],Computer Science; Engineering
From Guest to Family: An Innovative Framework for Enhancing Memorable Experiences in the Hotel Industry,2023,"Abdulaziz Alhamadani, Khadija Althubiti, Shailik Sarkar, Jianfeng He, Lulwah Alkulaib, Srishti Behal, M. Khan, Chang-Tien Lu","This paper presents an innovative framework developed to identify, analyze, and generate memorable experiences in the hotel industry. People prefer memorable experiences over traditional services or products in today's ever-changing consumer world. As a result, the hospitality industry has shifted its focus toward creating unique and unforgettable experiences rather than just providing essential services. Despite the inherent subjectivity and difficulties in quantifying experiences, the quest to capture and understand these critical elements in the hospitality context has persisted. However, traditional methods have proven inadequate due to their reliance on objective surveys or limited social media data, resulting in a lack of diversity and potential bias. Our framework addresses these issues, offering a holistic solution that effectively identifies and extracts memorable experiences from online customer reviews, discerns trends on a monthly or yearly basis, and utilizes a local LLM to generate potential, unexplored experiences. As the first successfully deployed, fast, and accurate product of its kind in the industry, This framework significantly contributes to the hotel industry's efforts to enhance services and create compelling, personalized experiences for its customers.",10.1145/3625007.3632331,"['JournalArticle', 'Book', 'Review']",Business; Computer Science
Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory,2025,"Nicole Smith-Vaniz, Harper Lyon, Lorraine Steigner, Ben Armstrong, Nicholas Mattei","Large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to measure differences in human participants along political, national, and cultural lines. While there has been some analysis of the responses of LLM with respect to political stance in role-playing scenarios, no work so far has directly assessed the moral leanings in the LLM responses, nor have they connected LLM outputs with robust human data. In this paper we analyze the distinctions between LLM MFT responses and existing human research directly, investigating whether commonly available LLM responses demonstrate ideological leanings  either through their inherent responses, straightforward representations of political ideologies, or when responding from the perspectives of constructed human personas. We assess whether LLMs inherently generate responses that align more closely with one political ideology over another, and additionally examine how accurately LLMs can represent ideological perspectives through both explicit prompting and demographic-based role-playing. By systematically analyzing LLM behavior across these conditions and experiments, our study provides insight into the extent of political and demographic dependency in AI-generated responses.",10.1609/aies.v8i3.36727,"['JournalArticle', 'Conference']",Computer Science; Political Science
ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users,2025,"Shahaf David, Yair Meidan, Ido Hersko, Daniel Varnovitzky, D. Mimran, Y. Elovici, A. Shabtai","Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.",10.48550/arXiv.2506.13980,['JournalArticle'],Computer Science
Improved Sequence Predictions using Knowledge Graph Embedding for Large Language Models,2023,"Rabina Khatun, Nilanjan Sinhababu","Large Language Models (LLM) have gained huge popularity recently due to their problem-solving capability in multiple domains. Technically LLMs can be considered a critical mixture of huge amounts of training data, smart and exhaustive prompt engineering, and word prediction models along with Reinforcement and Supervised learning mechanisms. Word prediction models are at the core of any Large Language Model. The latest word prediction techniques are sequential and transformer models. Transformers have overcome most of the drawbacks of sequential models with similar embedding knowledge. The literature survey shows little to no improvement in the embedding techniques. In this paper, we examined the existing word prediction models by replacing embedding models with an auto-engineered Knowledge Graph Embedding. This auto-engineered data representation shows drastic improvements in prediction quality. This mechanism also accelerates the prediction by providing more context information to the models with respect to the general embedding mechanism. Standard evaluation strategies are used to compare the model behavior.",10.1145/3639856.3639872,"['Book', 'JournalArticle', 'Review']",Computer Science
Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science,2025,"Xiaohua Wu, Xiaohui Tao, Wenjie Wu, Yuefeng Li, Lin Li","Social surveys in computational social science are well-designed by elaborate domain theories that can effectively reflect the interviewee's deep thoughts without concealing their true feelings. The candidate questionnaire options highly depend on the interviewee's previous answer, which results in the complexity of social survey analysis, the time, and the expertise required. The ability of large language models (LLMs) to perform complex reasoning is well-enhanced by prompting learning such as Chain-of-thought (CoT) but still confined to left-to-right decision-making processes or limited paths during inference. This means they can fall short in problems that require exploration and uncertainty searching. In response, a novel large language model prompting method, called Random Forest of Thoughts (RFoT), is proposed for generating uncertainty reasoning to fit the area of computational social science. The RFoT allows LLMs to perform deliberate decision-making by generating diverse thought space and randomly selecting the sub-thoughts to build the forest of thoughts. It can extend the exploration and prediction of overall performance, benefiting from the extensive research space of response. The method is applied to optimize computational social science analysis on two datasets covering a spectrum of social survey analysis problems. Our experiments show that RFoT significantly enhances language models' abilities on two novel social survey analysis problems requiring non-trivial reasoning.",10.48550/arXiv.2502.18729,"['JournalArticle', 'Review']",Computer Science
"A Comprehensive Survey on LLM-Powered Recommender Systems: From Discriminative, Generative to Multi-Modal Paradigms",2025,"Dina Nawara, R. Kashef","Large Language Models (LLMs) have become transformative tools in Natural Language Processing (NLP). They are increasingly being integrated into recommendation systems to address existing limitations such as data sparsity, novelty, cold start, and long-tail challenges. Unlike traditional recommendation techniques that rely on user-item interaction matrices, LLMs provide context-aware reasoning and multi-modal processing capabilities. However, existing research mainly focuses on fine-tuning and prompt engineering strategies without fully exploring hybrid models, retrieval-augmented generation (RAG), graph-enhanced recommendations, and evaluation methodologies. This survey offers a comprehensive and structured examination of LLM-based recommendation systems, categorizing them into discriminative, generative, hybrid, graph-enhanced, and multimodal paradigms. Additionally, we explore adaptive fine-tuning techniques, prompt engineering strategies, and retrieval-augmented generation (RAG) approaches that improve LLM performance in personalized recommendations. We also examine evaluation methodologies, including LLM-as-a-Judge frameworks, benchmark limitations, and fairness considerations. Finally, we present a detailed discussion of open challenges, such as hallucination, scalability, bias, and privacy, highlighting critical research gaps and opportunities for future exploration. This survey aims to guide researchers in navigating the evolving landscape of LLM-driven recommendation systems.",10.1109/ACCESS.2025.3599832,"['JournalArticle', 'Review']",Computer Science
A Predictive Model for Anticipated Hate and Speech Violence in Social Media: Large Language Model Approach,2025,"Gimhani Samindika Dissanayake, Sudesh Jayathunge Bandara, H. Abeysundara","This study explores the application of Large Language Models (LLMs), specifically a BERT-based architecture, coupled with predictive analytics for proactive hate speech and violence identification and forecasting in social media comment moderation. The research addresses the critical need to shift from reactive to proactive content moderation strategies in order to enhance digital safety and foster inclusivity. Using a dataset of 44,000 public comments from Facebook that are unfiltered and generalized, the methodology includes data collection through the Facebook Graph API, preprocessing phases, and model training using a balanced dataset to improve detection accuracy. The study highlights the importance of ethical deployment in artificial intelligence, noting the role of predictive analytics in identifying patterns and signals that indicate the existence of harmful content before its widespread circulation. From this, the results show that a balanced dataset helped the model to achieve strong performance metrics: accuracy of 82.63%, precision of 82.5%, recall of 82.88%, and F1 score of 82.69%. Results like this have shown the promise that advanced AI technologies hold when integrated into content moderation to successfully handle and pre-empt online hate speech and violence. This research helps to contribute to the larger discussion on ethical AI, responsible digital citizenship, and safer online communities through the promotion of proactive moderation systems.",10.51244/ijrsi.2025.12020028,['JournalArticle'],Computer Science; Political Science; Sociology
Can ChatGPT Pass Modern Control Theory Exam?,2024,"Itsuki Ogo, Masanobu Koga","Large language models (LLMs), such as GPT models, have been rapidly studied in recent years and are expected to be applied to academic fields such as mathematics and engineering. In this study, we examined how accurately ChatGPT (GPT-4o) can answer modern control theory questions at the undergraduate-level. A set of 98 questions on modern control theory was used to evaluate GPT-4os problem-solving ability on modern control theory. The results revealed that the GPT-4o showed a 49.0% correct response rate to the undergraduate-level modern control theory exercises, and that the correct response rate tended to be lower for problems involving calculations, especially those that require step-by-step thinking and complex computation. This may be attributed to the Transformer architecture of the GPT model, which generates answers based on probabilistic predictions. In this study, we proposed a method to improve response accuracy by developing a customized GPT which leverages prompt engineering methods to address these issues. In order to evaluate the proposed method, a question set consisting of 45 graduate school entrance exam questions on modern control theory was developed. The results of the evaluation showed that the correct response rate was improved by 26.6 points, yielding a 64.4% correct response rate.",10.23919/ICCAS63016.2024.10773183,['Conference'],Computer Science; Engineering
Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis,2025,"Maciej Szankin, V. Venkatasamy, Lihang Ying","Outdoor advertisements remain a critical medium for modern marketing, yet accurately verifying billboard text visibility under real-world conditions is still challenging. Traditional Optical Character Recognition (OCR) pipelines excel at cropped text recognition but often struggle with complex outdoor scenes, varying fonts, and weather-induced visual noise. Recently, multimodal Vision-Language Models (VLMs) have emerged as promising alternatives, offering end-to-end scene understanding with no explicit detection step. This work systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B, InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline (PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with synthetic weather distortions to simulate realistic degradation. Our results reveal that while selected VLMs excel at holistic scene reasoning, lightweight CNN pipelines still achieve competitive accuracy for cropped text at a fraction of the computational cost-an important consideration for edge deployment. To foster future research, we release our weather-augmented benchmark and evaluation code publicly.",10.48550/arXiv.2507.11730,"['JournalArticle', 'Review']",Computer Science
Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity on Human-LLM Collaboration,2025,"Matthew Russell, Aman Shah, Giles Blaney, Judith Amores, Mary Czerwinski, Robert J. K. Jacob","AI-based interactive assistants are advancing human-augmenting technology, yet their effects on users' mental and physiological states remain under-explored. We address this gap by analyzing how Copilot for Microsoft Word, a LLM-based assistant, impacts users. Using tasks ranging from objective (SAT reading comprehension) to subjective (personal reflection), and with measurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we measure Copilot's effects on users. We also evaluate users' performance with and without Copilot across tasks. In objective tasks, participants reported a reduction of workload and an increase in enjoyment, which was paired with objective performance increases. Participants reported reduced workload and increased enjoyment with no change in performance in a creative poetry writing task. However, no benefits due to Copilot use were reported in a highly subjective self-reflection task. Although no physiological changes were recorded due to Copilot use, task-dependent differences in prefrontal cortex activation offer complementary insights into the cognitive processes associated with successful and unsuccessful human-AI collaboration. These findings suggest that AI assistants' effectiveness varies with task type-particularly showing decreased usefulness in tasks that engage episodic memory-and presents a brain-network based hypothesis of human-AI collaboration.",10.48550/arXiv.2506.04167,['JournalArticle'],Computer Science; Psychology
KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy,2025,"Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho","The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.",10.48550/arXiv.2502.05651,"['JournalArticle', 'Conference']",Computer Science; Psychology
Motivation of University Students to Use LLMs to Assist with Online Consumption of Sustainable Products: An Analysis Based on a Hybrid SEMANN Approach,2025,"Junjie Yu, Wenjun Yan, Jiaxuan Gong, Siqin Wang, Ken Nah, Wei Cheng","This study investigates how university students adopt large language models (LLMs) for online consumption of sustainable products, integrating perceived value theory with the technology acceptance model (TAM). Cross-sectional survey data were analyzed using structural equation modeling (SEM) and artificial neural networks (ANNs). SEM results reveal partial mediation. Performance expectancy value (PEV) and information quality value (IQV) directly shape continue using intention (CUI). They also influence CUI indirectly through perceived ease of use (PEU) and perceived usefulness (PU). Green self-identity value (GSV) influences CUI both directly and via PEU, while trust transfer value (TTV) and green perceived value (GPV) affect CUI only via PEU. ANN findings confirm this hierarchy, as PU (86.7%) and PEU (85.7%) are the strongest predictors of CUI, followed by GSV (73.7%). Convergent evidence from both methods indicates that instrumental utility, effortless interaction, and sustainability identity congruence drive sustained LLM use in the context of online consumption of green products, whereas credibility cues and sustainability incentives play secondary roles. This study extends TAM by incorporating multidimensional value constructs and offers design recommendations for engaging and high-utility AI shopping platforms.",10.3390/app15148088,"['JournalArticle', 'Review']",Business; Computer Science; Environmental Science
Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought,2025,"Haimei Pan, Jiyun Zhang, Qinxi Wei, Xiongnan Jin, Xinkai Chen, Jie Cheng","Fire is a highly destructive disaster, but effective prevention can significantly reduce its likelihood of occurrence. When it happens, deploying emergency robots in fire-risk scenarios can help minimize the danger to human responders. However, current research on pre-disaster warnings and disaster-time rescue still faces significant challenges due to incomplete perception, inadequate fire situational awareness, and delayed response. To enhance intelligent perception and response planning for robots in fire scenarios, we first construct a knowledge graph (KG) by leveraging large language models (LLMs) to integrate fire domain knowledge derived from fire prevention guidelines and fire rescue task information from robotic emergency response documents. We then propose a new framework called Insights-on-Graph (IOG), which integrates the structured fire information of KG and Large Multimodal Models (LMMs). The framework generates perception-driven risk graphs from real-time scene imagery to enable early fire risk detection and provide interpretable emergency responses for task module and robot component configuration based on the evolving risk situation. Extensive simulations and real-world experiments show that IOG has good applicability and practical application value in fire risk detection and rescue decision-making.",10.48550/arXiv.2509.00054,['JournalArticle'],Computer Science; Engineering; Environmental Science
Entailment-Driven Privacy Policy Classification with LLMs,2024,"Bhanuka Silva, Dishanika Denipitiyage, Suranga Seneviratne, A. Mahanti, Aruna Seneviratne","While many online services provide privacy policies for end users to read and understand what personal data are being collected, these documents are often lengthy and complicated. As a result, the vast majority of users do not read them at all, leading to data collection under uninformed consent. Several attempts have been made to make privacy policies more user-friendly by summarising them, providing automatic annotations or labels for key sections, or by offering chat interfaces to ask specific questions. With recent advances in Large Language Models (LLMs), there is an opportunity to develop more effective tools to parse privacy policies and help users make informed decisions. In this paper, we propose an entailment-driven LLM-based framework to classify paragraphs of privacy policies into meaningful labels that are easily understood by users. The results demonstrate that our framework outperforms traditional LLM methods, improving the F1 score in average by 11.2%. Additionally, our framework provides inherently explainable and meaningful predictions.",10.1109/BuildSEC64048.2024.00010,"['JournalArticle', 'Conference']",Computer Science
Mitigating Bias in Large Language Models Leveraging Multi-Agent Scenarios,2025,"Jens Lnstedt, Tim Schlippe","Mitigating bias in large language models (LLMs) is essential, as biased outputs can perpetuate harmful stereotypes and negatively influence decision-making [1]. LLM-based multiagent scenarios, which have gained attention for their ability to simulate human-like collaboration in tasks such as decisionmaking and strategic planning [2], may reduce bias with advisory LLMs, similarly to how human ethics boards maintain fairness. Consequently, we evaluated three LLM-based multi-agent scenarios to mitigate biased responses to human prompts: (1) a single bias expert agent, (2) a team of bias expert agents, and (3) a simulated human ethics board. To measure bias reduction, we used a subset of the BBQ corpus, a bias benchmark corpus for question answering [3], focusing on eight bias types: physical appearance, disabiliy status, age, nationality status, race / ethnicity, sexual orientation, gender identity, and religion. Results show that all three scenarios reduced bias in LLM outputs by over 20% compared to a single-agent approach.",10.1109/ICNLP65360.2025.11108428,['Conference'],Computer Science
Zara: An LLM-based Candidate Interview Feedback System,2025,"Nima Yazdani, Aruj Mahajan, Ali Ansari","This paper introduces Zara, an AI-driven recruitment support system developed by micro1, as a practical case study illustrating how large language models (LLMs) can enhance the candidate experience through personalized, scalable interview support. Traditionally, recruiters have struggled to deliver individualized candidate feedback due to logistical and legal constraints, resulting in widespread candidate dissatisfaction. Leveraging OpenAI's GPT-4o, Zara addresses these limitations by dynamically generating personalized practice interviews, conducting conversational AI-driven assessments, autonomously delivering structured and actionable feedback, and efficiently answering candidate inquiries using a Retrieval-Augmented Generation (RAG) system. To promote transparency, we have open-sourced the approach Zara uses to generate candidate feedback.",10.48550/arXiv.2507.02869,['JournalArticle'],Computer Science
Advanced AI Techniques for Personalized Customer Support Using Intelligent Chatbots,2025,"D. P. Kumar, R. Mohana, I. Pratheema, Dr S J Vivekanandan","Advanced AI techniques, such as machine learning (ML) and natural language processing (NLP), have played a significant role in personalized customer care using smart chatbots. These chatbots employ customer data and past interactions to deliver tailored, context-aware responses, improving the user experience. Through understanding user intent and ongoing adaptation, AI-powered chatbots provide effective, real time support across multiple communication channels. Sentiment analysis adds more refinement to personalization by basing responses upon emotional tone. For complicated questions needing human handling, integration with Gemini API allows smooth escalation to live agents and an uninterrupted, responsive support experience. With continued advancements in AI capabilities, smart chatbots will fuel enhanced engagement, customer loyalty, and actionable business insights.",10.47392/irjaeh.2025.0393,['JournalArticle'],Computer Science
"ARTIFICIAL INTELLIGENCE IN ASSESSING THE ABILITY OF ADOLESCENTS TO MAKE CONSTRUCTIVE DECISIONS IN A STRESSFUL SITUATION 
(ACCORDING TO PROFESSOR M. O. OVCHARENKO)",2025,"M. Ovcharenko, Leonid Pinskyy, Yevhen Verbytskyi","The study addresses the pressing issue of assessing adolescents ability to make constructive decisions in stressful situations. Adolescence is a critical developmental period characterized by an increased risk of addictive behaviors, driven by low resilience and an impaired capacity to effectively cope with stress. The authors present a validated method  the Ovcharenko Constructive Decision-Making Capacity Index (CDMC)  which enables quantitative evaluation of adolescents decision-making constructiveness and prediction of susceptibility to maladaptive behavioral patterns. For the first time, artificial intelligence (GPT-4o, GPT-5) is proposed for automated recognition of completed paper questionnaires, calculation of the SPKR index, and provision of recommendations for practicing psychologists. Google Forms and a ranking algorithm in Google Sheets with QR-code access were developed, improving the efficiency of screening and enabling large-scale online surveys. The proposed innovative approach integrates traditional psychometric tools with modern digital technologies and can be incorporated into mental health care systems for early diagnosis and prevention of deviant behavioral patterns among adolescents.",10.36927/2079-0325-v33-is3-2025-6,"['JournalArticle', 'Review']",Computer Science; Psychology
"AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent",2025,"Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Zhong Zhang, Ya-Ting Lu, Yankai Lin, Zhiyuan Liu, Dahai Li, Cheng Qian","With the raid evolution of large language models and multimodal foundation models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that must be solved for mobile agents to deliver practical, scalable impact: (1) generalization across tasks, modalities, apps, and devices; (2) accuracy, specifically precise on-screen interaction and click targeting; (3) long-horizon capability for sustained, multi-step goals; and (4) efficiency, specifically high-performance runtime on resource-constrained devices. We present AppCopilot, a multimodal, multi-agent, general-purpose on-device assistant that operates across applications and constitutes a full-stack, closed-loop system from data to deployment. AppCopilot operationalizes this position through an end-to-end autonomous pipeline spanning data collection, training, deployment, high-quality and efficient inference, and mobile application development. At the model layer, it integrates multimodal foundation models with robust Chinese-English support. At the reasoning and control layer, it combines chain-of-thought reasoning, hierarchical task planning and decomposition, and multi-agent collaboration. At the execution layer, it enables user personalization and experiential adaptation, voice interaction, function calling, cross-app and cross-device orchestration, and comprehensive mobile app support. The system design incorporates profiling-driven optimization for latency, memory, and energy across heterogeneous hardware. Empirically, AppCopilot achieves significant improvements along all four dimensions: stronger generalization, higher-precision on-screen actions, more reliable long-horizon task completion, and faster, more resource-efficient runtime.",10.48550/arXiv.2509.02444,['JournalArticle'],Computer Science
Comparative Analysis of RAG-Based Open-Source LLMs for Indonesian Banking Customer Service Optimization Using Simulated Data,2025,"Hendra Lijaya, Patricia Ho, Handri Santoso","In the digital era, banks face challenges in delivering fast, accurate, and efficient customer service, especially for frequently asked simple questions. This study evaluates the effectiveness of three open-source Large Language Models (LLMs), namely Gemma2-9B-Sahabat-AI, Qwen2.5-14B-Instruct, and Mistral-Nemo-Instruct in supporting a Retrieval-Augmented Generation (RAG) question-answering system for the banking sector. Using 12,000 synthetic billing documents indexed with intfloat/multilingual-e5-large-instruct embeddings (1024 dimensions), model performance was assessed via semantic similarity metrics, LLM-as-a-Judge scores (GPT-4o-mini and Gemini 2.0 Flash), and human validation Gemma2-9B-Sahabat-AI achieved the highest semantic similarity score (0.9627), followed by Mistral (0.9614) and Qwen2.5 (0.9284). In LLM-as-a-Judge evaluations, Qwen2.5 ranked highest on GPT-4o-mini (92.2), while Gemma2 led under Gemini 2.0 Flash (88.4). Human evaluators gave perfect scores for factual questions (110), but all models struggled with arithmetic in question 13. Gemma2s average response time was 41 seconds, faster than Qwen2.5s 72 seconds and Mistrals 48 seconds, confirming Gemma2s balanced performance in accuracy, speed, and computational efficiency. These findings underscore the potential of locally operated open-source LLMs for banking applications, ensuring privacy and regulatory compliance. However, limitations include reliance on synthetic data, a narrow question set, and lack of user diversity. Future research should involve broader queries, real user testing, and numeric reasoning modules to ensure robust and scalable deployment in real-world banking customer service environments.",10.32736/sisfokom.v14i3.2383,['JournalArticle'],Business; Computer Science
AI Driven Chatbot Counsellor,2024,Prof. Rekha Kulkarni,"Speech and written information are fundamental to human communication. As a result, the majority of spoken and textual communication takes place on digital platforms like Twitter, Facebook, and WhatsApp, among others. Our model employs dual recurrent neural networks (RNNs) to encode the information from text and audio sequences, as spoken language and sound constitute emotional discourse. The emotion class is then predicted by combining the data from the two sources. Due to the complexity of speech emotion recognition, models that use audio properties to generate powerful classifiers have become increasingly important. Completing sentences or crafting sentences from a given starting word is a major aspect of natural language processing. In a way, it shows if a machine is capable of human creativity and mental processes. In order to assist handle diverse phrase generation challenges, we use natural language processing to train the machine for certain tasks. This is especially useful for application situations such as machine translation, automatic question answering, and summary creation. At the moment, OpenAI GPT and BERT are the most widely used language models for text generation and prediction. The approaches based on handwritten instructions, patterns, or statistical methods have been quickly superseded by the latest developments in deep learning and artificial intelligence, such as end-to-end trainable neural networks. This research presents a novel approach to deep neural learning-based chatbot creation. This approach builds a multilayer neural network to analyse and learn from the data. Furthermore, we employ supplementary limitations on the generation model to generate the correct response, which is capable of discerning the context of the discussion, the user's mood, and the anticipated response. This enables us to provide customised counselling replies depending on customer feedback. Through this study, two new corpora will be used to train the OpenAI GPT model, which will then be used to generate articles and long sentences. Finally, a comparison study will be carried out. Concurrently, we will use the BERT model to complete the task of context-based intermediate word prediction. Keywords : Artificial Intelligence, Data Science, NLP, Deep Learning, Machine Learning, GPT, Generative AI, Speech Synthesis",10.55041/ijsrem35303,['JournalArticle'],Computer Science; Psychology
Systematic Prompt Framework for Qualitative Data Analysis: Designing System and User Prompts,2025,"Aisvarya Adeseye, J. Isoaho, Mohammad Tahir","Prompt engineering has become an important aspect in optimizing the performance of large language models (LLMs) in diverse applications. This research proposes a systematic framework for system and user prompts by utilizing few-shot learning, chain-of-thought reasoning, role play and iterative refinement. The proposed framework was evaluated on open source LLMs, Llama, Gemma, and Phi, running on local machines to underscore their capability to enhance LLMs' outputs for qualitative data analysis for interview transcripts about security and privacy issues of gamification. Utilizing local LLMs eliminates concerns related to data leakage and privacy, making this approach particularly suitable for organizations that have privacy concerns with publicly available LLM solutions like ChatGPT, Gemini, DeepSeek etc. The LLM output demonstrated improved accuracy, consistency, and scalability in addressing security and privacy concerns with gamification. The validation using manual analysis with NVivo indicates less than 5% error margin for frequency analysis.",10.1109/ICHMS65439.2025.11154183,"['JournalArticle', 'Conference']",Computer Science
"Human Vulnerability, Machine Law: Ethical Risks and Global Governance Challenges in Generative AI",2025,Kaukab Ansari,"Generative Artificial Intelligence (GenAI) is rapidly transforming digital engagement, but its expanding capabilities introduce novel ethical and legal dilemmas. This paper investigates the under-examined risks of using GenAI models as informal life coaches and therapists, in addition to the other notable differences in regional AI governance, especially in Asia and Africa. Utilizing a mixed-methods design, this study integrates AI simulation testing of four leading models (ChatGPT, Gemini, Claude, and Grok) against the Columbia Suicide Severity Rating Scale (C-SSRS) combined with survey data from 592 users about using AI as life coach or therapist. Results indicate a difference between the AI models which prioritise safety; specifically, the AI model either disengaged from, or provided repetitive, or generic advice, when assessing a high-risk situation where a user might harm themself. The survey also questions the usefulness of AI chatbotswhere most users stated that they were not at all helpful (55.4%) in providing emotional support , users were never better  (56.7%) after using AI as a therapist and prefer human therapists (79.2%) for privacy reasons. Further, the analysis suggests a clear global split in policy; with the EU providing a proactive regulatory framework as opposed to legal and enforcement gaps in the Global South. Overall, this research suggests the use of GenAI for mental health support is premature and facilitates significant risk to vulnerable users. Finally, this paper argues for urgent, risk-based regulations, hybrid models of AI and human support and user-centred research, to create ethical guardrails for responsible innovations in AI.",10.36948/ijfmr.2025.v07i05.56786,"['JournalArticle', 'Review']",Computer Science; Law; Political Science; Psychology
"Collaborative Causal Inference and Multi-Agent Dynamic Intervention for ""Dual Carbon"" Public Opinion Driven by Reinforced Large Language Models and Diffusion Models",2025,Xin Chen,"Under the Dual Carbon goal, public opinion analysis is crucial for optimizing policy implementation and enhancing social consensus, yet it faces challenges such as insufficient multi-source data integration, limited causal modeling, and delayed interventions. This study proposes a collaborative framework integrating reinforcement learning-enhanced large language models (LLMs), diffusion models, and multi-agent systems (MASs). By constructing a four-dimensional causal network of policytechnologyeconomypublic sentiment, it analyzes multi-source data and simulates multi-agent interactions. The experimental results show that this framework outperforms Latent Dirichlet Allocation (LDA), Bidirectional Encoder Representations from Transformers (BERT), and Susceptible Infected Recovered (SIR) models in causal inference, dynamic intervention, and multi-agent collaboration. Reinforcement Learning from Human Feedback (RLHF) optimizes LLM outputs for reliable policy recommendations, with pass@10 showing strong correlations. This study provides scientific support for Dual Carbon policymaking and public opinion guidance, facilitating the green and low-carbon transition.",10.3390/systems13080689,['JournalArticle'],Computer Science; Economics; Environmental Science; Political Science
LineRetriever: Planning-Aware Observation Reduction for Web Agents,2025,"Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han L, Massimo Caccia, V'eronique Eglin, Alexandre Aussem, J'er'emy Espinas, Alexandre Lacoste","While large language models have demonstrated impressive capabilities in web navigation tasks, the extensive context of web pages, often represented as DOM or Accessibility Tree (AxTree) structures, frequently exceeds model context limits. Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history. This is particularly problematic for adaptive planning in web agents, where understanding the current state is essential for determining future actions. We hypothesize that embedding models lack sufficient capacity to capture plan-relevant information, especially when retrieving content that supports future action prediction. This raises a fundamental question: how can retrieval methods be optimized for adaptive planning in web navigation tasks? In response, we introduce \textit{LineRetriever}, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps. Unlike traditional retrieval methods that focus solely on semantic similarity, \textit{LineRetriever} explicitly considers the planning horizon, prioritizing elements that contribute to action prediction. Our experiments demonstrate that \textit{LineRetriever} can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.",10.48550/arXiv.2507.00210,['JournalArticle'],Computer Science
Data Redundancy Elimination and Noise Processing via Large Language Model Prompt Engineering,2024,"Jin Guang, Jing Chao, Tianqi Zong, Chen Siya, Chaoyuan Cui, Fan Jun","With the rapid advancement of artificial intelligence (AI), intelligent systems have been widely applied across various practical domains. During their experimental processes, data collection often occurs in complex environments, leading to challenges in ensuring data quality and integrity. Two significant issues are data redundancy and data noise, which can degrade model performance and impact the generalization ability and prediction accuracy of AI models. Traditional methods for handling these issues are either rule-based, relying heavily on domain knowledge, or machine learning-based, which require large volumes of high-quality training data and significant computational resources. In this paper, we propose a novel approach leveraging Large Language Models (LLMs) combined with the Sequential Chain optimization algorithm to address data redundancy elimination and noise processing. By designing automated prompt templates tailored to experimental data characteristics and utilizing LLMs' extensive knowledge and reasoning capabilities, our approach improves the effectiveness of preprocessing tasks. Additionally, the Sequential Chain technique enhances LLM processing per-formance, reducing the hallucination phenomenon and ensuring higher accuracy in data preprocessing. Our experimental results demonstrate that LLM-based methods can achieve results comparable to traditional techniques while offering greater scalability and adaptability. Future work could focus on developing more efficient LLMs with lower computational requirements and refining prompt engineering techniques to reduce the time investment needed, making these advanced methods more accessible and practical for a broader range of applications.",10.1109/BigDIA63733.2024.10808217,['Conference'],Computer Science; Engineering
AIced-Prep -AI Based Mock Interview Evaluator,2025,"Snehal Rajbhar, Siddhi Shelke, Ayush Singh, Yash Singh, Pravin Shinde","AIced Prep is an AI-driven system designed to provide individuals with realistic mock interviews and personalized feedback, particularly targeting freshers, job seekers, students, and recruitment agencies. The system aims to build confidence, improve performance, and enhance candidates' chances of success in interviews. To achieve this, the project integrates a Question Generation System using RetrievalAugmented Generation(RAG) and Large Language Models (LLMs) for generating relevant, unique, and non-repetitive questions. The project fo- cuses on two primary use cases: interview question generation and resume classification and mock interview. By leveraging the Mistral-7B LLM for contextually accurate question generation and FAISS for efficient vector-based retrieval, the system dynami- cally processes documents, including PDFs, to generate questions. The web-based interface, built using FastAPI and Streamlit, ensures users have an interactive and accessible experience. This hybrid approach addresses common challenges in question generation, such as repetitive questions and irrelevant content, by combining information retrieval techniques with generative AI methods. The ultimate goal of AIced Prep is to make quality interview preparation accessible to everyone, providing scalable, efficient, and personalized interview preparation solution.",10.1109/ICDICI66477.2025.11135157,['Conference'],Computer Science
Dynamic Context Adaptation for Consistent Role-Playing Agents with Retrieval-Augmented Generations,2025,"Jeiyoon Park, Yongshin Han, Minseop Kim, Kisu Yang","Recent advances in large language models (LLMs) have catalyzed research on role-playing agents (RPAs). However, the process of collecting character-specific utterances and continually updating model parameters to track rapidly changing persona attributes is resource-intensive. Although retrieval-augmented generation (RAG) can alleviate this problem, if a persona does not contain knowledge relevant to a given query, RAG-based RPAs are prone to hallucination, making it challenging to generate accurate responses. In this paper, we propose Amadeus, a training-free framework that can significantly enhance persona consistency even when responding to questions that lie beyond a character's knowledge. Amadeus is composed of Adaptive Context-aware Text Splitter (ACTS), Guided Selection (GS), and Attribute Extractor (AE). To facilitate effective RAG-based role-playing, ACTS partitions each character's persona into optimally sized, overlapping chunks and augments this representation with hierarchical contextual information. AE identifies a character's general attributes from the chunks retrieved by GS and uses these attributes as a final context to maintain robust persona consistency even when answering out-of-knowledge questions. To underpin the development and rigorous evaluation of RAG-based RPAs, we manually construct CharacterRAG, a role-playing dataset that consists of persona documents for 15 distinct fictional characters totaling 976K written characters, and 450 question-answer pairs. We find that our proposed method effectively models not only the knowledge possessed by characters, but also various attributes such as personality.",10.48550/arXiv.2508.02016,['JournalArticle'],Computer Science
Theatrical Language Processing: Exploring AI-Augmented Improvisational Acting and Scriptwriting with LLMs,2025,"Sora Kang, Joonhwan Lee","The increasing convergence of artificial intelligence has opened new avenues, including its emerging role in enhancing creativity. It is reshaping traditional creative practices such as actor improvisation, which often struggles with predictable patterns, limited interaction, and a lack of engaging stimuli. In this paper, we introduce a new concept, Theatrical Language Processing (TLP), and an AI-driven creativity support tool, Scribble$.$ai, designed to augment actors'creative expression and spontaneity through interactive practice. We conducted a user study involving tests and interviews with fourteen participants. Our findings indicate that: (1) Actors expanded their creativity when faced with AI-produced irregular scenarios; (2) The AI's unpredictability heightened their problem-solving skills, specifically in interpreting unfamiliar situations; (3) However, AI often generated excessively detailed scripts, which limited interpretive freedom and hindered subtext exploration. Based on these findings, we discuss the new potential in enhancing creative expressions in film and theater studies through an AI-driven tool.",10.23362/KOEN2025.07.25.1.099,['JournalArticle'],Art; Computer Science
Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition,2025,"Keito Inoshita, Rushia Harada","In the field of emotion recognition, the development of high-performance models remains a challenge due to the scarcity of high-quality, diverse emotional datasets. Emotional expressions are inherently subjective, shaped by individual personality traits, socio-cultural backgrounds, and contextual factors, making large-scale, generalizable data collection both ethically and practically difficult. To address this issue, we introduce PersonaGen, a novel framework for generating emotionally rich text using a Large Language Model (LLM) through multi-stage persona-based conditioning. PersonaGen constructs layered virtual personas by combining demographic attributes, socio-cultural backgrounds, and detailed situational contexts, which are then used to guide emotion expression generation. We conduct comprehensive evaluations of the generated synthetic data, assessing semantic diversity through clustering and distributional metrics, human-likeness via LLM-based quality scoring, realism through comparison with real-world emotion corpora, and practical utility in downstream emotion classification tasks. Experimental results show that PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, demonstrating its potential as a robust alternative for augmenting or replacing real-world emotional datasets.",10.48550/arXiv.2507.13380,['JournalArticle'],Computer Science
Perception vs reality: a multi-level framework for analyzing user perceptions of LLM-based chatbots,2025,"Yankuan Liu, Pei-Luen Patirck Rau","
 
 This study develops a multi-level framework to examine user perceptions of large language models (LLMs). By investigating the relationships among scenario characteristics (irreplaceability, audience and purpose), user perceptions (familiarity, trust and attitudes) and usage contexts (individual, organizational and societal), this study explores how users evaluate LLM-based chatbots in various contexts, addressing a gap in existing research on LLM applications and their user acceptance.
 
 
 
 Utilizing a mixed-methods approach, this study crafted eight diverse usage scenarios spanning niche entertainment to significant production support. A questionnaire survey captured users familiarity, trust and attitudes, enabling a systematic analysis of user perceptions at individual, organizational and societal levels. Hierarchical clustering and linear regression linked user perceptions with scenario characteristics, forming the basis of the proposed multi-level framework.
 
 
 
 Through the developed framework, this study identifies distinct perception patterns that systematically align with individual, organizational and societal levels of application. For instance, personal assistance scenarios receive higher perception scores, while societal-level applications reveal mixed user perceptions. These insights suggest that enhancing system performance and addressing user concerns are crucial for effective adoption.
 
 
 
 This study introduces a user-centric, multi-level framework for analyzing LLM perceptions across contexts. It highlights the critical role of user perspectives in shaping ethical, reliable and widely accepted LLM applications, offering practical guidance for developers, organizations and policymakers.
",10.1108/itp-03-2024-0403,"['JournalArticle', 'Review']",Computer Science; Linguistics
"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",2025,"Kongmeng Liew, Xiaoli Wu, Zhouqing Zhang",,10.1109/CAI64502.2025.00051,"['JournalArticle', 'Conference', 'Review']",Computer Science; Psychology
Codifying Character Logic in Role-Playing,2025,"Letian Peng, Jingbo Shang","This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. Each profile defines a set of functions parse_by_scene(scene) that outputs a list of logic-grounded assertions triggered_statements, using both explicit control structures (e.g., if-then-else) and condition checks like check_condition(scene, question), where each question is a semantically meaningful prompt about the scene (e.g.,""Is the character in danger?"") discriminated by the role-playing LLM as true, false, or unknown. This explicit representation offers three key advantages over traditional prompt-based profiles, which append character descriptions directly into text prompts: (1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning; (2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches; (3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve. To validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using NLI-based scoring to compare character responses against ground-truth actions. Our experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. Notably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing a scalable and efficient foundation for local deployment of role-play agents.",10.48550/arXiv.2505.07705,['JournalArticle'],Computer Science
Steel-LLM:From Scratch to Open Source - A Personal Journey in Building a Chinese-Centric LLM,2025,"Qingshui Gu, Shu Li, Tianyu Zheng, Zhaoxiang Zhang","Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.",10.48550/arXiv.2502.06635,['JournalArticle'],Computer Science; Linguistics
"The sociocultural roots of artificial conversations: The taste, class and habitus of generative AI chatbots",2025,"Ilir Rama, Massimo Airoldi","Research on AI has extensively considered biases related to gender and race. However, much less attention has been dedicated to another sociological tenet: that of class. Inspired by Bourdieus work on cultural stratification and distinction, this work sheds light on the sociocultural roots of artificial sociality, and on how these become manifest as habitus within the outputs of generative AI models. We conducted 39 interviews with three AI chatbots  ChatGPT, Gemini and Replika  after asking them to impersonate individuals with different occupational positions: highly skilled professionals, blue-collar workers, university professors in the humanities, construction workers, computer scientists and hairdressers. Our qualitative study shows class-based regularities in how popular AI chatbots represent the lifestyle and tastes of fictional personas in artificial conversations, partly mediated by infrastructural and design elements. The article proposes a sociological perspective on bias in artificial sociality and experiments with interview methods in the study of generative AI.",10.1177/14614448251338273,['JournalArticle'],Computer Science; Sociology
Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control,2025,Yuanchang Ye,"This study introduces a significance testing-enhanced conformal prediction (CP) framework to improve trustworthiness of large language models (LLMs) in multiple-choice question answering (MCQA). While LLMs have been increasingly deployed in disciplinary QA scenarios, hallucination and nonfactual generation substantially compromise response reliability. Although CP provides statistically rigorous marginal coverage guarantees for prediction sets, and significance testing offers established statistical rigor, their synergistic integration remains unexplored. To mitigate hallucination and factual inaccuracies, our framework integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs'black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves user-specified empirical miscoverage rates; (2) Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($\alpha$), validating APSS as an effective uncertainty metric. This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.",10.48550/arXiv.2508.10022,['JournalArticle'],Computer Science
Finetuning LLMs for Human Behavior Prediction in Social Science Experiments,2025,"Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein","Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at stanfordhci.github.io/socrates.",10.48550/arXiv.2509.05830,['JournalArticle'],Computer Science; Psychology
Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust,2025,"Hyo Jin Do, Werner Geyer","Large language models are known to produce outputs that are plausible but factually incorrect. To prevent people from making erroneous decisions by blindly trusting AI, researchers have explored various ways of communicating factuality estimates in AI-generated outputs to end-users. However, little is known about whether revealing content estimated to be less factual influences users' trust when compared to hiding it altogether. We tested four different ways of disclosing an AI-generated output with factuality assessments: transparent (highlights less factual content), attention (highlights factual content), opaque (removes less factual content), ambiguity (makes less factual content vague), and compared them with a baseline response without factuality information. We conducted a human subjects research (N=148) using the strategies in question-answering scenarios. We found that the opaque and ambiguity strategies led to higher trust while maintaining perceived answer quality, compared to the other strategies. We discuss the efficacy of hiding presumably less factual content to build end-user trust.",10.48550/arXiv.2508.07095,"['JournalArticle', 'Conference']",Computer Science; Psychology
"AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering",2025,"Onat Gngr, Roshan Sood, Harold Wang, Tajana Simunic","Large Language Models (LLMs) have recently demonstrated strong potential for cybersecurity question answering (QA), supporting decision-making in real-time threat detection and response workflows. However, their substantial computational demands pose significant challenges for deployment on resource-constrained edge devices. Quantization, a widely adopted model compression technique, can alleviate these constraints. Nevertheless, quantization may degrade model accuracy and increase susceptibility to adversarial attacks. Fine-tuning offers a potential means to mitigate these limitations, but its effectiveness when combined with quantization remains insufficiently explored. Hence, it is essential to understand the trade-offs among accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation framework designed to benchmark several state-of-the-art small LLMs under four distinct configurations: base, quantized-only, fine-tuned, and fine-tuned combined with quantization, specifically for cybersecurity QA. Our results demonstrate that quantization alone yields the lowest accuracy and robustness despite improving efficiency. In contrast, combining quantization with fine-tuning enhances both LLM robustness and predictive performance, achieving an optimal balance of accuracy, robustness, and efficiency. These findings highlight the critical need for quantization-aware, robustness-preserving fine-tuning methodologies to enable the robust and efficient deployment of LLMs for cybersecurity QA.",10.48550/arXiv.2509.13514,['JournalArticle'],Computer Science; Engineering
Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh,2024,"MD Thamed Bin Zaman Chowdhury, Moazzem Hossain, Md. Ridwanul Islam","Road accidents pose significant concerns globally. It leads to large financial losses, injuries, disabilities and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named Durghotona GPT that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers Prothom Alo, Dhaka Tribune and The Daily Star leveraging web scraping techniques. The collected news was then processed using the newest available LLMs. These LLMs are: GPT-4, GPT-3.5 and Llama-3. The framework efficiently extracts relevant information, categorizes reports and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors and communication gaps. The authors evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 96% accuracy in the authors evaluation. So, it can be considered as a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning and public health. The authors also developed an interface for Durghotona GPT for easy use as a part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.",10.1109/ICCIT64611.2024.11021969,"['JournalArticle', 'Conference']",Computer Science; Engineering; Environmental Science
Artificial Finance: How AI Thinks About Money,2025,"Orhan Erdem, Ragavi Pobbathi Ashok","In this paper, we explore how large language models (LLMs) approach financial decision-making by systematically comparing their responses to those of human participants across the globe. We posed a set of commonly used financial decision-making questions to seven leading LLMs, including five models from the GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We then compared their outputs to human responses drawn from a dataset covering 53 nations. Our analysis reveals three main results. First, LLMs generally exhibit a risk-neutral decision-making pattern, favoring choices aligned with expected value calculations when faced with lottery-type questions. Second, when evaluating trade-offs between present and future, LLMs occasionally produce responses that appear inconsistent with normative reasoning. Third, when we examine cross-national similarities, we find that the LLMs'aggregate responses most closely resemble those of participants from Tanzania. These findings contribute to the understanding of how LLMs emulate human-like decision behaviors and highlight potential cultural and training influences embedded within their outputs.",10.48550/arXiv.2507.10933,['JournalArticle'],Business; Computer Science; Economics
Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory,2025,"Shuning Zhang, Rongjun Ma, Ying Ma, Shixuan Li, Yiqun Xu, Xin Yi, Hewu Li","Large Language Models (LLMs) are increasingly integrating memory functionalities to provide personalized and context-aware interactions. However, user understanding, practices and expectations regarding these memory systems are not yet well understood. This paper presents a thematic analysis of semi-structured interviews with 18 users to explore their mental models of LLM's Retrieval Augmented Generation (RAG)-based memory, current usage practices, perceived benefits and drawbacks, privacy concerns and expectations for future memory systems. Our findings reveal diverse and often incomplete mental models of how memory operates. While users appreciate the potential for enhanced personalization and efficiency, significant concerns exist regarding privacy, control and the accuracy of remembered information. Users express a desire for granular control over memory generation, management, usage and updating, including clear mechanisms for reviewing, editing, deleting and categorizing memories, as well as transparent insight into how memories and inferred information are used. We discuss design implications for creating more user-centric, transparent, and trustworthy LLM memory systems.",10.48550/arXiv.2508.07664,"['JournalArticle', 'Review']",Computer Science; Psychology
A Feature Engineering Approach to Improve Clustering-Based Persona Generation,2025,"Thanatat Wongabut, Unhawa Ninrutsirikun, Chakarida Nukoolkit, Patcharaporn Lavangnananda, K. Warasup, C. Arpnikanondt","Personas are widely recognized as essential tools in user-centered design and human-computer interaction, enabling designers to deeply understand target users' behaviors, goals, and needs. With the increasing complexity and scale of digital systems, automated persona generation has emerged as a promising solution to streamline persona creation by leveraging user data, clustering algorithms, and large language models. Despite its potential, current methods face several limitations, including inadequate feature engineering, a lack of context-specific customization, and limited validation of persona relevance in real-world applications. This study aims to enhance the effectiveness of automated persona generation within the context of educational digital services by proposing a feature engineering-driven clustering approach. Using K-means clustering combined with dimension-based feature construction, we evaluate the clusters through silhouette analysis and assess the quality of personas based on cluster representativeness. The results demonstrate improved clustering cohesion and more representative persona profiles compared to baseline methods. The study contributes a structured methodology for generating data-driven personas tailored to educational environments, which benefits UX designers. However, limitations include the reliance on survey-based datasets and the scope confined to higher education in Thailand. Future research will explore the generalizability of the proposed approach across different domains, conduct cross-cultural validation of persona models, further assess persona quality through expert evaluations, and investigate the use of alternative large language models to enhance the quality and relevance of generated personas.",10.1109/SEAI65851.2025.11108775,"['Conference', 'Review']",Computer Science
"Mixed Crowd Navigation: Perception, Interaction, Planning, and Control",2025,"Jia Pan, Weizi Li, Wenxi Liu, Iftekharul Islam, Ke Guo, Yajue Yang, Shuai Zhang, Xuebo Ji, Dawei Wang","We comprehensively survey mixed crowd navigation, focusing on the integration of robotic agents with volitional crowds (humans or human-driven vehicles) to achieve system-wide benefits. The survey is organized following the perceptioninteractionplanningcontrol pipeline, examining four core components: (a) perceiving global crowd behavior from local robot observations through nonparticipant and participant observation methods; (b) modeling volitional agent responses via rule-based and data-driven interaction frameworks; (c) predicting crowd dynamics across microscopic, mesoscopic, and macroscopic scales using both traditional and machine learning approaches; and (d) synthesizing control policies that guide crowds toward desired states. Wed address critical challenges such as complex interaction modeling under partial observability, constrained robotic influence, and the need for multiscale behavioral consistency. Key applications span pedestrian crowd management and mixed traffic control. We also highlight emerging trends in mixed crowd navigation, including the use of deep reinforcement learning and foundation models, while identifying persistent challenges in human irrationality modeling, compliance prediction, and privacy-preserving algorithms for real-world deployment.",10.1146/annurev-control-032024-023929,"['JournalArticle', 'Review']",Computer Science; Engineering
Fitting the Message to the Moment: Designing Calendar-Aware Stress Messaging with Large Language Models,2025,"Pranav Rao, Maryam Taj, A. Mariakakis, Joseph Jay Williams, Ananya Bhattacharjee","Existing stress-management tools fail to account for the timing and contextual specificity of students' daily lives, often providing static or misaligned support. Digital calendars contain rich, personal indicators of upcoming responsibilities, yet this data is rarely leveraged for adaptive wellbeing interventions. In this short paper, we explore how large language models (LLMs) might use digital calendar data to deliver timely and personalized stress support. We conducted a one-week study with eight university students using a functional technology probe that generated daily stress-management messages based on participants' calendar events. Through semi-structured interviews and thematic analysis, we found that participants valued interventions that prioritized stressful events and adopted a concise, but colloquial tone. These findings reveal key design implications for LLM-based stress-management tools, including the need for structured questioning and tone calibration to foster relevance and trust.",10.48550/arXiv.2505.23997,['JournalArticle'],Computer Science; Psychology
Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities,2025,"Kieran Elrod, Katherine A. Flanigan, Mario Berg'es","Designing socially active streets has long been a goal of urban planning, yet existing quantitative research largely measures pedestrian volume rather than the quality of social interactions. We hypothesize that street view imagery -- an inexpensive data source with global coverage -- contains latent social information that can be extracted and interpreted through established social science theory. As a proof of concept, we analyzed 2,998 street view images from 15 cities using a multimodal large language model guided by Mehta's taxonomy of passive, fleeting, and enduring sociability -- one illustrative example of a theory grounded in urban design that could be substituted or complemented by other sociological frameworks. We then used linear regression models, controlling for factors like weather, time of day, and pedestrian counts, to test whether the inferred sociability measures correlate with city-level place attachment scores from the World Values Survey and with environmental predictors (e.g., green, sky, and water view indices) derived from individual street view images. Results aligned with long-standing urban planning theory: the sky view index was associated with all three sociability types, the green view index predicted enduring sociability, and place attachment was positively associated with fleeting sociability. These results provide preliminary evidence that street view images can be used to infer relationships between specific types of social interactions and built environment variables. Further research could establish street view imagery as a scalable, privacy-preserving tool for studying urban sociability, enabling cross-cultural theory testing and evidence-based design of socially vibrant cities.",10.48550/arXiv.2508.06342,"['JournalArticle', 'Review']",Computer Science; Sociology
Understanding PII Leakage in Large Language Models: A Systematic Survey,2024,"Shuai Cheng, Zhao Li, Shu Meng, Mengxia Ren, Haitao Xu, Shuai Hao, Chuan Yue, Fan Zhang","Large Language Models (LLMs) have demonstrated exceptional success across a variety of tasks, particularly in natural language processing, leading to their growing integration into numerous facets of daily life. However, this widespread deployment has raised substantial privacy concerns, especially regarding personally identifiable information (PII), which can be directly associated with specific individuals. The leakage of such information presents significant real-world privacy threats. In this paper, we conduct a systematic investigation into existing research on PII leakage in LLMs, encompassing commonly utilized PII datasets, evaluation metrics, and current studies on both PII leakage attacks and defensive strategies. Finally, we identify unresolved challenges in the current research landscape and suggest future research directions.",10.24963/ijcai.2025/1156,"['JournalArticle', 'Conference', 'Review']",Computer Science
Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment,2025,"Priyanka Dey, Yugal Khanter, Aayush Bothra, Jieyu Zhao, Emilio Ferrara","As LLMs become central to interactive applications, ranging from tutoring to mental health, the ability to express personality in culturally appropriate ways is increasingly important. While recent works have explored personality evaluation of LLMs, they largely overlook the interplay between culture and personality. To address this, we introduce CulturalPersonas, the first large-scale benchmark with human validation for evaluating LLMs'personality expression in culturally grounded, behaviorally rich contexts. Our dataset spans 3,000 scenario-based questions across six diverse countries, designed to elicit personality through everyday scenarios rooted in local values. We evaluate three LLMs, using both multiple-choice and open-ended response formats. Our results show that CulturalPersonas improves alignment with country-specific human personality distributions (over a 20% reduction in Wasserstein distance across models and countries) and elicits more expressive, culturally coherent outputs compared to existing benchmarks. CulturalPersonas surfaces meaningful modulated trait outputs in response to culturally grounded prompts, offering new directions for aligning LLMs to global norms of behavior. By bridging personality expression and cultural nuance, we envision that CulturalPersonas will pave the way for more socially intelligent and globally adaptive LLMs.",10.48550/arXiv.2506.05670,['JournalArticle'],Computer Science; Psychology
Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools,2025,"Panagiotis Lymperopoulos, Vasanth Sarathy","Modern Large Language Models (LLMs) often require external tools, such as machine learning classifiers or knowledge retrieval systems, to provide accurate answers in domains where their pre-trained knowledge is insufficient. This integration of LLMs with external tools expands their utility but also introduces a critical challenge: determining the trustworthiness of responses generated by the combined system. In high-stakes applications, such as medical decision-making, it is essential to assess the uncertainty of both the LLM's generated text and the tool's output to ensure the reliability of the final response. However, existing uncertainty quantification methods do not account for the tool-calling scenario, where both the LLM and external tool contribute to the overall system's uncertainty. In this work, we present a novel framework for modeling tool-calling LLMs that quantifies uncertainty by jointly considering the predictive uncertainty of the LLM and the external tool. We extend previous methods for uncertainty quantification over token sequences to this setting and propose efficient approximations that make uncertainty computation practical for real-world applications. We evaluate our framework on two new synthetic QA datasets, derived from well-known machine learning datasets, which require tool-calling for accurate answers. Additionally, we apply our method to retrieval-augmented generation (RAG) systems and conduct a proof-of-concept experiment demonstrating the effectiveness of our uncertainty metrics in scenarios where external information retrieval is needed. Our results show that the framework is effective in enhancing trust in LLM-based systems, especially in cases where the LLM's internal knowledge is insufficient and external tools are required.",10.48550/arXiv.2505.16113,"['JournalArticle', 'Conference']",Computer Science
A Study of Internet Users Heterogeneity Based on Large Language Models,2024,"Lihui Song, Haobo Zhang, Nan Jia","The emergence of big events is frequently followed by the existence of online public opinion, and netizens emotional comments heighten the hazards associated with the process of online public opinion transmission, resulting in social instability. Netizens emotionality is inextricably linked to their personality traits, and it is necessary to analyze their personalities in order to determine the corresponding public opinion countermeasures of government departments; thus, this paper conducts a study on netizen heterogeneity. A Graph Neural Network (GNN) personality prediction model is built on the Big Five personality model and the BERT model. The BERT model encodes the comment data and converts it into a graph neural network model; the dual attention mechanism extracts the relationship between the user and his/her diction, diction, and psychological keywords; the graph convolutional neural network extracts the features; and finally, the personality scores of the five dimensions of the Big Five personality model of Internet users are calculated. The study demonstrates that the graph neural network Internet user personality prediction model based on the BERT model has some advantages in the personality prediction task and can give data support for future Internet public opinion analysis.",10.1109/IIoTBDSC64371.2024.00026,['Conference'],Computer Science; Political Science
Towards Automated Classification of Adult Attachment Interviews in German Language Using the BERT Language Model,2025,"Teodor Stoev, Eva Flemming, Bernhard Strauss, Katja Petrowski, Carsten Spitzer, Kristina Yordanova","Attachment theory, pioneered by John Bowlby, has become a prominent psychological framework that aids in our comprehension of human behavior within close relationships. The Adult Attachment Interview (AAI), a semi-structured interview, provides a standardized method for assessing adult attachment styles, offering insights into an individuals attachment patterns, emotional regulation, and relational experiences. However, manual AAI classification is a labour-intensive and highly specialized task. Thus, automating this process can optimize the classification of individual attachment patterns and their psychological and relational implications, thereby increasing efficiency and accuracy in assessing attachment styles. In this work, we investigate the application of a BERT large language model and linguistic features for the automated classification of transcribed Adult Attachment Interviews conducted in German into the three categories: secure, dismissing, and preoccupied. The findings of our study indicate that using BERT embeddings alone can yield results comparable to, and in some cases better than, those achieved with traditional linguistic features. However, this effect should be interpreted with caution, as it does not hold consistently across all settings. Another key conclusion of our study is that augmenting the original AAI dataset with artificially generated interviews produced by state-of-the-art large language models generally improves predictive performance across a range of classification models.",10.1109/ACCESS.2025.3604573,['JournalArticle'],Computer Science; Linguistics; Psychology
System Approach to the Combined Use of Large Language Models and Classical Models in Foresight Tasks,2024,"V.V. Savastiyanov, M.V. Stoliar","Background. Large Language Models (LLMs) and their associated agents have spread wide technology and represent a significant advancement in recent times. These state-of-the-art models expose valuable potential, but they are not devoid of restrictions, inefficiencies, and limits. This article investigates the exploration of these constraints within specific domain areas and prediction problems as examples. 
Objective. The article highlights features offered by GPT-based models and compares the conclusions with classical methods of textual data analysis in classification tasks using the prediction methodology as an example. The purpose of the study is to develop a system approach to the combined use of traditional machine learning approaches as a practical alternative to LLMs in foresight tasks using the example of STEEP analysis, which provides an opportunity to obtain valuable information from textual data. 
Methods. The study is structured into four segments, each addressing distinct parts: Data Mining, text pre-processing using LLMs, text pre-processing utilizing Natural Language Processing (NLP) methods, and comparative analysis of results. Data Mining includes data collection and data pre-processing stages for train and test observations. For the utilization of LLMs, chains of thought techniques and prompt engineering were used. 
Results. Throughout this study, it was acknowledged that the LLMs can be used in combination with classical machine learning methodologies for domain-specific areas in STEEP analysis under Foresight tasks. The outcome revealed a model that was developed significantly faster and with less complexity compared to LLMs such as GPT and Mistral. Increasing the number of models employed leads to more stable results. 
Conclusions. The main result of the proceeding is that the patterns that reveal LLMs under certain settings can also be identified by classical models. Moreover, augmenting the deployment of LLMs during the data preparation stages contributes to heightened stability in outcomes. Using classical models combined with LLMs speeds up response times during inference and reduces operating costs for running models.",10.20535/kpisn.2024.1-4.315079,['JournalArticle'],Computer Science
"Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent",2025,"Xiaoyu Zhan, Xinyu Fu, Hao Sun, Yuanqi Li, Jie Guo, Yanwen Guo","The rapid advancement of large language models (LLMs) has enabled role-playing language agents to demonstrate significant potential in various applications. However, relying solely on prompts and contextual inputs often proves insufficient for achieving deep immersion in specific roles, particularly well-known fictional or public figures. On the other hand, fine-tuning-based approaches face limitations due to the challenges associated with data collection and the computational resources required for training, thereby restricting their broader applicability. To address these issues, we propose Test-Time-Matching (TTM), a training-free role-playing framework through test-time scaling and context engineering. TTM uses LLM agents to automatically decouple a character's features into personality, memory, and linguistic style. Our framework involves a structured, three-stage generation pipeline that utilizes these features for controlled role-playing. It achieves high-fidelity role-playing performance, also enables seamless combinations across diverse linguistic styles and even variations in personality and memory. We evaluate our framework through human assessment, and the results demonstrate that our method achieves the outstanding performance in generating expressive and stylistically consistent character dialogues.",10.48550/arXiv.2507.16799,['JournalArticle'],Computer Science; Linguistics
Reverse Engineering Human Preferences with Reinforcement Learning,2025,"Lisa Alazraki, Yi-Chern Tan, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo","The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework--known as LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited post hoc to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning--an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.",10.48550/arXiv.2505.15795,['JournalArticle'],Computer Science
Survey on the Role of Mechanistic Interpretability in Generative AI,2025,Leonardo Ranaldi,"The rapid advancement of artificial intelligence (AI) and machine learning has revolutionised how systems process information, make decisions, and adapt to dynamic environments. AI-driven approaches have significantly enhanced efficiency and problem-solving capabilities across various domains, from automated decision-making to knowledge representation and predictive modelling. These developments have led to the emergence of increasingly sophisticated models capable of learning patterns, reasoning over complex data structures, and generalising across tasks. As AI systems become more deeply integrated into networked infrastructures and the Internet of Things (IoT), their ability to process and interpret data in real-time is essential for optimising intelligent communication networks, distributed decision making, and autonomous IoT systems. However, despite these achievements, the internal mechanisms that drive LLMs reasoning and generalisation capabilities remain largely unexplored. This lack of transparency, compounded by challenges such as hallucinations, adversarial perturbations, and misaligned human expectations, raises concerns about their safe and beneficial deployment. Understanding the underlying principles governing AI models is crucial for their integration into intelligent network systems, automated decision-making processes, and secure digital infrastructures. This paper provides a comprehensive analysis of explainability approaches aimed at uncovering the fundamental mechanisms of LLMs. We investigate the strategic components contributing to their generalisation abilities, focusing on methods to quantify acquired knowledge and assess its representation within model parameters. Specifically, we examine mechanistic interpretability, probing techniques, and representation engineering as tools to decipher how knowledge is structured, encoded, and retrieved in AI systems. Furthermore, by adopting a mechanistic perspective, we analyse emergent phenomena within training dynamics, particularly memorisation and generalisation, which also play a crucial role in broader AI-driven systems, including adaptive network intelligence, edge computing, and real-time decision-making architectures. Understanding these principles is crucial for bridging the gap between black-box AI models and practical, explainable AI applications, thereby ensuring trust, robustness, and efficiency in language-based and general AI systems.",10.3390/bdcc9080193,"['JournalArticle', 'Review']",Computer Science
IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization,2025,"Yuzhuo Bai, Shitong Duan, Muhua Huang, Jing Yao, Zhenghao Liu, Peng Zhang, Tun Lu, Xiaoyuan Yi, Maosong Sun, Xing Xie","Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs'trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs'behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs'stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.",10.48550/arXiv.2508.08719,['JournalArticle'],Computer Science
Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues,2025,"Myke C. Cohen, Zhe Su, Hsien-Te Kao, Daniel Nguyen, Spencer Lynch, Maarten Sap, Svitlana Volkova","This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents'empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.",10.48550/arXiv.2506.15928,['JournalArticle'],Computer Science; Psychology
Simulating Policy Discussions with Digital Footprints and Large Language Models,2025,"Weiyao Meng, John Harvey, C. Carter, Georgiana Nica-Avram, James Goulding, Paul Frobisher, Mina Forrest, N/A","Introduction & BackgroundParliaments play a significant role in democratic decision-making, drawing on evidence from a wide range of stakeholders. This evidence data, published on official government websites, is a valuable archive of digital traces reflecting the policymaking process. Among the key data sources is Hansard, the UK Parliaments official verbatim record which captures the full transcripts of parliamentary debates and discussions, revealing diverse perspectives, stances and interactions shaped by participants political and social contexts. While datasets like Hansard offer rich insights into the policymaking process, their scale and varied dialogic structure present challenges for traditional analysis methods. 
Recent studies indicate large language models (LLMs) can exhibit intelligent and collaborative decision-making behaviours in social simulations. This makes them a useful tool for simulating complex discussions and analysing group dynamics. 
Objectives & ApproachThis study explores the potential of LLMs to simulate UK parliamentary debates, with a focus on speaker roles and stance-taking. The immediate objective is to assess whether LLMs can mimic the structure and dynamics of real debates. Longer-term, this work aims to lay the foundation for a more comprehensive deliberation sandbox which provides an experimental environment that supports multi-stakeholder negotiation and collaborative problem-solving in a low-risk setting. Such a tool could enhance transparency and support more inclusive, evidence-informed policymaking. 
We evaluate model performance by comparing LLM-generated debates with real parliamentary discussions. The study tests general-purpose LLMs (including Gemini 2.5 and ChatGPT-o3) and GovernmentGPT (an open-source LLM fine-tuned on Hansard). All models were prompted on the same topic (i.e., ultra-processed foods) to enable comparison. Each generated output includes a speakers party affiliation and speech content. 
Relevance to Digital FootprintsThe Hansard data in this study qualify as digital footprints, capturing the digital traces of stakeholder engagement in shaping public policy. By analysing and simulating these footprints, this study shows how such data can be used to model complex social interactions and support public understanding of policy discussion. 
ResultsTwo key patterns emerge from analysis. First, the simulated debates approximate the party composition of real parliamentary debates, i.e., the proportion of utterances by party broadly aligns with actual distribution. Second, many statements exhibit neutral or unclear stances, highlighting challenges in generating clear argumentative positions. 
Conclusions & ImplicationsThese initial results suggest that LLMs can reproduce key elements of real parliamentary debates, offering a promising step toward simulating more complex, multi-actor policy dialogues. Future work will seek to connect these outputs to public understanding more directly by introducing new stakeholder voices, simulating responses to citizen concerns, and identifying opportunities for consensus or clarification in contested policy areas such as sustainable food systems, public health, and wellbeing.",10.23889/ijpds.v10i5.3328,['JournalArticle'],Computer Science; Political Science
"AI-powered travel planning: exploring anthropomorphism, innovativeness and privacy concerns",2025,"Rohit Chauhan, Himanshu Jishtu","
 
 This study investigates how AI anthropomorphism, consumer innovativeness and data privacy concerns influence user attitudes and behavioral intentions toward AI-powered travel planning tools, framed within the theory of planned behavior (TPB). It positions AI anthropomorphism as a positive subjective norm, data privacy concerns as a negative subjective norm and consumer innovativeness as a proxy for perceived behavioral control.
 
 
 
 A structured survey was administered to 476 users of AI-powered large language models (LLMs) for travel-related tasks. Structural equation modeling (SEM) was used to examine the direct and mediating relationships among the constructs.
 
 
 
 AI anthropomorphism significantly enhances consumer innovativeness, indirectly promoting favorable attitudes and adoption intentions. However, it does not directly influence attitudes or intentions without the mediating effects of innovativeness and attitudes. Data privacy concerns negatively affect user attitudes but do not directly impede behavioral intentions, operating indirectly through cognitive evaluations.
 
 
 
 Theoretically, this study refines TPB by showing that AI-related social cues and risk factors operate through indirect pathways. Practically, the findings inform the design of AI travel tools, suggesting features like adaptive AI personas, explainable suggestions and co-creative itinerary tools to engage innovative users and address privacy concerns.
 
 
 
 This study extends TPB by integrating AI-specific constructs and psychological factors in tourism AI research. It offers a theory-driven and psychologically grounded model of AI adoption in tourism.
",10.1108/jhti-02-2025-0229,"['JournalArticle', 'Review']",Computer Science; Psychology
"Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots",2025,"Zahra Ashktorab, Alessandra Buccella, Jason DCruz, Zoe Fowler, Andrew Gill, Kei Yan Leung, P. D. Magnus, John Richards, Kush R. Varshney","As chatbots driven by large language models (LLMs) are increasingly deployed in everyday contexts, their ability to recover from errors through effective apologies is critical to maintaining user trust and satisfaction. In a preregistered study with Prolific workers (N=162), we examine user preferences for three types of apologies (rote, explanatory, and empathic) issued in response to three categories of common LLM mistakes (bias, unfounded fabrication, and factual errors). We designed a pairwise experiment in which participants evaluated chatbot responses consisting of an initial error, a subsequent apology, and a resolution. Explanatory apologies were generally preferred, but this varied by context and user. In the bias scenario, empathic apologies were favored for acknowledging emotional impact, while hallucinations, though seen as serious, elicited no clear preference, reflecting user uncertainty. Our findings show the complexity of effective apology in AI systems. We discuss key insights such as personalization and calibration that future systems must navigate to meaningfully repair trust.",10.48550/arXiv.2507.02745,['JournalArticle'],Computer Science; Linguistics
AI INTERVIEWER,2024,,"The AI Interviewer Chatbot is a comprehensive platform designed to assist users in preparing for job interviews through simulated interview sessions. Powered by Mistral-7b- Instruct and Sentence Transformers, it offers a seamless conversational experience, providing users with interview questions and feedback. Langchain orchestrates the backend processing and conversational flow, while Streamlit drives the user interface, ensuring an intuitive interaction. Operating efficiently on a CPU machine,the chatbot offers convenience, personalized learning, and confidence-building opportunities for users. With features including interview practice, feedback provision, customization options, and performance tracking, the AI Interviewer Chatbot serves as a valuable resource for job seekers looking to enhance their interview skills and improve their employability. Keywords Mistral-7b-Instruct (M7I)",10.55041/ijsrem34616,['JournalArticle'],Computer Science
Constructing Hybrid-Driven Agents for MOBA through Player Modeling: Integrating Behavioral Analysis and Narrative Persona in Human-Centered AI,2025,"Chenxi Yan, Yuntian Zhang","To address the challenges of rigid behavioral patterns, lack of semantic consistency, and insufficient emotional interaction in current MOBA game AI agents, this study proposes a Persona-Driven Game AI Agents (PDGAIA) framework based on the Human-Centered AI (HCAI). The framework integrates Dual-Channel Player Profiling, a Narrative Consistency Engine, and Hierarchical Reinforcement Learning (HRL) to enhance the personalization, immersion, and tactical adaptability of game AI. Using Tencents MOBA game Honor of Kings as a case study, player personas are constructed from large-scale match logs and survey data. By combining objective behavioral indicators and subjective psychological modeling, core player personality dimensions and their tactical preferences are identified. Based on narrative strategies, the player-modeled character prototypes are transformed into AI agents with consistent personas, leveraging Hunyuan LLMs for multimodal expression. An HRL framework is then employed to decouple tactical decision-making from action execution, enabling the development of stylized AI combat systems. This framework successfully ensures a coherent persona expression across combat behavior, voice tone, and tactical communication. Experimental results demonstrate that the dual-path approach, which combines data-driven modeling and narrative persona construction, significantly improves anthropomorphism and social presence, delivering a more immersive and intelligent gaming experience. The proposed framework provides a reusable HCI methodology for constructing Game AI agents.",10.54941/ahfe1006246,['Review'],Computer Science
LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing,2025,"Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He","Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.",10.48550/arXiv.2508.06388,['JournalArticle'],Computer Science; Linguistics
Mari: Automating Responses to Thai Legal Debt Queries Using Generative AI and Retrieval-Augmented Generation,2025,"Thitipa Nethassanai, Nichakon Seehabong, Saranrat Roteaim, P. Mongkolnam, B. Watanapa",,10.1109/ECTI-CON64996.2025.11101123,['Conference'],Computer Science; Law
Hostility Detection in UK Politics: A Dataset on Online Abuse Targeting MPs,2024,"Mugdha Pandya, Mali Jin, Kalina Bontcheva, Diana Maynard","Numerous politicians use social media platforms, particularly X, to engage with their constituents. This interaction allows constituents to pose questions and offer feedback but also exposes politicians to a barrage of hostile responses, especially given the anonymity afforded by social media. They are typically targeted in relation to their governmental role, but the comments also tend to attack their personal identity. This can discredit politicians and reduce public trust in the government. It can also incite anger and disrespect, leading to offline harm and violence. While numerous models exist for detecting hostility in general, they lack the specificity required for political contexts. Furthermore, addressing hostility towards politicians demands tailored approaches due to the distinct language and issues inherent to each country (e.g., Brexit for the UK). To bridge this gap, we construct a dataset of 3,320 English tweets spanning a two-year period manually annotated for hostility towards UK MPs. Our dataset also captures the targeted identity characteristics (race, gender, religion, none) in hostile tweets. We perform linguistic and topical analyses to delve into the unique content of the UK political data. Finally, we evaluate the performance of pre-trained language models and large language models on binary hostility detection and multi-class targeted identity type classification tasks. Our study offers valuable data and insights for future research on the prevalence and nature of politics-related hostility specific to the UK.",10.48550/arXiv.2412.04046,['JournalArticle'],Computer Science; Political Science
PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models,2025,"Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei","The rapid development of large language models (LLMs) is redefining the landscape of human-computer interaction, and their integration into various user-service applications is becoming increasingly prevalent. However, transmitting user data to cloud-based LLMs presents significant risks of data breaches and unauthorized access to personal identification information. In this paper, we propose a privacy preservation pipeline for protecting privacy and sensitive information during interactions between users and LLMs in practical LLM usage scenarios. We construct SensitiveQA, the first privacy open-ended question-answering dataset. It comprises 57k interactions in Chinese and English, encompassing a diverse range of user-sensitive information within the conversations. Our proposed solution employs a multi-stage strategy aimed at preemptively securing user information while simultaneously preserving the response quality of cloud-based LLMs. Experimental validation underscores our method's efficacy in balancing privacy protection with maintaining robust interaction quality. The code and dataset are available at https://github.com/ligw1998/PRIV-QA.",10.48550/arXiv.2502.13564,['JournalArticle'],Computer Science
"Can AI estimate product impressions? Comparison of consumers, designers, and AI models ratings of car wheels",2025,"Takahiro Yamaguchi, Hisao Ichikawa, Hiroyuki Sakai","Estimating consumer impressions of a products appearance is essential. However, this is not easy because of the variety in consumers tastes and differences in how consumers and designers experience design. Multimodal foundation models trained on datasets from the internet could be applicable for the estimation; however, it remains unclear if the models tastes are similar to those of consumers or experts like designers. Therefore, we conducted surveys in which consumers and designers rated the appearance of car wheels. In addition, a foundation model estimated the visual impression of the wheels. The models ratings were more similar to those provided by designers than consumers. Therefore, the models could have tastes similar to those of experts because the datasets could contain advertisements and reviews written by experts or product owners who have opinions on product appearance.",10.1017/pds.2025.10098,"['JournalArticle', 'Review']",Computer Science; Psychology
FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph,2025,"Zhangding Liu, Neda Mohammadi, John E. Taylor","Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.",10.48550/arXiv.2509.04772,['JournalArticle'],Computer Science; Engineering; Environmental Science
Show the Way: Accelerating General Aviation Accident Investigations through LLMs and HFACS,2024,"Qingli Liu, Yuqi Yan, Fan Li, Shanshan Feng","General Aviation (GA), with the highest accident and fatality rates in civil aviation, undergoes lengthy accident investigations that include site analysis, witness interviews, cause identification, and detailed reporting. These expert-driven processes, often extending for months or years, not only require extensive manpower but also delay vital accident prevention initiatives in GA. The advent of large language models (LLMs), with groundbreaking capabilities in understanding and generating complex text, offers a potential solution to these challenges. This study aims to conduct a General Aviation Accident Cause Automatic Prediction System (GA-ACAPS), which leverages witness narratives (established early in the investigation) through LLMs. The research utilizes 2250 GA accident reports from the National Transportation Safety Board (NTSB), employing the Human Factors Analysis and Classification System (HFACS) for structured accident causation predictions. Three preliminary experiments were conducted to compare the prediction performance of three different prompting methods before the formal experiment. The results from the preliminary experiments underscore that integrating witness narratives with basic accident information significantly boosts the performance of GA-ACAPS. This optimized prompt was thus implemented in the formal study. The formal experiment's findings demonstrate that GA-ACAPS is proficient in predicting unsafe acts and specific preconditions of unsafe acts like the physical environment and personal readiness. This study endorses the potential of GA-ACAPS to serve as a dependable tool for investigators, aiming to narrow down probable causes of accidents and thereby increase the efficiency of investigations. Moreover, the application of LLMs in GA accident analysis heralds a new era of innovative approaches and essential insights, contributing to the advancement of aviation safety.",10.54941/ahfe1005197,nan,Computer Science; Engineering
Explicit and Implicit Knowledge in Large-Scale Linguistic Data and Digital Footprints from Social Networks,2025,M.A. Pil'gun,"This study explores explicit and implicit knowledge in large-scale linguistic data and digital footprints from social networks. This research aims to develop and test algorithms for analyzing both explicit and implicit information in user-generated content and digital interactions. A dataset of social media discussions on avian influenza in Moscow (RF) was collected and analyzed (tokens: 1,316,387; engagement: 108,430; audience: 39,454,014), with data collection conducted from 1 March 2023, 00:00 to 31 May 2023, 23:59. This study employs Brand Analytics, TextAnalyst 2.32, ChatGPT o1, o1-mini, AutoMap, and Tableau as analytical tools. The findings highlight the advantages and limitations of explicit and implicit information analysis for social media data interpretation. Explicit knowledge analysis is more predictable and suitable for tasks requiring quantitative assessments or classification of explicit data, while implicit knowledge analysis complements it by enabling a deeper understanding of subtle emotional and contextual nuances, particularly relevant for public opinion research, social well-being assessment, and predictive analytics. While explicit knowledge analysis provides structured insights, it may overlook hidden biases, whereas implicit knowledge analysis reveals underlying issues but requires complex interpretation. The research results emphasize the importance of integrating various scientific paradigms and artificial intelligence technologies, particularly large language models (LLMs), in the analysis of social networks.",10.3390/bdcc9040075,['JournalArticle'],Computer Science; Environmental Science; Linguistics
Exploring the Socio-Technical Imaginary of Artificial General Intelligence in The Bard Large Language Model: A Narrative Analysis on Perspectives and Dialectics,2024,Ljubia Boji,"The 2022 release of ChatGPT sparked widespread discussions on Artificial General Intelligence (AGI). Through a detailed examination of an interview with Bard, a large language model, the study uncovers narratives of optimism and pessimism regarding AGI's future implications. It found a higher leaning towards optimism about AGI's potential effects, extending from education, arts, and relationships, to economy and space exploration. Conversely, pessimistic views pointed out potential downsides, such as unemployment, political instability, and media manipulation. The study also identified four primary AGI themes - the relationship between humans and AGI, AGI acquiring a physical form, AGI simulating a universe, and the responsible utilization of AGI. These insights aid in understanding the complex socio-technical imaginary surrounding AGI. The study has its limitations as it is based solely on the responses provided by Bard during its test phase. Additional research may reveal changes in AGI discourse representation as the model evolves.",10.21301/eap.v19i2.14,['JournalArticle'],Computer Science; Linguistics; Sociology
Argumentatively Coherent Judgmental Forecasting,2025,"Deniz Gorur, Antonio Rago, Francesca Toni","Judgmental forecasting employs human opinions to make predictions about future events, rather than exclusively historical data as in quantitative forecasting. When these opinions form an argumentative structure around forecasts, it is useful to study the properties of the forecasts from an argumentative perspective. In this paper, we advocate and formally define a property of argumentative coherence, which, in essence, requires that a forecaster's reasoning is coherent with their forecast. We then conduct three evaluations with our notion of coherence. First, we assess the impact of enforcing coherence on human forecasters as well as on Large Language Model (LLM)-based forecasters, given that they have recently shown to be competitive with human forecasters. In both cases, we show that filtering out incoherent predictions improves forecasting accuracy consistently, supporting the practical value of coherence in both human and LLM-based forecasting. Then, via crowd-sourced user experiments, we show that, despite its apparent intuitiveness and usefulness, users do not generally align with this coherence property. This points to the need to integrate, within argumentation-based judgmental forecasting, mechanisms to filter out incoherent opinions before obtaining group forecasting predictions.",10.48550/arXiv.2507.23163,['JournalArticle'],Computer Science
Beat ChatGPT at Fortune-TellingAn Attempt to Optimize a Large Language Model,2025,"Steven Cheng, Mingyang Han, Zonghong Lu, Weibo Xu, Jingyan Yu","In the context of the growing popularity of fortune-telling in modern Chinese society, this paper attempts to construct and optimize a language model specifically for fortune-telling and compares its performance with mainstream models such as ChatGPT and Llama. Due to the antiquity and cultural particularity of Chinese fortune-telling systems such as Zhou Yi and Eight Characters, they face unique challenges. We generated a customized Chinese fortune-telling data set through ChatGPT, crawled the data on multiple websites, continuously optimized the instructions to build specialized GPTs, and improved its output by continuously optimizing the question design and design. We then evaluated and compared the model's responses in the fields of career, finance, and health, and the results showed that the optimized model outperformed ChatGPT in terms of accuracy and depth of prediction. The paper also highlights the importance of preserving Chinese cultural traditions through AI and explores the user-friendliness of GPT-based solutions when training custom models.",10.54254/2755-2721/2025.21198,['JournalArticle'],Computer Science; Linguistics
From Embeddings to Explainability: A Tutorial on Large-Language-Model-Based Text Analysis for Behavioral Scientists,2025,"Rudolf Debelak, Timo K. Koch, Matthias Aenmacher, Clemens Stachl","Large language models (LLMs) are transforming research in psychology and the behavioral sciences by enabling advanced text analysis at scale. Their applications range from the analysis of social media posts to infer psychological traits to the automated scoring of open-ended survey responses. However, despite their potential, many behavioral scientists struggle to integrate LLMs into their research because of the complexity of text modeling. In this tutorial, we aim to provide an accessible introduction to LLM-based text analysis, focusing on the Transformer architecture. We guide researchers through the process of preparing text data, using pretrained Transformer models to generate text embeddings, fine-tuning models for specific tasks such as text classification, and applying interpretability methods, such as Shapley additive explanations and local interpretable model-agnostic explanations, to explain model predictions. By making these powerful techniques more approachable, we hope to empower behavioral scientists to leverage LLMs in their research, unlocking new opportunities for analyzing and interpreting textual data.",10.1177/25152459251351285,"['JournalArticle', 'Review']",Computer Science; Psychology
Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate,2025,"Ashim Gupta, Maitrey Mehta, Zhichao Xu, Vivek Srikumar","Large language models (LLMs) provide detailed and impressive responses to queries in English. However, are they really consistent at responding to the same query in other languages? The popular way of evaluating for multilingual performance of LLMs requires expensive-to-collect annotated datasets. Further, evaluating for tasks like open-ended generation, where multiple correct answers may exist, is nontrivial. Instead, we propose to evaluate the predictability of model response across different languages. In this work, we propose a framework to evaluate LLM's cross-lingual consistency based on a simple Translate then Evaluate strategy. We instantiate this evaluation framework along two dimensions of consistency: information and empathy. Our results reveal pronounced inconsistencies in popular LLM responses across thirty languages, with severe performance deficits in certain language families and scripts, underscoring critical weaknesses in their multilingual capabilities. These findings necessitate cross-lingual evaluations that are consistent along multiple dimensions. We invite practitioners to use our framework for future multilingual LLM benchmarking.",10.48550/arXiv.2505.21999,['JournalArticle'],Computer Science; Linguistics
Multi-Objective Recommendation in the Era of Generative AI: A Survey of Recent Progress and Future Prospects,2025,"Zihan Hong, Yushi Wu, Zhiting Zhao, Shanshan Feng, Jianghong Ma, Jiao Liu, Tianjun Wei","With the recent progress in generative artificial intelligence (Generative AI), particularly in the development of large language models, recommendation systems are evolving to become more versatile. Unlike traditional techniques, generative AI not only learns patterns and representations from complex data but also enables content generation, data synthesis, and personalized experiences. This generative capability plays a crucial role in the field of recommendation systems, helping to address the issue of data sparsity and improving the overall performance of recommendation systems. Numerous studies on generative AI have already emerged in the field of recommendation systems. Meanwhile, the current requirements for recommendation systems have surpassed the single utility of accuracy, leading to a proliferation of multi-objective research that considers various goals in recommendation systems. However, to the best of our knowledge, there remains a lack of comprehensive studies on multi-objective recommendation systems based on generative AI technologies, leaving a significant gap in the literature. Therefore, we investigate the existing research on multi-objective recommendation systems involving generative AI to bridge this gap. We compile current research on multi-objective recommendation systems based on generative techniques, categorizing them by objectives. Additionally, we summarize relevant evaluation metrics and commonly used datasets, concluding with an analysis of the challenges and future directions in this domain.",10.48550/arXiv.2506.16893,"['JournalArticle', 'Review']",Computer Science
Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey,2025,"Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets, Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma, Zhiyuan Liu, Paul Groth, Marcel Worring",,10.48550/arXiv.2505.18475,"['JournalArticle', 'Review']",Computer Science
From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents,2025,"Seongbo Jang, Minjin Jeon, Jaehoon Lee, Seonghyeon Lee, Dongha Lee, Hwanjo Yu","While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.",10.48550/arXiv.2506.14285,['JournalArticle'],Computer Science
The Roots of International Perceptions: Simulating US Attitude Changes Towards China with LLM Agents,2025,"Nicholas Sukiennik, Yichuan Xu, Yuqing Kan, J. Piao, Yuwei Yan, Chen Gao, Yong Li","The rise of LLMs poses new possibilities in modeling opinion evolution, a long-standing task in simulation, by leveraging advanced reasoning abilities to recreate complex, large-scale human cognitive trends. While most prior works focus on opinion evolution surrounding specific isolated events or the views within a country, ours is the first to model the large-scale attitude evolution of a population representing an entire country towards another -- US citizens'perspectives towards China. To tackle the challenges of this broad scenario, we propose a framework that integrates media data collection, user profile creation, and cognitive architecture for opinion updates to successfully reproduce the real trend of US attitudes towards China over a 20-year period from 2005 to today. We also leverage LLMs'capabilities to introduce debiased media exposure, extracting neutral events from typically subjective news contents, to uncover the roots of polarized opinion formation, as well as a devils advocate agent to help explain the rare reversal from negative to positive attitudes towards China, corresponding with changes in the way Americans obtain information about the country. The simulation results, beyond validating our framework architecture, also reveal the impact of biased framing and selection bias in shaping attitudes. Overall, our work contributes to a new paradigm for LLM-based modeling of cognitive behaviors in a large-scale, long-term, cross-border social context, providing insights into the formation of international biases and offering valuable implications for media consumers to better understand the factors shaping their perspectives, and ultimately contributing to the larger social need for bias reduction and cross-cultural tolerance.",10.48550/arXiv.2508.08837,['JournalArticle'],Computer Science; Political Science
Exploring AI-Driven Affective Avatars for Autistic Adults and Adults with Social Anxiety in Virtual Meetings,2025,"Tiffany D. Do, Martez E. Mott, John Tang, Sasa Junuzovic, Ann Paradiso, Edward Cutrell","Virtual meetings often exclude individuals who rely on text-based communication, such as autistic adults and those with social anxiety. This paper introduces a prototype that converts typed text into emotive avatars using LLM technology, which convey emotional tone through modulated vocal and facial expressions. We reflect on design choices for using LLMs in accessible meetings and discuss insights from our semi-structured interviews with 18 autistic adults and adults with social anxiety. Our qualitative analysis revealed the following key insights: 1) Participants found the avatars helpful in alleviating challenges like masking and exhaustion, with some noting that the avatars enhanced their communication, increasing participation and confidence; 2) While they valued the avatars affective capabilities, including both vocal and facial animations, they were sensitive to inaccuracies in vocal expression; and 3) Participants desired more personalized control over the avatars affect to balance societal expectations with authentic expression.",10.1145/3706599.3719885,"['JournalArticle', 'Book']",Computer Science; Psychology
Optimizing Group Activity Recognition With Actor Relation Graphs and GCN-LSTM Architectures,2025,"M. R. Tejonidhi, C. Aravinda, S. V. A. Kumar, C. K. Madhu, A. M. Vinod","The challenge of understanding and recognizing group activities through human behavior and interactions is a prominent issue in the realm of computer vision research. This area boasts a wide range of applications, including security surveillance, healthcare monitoring, and human-computer interaction systems. However, accurately deciphering complex group dynamics continues to pose significant difficulties. Traditional video-based activity recognition methods often grapple with persistent obstacles such as environmental noise, background clutter, and the inter-class similarities inherent in activity patterns. Recent advancements in this field have largely focused on spatial feature extraction, which alone proves inadequate for thorough group activity analysis. In response to these limitations, we propose a novel deep learning framework that effectively captures both temporal and spatial characteristics of group interactions. Our architecture employs a Convolutional Neural Network (CNN) with Inception-V3 as the foundational model for initial feature extraction. This is complemented by the development of an Actor Relation Graph (ARG) utilizing Zero Normalized Cross Correlation (ZNCC), which adeptly illustrates both appearance-based and positional relationships among participants. By integrating the ARG with a hybrid model that combines Graph Convolutional Network (GCN), Long Short-Term Memory (LSTM), and Attention mechanisms, our approach significantly enhances the extraction of spatial and relational features compared to conventional techniques. Experimental evaluations conducted on two benchmark datasetsthe Collective Activity Dataset (CAD) and the Volleyball datasetdemonstrate the efficacy of our framework. Our proposed model achieves state-of-the-art performance, attaining prediction accuracies of 94.32% and 94.47% on the CAD and Volleyball datasets, respectively, thereby surpassing existing methodologies in the realm of group activity recognition.",10.1109/ACCESS.2025.3552668,['JournalArticle'],Computer Science
An LLM-based Delphi Study to Predict GenAI Evolution,2025,"Francesco Bertolotti, Luca Mari","Predicting the future trajectory of complex and rapidly evolving systems remains a significant challenge, particularly in domains where data is scarce or unreliable. This study introduces a novel approach to qualitative forecasting by leveraging Large Language Models to conduct Delphi studies. The methodology was applied to explore the future evolution of Generative Artificial Intelligence, revealing insights into key factors such as geopolitical tensions, economic disparities, regulatory frameworks, and ethical considerations. The results highlight how LLM-based Delphi studies can facilitate structured scenario analysis, capturing diverse perspectives while mitigating issues such as respondent fatigue. However, limitations emerge in terms of knowledge cutoffs, inherent biases, and sensitivity to initial conditions. While the approach provides an innovative means for structured foresight, this method could be also considered as a novel form of reasoning. further research is needed to refine its ability to manage heterogeneity, improve reliability, and integrate external data sources.",10.48550/arXiv.2502.21092,['JournalArticle'],Computer Science; Political Science
Open-Universe Assistance Games,2025,"Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell","Embodied AI agents must infer and act in an interpretable way on diverse human goals and preferences that are not predefined. To formalize this setting, we introduce Open-Universe Assistance Games (OU-AGs), a framework where the agent must reason over an unbounded and evolving space of possible goals. In this context, we introduce GOOD (GOals from Open-ended Dialogue), a data-efficient, online method that extracts goals in the form of natural language during an interaction with a human, and infers a distribution over natural language goals. GOOD prompts an LLM to simulate users with different complex intents, using its responses to perform probabilistic inference over candidate goals. This approach enables rich goal representations and uncertainty estimation without requiring large offline datasets. We evaluate GOOD in a text-based grocery shopping domain and in a text-operated simulated household robotics environment (AI2Thor), using synthetic user profiles. Our method outperforms a baseline without explicit goal tracking, as confirmed by both LLM-based and human evaluations.",10.48550/arXiv.2508.15119,['JournalArticle'],Computer Science
Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes,2025,"Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner","Examining the alignment of large language models (LLMs) has become increasingly important, e.g., when LLMs fail to operate as intended. This study examines the alignment of LLMs with human values for the domain of politics. Prior research has shown that LLM-generated outputs can include political leanings and mimic the stances of political parties on various issues. However, the extent and conditions under which LLMs deviate from empirical positions are insufficiently examined. To address this gap, we analyze the factors that contribute to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them. Drawing on findings from cognitive science about representativeness heuristics, i.e., situations where humans lean on representative attributes of a target group in a way that leads to exaggerated beliefs, we scrutinize LLM responses through this heuristics' lens. We conduct experiments to determine how LLMs inflate predictions about political parties, which results in stereotyping. We find that while LLMs can mimic certain political parties' positions, they often exaggerate these positions more than human survey respondents do. Also, LLMs tend to overemphasize representativeness more than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggesting a potential vulnerability of LLMs that facilitates political stereotyping. We also test prompt-based mitigation strategies, finding that strategies that can mitigate representative heuristics in humans are also effective in reducing the influence of representativeness on LLM-generated responses.",10.48550/arXiv.2501.14294,"['JournalArticle', 'Review']",Computer Science; Linguistics; Political Science
LLM-PrivCheck : Detecting Privacy Policy Violations in Android Applications with LLMs,2025,"Zexiong Xiang, Shanshan Wang","With the widespread use of Android applications, frequent access to personal user data (such as location information and device identifiers) has become the norm. However, inconsistencies between privacy policy statements and actual app behavior often lead to significant privacy risks. This paper proposes a consistency detection method, LLM-PrivCheck, based on large language models (LLMs), which automatically checks whether the data collection statements in privacy policies align with the actual behavior of applications. LLM-PrivCheck utilizes LLMs for privacy policy parsing and combines taint analysis techniques to track data flows, ultimately achieving consistency detection through ontology mapping. Experimental results show that in tests on 960 commercial applications from Google Play, LLM-PrivCheck detected that 96.25% of the apps had inconsistencies between their privacy policies and actual behavior.",10.54254/2755-2721/2025.23103,['JournalArticle'],Computer Science
Research on the Acceptance Behavior of Accounting Professionals towards Large Language Models and Generative Artificial Intelligence: An Analysis Based on ChatGPT,2024,"Yawen Zhu, Bing Bai, Yangyang Zhang, Qun Su, Hexu Shang, Siying Mao","As generative artificial intelligence (AI), such as ChatGPT, permeates into our lives, its unparalleled application value has been showcased. This study delves into the accounting domain, examining professionals' acceptance behavior towards large language models (LLMs) and generative AI. Six hypotheses were proposed, and an initial technology acceptance model was developed based on the Unified Theory of Acceptance and Use of Technology (UTAUT) framework. Structural Equation Modeling (SEM) was employed to assess the research model, utilizing data from an empirical survey of 451 accounting professionals in China. The findings suggest that acceptance behavior is primarily influenced by perceived risk, personal innovation, community influence, and technological advancements. Notably, the moderating factor does not significantly impact usage intention.",10.1109/ICCSNT62291.2024.10776725,"['Conference', 'Review']",Business; Computer Science
"Speech Foundation Models and Crowdsourcing for Efficient, High-Quality Data Collection",2024,"Beomseok Lee, Marco Gaido, Ioan Calapodescu, Laurent Besacier, Matteo Negri","While crowdsourcing is an established solution for facilitating and scaling the collection of speech data, the involvement of non-experts necessitates protocols to ensure final data quality. To reduce the costs of these essential controls, this paper investigates the use of Speech Foundation Models (SFMs) to automate the validation process, examining for the first time the cost/quality trade-off in data acquisition. Experiments conducted on French, German, and Korean data demonstrate that SFM-based validation has the potential to reduce reliance on human validation, resulting in an estimated cost saving of over 40.0% without degrading final data quality. These findings open new opportunities for more efficient, cost-effective, and scalable speech data acquisition.",10.48550/arXiv.2412.11978,['JournalArticle'],Computer Science; Engineering; Linguistics
Enhancing Virtual Hiring Through AI-Driven Fake LipSync Detection and Career Chatbot,2025,"Tirthesh Patange, Swati Shinde, Pratik Nikat, Chaitanya Nalawade, Prathamesh Pandit",,10.1109/GINOTECH63460.2025.11077047,['Conference'],Computer Science
"Validation and Usability Assessment of The HapticPalm, a Wearable Device for Hand Palm Force Feedback",2024,"A. Villani, M. Dragusanu, Domenico Prattichizzo, M. Malvezzi","This paper presents the results of a series of tests for the assessment of a wearable haptic device for hand palm. This part of the hand is fundamental in several grasping and manipulation tasks, and in tactile exploration tasks, even if it is still less exploited in haptics applications than other parts of the hand. The assessed device has a parallel tendon-based mechanical structure actuated by three motors. The contact surface is modular can be easily changed to simulate the contact with different surface shapes. The aim of this work is to verify that all design specifications are respected according with user opinions in terms of realism, mobility, interchangeability and users acceptance, using different standardized survey protocols.",10.1109/HAPTICS59260.2024.10520842,"['JournalArticle', 'Review']",Computer Science; Engineering
Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning,2025,"Yekun Zhu, Guang Chen, Chengjun Mao","Large Language Models (LLMs) with chains-of-thought have demonstrated strong performance on an increasing range of tasks, particularly those involving complex logical reasoning. However, excessively long chains can lead to overthinking, causing computational waste and slower responses. This raises a question: can LLMs dynamically adjust the length of their reasoning processes based on task complexity? To address this, we propose the Think in Blocks framework, which enables adaptive reasoning-from zero to deep reasoning-by partitioning the reasoning process into a tunable number of blocks. Our main contributions are: (1) Establishing an explicit block-structured paradigm in which the model first predicts an integer reasoning budget-the number of blocks-and then partitions its reasoning accordingly; (2) Training an adaptive model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided Direct Preference Optimization, and Reinforcement Learning-that adjusts its reasoning depth to problem difficulty; (3) Exploiting the explicit block count to dynamically control reasoning depth at inference time, allowing flexible adjustment of chain-of-thought length during deployment.",10.48550/arXiv.2508.15507,['JournalArticle'],Computer Science
Personalized Game Difficulty with Large Language Models: a Preliminary Study,2025,"Xiaoxu Li, Yi Xia, R. Thawonmas","This paper presents a preliminary study on whether state-of-the-art (SOTA) large language models (LLMs), widely applied in various fields, can serve as dynamic difficulty adjustment (DDA) mechanisms in games. Personalized game mechanisms achieved through software and reflected via consumer electronics are critical for improving player experience. However, research on LLMs' usage in gaming has not addressed personalized game mechanisms, specifically in DDA. This paper picks GPT-4o as the SOTA LLM. Using random values to simulate the human player's skill strength, we investigate whether the LLM can provide a reasonable skill strength of the opponent in a turn-based game. Different instructions are used in the prompts to observe GPT-4o's responses, indicating significant potential to act as a DDA mechanism in games. Some prompts led GPT-40 to provide reasonable difficulty responses and follow the flow theory in games. In addition, pivotal limitations of GPT-40 in acting as DDA were exposed, suggesting prompt optimization and LLMs' fine-tuning as a future direction.",10.1109/ICCE63647.2025.10930119,"['JournalArticle', 'Conference']",Computer Science
Research on the roles of multiple social media platforms in the evolution of public events,2025,"Ruilian Han, Lu An, Wei Zhou, Gang Li","PurposeSocial media platforms (SMPs) are pivotal in information dissemination and molding public opinion. Various platforms exhibit differences and characteristics. It is necessary to compare and analyze the roles played by different platforms in the evolution of public events.Design/methodology/approachThis study develops a framework to evaluate the role of SMPs at different stages of public events. To calculate some of these indicators, the GPT-AP-TextRank topic model is constructed. The study further analyzes the correlation between indicators at different stages and SMPs role and compares SMPs different roles among the four stages.FindingsThe results reveal significant disparities in the role of different SMPs during public events. Weibo demonstrates notable performance during the outbreak, spread and recession stages of the event, exhibiting a strong influence on public event evolution. Bilibili, Douban, Zhihu and Baidu Tieba show relatively ordinary roles. In addition, compared to the spread stage, SMPs exhibit a stronger ability to influence event redirection in the initial stage, which is different from the original assumption of the study.Practical implicationsThe findings expose the powerful roles of SMPs in event evolution, providing valuable insights for enhancing public event governance.Originality/valueThis study proposes an evaluation method for SMPs role and introduces a novel GPT-AP-TextRank topic generation model for the indicator calculation.",10.1108/ajim-07-2024-0539,['JournalArticle'],Computer Science; Political Science
Prediksi Curah Hujan Di Kabupaten Bogor Menggunakan Long Short-Term Memory Dan Gemma 2,2025,"Indra Wijaya, Herlawati Herlawati, Rafika Sari","Bogor Regency is an area that often experiences prolonged rainfall, especially during the rainy season. High rainfall causes problems such as floods and landslides. Therefore, accurate rainfall prediction is important for various needs, especially in disaster mitigation. This study aims to implement the Long Short-Term Memory (LSTM) algorithm as a model for prediction of historical rainfall data and use the Large Language Model (LLM) GEMMA 2 to provide interpretation of prediction results and recommendations based on the prediction results. The methods used include data collection from the BMKG online data website totaling 1804 data, data pre-processing, model building, model performance evaluation, and interpretation of results using LLM. The results of this study show that LSTM is able to produce the best performance by showing MSE 201.92 mm, Root Mean Square Error (RMSE) of 14.21 mm. the RMSE value shows an average error of 14.21 mm. In addition, the interpretation provided by LLM GEEMA 2 to help understand the prediction and provide practical recommendations for disaster mitigation due to rainfall.",10.31603/komtika.v9i1.13639,['JournalArticle'],Computer Science; Environmental Science
Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning,2025,"Jeong-seok Oh, Jay-yoon Lee","Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks. We introduce Latent Self-Consistency (LSC), which selects the most semantically consistent response using learnable token embeddings. A lightweight forward generation of summary tokens increases inference time by less than 1% and requires no changes to the model architecture. Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form ones on average, while maintaining negligible computational overhead. These results position LSC as a practical consistency-selection method that works reliably across answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low Expected Calibration Error across both answer formats.",10.48550/arXiv.2508.18395,['JournalArticle'],Computer Science
Evaluating LLM Alignment on Personality Inference from Real-World Interview Data,2025,"Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin","Large Language Models (LLMs) are increasingly deployed in roles requiring nuanced psychological understanding, such as emotional support agents, counselors, and decision-making assistants. However, their ability to interpret human personality traits, a critical aspect of such applications, remains unexplored, particularly in ecologically valid conversational settings. While prior work has simulated LLM""personas""using discrete Big Five labels on social media data, the alignment of LLMs with continuous, ground-truth personality assessments derived from natural interactions is largely unexamined. To address this gap, we introduce a novel benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. Using this dataset, we systematically evaluate LLM performance across three paradigms: (1) zero-shot and chain-of-thought prompting with GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA architectures, and (3) regression using static embeddings from pretrained BERT and OpenAI's text-embedding-3-small. Our results reveal that all Pearson correlations between model predictions and ground-truth personality traits remain below 0.26, highlighting the limited alignment of current LLMs with validated psychological constructs. Chain-of-thought prompting offers minimal gains over zero-shot, suggesting that personality inference relies more on latent semantic representation than explicit reasoning. These findings underscore the challenges of aligning LLMs with complex human attributes and motivate future work on trait-specific prompting, context-aware modeling, and alignment-oriented fine-tuning.",10.48550/arXiv.2509.13244,['JournalArticle'],Computer Science; Psychology
Question Answering for Decisionmaking in Green Building Design: A Multimodal Data Reasoning Method Driven by Large Language Models,2024,"Yihui Li, Xiaoyue Yan, Hao Zhou, Borong Lin","In recent years, the critical role of green buildings in addressing energy consumption and environmental issues has become widely acknowledged. Research indicates that over 40% of potential energy savings can be achieved during the early design stage. Therefore, decision-making in green building design (DGBD), which is based on modeling and performance simulation, is crucial for reducing building energy costs. However, the field of green building encompasses a broad range of specialized knowledge, which involves significant learning costs and results in low decision-making efficiency. Many studies have already applied artificial intelligence (AI) methods to this field. Based on previous research, this study innovatively integrates large language models with DGBD, creating GreenQA, a question answering framework for multimodal data reasoning. Utilizing Retrieval Augmented Generation, Chain of Thought, and Function Call methods, GreenQA enables multimodal question answering, including weather data analysis and visualization, retrieval of green building cases, and knowledge query. Additionally, this study conducted a user survey using the GreenQA web platform. The results showed that 96% of users believed the platform helped improve design efficiency. This study not only effectively supports DGBD but also provides inspiration for AI-assisted design.",10.48550/arXiv.2412.04741,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Sports News and the Artificial-Intelligence-Generated Article: Examining Identity and the Influence of Human Versus Artificial-Intelligence Authorship on Perceptions of Credibility and Online Share Likelihood,2025,"Sean R. Sadri, Jessica L. Payne, Kenon A. Brown, Andrew C. Billings","To better understand factors that influence news credibility and online sharing likelihood of artificial-intelligence (AI)-generated articles, a national survey of U.S. adults was conducted (N=355). Using a 3 (authorship)2 (byline) factorial design, participants were asked to evaluate the credibility of a sports-news article written by a human author or an AI-language model (ChatGPT or Google Gemini), with a byline indicating thatthe article was written by a human author or AI, and assess the likelihood they would share the article on social media. Participant predictors for article credibility, source credibility, information accuracy, and article-share likelihood significantly differed based on demographics, identity characteristics, and cognitive heuristic factors, while authorship did not significantly influence credibility perceptions or share likelihood. Across both human- and AI-authored articles, level of sport fanship significantly influenced share likelihood and perceived information accuracy. Additionally, article credibility and source credibility were significantly influenced by positive attitudes toward AI, which may signal drastic changes in how audiences will interpret and engage with AI-generated news content.",10.1123/ijsc.2024-0208,"['JournalArticle', 'Review']",Computer Science
Navigating Heat Exposure: Simulation of Route Planning Based on Visual Language Model Agents,2025,"Haoran Ma, Kaihan Zhang, Jiannan Cai","Heat exposure significantly influences pedestrian routing behaviors. Existing methods such as agent-based modeling (ABM) and empirical measurements fail to account for individual physiological variations and environmental perception mechanisms under thermal stress. This results in a lack of human-centred, heat-adaptive routing suggestions. To address these limitations, we propose a novel Vision Language Model (VLM)-driven Persona-Perception-Planning-Memory (PPPM) framework that integrating street view imagery and urban network topology to simulate heat-adaptive pedestrian routing. Through structured prompt engineering on Gemini-2.0 model, eight distinct heat-sensitive personas were created to model mobility behaviors during heat exposure, with empirical validation through questionnaire survey. Results demonstrate that simulation outputs effectively capture inter-persona variations, achieving high significant congruence with observed route preferences and highlighting differences in the factors driving agents decisions. Our framework is highly cost-effective, with simulations costing 0.006USD and taking 47.81s per route. This Artificial Intelligence-Generated Content (AIGC) methodology advances urban climate adaptation research by enabling high-resolution simulation of thermal-responsive mobility patterns, providing actionable insights for climate-resilient urban planning.",10.48550/arXiv.2503.12731,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
From Precision to Perception: User-Centred Evaluation of Keyword Extraction Algorithms for Internet-Scale Contextual Advertising,2025,"Jingwen Cai, Sara Leckner, Johanna Bjrklund","Keyword extraction is a foundational task in natural language processing, underpinning countless real-world applications. A salient example is contextual advertising, where keywords help predict the topical congruence between ads and their surrounding media contexts to enhance advertising effectiveness. Recent advances in artificial intelligence, particularly large language models, have improved keyword extraction capabilities but also introduced concerns about computational cost. Moreover, although the end-user experience is of vital importance, human evaluation of keyword extraction performances remains under-explored. This study provides a comparative evaluation of three prevalent keyword extraction algorithms that vary in complexity: TF-IDF, KeyBERT, and Llama 2. To evaluate their effectiveness, a mixed-methods approach is employed, combining quantitative benchmarking with qualitative assessments from 552 participants through three survey-based experiments. Findings indicate a slight user preference for KeyBERT, which offers a favourable balance between performance and computational efficiency compared to the other two algorithms. Despite a strong overall preference for gold-standard keywords, differences between the algorithmic outputs are not statistically significant, highlighting a long-overlooked gap between traditional precision-focused metrics and user-perceived algorithm efficiency. The study highlights the importance of user-centred evaluation methodologies and proposes analytical tools to support their implementation.",10.48550/arXiv.2504.21667,"['JournalArticle', 'Review']",Computer Science
Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data,2025,"Jihong Zhang, Xinya Liang, Anqi Deng, Nicole Bonge, Lin Tan, Ling Zhang, Nicole Zarrett","Mixed methods research integrates quantitative and qualitative data but faces challenges in aligning their distinct structures, particularly in examining measurement characteristics and individual response patterns. Advances in large language models (LLMs) offer promising solutions by generating synthetic survey responses informed by qualitative data. This study investigates whether LLMs, guided by personal interviews, can reliably predict human survey responses, using the Behavioral Regulations in Exercise Questionnaire (BREQ) and interviews from after-school program staff as a case study. Results indicate that LLMs capture overall response patterns but exhibit lower variability than humans. Incorporating interview data improves response diversity for some models (e.g., Claude, GPT), while well-crafted prompts and low-temperature settings enhance alignment between LLM and human responses. Demographic information had less impact than interview content on alignment accuracy. These findings underscore the potential of interview-informed LLMs to bridge qualitative and quantitative methodologies while revealing limitations in response variability, emotional interpretation, and psychometric fidelity. Future research should refine prompt design, explore bias mitigation, and optimize model settings to enhance the validity of LLM-generated survey data in social science research.",10.48550/arXiv.2505.21997,"['JournalArticle', 'Review']",Computer Science; Psychology
GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs,2025,"Haibo Jin, Ruoxi Chen, Peiyan Zhang, Andy Zhou, Yang Zhang, Haohan Wang","As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance. To address this challenge, we introduce GUARD (\textbf{G}uideline \textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks''to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations. We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.",10.48550/arXiv.2508.20325,['JournalArticle'],Computer Science
"Interview Preparation Guide Generation Leveraging GPT-4, ZSL and Hybrid Techniques",2024,"Tharaka Ramu, Nenavath Srinivas Naik","This paper presents a novel approach for automating the creation of personalized interview preparation guides using GPT-4, Zero-Shot Learning (ZSL), and hybrid techniques. Our system dynamically generates tailored interview questions and preparation materials based on specific job descriptions and candidate profiles. By integrating advanced NLP with rule-based methods, it significantly enhances the relevance and quality of the content compared to traditional methods. We analyzed data from 10,000 job descriptions, 10,000 candidate profiles, and 20,000 interview questions to ensure high-quality results. The use of ZSL enables the generation of guides even for new and unfamiliar roles. Our approach achieved a 35% increase in question relevance and a 28% improvement in candidate-job matching accuracy compared to baseline methods. This research also paves the way for enhancing AI tools in HR, supporting personalized learning, performance reviews, and career planning.",10.1109/INSPECT63485.2024.10895997,"['Conference', 'Review']",Computer Science
EyeChat: An Eye-Gaze-Controlled Web-Browsing System for Head-Mounted Augmented Reality Displays,2025,"Mahya Mirbagheri, Tom Chau","Accessing Web content using conventional input methods can be difficult for people with motor impairments. Current solutions often use webcams to enable eye gaze control, which may cause interaction delays and inefficient browsing. EyeChat is an eye gaze-controlled interface for efficient Web content delivery via Microsoft HoloLens 2. The system features a text entry interface optimized for augmented reality and a hybrid chatbot to deliver accurate content. By combining rule-based and large language models, EyeChat provides real-time, contextually relevant responses. Its frequency-based circular text-entry interface centralizes eye movement around head gaze to minimize effort. Predictive features, including current-word, next-word, and phrase suggestions, enhance typing speed. This paper presents an initial technical evaluation of EyeChat with 12 typically developed adults. Non-disabled participants reported that 83.83% of responses met expectations. The system achieved a text-entry rate of 9.88 words per minute and 0.53 keystrokes per character with minimal practice. Eye gaze analysis showed 95% of points fell within a 24 field of view.",10.1109/ACCESS.2025.3606899,['JournalArticle'],Computer Science
Improving French Synthetic Speech Quality via SSML Prosody Control,2025,"Nassima Ouali, Awais Hussain Sani, Ruben Bueno, Jonah Dauvet, Tim Luka Horstmann, ric Moulines","Despite recent advances, synthetic voices often lack expressiveness due to limited prosody control in commercial text-to-speech (TTS) systems. We introduce the first end-to-end pipeline that inserts Speech Synthesis Markup Language (SSML) tags into French text to control pitch, speaking rate, volume, and pause duration. We employ a cascaded architecture with two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions and the other performs regression on prosodic targets, generating commercial TTS-compatible SSML markup. Evaluated on a 14-hour French podcast corpus, our method achieves 99.2% F1 for break placement and reduces mean absolute error on pitch, rate, and volume by 25-40% compared with prompting-only large language models (LLMs) and a BiLSTM baseline. In perceptual evaluation involving 18 participants across over 9 hours of synthesized audio, SSML-enhanced speech generated by our pipeline significantly improves naturalness, with the mean opinion score increasing from 3.20 to 3.87 (p<0.005). Additionally, 15 of 18 listeners preferred our enhanced synthesis. These results demonstrate substantial progress in bridging the expressiveness gap between synthetic and natural French speech. Our code is publicly available at https://github.com/hi-paris/Prosody-Control-French-TTS.",10.48550/arXiv.2508.17494,['JournalArticle'],Computer Science; Linguistics
Stock Market Forecasting: From Traditional Predictive Models to Large Language Models,2025,"Mahmoud Darwish, E. E. Hassanien, Amany H. B. Eissa",,10.1007/s10614-025-11024-w,"['JournalArticle', 'Review']",Business; Computer Science
The Role of Moderators in Transitioning From GenAI Chatbot Customer Experience To Customer Satisfaction in Digital Marketing,2024,"Vannam L E, Tien Hai Phung","This study investigates the role of moderators in transitioning from Generative AI (GenAI) chatbot customer experience to customer satisfaction in digital marketing. As digital marketing continues to grow with technological advancements, GenAI chatbots such as ChatGPT, Copilot, and Gemini have become essential tools for enhancing customer engagement and providing personalized experiences. Despite extensive research on the technical aspects of chatbots, there is limited understanding of how perceived personalization, relevance, and usefulness of GenAI chatbots impact customer satisfaction significantly when moderated by variables like familiarity with technology and organization type. A conceptual model is developed, and data is collected through a survey of 346 consumers who interact with GenAI chatbots. The data is analyzed using moderated regression analysis to test the proposed hypotheses. The findings of this study will enhance the understanding of factors influencing customer experience and provide practical insights for businesses aiming to improve customer satisfaction through effective GenAI chatbot integration. This research contributes to the existing literature on digital marketing and offers actionable recommendations for customizing GenAI chatbots to meet the needs of different organizational types.",10.46609/ijsser.2024.v09i07.028,"['JournalArticle', 'Review']",Business; Computer Science
Coue Application: User Emotional Reasoning Chain for Enhancing Brand Public Opinion Management,2024,"Jingwen Deng, Xintao Ding","This study aims to improve corporate brand public opinion management through user emotional reasoning, focusing on understanding users' psychological states and applying emotion analysis methods. Existing research typically uses end-to-end generative models, fine-tuning small pre-trained language models for emotion analysis. However, these methods lack fine-grained emotional state understanding, resulting in lower sentiment analysis accuracy and limited model interpretability. To address this, the study introduces a novel framework, Co De (Chaln-of-User-Emotlonal), based on large language models. CoDebreaks down emotion analysis into a phased reasoning process with three chains: emotion reasoning, public opinion trend, and response generation. By using background enhancement strategies, the method improves emotional state analysis and sentiment prediction, enabling real-time market sentiment monitoring and reducing brand public opinion crises.",10.1109/ICCWAMTIP64812.2024.10873745,['Conference'],Business; Computer Science
Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning,2025,"Bao Li, Xiaomei Zhang, Miao Xu, Zhaoxin Fan, Xiangyu Zhu, Zhen Lei","Generating 3D human poses from multimodal inputs such as images or text requires models to capture both rich spatial and semantic correspondences. While pose-specific multimodal large language models (MLLMs) have shown promise in this task, they are typically trained with supervised objectives such as SMPL parameter regression or token-level prediction, which struggle to model the inherent ambiguity and achieve task-specific alignment required for accurate 3D pose generation. To address these limitations, we propose Pose-RFT, a reinforcement fine-tuning framework tailored for 3D human pose generation in MLLMs. We formulate the task as a hybrid action reinforcement learning problem that jointly optimizes discrete language prediction and continuous pose generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning algorithm that performs group-wise reward normalization over sampled responses to guide joint optimization of discrete and continuous actions. Pose-RFT further incorporates task-specific reward functions to guide optimization towards spatial alignment in image-to-pose generation and semantic consistency in text-to-pose generation. Extensive experiments on multiple pose generation benchmarks demonstrate that Pose-RFT significantly improves performance over existing pose-specific MLLMs, validating the effectiveness of hybrid action reinforcement fine-tuning for 3D pose generation.",10.48550/arXiv.2508.07804,['JournalArticle'],Computer Science
Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities,2025,"Yunxiang Yan, Tomohiro Sawada, Kartik Goyal","While question-answering~(QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on \emph{cascaded question disclosure} that provides a more accurate estimate of the models'problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models. We further validate our findings by extensive ablation studies.",10.48550/arXiv.2507.23776,['JournalArticle'],Computer Science
"Analysis of the Content of ChatGPT's Memory: Types of Information, Security Implications, and User Perception",2025,"Nazmus Sadat, Nicholas Caporusso, My Doan, Vickey Ghimire, Bijay Dhungana, Jaljala Shrestha Lama","OpenAI's new memory feature enables ChatGPT to provide more personalized and relevant interactions by storing user information from the prompts and using it across conversations. While offering improved responses, the memory feature poses privacy and security challenges. This paper reports a three-fold study investigating ChatGPT's memory feature in more detail. First, we utilized the Knowledge-Attitude-Behavior model and distributed a survey to over 135 users to assess their awareness of ChatGPT's memory functionality, attitudes toward privacy implications, and the behavioral changes prompted by perceived risks. Secondly, memory content from over 55 user accounts was analyzed to evaluate the accuracy, relevance, and privacy of the stored data. Finally, we studied the distribution of the stored data across key categories to obtain insights into what kind of information ChatGPT considers relevant and stores. The findings reveal gaps in user understanding of the memory feature, the need for greater transparency, and the challenges of personalizing LLM agents while safeguarding privacy.",10.1109/MIPRO65660.2025.11131839,['Review'],Computer Science
C-LEIA: An Interactive LLM-based Simulator for Requirements Interviews,2025,"Nelson Lojo, Rafael Gonzlez, Rohan Philip, J. A. Parejo, Amador Durn Toro, Armando Fox, Pablo Fernandez","Requirements elicitation (RE) is a critical skill in software engineering, that was traditionally taught through transcripts of customer interviews. However, this approach does not develop the interviewing skills of the students. We present C-LEIA (Customer-Learning Enabling Intelligent Assistant), a tool leveraging Large Language Models (LLMs) to simulate realistic customer interviews in a chat-based interface. C-LEIA provides a scalable and immersive environment for students to practice formulating questions, conducting interviews, and gathering requirements. We conducted a study with 120 students comparing traditional transcript-based exercises with C-LEIA. Results indicate that C-LEIA offers an engaging, realistic experience that enhances RE interview skills. This demonstration showcases C-LEIAs features and highlights its potential to complement existing RE instruction.",10.1109/REW66121.2025.00012,['Conference'],Computer Science
Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches,2025,Patrick Y. Wu,"After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.",10.48550/arXiv.2506.06540,"['JournalArticle', 'Review']",Computer Science; Political Science
Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review,2024,"Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang","Public Code Review (PCR) is developed in the Software Question Answering (SQA) community, assisting developers in exploring high-quality and efficient review services. Current methods on PCR mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments. However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose K nowledge-guided P rompt learning for P ublic Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; and 2) knowledge and code prefix tuning which introduces knowledge guidance from fine-tuned large language models by soft prompt, and uses program dependence graph to characterize code snippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module. In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge guidance. Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request necessity prediction and by 1.4%-6.9% in the tag recommendation. The code implementation is released at https://github.com/WUT-IDEA/KP-PCR.",10.48550/arXiv.2410.21673,"['JournalArticle', 'Review']",Computer Science
APPLIED LINGUISTICS DRIVEN LARGE LANGUAGE MODEL FOR SARCASM RECOGNITION ON SOCIAL MEDIA CORPORA,2025,"A. M. Alashjaee, Alya Alshammari, M. Alzaidi, Nazir Ahmad Ijaz, Manar M. Almanea, Ahmed S. Salama","Sarcasm is a language phrase that expresses the opposite of what is stated, often used for mocking or offending. It is commonly seen on social media platforms day by day. The opinion analysis process is susceptible to errors due to the potential for sarcasm to alter the statements meaning. As automated social media research tools become more prevalent, the reliability problems of analytics have also increased. According to the prior study, sarcastic reports alone have greatly diminished the automatic Sentiment Analysis (SA) performance in complex systems platforms. Sarcasm detection utilizing Deep Learning (DL) contains training models to identify the nuanced linguistic cues that indicate sarcasm in text. Typically, this process applies large datasets annotated with sarcastic and non-sarcastic samples to teach models to discriminate between them. DL methodologies, namely Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformer methods like BERT or GPT, are widely applied due to their ability to capture intricate patterns in language. This model learns to detect sarcasm by discriminating exaggerated expressions, contextual incongruities, and semantic reversals frequently related to sarcastic remarks. Therefore, this study presents a Fractal Red-Tailed Hawk Algorithm with Hybrid Deep Learning-Driven Sarcasm Detection (RTHHDL-SD) technique on complex systems and social media platforms. The purpose of the RTHHDL-SD technique is to identify and classify the occurrence of sarcasm in social media text. In the RTHHDL-SD approach, data preprocessing is performed in four ways to transform input data into valuable design. Besides, the RTHHDL-SD technique applies the FastText word embedding approach to generate word embeddings. The RTHHDL-SD technique applies a Deep Neural Network (DNN) with bi-directional long short-term memory for sarcasm detection, called the deep BiLSTM model. The RTH method was utilized as the hyperparameter optimizer to enhance the detection performance of the deep BiLSTM model. Moreover, the large language model is used to estimate the outcomes of the social media corpora. The simulation outcomes of the RTHHDL-SD methodology are examined under Twitter and Headlines datasets. The investigational outcomes of the RTHHDL-SD methodology exhibited superior accuracy values of 89.10% and 92.77% with other approaches.",10.1142/s0218348x25400377,['JournalArticle'],Computer Science; Linguistics
SPOC: A Scale for Potential Operation Consequences of UI Interactions,2025,"Florian Eggenkemper, Teresa Rehers, Jana Swerew, Robert Mertens","Many approaches rely on a semantic understanding of screen contents to perform further actions on this information. This is relevant for test automation, robotic process automation, or context based assistive tasks (digital agents). Most of these approaches just analyze basic information about user interface elements but fall short in predicting the consequences of single user interface actions. Possible consequences of UI actions include (permanent) data loss, effects on the reputation of people (e.g. posting on social media), or other irreversible consequences. This makes it necessary to identify and analyze the potential consequences of operation on user interfaces to prevent automatic systems from making possibly fatal mistakes with a high impact on the human user. This paper presents a Scale for Potential Operation Consequences (SPOC) and demonstrates an automatic rating system based on Large Language Models (LLMs), that can be performed locally or in cloud implementations. The LLM-based ratings were tested against a human control group based on a questionnaire (n=35), and proved to perform as well as their human counterparts, making SPOC a robust and automation-friendly rating system for user interface element actions.",10.1109/AIxMM62960.2025.00011,['Conference'],Computer Science
Is LLM-Generated Code More Maintainable & Reliable than Human-Written Code?,2025,"Alfred Santa Molison, Marcia Moraes, Glaucia Melo, Fabio Santos, Wesley K. G. Assuno","Background: The rise of Large Language Models (LLMs) in software development has opened new possibilities for code generation. Despite the widespread use of this technology, it remains unclear how well LLMs generate code solutions in terms of software quality and how they compare to human-written code. Aims: This study compares the internal quality attributes of LLM-generated and human-written code. Method: Our empirical study integrates datasets of coding tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and SonarQube to assess software quality. The dataset comprises Python code solutions across three difficulty levels: introductory, interview, and competition. We analyzed key code quality metrics, including maintainability and reliability, and the estimated effort required to resolve code issues. Results: Our analysis shows that LLM-generated code has fewer bugs and requires less effort to fix them overall. Interestingly, fine-tuned models reduced the prevalence of high-severity issues, such as blocker and critical bugs, and shifted them to lower-severity categories, but decreased the model's performance. In competition-level problems, the LLM solutions sometimes introduce structural issues that are not present in human-written code. Conclusion: Our findings provide valuable insights into the quality of LLM-generated code; however, the introduction of critical issues in more complex scenarios highlights the need for a systematic evaluation and validation of LLM solutions. Our work deepens the understanding of the strengths and limitations of LLMs for code generation.",10.48550/arXiv.2508.00700,['JournalArticle'],Computer Science
"Career Link: AI for Resumes, Interviews, and Career Advice",2025,"Pratham Shenoy, Roshan Shiroorkar, A. Samani","Abstract: Career Link is an AI-powered career guidance platform that integrates multiple intelligent tools to assist users in every
step of their professional journey. The application offers an AI advisor for personalized career insights, an AI-powered resume
builder, a roadmap generator, and a mock interview simulator, providing a holistic user experience. Career Links architecture
leverages advanced language models such as Mistral and Googles Gemini to deliver context-aware career recommendations,
generate resume content, and simulate real-world interview scenarios. The system collects user inputs from psychometric
personality tests, educational background, and professional interests, then applies clustering techniques and prompt engineering
to generate detailed, tailored guidance. A case study involving a final-year student demonstrates how Career Link generates a
career fit report, an optimized resume, and adaptive interview questions, significantly improving the users preparedness and
self-awareness. This paper outlines the system's design, AI integration, and the real-world impact of combining psychometrics
with large language models for scalable, personalized career guidance",10.22214/ijraset.2025.69593,['JournalArticle'],Computer Science
Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents,2025,"Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Vivek Gupta, Dinesh Manocha","Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.",10.48550/arXiv.2506.01344,['JournalArticle'],Computer Science; Engineering
CogniPair: From LLM Chatbots to Conscious AI Agents - GNWT-Based Multi-Agent Digital Twins for Social Pairing - Dating & Hiring Applications,2025,"Wanghao Ye, Sihan Chen, Yiting Wang, Shwai He, Bowei Tian, Guoheng Sun, Ziyi Wang, Ziyao Wang, Yexiao He, Zheyu Shen, Meng Liu, Yuning Zhang, Mengnan Feng, Yang Wang, Siyuan Peng, Yilong Dai, Zhenle Duan, Hanzhang Qin, Ang Li","Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications. To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism. However, authentic digital twins require accurate personality initialization. We therefore develop a novel adventure-based personality test that evaluates true personality through behavioral choices within interactive scenarios, bypassing self-presentation bias found in traditional assessments. Building on these innovations, our CogniPair platform enables digital twins to engage in realistic simulated dating interactions and job interviews before real encounters, providing bidirectional cultural fit assessment for both romantic compatibility and workplace matching. Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies. This work advances psychological authenticity in LLM agents and establishes a foundation for intelligent dating platforms and HR technology solutions.",10.48550/arXiv.2506.03543,['JournalArticle'],Computer Science; Psychology
Large Language Model Enhanced Multi-UAV Direct Cross-boundary Maritime Data Collection Scheme,2025,"Yang Zhao, Hanjiang Luo, Hang Tao, Jinyin Li, Chao Liu, Jiehan Zhou","The cross-boundary communication between unmanned aerial vehicles (UAVs) and autonomous underwater vehicles (AUVs) constitutes a pivotal component in achieving full coverage of the space-air-ground-sea integrated network of 6G. In such hybrid networks, it is imperative to intelligently plan the trajectory of UAVs by exploiting the deep reinforcement learning (DRL) technique and direct optical wireless communication (OWC) technology to ensure timely data collection. However, the traditional DRL technique suffers from issues such as sparse rewards and low sampling efficiency, which leads to a decrease in the freshness of data. To address these issues, in this paper, we investigate the trajectory planning problem for swarms of UAVs conducting data collection with age of information awareness. Leveraging the prior knowledge of large language models (LLMs) and multi-agent reinforcement learning technique, we propose a novel multi-UAV maritime data collection scheme. Firstly, we extract the prior knowledge strategies of LLMs in the form of expert trajectories through structured prompts. Then, initial strategy models are rapidly generated for UAVs through multi-agent behavior cloning method, which reduce ineffective exploration and accelerates learning. Finally, the MATD3 algorithm is used to fine-tune these strategy models, enhancing their policy learning capability in sparse reward environments. We also conduct simulations to validate the effectiveness of the proposed algorithms.",10.1109/ICCCN65249.2025.11133883,"['JournalArticle', 'Conference']",Computer Science; Engineering; Environmental Science
Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation,2025,"James Mooney, Josef Woldense, Zheng Robert Jia, S. Hayati, My Ha Nguyen, Vipul Raheja, Dongyeop Kang","The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.",10.48550/arXiv.2509.03736,"['JournalArticle', 'Review']",Computer Science; Psychology
Innovative Multi-Channel Sound System Based on Multiple Mobile Devices with Frequency Domain Processing,2024,"Chun-Chieh Yang, Chien-Wen Cheng","Traditional speakers are usually mounted in a fixed position, with limited or non-adjustable spectrum control capabilities, and they usually focus on stereo sound, leading to a fixed manner of sound playback. This study attempts to develop a prototype for an innovative listening environment by integrating multiple mobile devices through OSC technology, intended as a foundational model for developing more detailed control. Users can distribute different frequency bands to the speakers of these mobile devices via a simple UI interface on a webpage, based on personal preferences, and complemented with subwoofers with its content specifically segregated through audio crossover, ensuring precise low-frequency sound delivery. Additionally, users can dynamically alter the positions of various frequency bands by moving mobile phones, enhancing their control over the audio environment. Upon completion of the system, usability surveys and interviews were conducted, revealing that playing sound across multiple mobile devices enhances the sense of spatiality and immersion during listening.",10.1109/ICCE-Taiwan62264.2024.10674210,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering
International Workshop on Online and Adaptive Recommender Systems (OARS 2024),2024,"Xiquan Cui, Vachik S. Dave, Yi Su, Khalifeh Al Jadda, Srijan Kumar, Julian J. McAuley, Tao Ye, Stephen Guo, Chip Huyen","Recommender system (RecSys) plays important roles in helping users navigate, discover, and consume massive and highly-dynamic information. Today, many RecSys solutions deployed in the real world rely on categorical user-profiles and/or pre-calculated recommendation actions that stay static during a user session. However, recent trends suggest that RecSys need to model user intent in real time and constantly adapt to meet user needs at the moment or change user behavior in situ. There are three primary drivers for this emerging need of online adaptation. First, in order to meet the increasing demand for a better personalized experience, the personalization dimensions and space will grow larger and larger. It would not be feasible to pre-compute recommended actions for all personalization scenarios beyond a certain scale. Second, in many settings the system does not have user prior history to leverage. Estimating user intent in real time is the only feasible way to personalize. As various consumer privacy laws tighten, it is foreseeable that many businesses will reduce their reliance on static user profiles. Therefore, it makes the modeling of user intent in real time an important research topic. Third, a user's intent often changes within a session and between sessions, and user behavior could shift significantly during dramatic events. Therefore, it is important to investigate more on online and adaptive recommender system (OARS) that can adapt in real time to meet user needs and be robust against distribution shifts. Every year, the organizers survey the most important topics for OARS and propose a new workshop program. In light of the recent advancement of LLMs and foundation models in RecSys, in this new edition, we decide to formally add the new topic of foundation and LLM models in OARS. We will invite experts and papers in the field to facilitate its further advancement. Our workshop offers a focused discussion of the new study and application of OARS, and will bring together an interdisciplinary community of researchers and practitioners from both industry and academia to discuss on new topics in the area, grow a community, and push the direction forward.",10.1145/3627673.3679083,"['JournalArticle', 'Book', 'Conference', 'Review']",Computer Science
Synthetic Data Generation for Screen Time and App Usage,2025,"Gustavo Kruger, Nikhil Sachdeva, Michael Sobolev","Smartphone usage data can provide valuable insights for understanding interaction with technology and human behavior. However, collecting large-scale, in-the-wild smartphone usage logs is challenging due to high costs, privacy concerns, under representative user samples and biases like non-response that can skew results. These challenges call for exploring alternative approaches to obtain smartphone usage datasets. In this context, large language models (LLMs) such as Open AI's ChatGPT present a novel approach for synthetic smartphone usage data generation, addressing limitations of real-world data collection. We describe a case study on how four prompt strategies influenced the quality of generated smartphone usage data. We contribute with insights on prompt design and measures of data quality, reporting a prompting strategy comparison combining two factors, prompt level of detail (describing a user persona, describing the expected results characteristics) and seed data inclusion (with versus without an initial real usage example). Our findings suggest that using LLMs to generate structured and behaviorally plausible smartphone use datasets is feasible for some use cases, especially when using detailed prompts. Challenges remain in capturing diverse nuances of human behavioral patterns in a single synthetic dataset, and evaluating tradeoffs between data fidelity and diversity, suggesting the need for use-case-specific evaluation metrics and future research with more diverse seed data and different LLM models.",10.48550/arXiv.2509.13892,['JournalArticle'],Computer Science
A Brief Survey on Temporal Reasoning Based on Large Language Models,2024,"Panfeng Zhang, Huan Zhang, Xiaoke Wang, Fu Zhang, Fan Yu","Temporal reasoning is a pivotal mechanism for understanding the world around us, enabling inference, prediction, and deduction of temporal relationships among events. The advent of Large Language Models (LLMs) has sparked considerable interest in research on temporal reasoning utilizing these models. These models, trained on massive datasets, acquire potent representational capabilities, allowing them to learn temporal patterns and perform inference and prediction on complex temporal data. We provide a brief overview of recent research on temporal reasoning based on LLMs, exploring the capabilities of LLMs in temporal reasoning and outlining future directions. We particularly focus on four major research areas: Time Series Forecasting, Temporal Question Answering, Temporal Knowledge Graph and Assessing Temporal Reasoning Capability in LLMs. Through this review, we aim to offer new insights and perspectives for research and applications in the field of temporal reasoning, further advancing research on LLMs in temporal reasoning.",10.1109/ACAIT63902.2024.11021814,"['Conference', 'Review']",Computer Science
LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models,2025,"Ryan Chen, Youngmin Ko, Zeyu Zhang, Catherine Cho, Sunny Chung, Mauro Giuffr'e, Dennis L. Shung, Bradly C. Stadie","We introduce LAMP (Linear Attribution Mapping Probe), a method that shines light onto a black-box language model's decision surface and studies how reliably a model maps its stated reasons to its predictions through a locally linear model approximating the decision surface. LAMP treats the model's own self-reported explanations as a coordinate system and fits a locally linear surrogate that links those weights to the model's output. By doing so, it reveals which stated factors steer the model's decisions, and by how much. We apply LAMP to three tasks: sentiment analysis, controversial-topic detection, and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs exhibit locally linear decision landscapes. In addition, these surfaces correlate with human judgments on explanation quality and, on a clinical case-file data set, aligns with expert assessments. Since LAMP operates without requiring access to model gradients, logits, or internal activations, it serves as a practical and lightweight framework for auditing proprietary language models, and enabling assessment of whether a model behaves consistently with the explanations it provides.",10.48550/arXiv.2505.11772,['JournalArticle'],Computer Science; Linguistics
PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG,2025,"Muhammed Rizwan, Lars Carlsson, Mohammad Loni","The introduction of Large Language Models (LLMs) has significantly transformed Natural Language Processing (NLP) applications by enabling more advanced analysis of customer personas. At Volvo Construction Equipment (VCE), customer personas have traditionally been developed through qualitative methods, which are time-consuming and lack scalability. The main objective of this paper is to generate synthetic customer personas and integrate them into a Retrieval-Augmented Generation (RAG) chatbot to support decision-making in business processes. To this end, we first focus on developing a persona-based RAG chatbot integrated with verified personas. Next, synthetic personas are generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and evaluated based on completeness, relevance, and consistency using McNemar's test. In the final step, the chatbot's knowledge base is augmented with synthetic personas and additional segment information to assess improvements in response accuracy and practical utility. Key findings indicate that Few-Shot prompting outperformed CoT in generating more complete personas, while CoT demonstrated greater efficiency in terms of response time and token usage. After augmenting the knowledge base, the average accuracy rating of the chatbot increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants found the updated system useful in business contexts.",10.48550/arXiv.2505.17156,['JournalArticle'],Business; Computer Science
AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs,2025,"Xuanwen Ding, Chengjun Pan, Zejun Li, Jiwen Zhang, Siyuan Wang, Zhongyu Wei","Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model's real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process. Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench.",10.48550/arXiv.2505.21389,['JournalArticle'],Computer Science
Research on Psychological Test based on Large Language Model,2024,"Zhengzheng Liu, Yunfeng Kang, Xinying Li","Traditional questionnaire-based psychological tests have many drawbacks, such as the subjects inaccurate understanding of the questions, incomplete answers, disguised intentions, one-way communication, closed answers, etc. This paper proposes a psychological test system design scheme based on pre-trained large language model (LLM), which makes use of LLMs ability to learn and understand natural language and generate output. Complete the psychological test with the subjects through natural language human-computer dialogue.Taking the Cattell 16 Personality Factors Test (16PF) as an example, a natural corpus for large model training of psychological test was researched and compiled, and the psychological test model of two factors in 16PF was developed. Through test comparison, the psychological test model is better than the traditional questionnaire test effect.",10.1109/RAIIC61787.2024.10670817,['Conference'],Computer Science; Psychology
How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org),2025,Jin Kim,"As large language models (LLMs) like ChatGPT become increasingly integrated into our everyday lives--from customer service and education to creative work and personal productivity--understanding how people interact with these AI systems has become a pressing issue. Despite the widespread use of LLMs, researchers lack standardized tools for systematically studying people's interactions with LLMs. To address this issue, we introduce GPT for Researchers (G4R), or g4r.org, a free website that researchers can use to easily create and integrate a GPT Interface into their studies. At g4r.org, researchers can (1) enable their study participants to interact with GPT (such as ChatGPT), (2) customize GPT Interfaces to guide participants' interactions with GPT (e.g., set constraints on topics or adjust GPT's tone or response style), and (3) capture participants' interactions with GPT by downloading data on messages exchanged between participants and GPT. By facilitating study participants' interactions with GPT and providing detailed data on these interactions, G4R can support research on topics such as consumer interactions with AI agents or LLMs, AI-assisted decision-making, and linguistic patterns in human-AI communication. With this goal in mind, we provide a step-by-step guide to using G4R at g4r.org.",10.48550/arXiv.2503.18303,['JournalArticle'],Computer Science; Linguistics
Design of Persona-Based Interactive Interfaces and Their Impact on Human Self-Perception,2025,Xinzhu Li,"Grounded in the frameworks of affective computing and large-language-model technology, this paper systematically sorts out the conceptual evolution and core techniques of persona-based interactive interfaces (PBIIs) and, by means of a randomized controlled experiment, verifies their comprehensive effect on users self-perception. The study first constructs a four-level closed-loop architecture of Perception  Understanding  Generation  Feedback. Sixty university students are then recruited for a 14-day intervention; pre- and post-tests using a Self-Efficacy Scale and an Emotion-Awareness Test are compared. Results show a significant rise in self-efficacy for the experimental group (M = +0.70, p < 0.01) and a 12 % increase in emotion-recognition accuracy, validating the synergistic mechanism of affective mirroring and verbal persuasion. Qualitative interviews further reveal potential risks such as emotional dependence and cognitive dissonance. In view of these findings, this paper proposes PBII design principles centred on uncertainty management and user-agency cues, offering theoretical and practical reference for responsible application in education and mental-health fields.",10.70393/616a736d.323936,['JournalArticle'],Computer Science; Psychology
A Career Interview Dialogue System using Large Language Model-based Dynamic Slot Generation,2024,"Ekai Hashimoto, Mikio Nakano, Takayoshi Sakurai, Shun Shiramatsu, Toshitake Komazaki, Shiho Tsuchiya","This study aims to improve the efficiency and quality of career interviews conducted by nursing managers. To this end, we have been developing a slot-filling dialogue system that engages in pre-interviews to collect information on staff careers as a preparatory step before the actual interviews. Conventional slot-filling-based interview dialogue systems have limitations in the flexibility of information collection because the dialogue progresses based on predefined slot sets. We therefore propose a method that leverages large language models (LLMs) to dynamically generate new slots according to the flow of the dialogue, achieving more natural conversations. Furthermore, we incorporate abduction into the slot generation process to enable more appropriate and effective slot generation. To validate the effectiveness of the proposed method, we conducted experiments using a user simulator. The results suggest that the proposed method using abduction is effective in enhancing both information-collecting capabilities and the naturalness of the dialogue.",10.48550/arXiv.2412.16943,"['JournalArticle', 'Conference']",Computer Science
Cash or Comfort? How LLMs Value Your Inconvenience,2025,"Mateusz Cedro, Timour Ichmoukhamedov, Sofie Goethals, Yifan He, James Hinns, David Martens","Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users'behalf.",10.48550/arXiv.2506.17367,['JournalArticle'],Computer Science; Psychology
Neural embedding of beliefs reveals the role of relative dissonance in human decision-making,2024,"Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An",,10.48550/arXiv.2408.07237,"['JournalArticle', 'Review']",Computer Science; Psychology
"AdaptAI: A Personalized Solution to Sense Your Stress, Fix Your Mess, and Boost Productivity",2025,"Rushiraj Gadhvi, Soham Petkar, P. Desai, Shreyas Ramachandran, Siddharth Siddharth","Personalization is a critical yet often overlooked factor in boosting productivity and well-being in knowledge-intensive workplaces to better address individual preferences. Existing tools typically offer uniform guidance, whether auto-generating email responses or prompting break reminders, without accounting for individual behavioral patterns or stress triggers. We introduce AdaptAI, a multimodal AI solution combining egocentric vision and audio, heart and motion activities, and the agentic workflow of Large Language Models (LLMs) to deliver highly personalized productivity support and context-aware well-being interventions. AdaptAI not only automates peripheral tasks (e.g., drafting succinct document summaries, replying to emails, etc.) but also continuously monitors the users unique physiological and situational indicators to dynamically tailor interventions, such as micro-break suggestions or exercise prompts, at the exact point of need. In a preliminary study with 15 participants, AdaptAI demonstrated significant improvements in task throughput and user satisfaction by anticipating user stressors and streamlining daily workflows.",10.1145/3706599.3720284,"['Book', 'JournalArticle']",Computer Science; Psychology
Measuring Large Language Models Dependency: Validating the Arabic Version of the LLM-D12 Scale,2025,"S. Alshakhsi, Ala Yankouskaya, Magnus Liebherr, Raian Ali","There is an urgent need for reliable, culturally validated instruments to assess psychological responses to AI in general and large language models (LLMs). This need is global issue, but it is especially urgent among Arabic-speaking populations, where AI and LLMs adoption is accelerating, yet psychometric tools remain limited. This study presents the first validation of the LLM-D12, a dual-dimensional scale assessing Instrumental and Relationship Dependency on LLMs, in an Arab sample. A total of 250 Arab participants completed the Arabic version of the LLM-D12. Confirmatory Factor Analysis confirms the original 2-factor structure of LLM-D12 with all items showing good loading of corresponding Instrumental and Relationship Dependency. The scale showed good to excellent internal reliability (Cronbach alpha is 0.90 for Total, 0.85 for Instrumental Dependency, and 0.90 for Relationship Dependency). External validation revealed that Instrumental Dependency was positively associated with AI acceptance and internet addiction, while Relationship Dependency was linked to lower need for cognition and greater trustworthiness of LLM, demonstrating sensitivity of this instrument to different use and personal factors. These findings confirm that Arabic LLM-D12 is a psychometrically sound, culturally appropriate instrument, offering a necessary tool for research, education, and policy concerning AI and LLMs engagement in Arab contexts.",10.48550/arXiv.2508.17063,['JournalArticle'],Computer Science; Linguistics; Psychology
Investigating Effects of Multimodal Topic-continuance Recognition on Human-Robot Interviewing Interaction,2024,"Fuminori Nagasawa, Shogo Okada","This study's long-term goal is the development of a communication robot as a partner that can keep talking about specific things about which the user would like to talk and in which they are interested. To achieve this goal, we developed an interviewer robot that adapts topics based on the user's multimodal attitudes. The robot, utilizing the Japanese GPT-NeoX-3.6, selects questions based on the estimated topic continuance level. We regard the topic continuance level as the degree of the user's speaking willingness (willingness to continue the current topic). This paper aims to validate the multimodal topic continuance recognition model and its adaptive question selection strategy. First, we trained the model on the ""Hazumi"" dialog corpus, which includes user multimodal behavior in human-virtual agent interactions. Second, 10 participants were interviewed with the robot equipped with the trained model. After the interviews, we asked the participants if the topic continuance/change by the robot was appropriate and validated the estimation accuracy.",10.1145/3610978.3640662,"['JournalArticle', 'Book']",Computer Science
LLM4Rail: An LLM-Augmented Railway Service Consulting Platform,2025,"Zhuo Li, Xianghuai Deng, Chiwei Feng, Hanmeng Li, Shenjie Wang, Haichao Zhang, Teng Jia, Conlin Chen, Louis Linchun Wu, Jia Wang","Large language models (LLMs) have significantly reshaped different walks of business. To meet the increasing demands for individualized railway service, we develop LLM4Rail - a novel LLM-augmented railway service consulting platform. Empowered by LLM, LLM4Rail can provide custom modules for ticketing, railway food&drink recommendations, weather information, and chitchat. In LLM4Rail, we propose the iterative""Question-Thought-Action-Observation (QTAO)""prompting framework. It meticulously integrates verbal reasoning with task-oriented actions, that is, reasoning to guide action selection, to effectively retrieve external observations relevant to railway operation and service to generate accurate responses. To provide personalized onboard dining services, we first construct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible takeout dataset tailored for railway services. CRFD-25 covers a wide range of signature dishes categorized by cities, cuisines, age groups, and spiciness levels. We further introduce an LLM-based zero-shot conversational recommender for railway catering. To address the unconstrained nature of open recommendations, the feature similarity-based post-processing step is introduced to ensure all the recommended items are aligned with CRFD-25 dataset.",10.48550/arXiv.2507.23377,['JournalArticle'],Computer Science; Engineering
APEE: Assessing the Personality Expressions of LLM-Driven Role Play Agent Beyond Self-Perception,2025,"Kun Li, Chenwei Dai, Wei Zhou, Songlin Hu","Large language models (LLMs) have demonstrated significant progress in role-playing tasks, yet evaluating their ability to simulate personality traits remains a challenge. Traditional psychological questionnaires-based method have been used to assess LLMs' personality traits. However, these approaches have limitations when applied to LLM-driven role-playing agents (RPAs), as they are designed for humans and rely on stable, self-assessed personality traits. To bridge this gap, we extend simple self-perception questionnaires to more objective, real-world evaluations. In this paper, we introduce APEE, a new dataset consisting of 473 instances across three real-world scenario types: practical goal planning, social media behavior, and leaderless group discussions (LGD). In addition to evaluating whether LLMs adhere to predefined character traits, we introduce two key metrics: Stability and Differentiation. These metrics assess how consistently LLMs express personality traits across different scenarios (Stability) and how effectively they differentiate their behavior when assuming multiple roles (Differentiation). We conducted experiments on 339 different roles using 11 advanced LLMs with the APEE dataset. Discuss the impact of factors such as model size and architecture. Code and dataset are available at https://github.com/linkseed18612254945/APEE_Personality.",10.1109/CSCWD64889.2025.11033568,"['JournalArticle', 'Conference']",Computer Science; Psychology
PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting,2025,"Caitlin Cisar, Emily Sheffield, Joshua Drake, Alden Harrell, Subramanian Chidambaram, Nikita Nangia, Vinayak Arannil, Alex Williams","Generative AI applications commonly leverage user personas as a steering mechanism for synthetic data generation, but reliance on natural language representations forces models to make unintended inferences about which attributes to emphasize, limiting precise control over outputs. We introduce PILOT (Psychological and Linguistic Output Targeting), a two-phase framework for steering large language models with structured psycholinguistic profiles. In Phase 1, PILOT translates natural language persona descriptions into multidimensional profiles with normalized scores across linguistic and psychological dimensions. In Phase 2, these profiles guide generation along measurable axes of variation. We evaluate PILOT across three state-of-the-art LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas under three conditions: Natural-language Persona Steering (NPS), Schema-Based Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate that schema-based approaches significantly reduce artificial-sounding persona repetition while improving output coherence, with silhouette scores increasing from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals a fundamental trade-off: SBS produces more concise outputs with higher topical consistency, while NPS offers greater lexical diversity but reduced predictability. HPS achieves a balance between these extremes, maintaining output variety while preserving structural consistency. Expert linguistic evaluation confirms that PILOT maintains high response quality across all conditions, with no statistically significant differences between steering approaches.",10.48550/arXiv.2509.15447,['JournalArticle'],Computer Science; Linguistics; Psychology
LLMs are Introvert,2025,"Litian Zhang, Xiaoming Zhang, Bingyu Yan, Ziyi Zhou, Bo Zhang, Zhenyu Guan, Xi Zhang, Chaozhuo Li","The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents'evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.",10.48550/arXiv.2507.05638,['JournalArticle'],Computer Science; Psychology
MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models,2025,"Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang","Automated depression diagnosis aims to analyze multimodal information from interview videos to predict participants'depression scores. Previous studies often lack clear explanations of how these scores were determined, limiting their adoption in clinical practice. While the advent of LLMs provides a possible pathway for explainable depression diagnosis, current LLMs capable of processing multimodal data lack training on interview data, resulting in poor diagnostic performance when used directly. In this paper, we propose a novel multimodal large language model (MLlm-DR) that can understand multimodal information inputs and supports explainable depression diagnosis. MLlm-DR integrates a smaller LLMs and a lightweight query module (LQ-former). Specifically, the smaller LLMs is designed to generate depression scores and corresponding evaluation rationales. To enhance its logical reasoning for domain-specific tasks while maintaining practicality, we constructed a robust training dataset to fine-tune it. Meanwhile, the LQ-former captures depression-related features from speech and visual data, aiding the model's ability to process multimodal information, to achieve comprehensive depression diagnosis. Our approach achieves state-of-the-art results on two interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its effectiveness and superiority.",10.48550/arXiv.2507.05591,['JournalArticle'],Computer Science; Psychology
Edge intelligence unleashed: a survey on deploying large language models in resource-constrained environments,2025,"Serhii Semerikov, T. Vakaliuk, O. Kanevska, O.A. Ostroushko, Andrii O. Kolhatin","Edge computing environments face unprecedented challenges in deploying large language models due to severe resource constraints, latency requirements, and privacy concerns that traditional cloud-based solutions cannot address. Current approaches struggle with the fundamental mismatch between LLMs' computational demands - requiring gigabytes of memory and billions of operations - and edge devices' limited capabilities, resulting in either degraded performance or infeasible deployments. This survey presents a systematic analysis of emerging techniques that enable efficient LLM deployment at the edge through four complementary strategies: model compression via quantisation and pruning that reduces memory footprint by up to 75% while maintaining accuracy, knowledge distillation frameworks achieving 4000 parameter reduction with comparable performance, edge-cloud collaborative architectures like EdgeShard delivering 50% latency reduction through intelligent workload distribution, and hardware-specific optimisations leveraging specialised accelerators. Extensive evaluation across multiple real-world testbeds demonstrates that hybrid edge-microservices architectures achieve 46% lower P99 latency and 67% higher throughput compared to monolithic approaches, while supporting 10,000 concurrent users with 100 ms latency constraints and reducing bandwidth consumption by 99.5% through selective cloud offloading. These advancements enable transformative applications in healthcare monitoring, autonomous systems, real-time IoT analytics, and personalised AI services, fundamentally reshaping how intelligence is delivered at the network edge while preserving privacy and ensuring responsiveness critical for next-generation computing paradigms.",10.55056/jec.1000,"['JournalArticle', 'Review']",Computer Science
User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies,2025,"Jennifer King, Kevin Klyman, Emily Capstick, Tiffany Saade, Victoria Hsieh","Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on the California Consumer Privacy Act, we develop a novel qualitative coding schema that we apply to each developers relevant privacy policies to compare data collection and use practices across the six companies. We find that all six developers appear to employ their users' chat data to train and improve their models by default, and that some retain this data indefinitely. Developers may collect and train on personal information disclosed in chats, including sensitive information such as biometric and health data, as well as files uploaded by users. Four of the six companies we examined appear to include children's chat data for model training, as well as customer data from other products. On the whole, developers' privacy policies often lack essential information about their practices, highlighting the need for greater transparency and accountability. We address the implications of users' lack of consent for the use of their chat data for model training, data security risks arising from indefinite chat data retention, and issues with training on children's chat data. We conclude by providing recommendations to policymakers and developers to address the data privacy challenges posed by LLM-powered chatbots.",10.48550/arXiv.2509.05382,"['JournalArticle', 'Conference']",Computer Science; Linguistics
LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios,2025,"Bingxi Zhao, Lin Geng Foo, Ping Hu, Christian Theobalt, Hossein Rahmani, Jun Liu","Recent advances in the intrinsic reasoning capabilities of large language models (LLMs) have given rise to LLM-based agent systems that exhibit near-human performance on a variety of automated tasks. However, although these systems share similarities in terms of their use of LLMs, different reasoning frameworks of the agent system steer and organize the reasoning process in different ways. In this survey, we propose a systematic taxonomy that decomposes agentic reasoning frameworks and analyze how these frameworks dominate framework-level reasoning by comparing their applications across different scenarios. Specifically, we propose an unified formal language to further classify agentic reasoning systems into single-agent methods, tool-based methods, and multi-agent methods. After that, we provide a comprehensive review of their key application scenarios in scientific discovery, healthcare, software engineering, social simulation, and economics. We also analyze the characteristic features of each framework and summarize different evaluation strategies. Our survey aims to provide the research community with a panoramic view to facilitate understanding of the strengths, suitable scenarios, and evaluation practices of different agentic reasoning frameworks.",10.48550/arXiv.2508.17692,"['JournalArticle', 'Review']",Computer Science
Can Large Language Models Trigger a Paradigm Shift in Travel Behavior Modeling? Experiences with Modeling Travel Satisfaction,2025,"Pengfei Xu, Donggen Wang","As a specific domain of subjective well-being, travel satisfaction has attracted much research attention recently. Previous studies primarily use statistical models and, more recently, machine learning models to explore the determinants of travel satisfaction. Both approaches require data from sufficient sample sizes and correct prior statistical assumptions. The emergence of Large Language Models (LLMs) offers a new modeling approach that can overcome the shortcomings of the existing methods. Pre-trained on extensive datasets, LLMs have strong capabilities in contextual understanding and generalization, significantly reducing their dependence on large quantities of task-specific data and stringent statistical assumptions. The primary challenge in applying LLMs lies in addressing the behavioral misalignment between LLMs and human behavior. Using data on travel satisfaction from a household survey in shanghai, this study identifies the existence and source of misalignment and develop methods to address the misalignment issue. We find that the zero-shot LLM exhibits behavioral misalignment, resulting in relatively low prediction accuracy. However, few-shot learning, even with a limited number of samples, allows the model to outperform baseline models in MSE and MAPE metrics. This misalignment can be attributed to the gap between the general knowledge embedded in LLMs and the specific, unique characteristics of the dataset. On these bases, we propose an LLM-based modeling approach that can be applied to model travel behavior using samples of small sizes. This study highlights the potential of LLMs for modeling not only travel satisfaction but also broader aspects of travel behavior.",10.48550/arXiv.2505.23262,"['JournalArticle', 'Review']",Computer Science
CHAT GPT ASSISTANT,2024,S. Raees,"The capacity of ChatGPT, a cutting-edge conversational AI model, to produce text responses that resemble those of a human being in a range of conversational contexts has attracted a lot of attention. By using extensive pretraining on a variety of textual data, ChatGPT generates discourse with impressive fluency and coherence, opening up new possibilities for improving human-computer interaction. In this study, we give a thorough analysis of ChatGPT's capabilities and constraints, examining how well it generates interesting and natural-sounding discussions. We explore several facets of ChatGPT's dialogue production through empirical evaluation and qualitative analysis, such as its flexibility across domains, responsiveness to input cues, and propensity to produce a range of contextually relevant and varied responses. Additionally, we look at methods for adjusting ChatGPT to certain conversational tasks and evaluate how different training regimens affect its Index Terms Data Collection and Preprocessing, Machine Learning Models based, Prediction and Visualization, Real-time Update.",10.55041/ijsrem30218,['JournalArticle'],Computer Science
Belle: Large Language Model-Based Conversational Recommender Systems for Beauty Products,2025,"Elvira Wulandari, Z. Baizal","The beauty product industry has become one of the sectors influenced by technological advancements and changes in consumer behavior. In this industry, the main challenges include the wide variety of products, diverse consumer preferences, and the difficulty consumers select the right products when shopping online. Conventional recommender systems often struggle to provide personalized and relevant suggestions tailored to individual needs, leading to a less satisfying shopping experience. To address this issue, we introduce Belle, a beauty product recommender system that integrates a Large Language Model (LLM) with a Conversational Recommender System (CRS). Belle is designed to provide more personalized and dynamic beauty product recommendations by utilizing the fine-tuned GPT-4o model, which better understands user preferences and supports more natural conversational interactions. Belle was developed through a fine-tuning process on the GPT-4o model using a dataset of 53,511 tokens over three epochs. This process significantly improved, with a training loss of 0.5344 and a validation loss of 0.3643. Semantic similarity evaluation showed improved results compared to prefine-tuning values, with the final BERTScore (Precision: 0.8676, Recall: 0.9182, F1: 0.8921) and Cosine Similarity (0.7227). Additionally, the Distinct@1 value increased from 0.7089 to 0.9081, indicating more diverse and natural responses after fine-tuning. By leveraging the strengths of LLM and CRS, Belle demonstrates that the beauty product recommendations generated are more personalized, flexible, and relevant to each user.",10.1109/ICADEIS65852.2025.10933248,['Conference'],Computer Science
Addressing class imbalance in text classification with LLMs: A prompt-based GPT-2 approach,2025,"Laouni Mahmoudi, Mohammed Salem, N. Alharbe","The advent of Generative Pre-trained Transformer (GPT) has significantly impacted various downstream natural language processing (NLP) tasks, showcasing its remarkable capabilities in language understanding and generation. It has demonstrated state-of-the-art performance in areas such as machine translation, text summarisation and question answering. In this study, we focus on addressing the issue of skewed data sets, which is a challenge across multiple NLP tasks. Conventional balancing techniques including undersampling and oversampling are fraught with limitations and may result in biased predictions and insufficient representation of minority classes. In response, we propose an innovative approach harnessing GPTs capabilities to generate synthetic samples. We evaluate our approach on a downstream multi-class text classification task and demonstrate significant performance improvements over conventional techniques and state-of-the-art methods, surpassing the accuracy by more than 10%. These findings underscore the potential of GPT to revolutionise data set balancing, thereby augmenting the performance of downstream NLP tasks.",10.1177/01655515251362340,['JournalArticle'],Computer Science
An intelligent enterprise system with processing and verification of business documents using big data and AI,2025,"G. Balakayeva, M. Zhanuzakov, U. Zhapbasbayev, K. Nurlybayeva","
 The increasing demand for operational efficiency and data integrity has led enterprises to prioritize the digital transformation of internal workflows. This is done through automation of document-related business processes. This study proposes an intelligent enterprise system that integrates artificial intelligence and big data technologies for the automated generation, validation, and approval of business documents. The motivation behind this work derives from the need to reduce human error, enhance accuracy, and accelerate document turnaround times in enterprise environments. The authors employ large language models to automatically generate document templates and a fine-tuned bidirectional encoder representations from transformer-based classifier for validating document content. Big data tools such as Apache Spark are used for processing and cleaning large volumes of enterprise documents. Additionally, low-confidence predictions are handled through a human-in-the-loop mechanism to ensure high reliability. The research process involves system design, data collection from over 9,000 real enterprise documents, model training, and integration into a business process management system. Experimental results show that the proposed approach improves document processing efficiency while maintaining data quality. This article presents a unified framework and implementation methodology that can be adapted for broader enterprise automation needs. The authors use data from thermal grid enterprises as an example for testing the developed intelligent models.",10.1515/jisys-2024-0446,['JournalArticle'],Business; Computer Science
Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs,2025,"Eyal German, Sagiv Antebi, Daniel Samira, A. Shabtai, Y. Elovici","Large language models (LLMs) are increasingly trained on tabular data, which, unlike unstructured text, often contains personally identifiable information (PII) in a highly structured and explicit format. As a result, privacy risks arise, since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). While existing MIA methods primarily target textual content, their efficacy and threat implications may differ when applied to structured data, due to its limited content, diverse data types, unique value distributions, and column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Tab-MIA comprises five data collections, each represented in six different encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs finetuned with tabular data across multiple encoding formats. In the evaluation, we analyze the memorization behavior of pretrained LLMs on structured data derived from Wikipedia tables. Our findings show that LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases. Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs.",10.48550/arXiv.2507.17259,['JournalArticle'],Computer Science
Value-Sensitive Disagreement Analysis for Online Deliberation,2024,"Michiel van der Meer, Piek Vossen, C. Jonker, P. Murukannaiah",". Disagreements are common in online societal deliberation and may be crucial for effective collaboration, for instance in helping users understand opposing viewpoints. Although there exist automated methods for recognizing disagreement, a deeper understanding of factors that inuence disagreement is currently missing. We investigate a hypothesis that differences in personal values inuence disagreement in online discussions. Using Large Language Models (LLMs) for estimating both proles of personal values and disagreement, we conduct a large-scale experiment involving 11.4M user comments. We nd that the dissimilarity of value proles correlates with disagreement only in specic cases, but that incorporating self-reported value proles changes these results to be more undecided.",10.3233/FAIA240231,['JournalArticle'],Computer Science
Accelerating the Development of Machine Voice User Interfaces with Immersive Environments,2025,"Polina Hfner, Frithjof Eisenlohr, F. L. Michels, Abhijit Karande, Michael Grethler","This paper proposes a conceptual framework leveraging VR technologies to develop large language model (LLM)-based Voice User Interfaces (VUIs) for industrial applications, addressing key adoption challenges. By integrating virtual twins and immersive environments, the framework supports essential development steps, including data collection, LLM training, usability testing, and iterative design validation. The study shows how VR enables realistic simulations, faster development, and reduces the dependency on physical prototypes in the VUI development, offering improvements in efficiency and user-centered design. Initial findings of the proof-of-concept study highlight the frameworks potential to address industrial requirements.",10.1109/VRW66409.2025.00390,"['JournalArticle', 'Conference']",Computer Science; Engineering
Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games,2025,"Andrs Isaza-Giraldo, Paulo Bala, Lucas Pereira","The evaluation of open-ended responses in serious games presents a unique challenge, as correctness is often subjective. Large Language Models (LLMs) are increasingly being explored as evaluators in such contexts, yet their accuracy and consistency remain uncertain, particularly for smaller models intended for local execution. This study investigates the reliability of five small-scale LLMs when assessing player responses in \textit{En-join}, a game that simulates decision-making within energy communities. By leveraging traditional binary classification metrics (including accuracy, true positive rate, and true negative rate), we systematically compare these models across different evaluation scenarios. Our results highlight the strengths and limitations of each model, revealing trade-offs between sensitivity, specificity, and overall performance. We demonstrate that while some models excel at identifying correct responses, others struggle with false positives or inconsistent evaluations. The findings highlight the need for context-aware evaluation frameworks and careful model selection when deploying LLMs as evaluators. This work contributes to the broader discourse on the trustworthiness of AI-driven assessment tools, offering insights into how different LLM architectures handle subjective evaluation tasks.",10.48550/arXiv.2504.12333,['JournalArticle'],Computer Science
SimLiquid: A SimulationBased Liquid Perception Pipeline for Robot Liquid Manipulation,2025,"Yan Huang, Jiawei Zhang, Ran Yu, Shoujie Li, Wenbo Ding","Transparent liquid volume estimation is crucial for robot manipulation tasks, such as pouring. However, estimating the volume of transparent liquids is a challenging problem. Most existing methods primarily focus on data collection in the real world, and the sensors are fixed to the robot body for liquid volume estimation. These approaches limit both the timeliness of the research process and the flexibility of perception. In this paper, we present SimLiquid20k, a highfidelity synthetic data set for liquid volume estimation, and propose a YOLObased multitask network trained on fully synthetic data for estimating the volume of transparent liquids. Extensive experiments demonstrate that our method can effectively transfer from simulation to the real world. In scenarios involving changes in background, viewpoint, and container variations, our approach achieves an average error of 5% in realworld volume estimation. In addition, our work conducts two application experiments integrating with GPT4, showcasing the potential of our method in service robotics. The accompanying videos and supporting Information are available at https://simliquid.github.io/.",10.1002/rob.22548,['JournalArticle'],Computer Science; Engineering
Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization,2025,"Vera Neplenbroek, Arianna Bisazza, R. Fern'andez","Generative Large Language Models (LLMs) infer user's demographic information from subtle cues in the conversation -- a phenomenon called implicit personalization. Prior work has shown that such inferences can lead to lower quality responses for users assumed to be from minority groups, even when no demographic information is explicitly provided. In this work, we systematically explore how LLMs respond to stereotypical cues using controlled synthetic conversations, by analyzing the models'latent user representations through both model internals and generated answers to targeted user questions. Our findings reveal that LLMs do infer demographic attributes based on these stereotypical signals, which for a number of groups even persists when the user explicitly identifies with a different demographic group. Finally, we show that this form of stereotype-driven implicit personalization can be effectively mitigated by intervening on the model's internal representations using a trained linear probe to steer them toward the explicitly stated identity. Our results highlight the need for greater transparency and control in how LLMs represent user identity.",10.48550/arXiv.2505.16467,['JournalArticle'],Computer Science; Sociology
Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation,2025,"Julia Ann Mathew, Suining He","The increasing popularity of outdoor recreational activities (such as hiking and biking) has boosted the demand for a conversational AI system to provide informative and personalized suggestion on outdoor trails. Challenges arise in response to (1) how to provide accurate outdoor trail information via conversational AI; and (2) how to enable usable and efficient recommendation services. To address above, this paper discusses the preliminary and practical lessons learned from developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG). To gain concrete system insights, we have performed case studies with the outdoor trails in Connecticut (CT), US. We have conducted web-based data collection, outdoor trail data management, and LLM model performance studies on the RAG-based recommendation. Our experimental results have demonstrated the accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG.",10.48550/arXiv.2508.05652,['JournalArticle'],Computer Science
Automated Developer Pattern Analysis and Code Suggestions with AI,2025,"Manjula G, Yashwanth A, Nitish Kp","The rapid advancement of software development tools that elevates productivity, refines code quality, diminishes human error. This project confronts these challenges by integrating artificial intelligence (AI) to automate the analysis of the developer patterns and provide appropriate code suggestions. By harnessing machine learning algorithms, natural language processing and making the most of the Qwen 2.5 coder, we can assiduously survey individual coding habits, study recurring patterns, and detect inefficiencies in real-time. This AI-driven framework tailors itself to unique coding styles while mining insights from diverse repositories of open-source code to generate contextually optimized solutions. Key features comprise of semantic code analysis, predictive pattern recognition, and automated refactoring suggestions, all designed to streamline workflows and elevate consistent, high quality software enhancement. This work ensures for scalable, intelligent coding assistant that empowers developers across diverse programming environments.",10.59256/indjcst.20250401037,"['JournalArticle', 'Review']",Computer Science
Can ChatGPT Outperform Humans in Faking a Personality Assessment While Avoiding Detection?,2025,"C. Robie, Jane Phillips, J. Bourdage, Neil D. Christiansen, Patrick D. Dunlop, Stephen D. Risavy, Andrew B. Speer","Large language models (LLMs), such as ChatGPT, have reshaped opportunities and challenges across various fields, including human resources (HR). Concerns have arisen about the potential for personality assessment manipulation using LLMs, posing a risk to the validity of these tools. This threat is a reality: recent research suggests that manycandidates are using AI to complete prehire assessments. This study addresses this problem by examining whether ChatGPT can outperform humans in faking personality assessments while avoiding detection. To explore this, two experiments were conducted focusing on assessing jobrelevant traits, with and without coaching, and with two methods of identifying faking, specifically using an impression management (IM) measure and an overclaiming questionnaire (OCQ). For each study, we used responses from 100 working adults recruited via the Prolific platform, which were compared to 100 replications from ChatGPT. The results revealed that while ChatGPT showed some ability to manipulate assessments, without coaching it did not consistently outperform humans. Coaching had a minimal impact on reducing IM scores for either humans or ChatGPT, but reduced OCQ bias scores for ChatGPT. These findings highlight the limitations of current faking detection measures and emphasize the need for further research to refine methods for ensuring the integrity of personality assessments in HR, particularly as artificial intelligence becomes more available to candidates.",10.1111/ijsa.70015,['JournalArticle'],Computer Science; Psychology
0429 DreamGPT: Validation of ChatGPT as a Tool for Emotion Analysis in Dream Reports,2025,"G. Baber, Nancy Hamilton, M. Gratton","
 
 
 Dream reports provide unique insights into emotional processing during sleep. While self-reports are the gold standard for assessing dream affect, natural language processing (NLP) tools like ChatGPT may offer scalable alternatives. This study evaluated ChatGPTs ability to estimate positive and negative affect from dream reports, comparing its performance against self-reported ratings.
 
 
 
 A total of 136 participants provided one dream report each. Participants rated their positive and negative dream affect on a 010 scale. ChatGPT 3.5 Turbo was accessed via the OpenAI API using Python, where each dream report was analyzed and rated on identical scales. The models outputs were programmatically appended to a CSV file for further analysis. Agreement between ChatGPT and self-reports was assessed using intra-class correlation coefficients (ICC3k) for consistency, mean absolute error (MAE) for deviation, and Bland-Altman plots for visual inspection of agreement.
 
 
 
 For positive affect ratings, ChatGPT demonstrated excellent agreement with self-reports (ICC3k = 0.844, 95% CI [0.781, 0.889], p <.001), with an MAE of 1.778. Negative affect ratings similarly showed excellent agreement (ICC3k = 0.857, 95% CI [0.799, 0.898], p <.001), with an MAE of 1.681. Bland-Altman plots for both affective dimensions indicated no systematic bias and acceptable limits of agreement upon visual inspection.
 
 
 
 ChatGPT demonstrated strong agreement with self-reported dream affect ratings, supporting its potential as a scalable tool for analyzing emotional content in dream reports. These findings suggest that large language models can provide valid and reliable estimates of dream affect, which may advance sleep and affective science.
 
 
 
 
",10.1093/sleep/zsaf090.0429,['JournalArticle'],Computer Science; Psychology
Synthetic Founders: AI-Generated Social Simulations for Startup Validation Research in Computational Social Science,2025,Jorn K. Teutloff,"We present a comparative docking experiment that aligns human-subject interview data with large language model (LLM)-driven synthetic personas to evaluate fidelity, divergence, and blind spots in AI-enabled simulation. Fifteen early-stage startup founders were interviewed about their hopes and concerns regarding AI-powered validation, and the same protocol was replicated with AI-generated founder and investor personas. A structured thematic synthesis revealed four categories of outcomes: (1) Convergent themes - commitment-based demand signals, black-box trust barriers, and efficiency gains were consistently emphasized across both datasets; (2) Partial overlaps - founders worried about outliers being averaged away and the stress of real customer validation, while synthetic personas highlighted irrational blind spots and framed AI as a psychological buffer; (3) Human-only themes - relational and advocacy value from early customer engagement and skepticism toward moonshot markets; and (4) Synthetic-only themes - amplified false positives and trauma blind spots, where AI may overstate adoption potential by missing negative historical experiences. We interpret this comparative framework as evidence that LLM-driven personas constitute a form of hybrid social simulation: more linguistically expressive and adaptable than traditional rule-based agents, yet bounded by the absence of lived history and relational consequence. Rather than replacing empirical studies, we argue they function as a complementary simulation category - capable of extending hypothesis space, accelerating exploratory validation, and clarifying the boundaries of cognitive realism in computational social science.",10.48550/arXiv.2509.02605,['JournalArticle'],Computer Science; Psychology; Sociology
Implementing persona in the business sector by a universal explainable AI framework based on byte-pair encoding,2025,"Zhenyao Liu, Yu-Lun Liu, Wei-Chang Yeh, Chia-Ling Huang","In the commercial realm, particularly for businesses targeting consumers
 (B2C), the challenge of acquiring and retaining valuable potential customers
 is paramount. As chip technology continues to advance at breakneck speed, in
 line with Moore?s Law, various innovative AI technologies have emerged, yet
 this also highlights the infamous ?black-box? issue. Naturally, this has
 paved the way for the rise of Explainable AI (XAI) and machine learning. In
 response, this study proposes a universal explainability framework to tackle
 both the black-box conundrum and the limitation of customer list sizes. The
 framework leverages the fundamental Byte-Pair Encoding (BPE) algorithm from
 large language models to tokenize natural language data, integrating the
 results into customer data as feature columns, thereby constructing
 comprehensive Persona. Crucially, domain experts are involved in the
 model-building process, selecting and recommending features. These experts
 utilize depth-first search to identify additional, similar feature columns,
 which are then used as target categories for machine learning models. The
 final step involves classification tasks and prediction evaluations. The
 proposed framework demonstrates its effectiveness and generalizability
 through validation on public datasets, increasing the number of potential
 customers by 7.5 times compared to traditional modeling approaches. In case
 studies, the framework outperforms customer lists generated by experts based
 on past experience, yielding 2.4 times more customers, 3.8 times higher
 response rates, and 9 times more total respondents. More importantly, both
 the model-building process and predictive outcomes are interpretable through
 domain knowledge, enabling businesses to transfer experience and expertise,
 thus laying a solid foundation for large language models within the
 industry.",10.2298/csis241130068l,['JournalArticle'],Business; Computer Science
The promise and peril of generative AI for organizational selection and socialization,2025,"James Y. Chu, S. Srivastava",,10.1007/s41469-025-00193-5,['JournalArticle'],Business; Computer Science
Context Attribution with Multi-Armed Bandit Optimization,2025,"Deng Pan, K. Murugesan, Nuno Moniz, Nitesh V. Chawla","Understanding which parts of the retrieved context contribute to a large language model's generated answer is essential for building interpretable and trustworthy generative QA systems. We propose a novel framework that formulates context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each context segment is treated as a bandit arm, and we employ Combinatorial Thompson Sampling (CTS) to efficiently explore the exponentially large space of context subsets under a limited query budget. Our method defines a reward function based on normalized token likelihoods, capturing how well a subset of segments supports the original model response. Unlike traditional perturbation-based attribution methods such as SHAP, which sample subsets uniformly and incur high computational costs, our approach adaptively balances exploration and exploitation by leveraging posterior estimates of segment relevance. This leads to substantially improved query efficiency while maintaining high attribution fidelity. Extensive experiments on diverse datasets and LLMs demonstrate that our method achieves competitive attribution quality with fewer model queries.",10.48550/arXiv.2506.19977,['JournalArticle'],Computer Science
Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations,2025,"Martin Benderoth, Patrick Gebhard, Christian Keller, C. B. Nakhosteen, Stefan Schaffer, T. Schneeberger","This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverbal, nonverbal), simulating expert roles in various organizational contexts. SIAs engage employees in empathic, natural-language dialogues, helping them externalize insights that might otherwise remain unspoken. Their success hinges on building trust, as employees are often hesitant to share tacit knowledge without assurance of confidentiality and appreciation. Key technologies include Large Language Models (LLMs) for generating context-relevant dialogue, Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and Chain-of-Thought (CoT) prompting to guide structured reflection. These enable SIAs to actively elicit knowledge, uncover implicit assumptions, and connect insights to broader organizational contexts. Potential applications span onboarding, where SIAs support personalized guidance and introductions, and knowledge retention, where they conduct structured interviews with retiring experts to capture heuristics behind decisions. Success depends on addressing ethical and operational challenges such as data privacy, algorithmic bias, and resistance to AI. Transparency, robust validation, and a culture of trust are essential to mitigate these risks.",10.48550/arXiv.2508.19942,['JournalArticle'],Business; Computer Science
A Tool for Researching how AI Affects Information Seeking,2025,"Alamir Novin, Georgia Towne","Information Scholars are calling for more standardization of the metrics quantifying the information-seeking processes (ISP). Standardizing measures for online searching is challenging due to search pages including the dynamic nature of generative AI adjacent to personalized search results. AI-chatbots occasionally use queries as prompts to generate text using large-language models (LLM). Search engines also occasionally include Retrieval-Augmented Generation (RAG) results adjacent to personalized search results. This interface hinders the level of control in experiments when two similar queries are treated as prompts towards very different results and text-generations. In addition, experiment findings are less comparable when researchers use different combinations of software for data log collections (e.g., click-throughs and time-on-site), analysis (e.g., NVivo and Qualtrics), and visualizations. These different combinations of platforms can lead to different statistical results or even obnubilate the research metrics. To address these discrepancies, this study introduces a method with three key automations to assist with standardizing ISP experiments on search pages with LLMs and RAG: 1) Controls: A system designed for Randomized Control Trial Experiments and A/B Tests by simulating Googles search AI and algorithms; 2) Data Collection: Automatic participant log data collection; 3) Analysis and Visualization: Presentation of statistically significant differences in both quantitative and qualitative data, with results visualized alongside proper formatting (e.g., APA citations of the p-value). Preliminary feedback from Information Scholars has been promising, with 86% expressing sufficient value in the proposed method to consider using the software for their future projects.",10.1145/3698204.3716477,"['Book', 'JournalArticle']",Computer Science
Towards Trustworthy Deep Learning,2024,Tsui-Wei Weng,"Deep neural networks (DNNs) have achieved unprecedented success across many scientific and engineering fields in the last decades. Despite its empirical success, unfortunately, recent studies have shown that there are various failure modes and blindspots in DNN models which may result in unexpected serious failures and potential harms, e.g. the existence of adversarial examples and small perturbations. This is not acceptable especially for safety critical and high stakes applications in the real-world, including healthcare, self-driving cars, aircraft control systems, hiring and malware detection protocols. Moreover, it has been challenging to understand why and when DNNs will fail due to their complicated structures and black-box behaviors. Lacking interpretability is one critical issue that may seriously hinder the deployment of DNNs in high-stake applications, which need interpretability to trust the prediction, to understand potential failures, and to be able to mitigate harms and eliminate biases in the model.


To make DNNs trustworthy and reliable for deployment, it is necessary and urgent to develop methods and tools that can (i) quantify and improve their robustness against adversarial and natural perturbations, and (ii) understand their underlying behaviors and further correct errors to prevent injuries and damages. These are the important first steps to enable Trustworthy AI and Trustworthy Machine Learning. In this talk, I will survey a series of research efforts in my lab contributed to tackling the grand challenges in (i) and (ii). In the first part of my talk, I will overview our research effort in Robust Machine Learning since 2017, where we have proposed the first attack-agnostic robustness evaluation metric, the first efficient robustness certification algorithms for various types of perturbations, and efficient robust learning algorithms across supervised learning to deep reinforcement learning. 


In the second part of my talk, I will survey a series of exciting results in my lab on accelerating interpretable machine learning and explainable AI. Specifically, I will show how we could bring interpretability into deep learning by leveraging recent advances in multi-modal models. I'll present recent works in our group on automatically dissecting neural networks with open vocabulary concepts, designing interpretable neural networks without concept labels, and briefly overview our recent efforts on demystifying black-box DNN training process, automated neuron explanations for Large Language Models and the first robustness evaluation of a family of neuron-level interpretation techniques.",10.1609/aaai.v38i20.30298,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering
Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble,2025,"Bingchen Wang, Zi-Yu Khoo, K. H. Low","Large language models (LLMs) have demonstrated promise in emulating human-like responses across a wide range of tasks. In this paper, we propose a novel alignment framework that treats LLMs as agent proxies for human survey respondents, affording a cost-effective and steerable solution to two pressing challenges in the social sciences: the rising cost of survey deployment and the growing demographic imbalance in survey response data. Drawing inspiration from the theory of revealed preference, we formulate alignment as a two-stage problem: constructing diverse agent personas called endowments that simulate plausible respondent profiles, and selecting a representative subset to approximate a ground-truth population based on observed data. To implement the paradigm, we introduce P2P, a system that steers LLM agents toward representative behavioral patterns using structured prompt engineering, entropy-based sampling, and regression-based selection. Unlike personalization-heavy approaches, our alignment approach is demographic-agnostic and relies only on aggregate survey results, offering better generalizability and parsimony. Beyond improving data efficiency in social science research, our framework offers a testbed for studying the operationalization of pluralistic alignment. We demonstrate the efficacy of our approach on real-world opinion survey datasets, showing that our aligned agent populations can reproduce aggregate response patterns with high fidelity and exhibit substantial response diversity, even without demographic conditioning.",10.48550/arXiv.2509.11311,"['JournalArticle', 'Review']",Computer Science
Leveraging Behavioural Biometrics for Predicting Employee Engagement and Retention: A Theoretical Framework for Successful Implementation,2025,Pankaj Kumar,"In today's dynamic work environments, employee engagement and retention are very much important for organizational success. Traditional assessment tools like surveys and performance reviews are now very much insufficient in capturing the real-time behavioural changes of the employees. This paper discusses the use of behavioural biometrics which is a study that monitors human digital behaviour patterns as a predictive tool for analysing their other behaviour, which can also be linked to study the employee engagement and retention. Behavioural biometric measures like the keystroke dynamics, mouse movement, voice tone, and screen navigation, offer a non-intrusive, continuous monitoring of user behaviour. By carefully analysing all the behavioural patterns through the application of artificial intelligence and machine learning frameworks, organizations can proactively identify the signs of disengagement, interest to work, or the intent to leave of their employees. This paper proposes a step-wise theoretical framework that is integrating the behavioural biometrics with human resource behaviour analytics for making the predictive insights. Furthermore, the study in this research paper also outlines the implementation challenges such as privacy concerns, data ethics, and system integration, and their possible solutions or way-forward. This paper also contributes to the theoretical advancement in the Individual Behavioural study, HR Tech and organizational behaviour study and provides a foundational model for the organizations which are aiming to apply behavioural analytics into employee management. Ultimately, utilizing the concepts of behavioural biometrics will also redefine the employee engagement strategy by supporting the shift from a reactive management to making it more of a proactive approach, which will enable the organizations to retain the top performing talent and foster a culture of continuous engagement.",10.36948/ijfmr.2025.v07i04.51457,"['JournalArticle', 'Review']",Business; Computer Science; Psychology
Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model,2025,"Dizhan Xue, Jing Cui, Shengsheng Qian, Chuanrui Hu, Changsheng Xu","Short-video platforms have gained immense popularity, captivating the interest of millions, if not billions, of users globally. Recently, researchers have highlighted the significance of analyzing the propagation of short-videos, which typically involves discovering commercial values, public opinions, user behaviors, etc. This paper proposes a new Short-video Propagation Influence Rating (SPIR) task and aims to promote SPIR from both the dataset and method perspectives. First, we propose a new Cross-platform Short-Video (XS-Video) dataset, which aims to provide a large-scale and real-world short-video propagation network across various platforms to facilitate the research on short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926 samples, and 535 topics across 5 biggest Chinese platforms, annotated with the propagation influence from level 0 to 9. To the best of our knowledge, this is the first large-scale short-video dataset that contains cross-platform data or provides all of the views, likes, shares, collects, fans, comments, and comment content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a novel three-stage training mechanism, to bridge heterogeneous graph-structured data with the powerful reasoning ability and knowledge of Large Language Models (LLMs). Our NetGPT can comprehend and analyze the short-video propagation graph, enabling it to predict the long-term propagation influence of short-videos. Comprehensive experimental results evaluated by both classification and regression metrics on our XS-Video dataset indicate the superiority of our method for SPIR.",10.48550/arXiv.2503.23746,['JournalArticle'],Computer Science
An Intelligent Docent System with a Small Large Language Model (sLLM) Based on Retrieval-Augmented Generation (RAG),2025,"Taemoon Jung, Inwhee Joe","This study designed and empirically evaluated a method to enhance information accessibility for museum and art gallery visitors using a small Large Language Model (sLLM) based on the Retrieval-Augmented Generation (RAG) framework. Over 199,000 exhibition descriptions were collected and refined, and a question-answering dataset consisting of 102,000 pairs reflecting user personas was constructed to develop DocentGemma, a domain-optimized language model. This model was fine-tuned through Low-Rank Adaptation (LoRA) based on Googles Gemma2-9B and integrated with FAISS and OpenSearch-based document retrieval systems within the LangChain framework. Performance evaluation was conducted using a dedicated Q&A benchmark for the docent domain, comparing the model against five commercial and open-source LLMs (including GPT-3.5 Turbo, LLaMA3.3-70B, and Gemma2-9B). DocentGemma achieved an accuracy of 85.55% and a perplexity of 3.78, demonstrating competitive performance in language generation and response accuracy within the domain-specific context. To enhance retrieval relevance, a Spatio-Contextual Retriever (SC-Retriever) was introduced, which combines semantic similarity and spatial proximity based on the users query and location. An ablation study confirmed that integrating both modalities improved retrieval quality, with the SC-Retriever achieving a recall@1 of 53.45% and a Mean Reciprocal Rank (MRR) of 68.12, representing a 17.5 20% gain in search accuracy compared to baseline models such as GTE and SpatialNN. System performance was further validated through field deployment at three major exhibition venues in Seoul (the Seoul History Museum, the Hwan-ki Museum, and the Hanseong Baekje Museum). A user test involving 110 participants indicated high response credibility and an average satisfaction score of 4.24. To ensure accessibility, the system supports various output formats, including multilingual speech and subtitles. This work illustrates a practical application of integrating LLM-based conversational capabilities into traditional docent services and suggests potential for further development toward location-aware interactive systems and AI-driven cultural content services.",10.3390/app15179398,['JournalArticle'],Art; Computer Science
Infinite Leagues Under the Sea: Photorealistic 3D Underwater Terrain Generation by Latent Fractal Diffusion Models,2025,"Tianyi Zhang, Weiming Zhi, Joshua Mangelson, Matthew Johnson-Roberson","This paper tackles the problem of generating representations of underwater 3D terrain. Off-the-shelf generative models, trained on Internet-scale data but not on specialized underwater images, exhibit downgraded realism, as images of the seafloor are relatively uncommon. To this end, we introduce DreamSea, a generative model to generate hyper-realistic underwater scenes. DreamSea is trained on real-world image databases collected from underwater robot surveys. Images from these surveys contain massive real seafloor observations and covering large areas, but are prone to noise and artifacts from the real world. We extract 3D geometry and semantics from the data with visual foundation models, and train a diffusion model that generates realistic seafloor images in RGBD channels, conditioned on novel fractal distribution-based latent embeddings. We then fuse the generated images into a 3D map, building a 3DGS model supervised by 2D diffusion priors which allows photorealistic novel view rendering. DreamSea is rigorously evaluated, demonstrating the ability to robustly generate large-scale underwater scenes that are consistent, diverse, and photorealistic. Our work drives impact in multiple domains, spanning filming, gaming, and robot simulation.",10.48550/arXiv.2503.06784,"['JournalArticle', 'Review']",Computer Science; Engineering; Environmental Science
Evaluating Silicon Sampling: LLM Accuracy in Simulating Public Opinion on Facial Recognition Technology,2025,Charles Ma,"Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like responses, prompting exploration into their potential for social science research. ""Silicon sampling,"" a method where LLMs are queried after being prompted with personas, has emerged as a possible alternative to traditional survey methods, especially given the increasing challenges associated with declining survey participation rates and rising costs. However, the accuracy of silicon sampling remains a subject of debate.This study examines the effectiveness of silicon sampling in replicating survey results on public acceptance toward facial recognition technology (FRT). The research builds upon the work of Kostka et al. (2021)*, who conducted a multinational survey across Germany, China, the United Kingdom, and the United States, analyzing public opinion on FRT alongside socio-demographic data and key contextual factors, including perceived consequences, utility, and reliability of the technology.The study addresses two research questions: (1) Can LLMs simulate an individual's surveyed opinions on FRT when prompted with a persona using only demographic information? (2) Can LLMs simulate an individual's surveyed opinions on FRT when prompted with a persona using both demographic and relevant contextual information?The research employs three LLMs: GPT-4o, Claude 3.5, and the open-source DeepSeek V3. It compares the LLM-generated responses to the original survey data, assessing the degree of alignment under three prompting conditions: demographic-only, contextual information-only, and demographic-plus-contextual information. To initially evaluate alignment, the differences between the percentages of each level of FRT acceptance were calculated. Additional metrics such as accuracy, mean absolute error, and F1-Scores are included in the extended paper. Preliminary results from GPT-4o and Claude 3.5 suggest that prompts incorporating both demographic and contextual information yield simulated responses that closely align with the original survey data. Consistent with prior findings, prompts based solely on demographics produce significantly less accurate results. By comparing closed-source models (GPT and Claude) with an open-source alternative (DeepSeek), the study also examines potential differences in reliability between these types of models. Multiple runs for each model are included to assess output variability and reproducibility within and between models.By demonstrating the importance of incorporating relevant contextual information into prompts, the study provides valuable insights into optimizing the silicon sampling technique and the accuracy of LLM-generated responses in survey simulations. Ultimately, this investigation advances the understanding of the capabilities and limitations of LLMs as tools for studying public opinion, particularly in the context of technology acceptance, and informs the development of best practices for utilizing silicon sampling in future research. The results suggest that, with careful prompting, silicon sampling can offer a viable and cost-effective alternative to traditional survey methods, potentially mitigating challenges related to declining response rates and increasing costs.*Kostka, G., Steinacker, L., & Meckel, M. (2021). Between security and convenience: Facial recognition technology in the eyes of citizens in China, Germany, the United Kingdom, and the United States. Public Understanding of Science, 30(6), 671690. https://doi.org/10.1177/09636625211001555",10.54941/ahfe1006738,['Review'],Computer Science; Political Science; Psychology
Classification of Greek News Articles by Text Type Using Open Large Language Models,2025,"Ismini Veneti, Iraklis Varlamis, Georgios Th. Papadopoulos","The automatic classification of Greek news articles presents unique challenges due to the relatively low availability of annotated datasets and the under-representation of the Greek language in large-scale language models. While previous studies have primarily focused on topic-based classification, this work investigates the classification of articles based on their text type (e.g. articles, letters, reviews, interviews, etc.).We evaluate the zero-shot classification capabilities of open large language models (LLMs), including LLaMA 3.1, Aya, and the Greek-LLM Meltemi, comparing their performance against a supervised approach that employs a fine-tuned BERT baseline for Greek. Our experiments explore various prompt engineering techniques that incorporate structured output and LLM-based reasoning, to improve classification performance.Results indicate that while generic LLMs show some promise, their classification errors are often skewed by prediction biases, with confusion matrices revealing difficulties in differentiating between certain categories. Moreover, the fine-tuned Greek BERT model significantly outperforms the zero-shot LLMs across all evaluation metrics, reinforcing the importance of domain-specific training for low-resource languages. Our findings highlight the need for further fine-tuning and dataset expansion to improve LLM classification reliability in Greek text-processing tasks.",10.1109/EEITE65381.2025.11166401,"['Conference', 'Review']",Computer Science; Linguistics
Analyzing the Impact of Various Indexing Techniques on Retrieval-Augmented Generation (RAG) Performance in Closed-Domain Question Answering,2025,"Buhari Bashir, Ismail Ahmad Ibrahim, Ameenu Abdulhameed Rabiu, Salihu Abdullahi Salihu Abdullahi","Rapid and recent advancements in large language models (LLMs) have become the driving force of many Natural Language Processing (NLP) applications, revolutionizing tasks such as Question Answering (QA) with the aim of improving human-computer interaction. LLMs have been explored to be capable of predicting reasonably good answers for provided questions using its ability to memorize information seen during training of the model. But to what extent can these models remember training data? Retrieval-Augmented Generation (RAG) augments LLMs with knowledge to avoid relying on their memorization capabilities by combining generative capabilities of LLMs with retrieval-based methods to enhance answer accuracy and relevance of answers. With or without RAG, the aim is to mitigate the generation of plausible but non factual responses by LLMs which is hallucination. RAG has demonstrated significant performance in reducing hallucination and improving accuracy, speed and relevance of LLM generation in QA tasks. This research evaluates how different indexing methods such as BM25, DPR, and hybrid techniques affect RAG system performance in closed-domain QA, examining accuracy of retrieval and generation using performance metrics such as hit rate, precision, faithfulness and f1-score. The study also considers passage length variability and domainspecific adaptation across multiple datasets and domains with the hypothesis that focusing on a single domain can enhance system performance. Experiments are carried out on multiple datasets within both closed and open domains, using pre-trained models. This study explores optimal indexing strategies for RAG systems, balancing accuracy and efficiency. Results show that increasing passage length and retrieved samples improves hit rate and recall but reduces precision. The best performance achieved was 91% Hit-rate@9 and 93% relevancy using the BioASQ dataset.",10.57233/ijsgs.v11i2.872,['JournalArticle'],Computer Science
Modelling the Identification and Classification of Military Air Objects Based on Machine Learning,2024,"Dmytro Uhryn, Danylo Shved","The article is devoted to the urgent problem of developing systems for intelligent identification of military aircraft based on artificial intelligence, machine learning and deep learning technologies as an important task for ensuring national security and increasing the efficiency of military operations. The necessity of such systems capable of automatically accurately recognizing and classifying aircraft in images is substantiated. Their advantages over traditional methods are highlighted: higher performance, speed, accuracy, elimination of the human factor. The critical importance of implementing innovative deep learning solutions to identify threats and increase the effectiveness of military operations is emphasised. Modern methods and tools for object recognition in visual data are analysed. The proposed method of collecting and pre-processing data for model training is described in detail, and a diagram of the key stages of developing a high-precision recognition system based on YOLOv8 is presented. The process of forming a high-quality training dataset from public sources and own aerial survey/satellite images using Roboflow for object annotation, creating subsets for training/validation/testing in the YOLO format is presented. Satisfactory results of fast recognition of military aircraft with high classification probabilities are demonstrated. A comparative analysis of the YOLOv8, R-CNN and GPT-4 models is presented, which shows the advantage of YOLOv8 in terms of forecasting accuracy and speed. The created model management system for setting hyperparameters, selecting object categories, and launching training/prediction processes is described. The results of testing the trained YOLOv8 are presented, which confirmed its high efficiency in accurately detecting targets in difficult conditions due to advanced deep learning algorithms. The optimality of YOLOv8 for solving the problem of military aircraft identification is substantiated.",10.31861/sisiot2024.1.01001,"['JournalArticle', 'Review']",Computer Science; Engineering
Personality Prediction from Life Stories using Language Models,2025,"Rasiq Hussain, Jerry Ma, Rithik Khandelwal, Joshua Oltmanns, Mehak Gupta","Natural Language Processing (NLP) offers new avenues for personality assessment by leveraging rich, open-ended text, moving beyond traditional questionnaires. In this study, we address the challenge of modeling long narrative interview where each exceeds 2000 tokens so as to predict Five-Factor Model (FFM) personality traits. We propose a two-step approach: first, we extract contextual embeddings using sliding-window fine-tuning of pretrained language models; then, we apply Recurrent Neural Networks (RNNs) with attention mechanisms to integrate long-range dependencies and enhance interpretability. This hybrid method effectively bridges the strengths of pretrained transformers and sequence modeling to handle long-context data. Through ablation studies and comparisons with state-of-the-art long-context models such as LLaMA and Longformer, we demonstrate improvements in prediction accuracy, efficiency, and interpretability. Our results highlight the potential of combining language-based features with long-context modeling to advance personality assessment from life narratives.",10.48550/arXiv.2506.19258,['JournalArticle'],Computer Science; Psychology
Generating high-quality and diverse synthetic datasets with large language models: A survey,2025,Abinandaraj Rajendran,"Large Language Models (LLMs) are increasingly leveraged to generate synthetic datasets that overcome challenges in real-world data collection, including privacy risks, imbalance, and scarcity. This paper surveys recent developments in LLM-based synthetic data generation, emphasizing techniques that improve diversity, task alignment, and reliabilitycrucial factors in high-stakes domains such as predictive maintenance. We categorize state-of-the-art approaches into four methodological pillars: prompt engineering, multi-step generation pipelines, quality control through data curation, and rigorous evaluation methods. Structured generation workflows and controlled prompting strategies significantly enhance output coherence and domain relevance, while self-correction mechanisms and diversity-aware metrics contribute to higher dataset fidelity. Despite progress, open challenges persist, including bias propagation, limited generalization across tasks and modalities, and the need for robust ethical safeguards. We outline promising future directionssuch as integrating external knowledge, expanding to multilingual and multimodal settings, and fostering human-AI collaborationfor advancing synthetic data generation using LLMs.",10.30574/wjaets.2025.15.2.0652,"['JournalArticle', 'Review']",Computer Science; Linguistics
Open-Ended NPC Dialogue Favors Casual Players: A Pilot Comparison of Three LLM-Driven Dialogue Systems,2025,"Rasmus Ploug, Emil Rimer, Anthon Kristian Skov Petersen, Marco Scirea","Non-player character (NPC) dialogue plays a crucial role in shaping the player experience in narrativedriven video games, influencing agency, immersion and story engagement. Despite the recent advancements in large language models (LLMs) for dynamic dialogue generation, few empirical studies have compared their impact across different dialogue system designs. This pilot study explores how LLM-driven dialogue systems affect the player experience using a custom-developed role-playing game (RPG) featuring four different dialogue designs; static control (CV), rephrase (A), hybrid (B) and fully open-ended (C). Behavioral data and post-game questionnaires were collected from 64 participants. Results indicate that fully open-ended dialogues led to significantly longer dialogue interactions and higher overall engagement, particularly among casual players, with the survey feedback highlighting its immersive and natural tone. These findings suggest that fully open-ended LLM-based dialogue in video games can enhance narrative depth and player involvement.",10.1109/CoG64752.2025.11114150,"['JournalArticle', 'Conference', 'Review']",Computer Science
Development of an intelligent system for automatic response to customer reviews on marketplaces with using LLM,2024,"Dmitrii Sergeevich Dobrovolskii, Vitaliy Sergeevich Oleynikov, Ekaterina Sergeevna Rats","The article discusses an approach to the development of an intelligent system designed for automatic responses to customer reviews on marketplaces, utilizing advanced artificial intelligence technologies, specifically large language models (LLMs). The relevance of this work is driven by the growing need for automating customer interactions for e-commerce companies and reducing the time spent processing a large volume of reviews, which requires the application of high-tech solutions. The problem faced by companies is the need to respond quickly and effectively to a large number of reviews, which in a traditional setup demands significant effort from support staff. The methods include data collection and preprocessing, as well as the use of the GPT-4 large language model based on transformer architecture for sentiment analysis and generating personalized responses. The model analyzes the emotional tone of reviews and formulates responses that consider context, tone, and subtext, making communication with customers more natural and effective. The research results demonstrated a significant improvement in customer interaction quality, a reduction in review processing time, and a decrease in operator workload. The conclusions confirm that implementing this system in a company can significantly enhance the quality of customer service.",10.18413/2518-1092-2024-9-4-0-9,"['JournalArticle', 'Review']",Computer Science
The Impact of Geopolitical Risks on Equity Markets and Financial Stress: A Comparative Analysis of Emerging and Advanced Economies,2025,Amirali Nasouri,": Purpose: This study aims to comprehensively analyze and compare the impacts of geopolitical risks on equity markets and financial stress in both emerging and advanced economies. We seek to understand how GPR affects market returns, volatility, and overall financial stability across different economic landscapes. Design/Methodology/Approach: We employ a multi-faceted approach utilizing various econometric techniques. These include the GARCH-MIDAS model for volatility prediction, lead-lag regression, Markov regime-switching model, and panel quantile estimation. We use the geopolitical threats (GPT) index of Caldara and Iacoviello and consider both composite and decomposed GPR indices. Our analysis covers major emerging economies and the G7 countries, using daily stock returns and monthly GPR data. Findings: Our findings reveal significant disparities in how geopolitical risks impact emerging and advanced economies. The U.S. equity market, particularly in the information technology and financial sectors, shows positive returns during high geopolitical threats. In contrast, emerging markets exhibit increased stock market volatility in response to GPR. Financial stress in emerging economies intensifies with increased GPR, especially when financial conditions are already strained. Advanced economies primarily witness GPR effects in their stock markets. Practical Implications: The study also identifies optimal hedging strategies, such as between the U.S. market and gold, during periods of high geopolitical threats. These results have important implications for investment strategies and policy decisions in the face of geopolitical uncertainties. Originality value: Geopolitical risks (GPR) significantly influence global financial markets. However, their impacts differ across economies and market segments. Previous studies have shown varied effects of GPR on asset prices, stock market volatility, and financial stress in different economic contexts.",10.35808/ijeba/873,['JournalArticle'],Business; Economics; Political Science
Automatic Toxicity Evaluation for Human-LLM Conversations in Flexible Manufacturing System With Duplex Fine-Tuned LLMs,2025,"Bo Wang, Chao Wang, Zan Zhou, Yi Sun, Shujie Yang, Yonghui Huang, Yuning Cui, Yuchen Wang, Yasser D. Al-Otaibi, Ali Kashif Bashir, Changqiao Xu","Flexible manufacturing systems (FMS), empowered by the Industrial Internet of Things (IIoT), have become a cornerstone of Industry 6.0 by enabling dynamic production adaptation, real-time equipment monitoring, and intelligent scheduling. As these systems increasingly incorporate large language models (LLMs) to support functions such as knowledge querying, decision assistance, and predictive maintenance, ensuring the safety and reliability of human-LLM conversations has become a pressing concern. Specifically, LLMs may generate toxic, biased, or privacy-violating outputs when interacting with sensitive IIoT data and production logic, potentially compromising operational safety. To address this challenge, we propose AugLLMSen, an automated toxicity evaluation framework tailored to the IIoT-driven FMS context. AugLLMSen integrates a question automatic expansion mechanism (Q-Judge) and an output toxicity evaluation model (O-Judge) into a closed-loop pipeline, enabling large-scale assessment of LLM safety across diverse industrial scenarios. Experimental results on open- and closed-source LLMs demonstrate the effectiveness and accuracy of our approach in identifying toxic responses and guiding safe deployment of LLMs in flexible manufacturing environments.",10.1109/JIOT.2025.3592453,['JournalArticle'],Computer Science; Engineering; Environmental Science
Exploring LLMs as a means for conducting Bias-free technical Interviews,2024,"Manodnya K H, Dishita Mahendrakar, Aadithya Krishna Vamshi V, Animesh Giri, Pranay B V","We present a novel approach to conducting technical interviews using a conversational AI agent. Traditional technical interviews have constraints like interviewer biases, human errors, and subjective evaluations. To address these issues, we propose a conversational AI agent that conducts and assesses technical interviews in a fair and objective manner. The conversational AI agent uses natural language processing (NLP) techniques and machine learning algorithms to simulate a real-time conversation between the interviewer and the interviewee. The agent is designed to ask relevant technical questions and evaluate the candidate's responses based on predefined criteria. We conducted experiments to evaluate the effectiveness of the conversational AI agent in conducting technical interviews. The results showed that the agent was able to conduct interviews efficiently, provide objective evaluations, and eliminate biases. Moreover, the agent was discovered to be reliable and consistent in its assessments, turning it into a useful resource for recruiting managers and recruiters.",10.1109/ISML60050.2024.11007345,['Conference'],Computer Science
The method of synthetic focus groups in the context of digital transformation of sociological research,2025,"A. G. Filipova, E. E. Abrosimova, O. G. Zubova","
 The subject of the research is the use of synthetic focus groups (SFG), created using large language models (LLM), within the framework of the digital transformation of sociological analysis. The object of the research is innovative methods for collecting and interpreting qualitative data, aimed at simulating group discussions without the participation of real respondents. The article discusses the heuristic potential of SFG, the possibilities of applying the method in conditions of limited access to respondents, and its relevance for studying attitudes toward artificial intelligence in the field of higher education. Special attention is paid to the ways of organizing interaction with language models through a system of prompts, the formation of participant roles, and the analysis of the obtained discursive positions. The article also addresses the ethical and methodological challenges arising from the use of synthetic participants for research purposes. The methodology combines theoretical analysis of scientific publications with practical modeling of SFG, implemented through sequential prompts to language models and subsequent interpretation of the generated data. The scientific novelty of the work lies in the testing of the synthetic focus group (SFG) method as an innovative tool for collecting qualitative information using generative language models (LLM) such as Gemini, Qwen, Llama, Deepseek, and Mistral. The study pays particular attention to modeling SFG based on user prompts aimed at exploring the attitudes of students and faculty toward artificial intelligence in the higher education system. All models used emphasized the methodological limitations of SFG: the possibility of data distortion, the necessity of clearly indicating the synthetic nature of the participants, and the need to complement such studies with traditional methods. Such methods can be useful in situations requiring rapid hypothesis generation, preliminary testing of research scenarios, as well as in educational and expert practice. At the same time, the necessity of a critical approach to the use of SFG is emphasized, especially in the context of the validity and representativeness of the obtained data.
",10.25136/2409-7144.2025.5.74430,['JournalArticle'],Computer Science; Sociology
Agentic LLM Workflows for Personalized User Experience Questionnaire Generation,2024,"Yeonwoo Kim, Junhyeok Lee, Juhyuk Han, Minjae Kim, Howook Lee, Won Hee Lee","Effective user experience (UX) evaluation requires personalized assessment methods that adapt to individual user characteristics and real-time context. This study introduces the User Experience Questionnaire generation workflow using multiple LLMs (UEQ-mLLM), a system that generates tailored questionnaires based on user data collected through a multimodal interactive dashboard. By leveraging user information and states, UEQ-mLLM generates questionnaires that enhance the accuracy and depth of UX evaluations. Comparative analysis against a single LLM-based approach using the G-Eval framework demonstrated a significant performance improvement of 20.62% for UEQ-mLLM. This work highlights the potential of utilizing multiple LLMs to generate effective UX questionnaires and contributes to the advancement of user-centered design methodologies.",10.1109/ICCE-Asia63397.2024.10773955,['Conference'],Computer Science
Simulating Prosocial Behavior and Social Contagion in LLM Agents under Institutional Interventions,2025,"Yujia Zhou, Hexi Wang, Qingyao Ai, Zhen Wu, Yiqun Liu","As large language models (LLMs) increasingly serve as autonomous agents in social contexts, understanding their capacity for prosocial behavior becomes essential. We present ProSim, a simulation framework designed to examine how prosocial behavior emerges, adapts, and erodes in LLM-based agents under diverse social and institutional conditions. The framework comprises four components: individual simulation, scenario simulation, interaction simulation, and intervention simulation. We conduct three progressive studies to evaluate prosocial alignment. First, we show that LLM agents can demonstrate stable and context-sensitive prosocial behavior across diverse scenarios and adapt their responses under normative policy interventions. Second, we find that agents engage in fairness-based third-party punishment and respond systematically to variations in inequity magnitude and enforcement cost. Third, we show that policy-induced inequities suppress prosocial behavior, propagate through social networks, and are mediated by agents' perceptions of unfairness. These findings lay the groundwork for evaluating social alignment and modeling institutional dynamics in agent-driven societies.",10.48550/arXiv.2505.15857,['JournalArticle'],Computer Science; Psychology; Sociology
A Review of Developmental Interpretability in Large Language Models,2025,Ihor Kendiukhov,"This review synthesizes the nascent but critical field of developmental interpretability for Large Language Models. We chart the field's evolution from static, post-hoc analysis of trained models to a dynamic investigation of the training process itself. We begin by surveying the foundational methodologies, including representational probing, causal tracing, and circuit analysis, that enable researchers to deconstruct the learning process. The core of this review examines the developmental arc of LLM capabilities, detailing key findings on the formation and composition of computational circuits, the biphasic nature of knowledge acquisition, the transient dynamics of learning strategies like in-context learning, and the phenomenon of emergent abilities as phase transitions in training. We explore illuminating parallels with human cognitive and linguistic development, which provide valuable conceptual frameworks for understanding LLM learning. Finally, we argue that this developmental perspective is not merely an academic exercise but a cornerstone of proactive AI safety, offering a pathway to predict, monitor, and align the processes by which models acquire their capabilities. We conclude by outlining the grand challenges facing the field, such as scalability and automation, and propose a research agenda for building more transparent, reliable, and beneficial AI systems.",10.48550/arXiv.2508.15841,"['JournalArticle', 'Review']",Computer Science; Linguistics
ASCENT: Autonomous Skill Learning Toward Complex Embodied Tasks With Foundation Models,2025,"Haolin Wu, Yuecheng Liu, Junyi Dong, Heng Zhang, Sitong Mao, Hesheng Wang, Weigang Wu, Shunbo Zhou","Collecting data from simulated scenarios for training robotic skills provides a safer and more controllable alternative to real-world environments. However, it demands considerable effort, including the manual construction of simulation environments, the careful design of tasks, and the challenge of obtaining effective trajectories. These limitations hinder the efficiency of data collection from simulated scenarios. In this paper, we leverage the prior knowledge of Large Language Models (LLMs) and Large Multimodal Models (LMMs) to generate simulated scenarios and embodied tasks. We introduce a novel framework, ASCENT (Autonomous Skill learning toward Complex Embodied tasks with fouNdaTion models), designed to efficiently accomplish these tasks and generate trajectory data. ASCENT features a fully autonomous skill learning mechanism based on AI agent. During task training, the AI agent identifies suitable atomic skills from an atomic skill library to either directly complete the task or serve as an initial policy for further training. Newly acquired atomic skills are subsequently added to the library. To address training failures and enhance efficiency, the AI agent uses an LLM to automatically optimize the skill training process based on feedback received from simulations. Experimental results indicate that the number of training steps required for learning new tasks can be reduced by up to 65.9 %.",10.1109/ICRA55743.2025.11127927,"['JournalArticle', 'Conference']",Computer Science; Engineering
Towards a Proper Evaluation of Automated Conversational Systems,2023,"Abraham Sanders, Mara Schwartz, Albert Ling Sheng Chang, Shannon Briggs, J. Braasch, Dakuo Wang, Mei Si, T. Strzalkowski","Efficient evaluation of dialogue agents is a major problem in conversational AI, with current research still relying largely on human studies for method validation. Recently, there has been a trend toward the use of automatic self-play and bot-bot evaluation as an approximation for human ratings of conversational systems. Such methods promise to alleviate the time and financial costs associated with human evaluation, and current proposed methods show moderate to strong correlation with human judgements. In this study, we further investigate the fitness of end-to-end self-play and bot-bot interaction for dialogue system evaluation. Specifically, we perform a human study to confirm self-play evaluations of a recently proposed agent that implements a GPT-2 based response generator on the Persuasion For Good charity solicitation task. This agent leverages Progression Function (PF) models to predict the evolving acceptability of an ongoing dialogue and uses dialogue rollouts to proactively simulate how candidate responses may impact the future success of the conversation. The agent was evaluated in an automatic self-play setting, using automatic metrics to estimate sentiment and intent to donate in each simulated dialogue. This evaluation indicated that sentiment and intent to donate were higher (p < 0.05) across dialogues involving the progression-aware agents with rollouts, compared to a baseline agent with no rollout-based planning mechanism. To validate the use of self-play in this setting, we follow up by conducting a human evaluation of this same agent on a range of factors including convincingness, aggression, competence, confidence, friendliness, and task utility on the same Persuasion For Good solicitation task. Results show that human users agree with previously reported automatic self-play results with respect to agent sentiment, specifically showing improvement in friendliness and confidence in the experimental condition; however, we also discover that for the same agent, humans reported a lower desire to use it in the future compared to the baseline. We perform a qualitative sentiment analysis of participant feedback to explore possible reasons for this, and discuss implications for self-play and bot-bot interaction as a general framework for evaluating conversational systems.",10.54941/ahfe1003276,nan,Computer Science
A Novel Method based on Large Language Model for MBTI Classification: A Novel MBTI Classification Method,2024,"Pei-Lun Li, Xiaomeng Liu, Yongxing Wang","Personality analysis has a wide and important application in psychology, helping us explain various psychological phenomena and the developmental laws of personality. Accurately identifying personality will contribute to research in fields such as cognitive science, public opinion analysis, and cybersecurity. The most widely used models in the field of personality recognition are the Big Five personality model and MBTI model in trait genre. With the rise of social media and Large Language Model(LLM), massive corpora and deep learning models have been used for personality analysis and have achieved good results. In view of this, this article introduces a novel method based on transformer-based pre-trained language model named mDeBERTa and uses the MBTI-500 and MBTI-1 datasets as examples to demonstrate the advantages of this model over traditional SVM models and BERT models. At the same time, we explore the possibility of using large models and multimodal data for more accurate personality analysis.",10.1145/3675249.3675253,"['Book', 'Conference']",Computer Science; Psychology
CRICKET CONNECT,2025,Dr.M. Shalima Sulthana,"Abstract Cricket ConnectCricket Connect is a dynamic, user-friendly web platform designed to serve as a one-stop destination for all cricket enthusiasts. Built using HTML, CSS, JavaScript, MongoDB and Python, this platform integrates multiple cricket-related features, including a specialized chatbot that answers cricket- specific queries, real-time live scores, IPL updates, and match information.

The system offers user authentication via Login and Signup pages, enabling personalized access and interaction. It also features live app functionality and a chat discussion module where users can engage in real-time conversations about ongoing matches, teams, and player performances.

At the core of the platform is the cricket chatbot using Gemini ai, which delivers fast, relevant responses to questions about cricket rules, schedules, players, and history. The live score module fetches real-time data from external cricket APIs, keeping users updated with the latest developments. The IPL section presents dedicated content, match results, and team statistics in an intuitive and visually engaging layout.

The frontend ensures a smooth and responsive user experience through modern UI/UX practices using HTML, CSS, and JavaScript, while Python handles server-side logic, data integration, and chatbot intelligence.

Cricket Connect aims to create an interactive, real-time, and informative ecosystem that brings cricket fans together through technology.",10.55041/ijsrem50214,['JournalArticle'],Computer Science
AI-Powered Hybrid Recommendation System for E-Commerce,2025,"Rakesh Kumar M, S. S, Shriram Kumar A N, Kumar P","Personalization is becoming crucial in the rapidly evolving world of e-commerce in order to boost user engagement and sales. The cold-start problem, data sparsity, and providing static and repetitive recommendations that dont adapt to changing tastes are the main issues that traditional recommender systems face as the volume of items and interactions increases. Inappropriate suggestions and ineffective shopping experiences cause annoyance. In the midst of these challenges, this paper proposes a hybrid form of recommender system that combines collaborative filtering with content-based filtering, utilizing the Llama Index, CNNs in image search functionality, and VADER to determine sentiment. Instead of analyzing only patterns in user behavior for collaborative filtering, this system suggests products liked by similar users. It lists product attributes for content-based filtering, which can help recommend products the user has already interacted with. This strategy will ensure a greater level of accuracy and diversity in the recommendations by combining multiple approaches into one. In addition, the CNN-based image searching feature can enhance personalization by suggesting visually appealing products according to user preferences, especially in visually-driven categories like fashion and home decor. To ensure that consumers receive only top-notch recommendations, this system also implements a sophisticated sentiment analysis-based filtering system that favors products with favorable ratings. One of the major additions to this system is a smart chatbot that enables real-time interaction, product availability inquiries, and immediate responses based on a current database. This chatbot not only simplifies the shopping process but also dynamically adjusts to user questions, further enhancing recommendation accuracy. It makes shopping more enjoyable, increases customer satisfaction, and boosts sales conversion rates.",10.1109/ICAISS61471.2025.11042218,['Conference'],Business; Computer Science
AI Guarded Code Editor: Verifying Authentic Human Input,2025,"Aditi Raghunandan, Aishwarya A G, Mohammad Adnan Khan, Nishant S, K. Panimozhi","AI Tools such as ChatGPT, GPT-4 and GitHub Copilot change the very nature of how developers build software by greatly helping in the quick generation of code. While this proves to be efficient for professionals, it poses challenges in the competitive programming space. Generative AI is greatly misused by candidates in coding tests and online interviews. We will design a guarded code editor that detects the probability of usage of GenAI in code submissions with specialized AI detection models that use unique metrics, such as long AI-generated keywords, unusual number of utility functions, and very verbose comments that have been shown to be associated with usual AI-generated content. We will focus on stylistic and structural markers such as redundant comments, too general or too redundant variable names, and predictable formatting to realize a lightweight and scalable detection framework that fits into a user-friendly code editor. Our system also includes mechanisms for immediate feedback that inform the users on the reasons for differences in AI-generated and human-written code, thereby promoting better integrity of code in their own practices. This solution addresses a few critical concerns identified in some recent work that detection tools have limitations to the availability of responding to a domain-specific method responsive to how it would likely be combated using misapplication of the AI system. In other words, with a code editor that does not accept the direct submission of AI-generated code, the proposed solution will most definitely prove effective in both bringing forward practicality and ethical means while ensuring originality and improvement of quality in academic and competitive programming.",10.1109/COMP-SIF65618.2025.10969913,['Conference'],Computer Science
Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty,2025,"Yoshee Jain, John Hollander, Amber He, Sunny Tang, Liang Zhang, John Sabatini","Reading comprehension is a key for individual success, yet the assessment of question difficulty remains challenging due to the extensive human annotation and large-scale testing required by traditional methods such as linguistic analysis and Item Response Theory (IRT). While these robust approaches provide valuable insights, their scalability is limited. There is potential for Large Language Models (LLMs) to automate question difficulty estimation; however, this area remains underexplored. Our study investigates the effectiveness of LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of reading comprehension questions using the Study Aid and Reading Assessment (SARA) dataset. We evaluated both the accuracy of the models in answering comprehension questions and their ability to classify difficulty levels as defined by IRT. The results indicate that, while the models yield difficulty estimates that align meaningfully with derived IRT parameters, there are notable differences in their sensitivity to extreme item characteristics. These findings suggest that LLMs can serve as the scalable method for automated difficulty assessment, particularly in dynamic interactions between learners and Adaptive Instructional Systems (AIS), bridging the gap between traditional psychometric techniques and modern AIS for reading comprehension and paving the way for more adaptive and personalized educational assessments.",10.48550/arXiv.2502.17785,['JournalArticle'],Computer Science; Linguistics
Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards,2025,"Ting Yang, Li Chen, Huimin Wang","Emotional Support Conversation (ESC) systems aim to alleviate users'emotional difficulties and provide long-term, systematic support for emotional well-being. However, most large language model (LLM)-based ESC systems rely on predefined strategies, which limits their effectiveness in complex, real-life scenarios. To enable flexible responses to diverse emotional problem scenarios, this paper introduces a novel end-to-end framework (RLFF-ESC) that directly learns enduring emotionally supportive response skills using reinforcement learning. For sustained emotional support, we first employ an LLM-based multi-agent mechanism to simulate future dialogue trajectories and collect future-oriented rewards. We then train a future-oriented reward model, which is subsequently used to train the emotional support policy model. Additionally, we incorporate an explicit reasoning process during response generation to further enhance the quality, relevance, and contextual appropriateness of the system's responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two public ESC datasets. Experimental results demonstrate that RLFF-ESC consistently outperforms existing baselines in terms of goal completion and response quality.",10.48550/arXiv.2508.12935,['JournalArticle'],Computer Science
Simulation Analysis of Low PAPR FFT-based OFDM Through Nakagami Channel,2022,"Vincent Vincent, A. Suhartomo","In response to the inefficiency of power amplifier in conventional Orthogonal Frequency Division Multiplexing (OFDM) system, a low Peak-to-Average Power Ratio (PAPR) scenario can be obtained by implementing several PAPR reduction techniques. Partial Transmit Sequence (PTS) is a well-known instance to provide zero distortion for the system, while clipping is one of the simplest methods but it is sacrificing the error rate performance. In this work, Authors focused on analyzing the implementation of PTS and Palm Date Leaf clipping technique on OFDM system with 16-, 64-, 256-, and 1024-QAM under various Nakagami channel. In addition, the High-Power Amplifier (HPA) was also considered by using Solid State Power Amplifier (SSPA) model. It was found that linear HPA serves as the lower boundary for non-linear HPA, and system under high saturation level performs similarly. In physical properties, high saturation level is equivalent to higher power consumption in amplifier. Thus, optimal level was obtained by considering taking the one with moderate BER performance. After the systems PAPR was reduced, the optimum saturation level of HPA was found to be 5 dB, 8 dB, 9 dB, and 11 dB for each Quadrature Amplitude Modulation (QAM) under Nakagami channel with m value of 1.5. However, this value was found to be ample for lower m value such as 1 or 0.7. In this case, the system required lower saturation level on the HPA for Nakagami channel with lower m value.",10.33021/icsecc.v1i1.4163,"['JournalArticle', 'Conference']",Computer Science; Engineering
Knowledge-Augmented Intelligent Bot Framework for Enabling Accessible Design,2025,"Diya Saha, Tirthankar Dasgupta, Manjira Sinha, Sumeet Agrawal, Shreedhar Vellayaraj, Charudatta Jadhav","This paper proposes the development of a WCAG-compliant chatbot capable of generating multimodal content to enhance usability for all users. While LLM-based chatbots excel in generating varied responses, they often struggle with ambiguous or incomplete queries, leading to misaligned outputs. We introduce a framework that formulates domain-specific, persona-driven follow-up questions to clarify ambiguities, utilizing knowledge graphs and human feedback. The system refines queries before generating responses by employing a domain-Specific Multilayer Hierarchical Relational Graph (MHRG) to model user intent. Our preliminary evaluations indicate that the Accessibility Bot improves response relevance and quality as compared to existing techniques.",10.1609/icwsm.v19i1.35966,['JournalArticle'],Computer Science
Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI,2025,"Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores","This study examines the capacity of large language models (LLMs) to support phenomenological qualitative analysis of first-person experience in Borderline Personality Disorder (BPD), understood as a disorder of temporality and selfhood. Building on a prior human-led thematic analysis of 24 inpatients'life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5 Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the original investigators. The models were evaluated with blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients, and multidimensional validity ratings (credibility, coherence, substantiveness, and groundness in data). Results showed variable overlap with the human analysis, from 0 percent in GPT to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient (0.21-0.28). However, the models recovered themes omitted by humans. Gemini's output most closely resembled the human analysis, with validity scores significantly higher than GPT and Claude (p<0.0001), and was judged as human by blinded experts. All scores strongly correlated (R>0.78) with the quantity of text and words per theme, highlighting both the variability and potential of AI-augmented thematic analysis to mitigate human interpretative bias.",10.48550/arXiv.2508.19008,['JournalArticle'],Computer Science; Psychology
Innovating Oil and Gas Field Operations - Harnessing the Power of Generative Ai for Supporting Workforce Towards Achieving Autonomous Operations,2024,"N. Reddicharla, Mayada Ali Sultan Ali","
 In today's dynamic and competitive oil and gas industry, the integration of Artificial Intelligence (AI) has emerged as a game-changer, offering unparalleled opportunities for optimization, cost reduction, and operational excellence. The main objective of autonomous operations is to minimize manual interactions and maximize self-directed plant operations. ADNOC Onshore has implemented generative AI agents in daily maintenance and production operations to boost workforce productivity in the journey of achieving autonomous operations. This paper explains the use cases, challenges, AI architecture & data security in deployment.
 Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. GPT-4 Turbo is a large multimodal model (accepting text or image inputs and generating text) that can solve difficult problems with greater accuracy and advanced reasoning capabilities. The scope includes empowering reliability, maintenance, and operations professionals to draw insights from equipment manuals, asset operating manuals and operating procedures, maintenance records, and safety & integrity manuals. This in-house solution with support across structured and unstructured data, an LLM-agnostic architecture, deterministic responses with source references, and granular access controls. The solution has been integrated ERP SAP system and sensor time series PI system, data historians for integrated context. A unique automated contextualization engine has been used based on oil and gas specific vocabulary to bring context to their operations. A conversational interactive agent has been built for user interactions.
 The maintenance and operations engineer can receive suggestions on the proper steps to identify the root cause based on OEM product manuals, previous events, and current performance. This Generative AI solution accelerates time to insight for operators by equipping teams to streamline maintenance operations and Investigate maintenance records with generative AI to troubleshoot operations challenges more efficiently. The internal study showed that operational productivity has increased by 20% after this solution's implementation. For the model to understand industrial environments, it would require retraining the model on industrial data. Using existing models on uncontextualized, unstructured industrial data significantly increases the risk of incorrect and untrustworthy answers  referred to as AI hallucinations. Another significant challenge lies in the dependence on the quality and quantity of available data for training. AI models require extensive and representative datasets to produce accurate and reliable predictions.
 Large language models are a type of artificial intelligence (AI) model designed to understand and generate human language. These models are built upon deep learning architectures, particularly transformer architectures. Generative AI can play a significant role in oil and gas asset operations towards the goal of achieving autonomous operations.",10.2118/222046-ms,nan,Computer Science; Engineering; Environmental Science
ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments,2025,"Shreya Gummadi, M. V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary","The advancement of robotics and autonomous navigation systems hinges on the ability to accurately predict terrain traversability. Traditional methods for generating datasets to train these prediction models often involve putting robots into potentially hazardous environments, posing risks to equipment and safety. To solve this problem, we present ZeST, a novel approach leveraging visual reasoning capabilities of Large Language Models (LLMs) to create a traversability map in real-time without exposing robots to danger. Our approach not only performs zero-shot traversability and mitigates the risks associated with real-world data collection but also accelerates the development of advanced navigation systems, offering a cost-effective and scalable solution. To support our findings, we present navigation results, in both controlled indoor and unstructured outdoor environments. As shown in the experiments, our method provides safer navigation when compared to other state-of-the-art methods, constantly reaching the final goal.",10.48550/arXiv.2508.19131,['JournalArticle'],Computer Science; Engineering
Leveraging Computer Vision and Visual LLMs for Cost-Effective and Consistent Street Food Safety Assessment in Kolkata India,2025,"Alexey Chernikov, Klaus Ackermann, Caitlin Brown, Denni Tommasi","Ensuring street food safety in developing countries is crucial due to the high prevalence of foodborne illnesses. Traditional methods of food safety assessments face challenges such as resource constraints, logistical issues, and subjective biases influenced by surveyors personal lived experiences, particularly when interacting with local communities. For instance, a local food safety inspector may inadvertently overrate the quality of infrastructure due to prior familiarity or past purchases, thereby compromising objective assessment. This subjectivity highlights the necessity for technologies that reduce human biases and enhance the accuracy of survey data across various domains.
This paper proposes a novel approach based on a combination of Computer Vision and a lightweight Visual Large Language Model (VLLM) to automate the detection and analysis of critical food safety infrastructure in street food vendor environments at a field experiment in Kolkata, India. The system utilises a three-stage object extraction pipeline from the video to identify, extract and select unique representations of critical elements such as hand-washing stations, dishwashing areas, garbage bins, and water tanks. These four infrastructure items are crucial for maintaining safe food practices, irrespective of the specific methods employed by the vendors. A VLLM then analyses the extracted representations to assess compliance with food safety standards. Notably, over half of the pipeline can be processed using a user's smartphone, significantly reducing government server workload. By leveraging this decentralised approach, the proposed system decreases the analysis cost by many orders of magnitude compared to alternatives like ChatGPT or Claude 3.5. Additionally, processing data on local government servers provides better privacy and security than cloud platforms, addressing critical ethical considerations. This automated approach significantly improves efficiency, consistency, and scalability, providing a robust solution to enhance public health outcomes in developing regions.",10.1609/aaai.v39i27.35008,"['JournalArticle', 'Conference', 'Review']",Computer Science; Environmental Science
Continuous Learning Conversational AI: A Personalized Agent Framework via A2C Reinforcement Learning,2025,"M. Nandakishor, M. Anjali","Creating personalized and adaptable conversational AI remains a key challenge. This paper introduces a Continuous Learning Conversational AI (CLCA) approach, implemented using A2C reinforcement learning, to move beyond static Large Language Models (LLMs). We use simulated sales dialogues, generated by LLMs, to train an A2C agent. This agent learns to optimize conversation strategies for personalization, focusing on engagement and delivering value. Our system architecture integrates reinforcement learning with LLMs for both data creation and response selection. This method offers a practical way to build personalized AI companions that evolve through continuous learning, advancing beyond traditional static LLM techniques.",10.48550/arXiv.2502.12876,['JournalArticle'],Computer Science
AI Judging Architecture for Well-Being: Large Language Models Simulate Human Empathy and Predict Public Preference,2025,"Nicholas Boys Smith, N. Salingaros","Large language models (LLMs) judge three pairs of architectural design proposals which have been independently surveyed by opinion polls: department store buildings, sports stadia, and viaducts. A tailored prompt instructs the LLM to use specific emotional and geometrical criteria for separate evaluations of image pairs. Those independent evaluations agree with each other. In addition, a streamlined evaluation using a single descriptor friendliness yields the same results while offering a rapid screening measure. In all cases, the LLM consistently selects the more human-centric design, and the results align closely with independently conducted public opinion poll surveys. This agreement is significant in improving designs based upon human-centered principles. AI helps to illustrate the correlational effect: living geometry  positive-valence emotions  public preference. The AI-based model therefore provides empirical evidence for a deep biological link between geometric structure and human emotion that warrants further investigation. The convergence of AI judgments, neuroscience, and public sentiment highlights the diagnostic power of criteria-driven evaluations. With intelligent prompt engineering, LLM technology offers objective, reproducible architectural assessments capable of supporting design approval and policy decisions. A low-cost tool for pre-occupancy evaluation unifies scientific evidence with public preference and can inform urban planning to promote a more human-centered built environment.",10.3390/designs9050118,"['JournalArticle', 'Review']",Computer Science; Psychology
The Impact of Machine Intelligence on the Culture of Intercorporate Communication,2025,"Tetiana I. Demianenko, Pavlo A. Malinnikov","The aim of the article is to investigate the transformational impact of machine intelligence on the culture of intercorporate communication, as well as to analyze new forms of digital interaction between companies. Analyzing the impact of machine intelligence, it should be noted that the integration of intelligent technologies fundamentally transforms the nature and content of interactions between organizations. The use of artificial intelligence systems contributes to the automation of communication processes, increases the efficiency of information exchange, and fosters the formation of new norms and values in corporate interaction. Machine intelligence not only optimizes the technical aspects of data exchange but also influences the ethical, behavioral, and cultural components of intercorporate cooperation. As a result, the structure of communication models changes, and new approaches to trust, transparency, and mutual responsibility among participants in organizational interaction are formed. It is proved that the integration of machine intelligence, particularly large language models and autonomous function calling mechanisms, contributes to the acceleration of information exchange, increased operational transparency, and improved data management efficiency, while also raising new questions regarding privacy protection and ethics. As a result of the research, a conceptual model has been developed that integrates technological, organizational, and ethical factors to optimize intercorporate communication processes. The significance of modern machine intelligence technologies as a tool for transforming traditional communication strategies has been revealed, ensuring a more flexible, faster, and personalized information exchange between organizations. Special attention has been paid to how integrated solutions based on natural language command processing simplify data exchange, accelerate response to inquiries, and ensure better coordination in the fields of marketing and logistics.",10.32983/2222-4459-2025-3-337-343,['JournalArticle'],Business; Computer Science
A Novel Agent-Based Framework for Conversational Data Analysis and Personal AI Systems,2024,"Bartosz Kurylek, Arthur Camara, Akash Nandi, E. Markopoulos","This paper introduces a novel agent-based framework that leverages conversational data to enhance Large Language Models (LLMs) with personalized knowledge, enabling the creation of Artificial Personal Intelligence (API) systems. The proposed framework addresses the challenge of collecting and analysing unstructured conversational data by utilizing LLM agents and embeddings to efficiently process, organize, and extract insights from conversations. The system architecture integrates knowledge data aggregation and agent-based conversational data extraction. The knowledge data aggregation method employs LLMs and embeddings to create a dynamic, multi-level hierarchy for organizing information based on conceptual similarity and topical relevance. The agent-based component utilizes an LLM Agent to handle user queries, extracting relevant information and generating specialized theme datasets for comprehensive analysis. The framework's effectiveness is demonstrated through empirical analysis of real-world conversational data and a user survey. However, limitations such as the need for further testing of scalability and performance under large-scale, real-world conditions and potential biases introduced by LLMs are acknowledged. Future research should focus on extensive real-world testing and the integration of additional conversational qualities to further enhance the framework's capabilities, ultimately enabling more personalized and context-aware AI assistance.",10.54941/ahfe1004649,['Review'],Computer Science
Intelligent accounting question-answering robot based on a large language model and knowledge graph,2025,"Shengyun Shi, Guoxi Li, Yong Wang","Abstract In the wave of digital transformation, enterprises have an increasing demand for fast and accurate access to financial information. The conventional accounting service model often relies on manual operations, which are not only inefficient but also susceptible to errors. This study aims to design an intelligent accounting question-answering robot based on a large language model and knowledge graph. To build a complete knowledge graph, this study uses the attention mechanism and convolutional neural network to build a connection prediction model and completes the accounting question-answering knowledge graph. After that, the bidirectional gated loop unit is used to improve the large language model so as to further improve the correlation between knowledge and explore potential information. The results denoted that the developed method had a question-answering accuracy of 94.6%, and the answers covered 95.2% of the domain range. The response time was only 120ms, which was faster than other models and enhanced the user experience. Moreover, the user satisfaction score of Model 1 was 9.2 points. It is expected that the designed bot will be helpful for enterprises to obtain quick financial information and improve accounting service efficiency.",10.1515/nleng-2024-0087,['JournalArticle'],Business; Computer Science
VChatter: Exploring Generative Conversational Agents for Simulating Exposure Therapy to Reduce Social Anxiety,2025,"Han Zhang, KaWing Tsang, Zhenhui Peng","Many people struggle with social anxiety, feeling fear, or even physically uncomfortable in social situations like talking to strangers. Exposure therapy, a clinical method that gradually and repeatedly exposes individuals to the source of their fear and helps them build coping mechanisms, can reduce social anxiety but traditionally requires human therapists' guidance and constructions of situations. In this paper, we developed a multi-agent system VChatter to explore large language models(LLMs)-based conversational agents for simulating exposure therapy with users. Based on a survey study (N=36) and an expert interview, VChatter includes an Agent-P, which acts as a psychotherapist to design the exposure therapy plans for users, and two Agent-Hs, which can take on different interactive roles in low, medium, and high exposure scenarios. A six-day qualitative study (N=10) showcases VChatter's usefulness in reducing users' social anxiety, feelings of isolation, and avoidance of social interactions. We demonstrated the feasibility of using LLMs-based conversational agents to simulate exposure therapy for addressing social anxiety and discussed future concerns for designing agents tailored to social anxiety.",10.48550/arXiv.2506.03520,"['JournalArticle', 'Review']",Computer Science; Psychology
Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes,2025,"Quintin Myers, Yanjun Gao","Large language models (LLMs) are increasingly proposed for detecting and responding to violent content online, yet their ability to reason about morally ambiguous, real-world scenarios remains underexamined. We present the first study to evaluate LLMs using a validated social science instrument designed to measure human response to everyday conflict, namely the Violent Behavior Vignette Questionnaire (VBVQ). To assess potential bias, we introduce persona-based prompting that varies race, age, and geographic identity within the United States. Six LLMs developed across different geopolitical and organizational contexts are evaluated under a unified zero-shot setting. Our study reveals two key findings: (1) LLMs surface-level text generation often diverges from their internal preference for violent responses; (2) their violent tendencies vary across demographics, frequently contradicting established findings in criminology, social science, and psychology.",10.48550/arXiv.2506.20822,['JournalArticle'],Computer Science; Psychology; Sociology
Real-Time Employee Attrition Monitoring and Recommendation System,2025,"Jaya Santhiyaa A E, Shadhir Mohamed S, Shalini S, R. Meena, Kalaiselvi. T","Employee attrition is an important challenge that impacts organizational productivity, staff morale, and financial stability. Increased attrition rates result in higher costs of recruitment, workflow disturbances, and reduced efficiency overall. The paper presents a Real-Time Employee Attrition Monitoring and Recommendation System that combines rule-based scoring and AIanalysis through GPT-4 for forecasting and avoiding attrition risks. The system captures employee survey responses, assigns quantifiable scores along predetermined metrics such as job satisfaction, work-life balance, and career development, and processes structured scores through GPT-4 to create tailored recommendations for both employees and HR teams. A Firebase backend provides secure data storage, real-time updating, authentication, and visualization in the form of interactive dashboards, allowing HR teams to study workforce trends and undertake proactive retention measures. The results show that this hybrid model enhances early attrition detection by 30% over conventional approaches, enabling timely $H R$ interventions and enhanced workforce retention. The performance of the system is measured on the basis of attrition prediction accuracy, response time, and intervention success rate.",10.1109/ICIRCA65293.2025.11089919,"['Conference', 'Review']",Business; Computer Science
Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods,2024,"Eugene Vyborov, Oleksiy O. Osypenko, Serge Sotnyk","There are various methods for adapting LLMs to different domains. The most common methods are prompting, finetuning, and RAG. In this work, we explore the possibility of adapting a model using one of the PEFT methods - QLoRA. The experiment aims to simulate human responses based on their interviews. The simulation quality is assessed by comparing the quality of the style and the quality of the generated facts.",10.48550/arXiv.2406.08582,['JournalArticle'],Computer Science; Linguistics
Finetuning Large Language Models for Suicide Risk Level Assessment on Social Media,2024,"Lei Cao, Zihan Wei, Zhexuan Wei, Xixian Chen","Suicide remains a pressing global issue, and early detection of individuals at risk is vital for prevention. Traditional methods of suicide risk assessment rely heavily on questionnaires and interviews. However, social media offers a rich source of realtime data, providing an opportunity for automated suicide risk detection. In this study, we propose a novel method leveraging large language models (LLMs) to assess suicide risk based on social media activity. Our approach integrates RoBERTa, Llama 3.1, and Gemma 2 through a voting mechanism to enhance predictive performance. Results indicate that our model achieves an F1 score of 0.7006, outperforming traditional machine learning and deep learning models. We also observe that while fine-tuning large models on limited data yields better results than pre-trained models, increasing model size does not always improve performance. The use of a voting mechanism significantly improves the accuracy of the predictions, demonstrating the utility of multi-model approaches in suicide risk detection.",10.1109/BigData62323.2024.10825211,"['JournalArticle', 'Conference']",Computer Science; Psychology
"Exploring Generative Artificial Intelligence Usage in the DPWH: Platforms, Extent of Adoption, and Areas of Application",2025,"Bien, Garcia, Muchamiel, Raquel, Vicario, Bation, Pudan","Artificial intelligence (AI) transforms how we work, think, and lead. In the Philippines, almost half of the
population uses generative artificial intelligence (GenAI) tools at least once a month for work and personal
purposes. Despite growing adoption of GenAI, there isn't much information about how it is used in government
offices, which still rely heavily on traditional, paper-based methods. This study examined how much GenAI is
being used by the Department of Public Works and Highways (DPWH) employees, focusing on areas of
application, perceived impacts, and barriers to integration. Data were collected from 50 respondents using Google
Forms through a non-probability convenience sampling. Generalist tools like ChatGPT were widely adopted
across functions, while specialized tools like Canva, Grammarly, and Quillbot were only used by a few. On the
other hand, tools like Firefly, Midjourney, Claude, and Perplexity remained underutilized. Statistical analysis
confirmed task-specific patterns of GenAI use, where the chi-square test showed a significant association between
task type and tool usage, (48, N = 232) = 91.17, p < .001. The overall index ( = 2.07, SD = 0.78) reflects a
low-to-moderate level of adoption, implying that while GenAI is already introduced in the workplace, its use
continues to emerge and has not yet reached full integration into daily operations. Current applications were
concentrated in correspondence, documentation, and research. The study recommends policy development,
training, and ethical guidelines tailored to government operations to enable strategic and sustainable integration.",10.63941/dit.adsimrj.2025.1.3.59,['JournalArticle'],Business; Computer Science; Sociology
Balancing Innovation and Privacy: A Red Teaming Approach to Evaluating Phone-Based Large Language Models under AI Privacy Regulations,2023,"M. Pujari, A. K. Pakina, A. Goel, A. K. Pakina","The rapid deployment of large language models (LLMs) on mobile devices has introduced significant privacy concerns, particularly regarding data collection, user profiling, and compliance with evolving AI regulations such as the GDPR and the AI Act. While these on-device LLMs promise improved latency and user experience, their potential to inadvertently leak sensitive information remains understudied. This paper proposes a red teaming framework to systematically assess the privacy risks of phone-based LLMs, simulating adversarial attacks to identify vulnerabilities in model behavior, data storage, and inference processes. 
We evaluate popular mobile LLMs under scenarios such as prompt injection, side-channel exploitation, and unintended memorization, measuring their compliance with strict privacy-by-design principles. Our findings reveal critical gaps in current safeguards, including susceptibility to context-aware deanonymization and insufficient data minimization. We further discuss regulatory implications, advocating for adaptive red teaming as a mandatory evaluation step in AI governance. By integrating adversarial testing into the development lifecycle, stakeholders can preemptively align phone-based AI systems with legal and ethical privacy standards while maintaining functional utility.",10.56127/ijst.v2i3.1956,['JournalArticle'],Computer Science
Bigger But Not Better: Small Neural Language Models Outperform Large Language Models in Detection of Thought Disorder,2025,"Changye Li, Weizhe Xu, Serguei V. S. Pakhomov, Ellen R. Bradley, Dror Ben-Zeev, Trevor Cohen","Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum disorders. Recently, clinical estimates of the severity of disorganized thinking have been shown to correlate with measures of how difficult speech transcripts would be for large language models (LLMs) to predict. However, LLMs' deployment challenges -- including privacy concerns, computational and financial costs, and lack of transparency of training data -- limit their clinical utility. We investigate whether smaller neural language models can serve as effective alternatives for detecting positive formal thought disorder, using the same sliding window based perplexity measurements that proved effective with larger models. Surprisingly, our results show that smaller models are more sensitive to linguistic differences associated with formal thought disorder than their larger counterparts. Detection capability declines beyond a certain model size and context length, challenging the common assumption of ``bigger is better'' for LLM-based applications. Our findings generalize across audio diaries and clinical interview speech samples from individuals with psychotic symptoms, suggesting a promising direction for developing efficient, cost-effective, and privacy-preserving screening tools that can be deployed in both clinical and naturalistic settings.",10.48550/arXiv.2503.20103,['JournalArticle'],Computer Science; Psychology
A Comparative Study of Large Language Models and Human Personality Traits,2025,"Wang Jiaqi, Wang bo, Guo fa, Cheng cheng, Yang li","Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.",10.48550/arXiv.2505.14845,['JournalArticle'],Computer Science; Linguistics; Psychology
Spelling correction with large language models to reduce measurement error in open-ended survey responses,2025,"Maxwell B Allamong, Jongwoo Jeong, Paul M. Kellstedt","Open-ended survey questions have a long history in public opinion research and are seeing a renewed interest as computing power and tools of text analysis proliferate. A major challenge in performing text analyses on open-ended responses is that the documentsespecially if transcribed or collected through web surveysmay contain measurement error in the form of misspellings which are not easily corrected in a reliable and systematic manner. This paper provides evidence that large language models (LLMs), specifically OpenAIs GPT-4o, offer a flexible, dependable, and low-cost solution to correcting misspellings in open-ended responses. We demonstrate the efficacy of this approach with open-ended responses about the Democratic and Republican parties from the 19962020 American National Election Studies, where GPT is shown to correct 85%90% of misspellings identified by human coders in a sample of responses. Following spelling correction on 50,000 responses, we document several consequential changes to the data. First, we show that spelling correction reduces the number of unique and single-use tokens while increasing the number of words matched to a sentiment dictionary. Then, to highlight the potential benefits and limitations of spelling correction we show improved out-of-sample prediction accuracy from a text-based machine learning classifier. Finally, we show a significantly larger degree of emotionality is captured in the spelling-corrected texts, though the size of this measures relationship with a known correlate in political interest remains relatively unchanged. Our findings point to LLMs as an effective tool for reducing measurement error by correcting misspellings in open-ended survey responses.",10.1177/20531680241311510,"['JournalArticle', 'Review']",Computer Science; Political Science
Illuminate a Novel Approach for Depression Detection with Explainable Analysis and Proactive Therapy using Prompt Engineering.,2024,Aryan Agrawal,"Traditional methods of depression detection on social media forums can classify whether a user is depressed, but they often lack the capacity for human-like explanations and interactions. This paper proposes a next-generation paradigm for depression detection and treatment strategies. This research employs three advanced Large Language Models (LLMs) - Generative Pretrained Transformer 4 (GPT - 4), Llama 2 chat, and Gemini, each fine-tuned using specially engineered prompts to effectively diagnose, explain, and suggest therapeutic interventions for depression. These prompts are designed to guide the models in analyzing textual data from clinical interviews and online forums, ensuring nuanced and context-aware responses. The study introduces a novel approach to prompt engineering, utilizing a few-shot prompting methodology for the Diagnosis and Explanation component. This technique is optimized to provide Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition, (DSM-5) based analysis and explanation, enhancing the models ability to identify and articulate depressive symptoms accurately. For the Interaction and Learning aspect, the models engage in empathetic dialogue management, guided by resources from Psychology Database and a Cognitive Behavioral Therapy (CBT) guide. This facilitates meaningful interactions with individuals facing major depressive disorders, fostering a supportive and understanding environment. Furthermore, the research innovates in Case Conceptualization and Treatment, creating the Illuminate Database to guide the models in offering personalized therapy. This database is enriched with various CBT modules, encompassing case conceptualization, treatment planning, and therapeutic techniques. The models utilize this information to offer structured, actionable steps for addressing mental health issues. The quantitative analysis of the study highlights the effectiveness of these LLMs, demonstrated through metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) across different test sets. This comprehensive approach, blending cutting-edge AI with established psychological methodologies, illuminates new possibilities in mental health care, showcasing the potential of LLMs in revolutionizing diagnosis and treatment strategies.",10.33140/ijp.09.02.02,['JournalArticle'],Computer Science; Psychology
A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic Data Collection in Multi-Agent Collaborative Environments Driven by LLMs,2025,"Xingyu Xiao, Peng Chen, Qianqian Jia, Jiejuan Tong, Jingang Liang, Haitao Wang","HRA (Human Reliability Analysis) data is crucial for advancing HRA methodologies. however, existing data collection methods lack the necessary granularity, and most approaches fail to capture dynamic features. Additionally, many methods require expert knowledge as input, making them time-consuming and labor-intensive. To address these challenges, we propose a new paradigm for the automated collection of HRA data. Our approach focuses on key indicators behind human error, specifically measuring workload in collaborative settings. This study introduces a novel, scenario-driven method for workload estimation, leveraging fine-tuned large language models (LLMs). By training LLMs on real-world operational data from high-temperature gas-cooled reactors (HTGRs), we simulate human behavior and cognitive load in real time across various collaborative scenarios. The method dynamically adapts to changes in operator workload, providing more accurate, flexible, and scalable workload estimates. The results demonstrate that the proposed WELLA (Workload Estimation with LLMs and Agents) outperforms existing commercial LLM-based methods in terms of prediction accuracy.",10.48550/arXiv.2502.00022,['JournalArticle'],Computer Science; Engineering
VOCALAI : An intelligent virtual personal voice assistant for smart interaction,2025,Dr.C.Srinivasa Kumar,"AbstractThe evolution of virtual personal assistants (VPAs) has been significantly influenced by advancements in voice command recognition and response optimization. Modern systems leverage sophisticated technologies such as Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and Text-to-Speech (TTS) synthesis to facilitate seamless human-computer interactions. These integrations enable VPAs to comprehend and process voice inputs, interpret user intent, and generate contextually appropriate responses.

Recent developments have introduced multimodal capabilities, allowing VPAs to engage in voice, text, and visual interactions. For instance, OpenAI's GPT-4o model supports real-time voice conversations, providing users with dynamic and natural interactions. This advancement enhances the VPA's ability to manage a wide spectrum of tasksfrom answering questions and managing calendars to niche functions like coding.

Furthermore, the integration of VPAs with hardware platforms, such as the ESP32 microcontroller, has facilitated the development of intelligent voice interfaces. These systems utilize cloud APIs and conversational intelligence to deliver comprehensive solutions for voice-based interactions, enhancing productivity across various environments.

Despite these advancements, challenges persist in ensuring the accuracy, security, and privacy of voice interactions. Addressing issues related to data protection and system vulnerabilities is crucial for the continued success and adoption of voice-enabled VPAs.

In conclusion, the integration of voice command recognition and response optimization in VPAs represents a significant leap towards more intuitive and efficient human-computer interactions. Ongoing research and development in this field are essential to overcome existing challenges and unlock the full potential of voice-enabled technologies.",10.55041/ijsrem50134,['JournalArticle'],Computer Science
"Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria",2025,"Keita Kiuchi, Yoshikazu Fujimoto, Hideyuki Goto, Tomonori Hosokawa, Makoto Nishimura, Yosuke Sato, Izumi Sezai","This study provides the first comprehensive evaluation of large language model (LLM) performance across three counseling roles in Japanese-language therapeutic contexts. We simultaneously assessed counselor artificial intelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured Multi-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations, and evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human experts (n = 15) with extensive counseling experience evaluated AI-generated dialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding Manual 4.2.1. Notably, SMDP implementation significantly enhanced counselor AI performance across all MITI global ratings compared with zeroshot prompting, with no significant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed comparable performance to human raters for Cultivating Change Talk but systematically overestimated Softening Sustain Talk and the overall quality metrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3 focused on technical proficiency, and Sonnet prioritized emotional expression. Client AI simulations exhibited a limited emotional range and unnaturally high compliance, indicating the need for enhanced realism. These findings establish benchmarks for AI-assisted counseling in non-English contexts and identify critical areas for improvement through advanced prompt engineering, retrieval-augmented generation, and targeted fine-tuning, with important implications for developing culturally sensitive AI mental health tools.",10.48550/arXiv.2507.02950,['JournalArticle'],Computer Science; Psychology
A Systematic Survey of Human Ear Biometrics Recognition by using 2D images,2023,"H. A. Elmarzaki, Bothaina F. Gargoum, Amina A. Abdo","Nowadays, one of the most serious issues is secure verification, especially with the advent of artificial intelligence and machine learning and deep learning algorithms. As a result, the research field of recognizing human biometrics has grown significantly, and various types of algorithms have been proposed to help with security. From the available literature, we noticed that researchers were focusing on various systematic surveys for the recognition of human biometrics, including faces, eyes, finger prints, palms, and etc. Despite the popularity of machine and deep learning methods in the field of human hearing prediction, surveys of such methods are rare. In this work, we present an up-to-date survey of the ear recognition field by using 2D images and display the most relevant papers from the most popular databases by applying different techniques based on several criteria. This review presents an overview of the similarities and differences between different works and provides a statistical analysis view of common techniques commonly used in ear recognition literature. The results presented in this report provide insight on the patterns of this field's research.",10.1109/MI-STA57575.2023.10169473,"['Conference', 'Review']",Computer Science; Engineering
Domain-Specific Conversational Agents: Revolutionizing Customer Service,2025,Swapnil Hemant Thorat,"Domain-specific conversational agents are transforming customer service by addressing the limitations of traditional models that struggle with high volumes, inconsistent quality, and scaling challenges. These intelligent systems leverage fine-tuned Large Language Models on industry-specific data to understand complex terminology, navigate specialized processes, deliver personalized interactions, and manage multi-turn conversations while recognizing when human intervention is needed. The technical architecture requires sophisticated natural language understanding, seamless system integration, robust context management, graceful handover mechanisms, and scalable infrastructure. Implementation follows a structured approach encompassing data collection, model fine-tuning, integration development, conversation design, testing, and deployment with continuous monitoring. Emerging trends include multimodal interactions, proactive service capabilities, enhanced emotional intelligence, and sophisticated learning mechanisms that adapt from each interaction. These advancements create a balanced ecosystem where AI efficiently handles routine tasks while humans focus on complex situations requiring expertise and empathy.",10.71097/ijsat.v16.i1.2755,['JournalArticle'],Business; Computer Science
Advancing Palmistry with AI: Integrating Machine Learning and Deep Learning for Predictive Analysis,2025,Saloni Shrivastava,"This paper explores the intersection of palmistry and artificial intelligence (AI), focusing on the integration of machine learning (ML) and deep learning (DL) techniques to enhance predictive analysis. Palmistry, an ancient practice of interpreting an individuals future and personality through the study of palm lines and shapes, has traditionally been subjective and qualitative. This paper reviews the current state of palmistry, AI, ML, and DL, and discusses how these technologies can be applied to develop objective, data-driven palmistry models. This survey aims to provide a comprehensive understanding of the opportunities, challenges, and future directions in this emerging field.",10.36948/ijfmr.2025.v07i03.49010,"['JournalArticle', 'Review']",Computer Science
Potential of large language model-powered nudges for promoting daily water and energy conservation,2025,"Zonghan Li, Song Tong, Yi Liu, Kaiping Peng, Chunyan Wang","The increasing amount of pressure related to water and energy shortages has increased the urgency of cultivating individual conservation behaviors. While the concept of nudging, i.e., providing usage-based feedback, has shown promise in encouraging conservation behaviors, its efficacy is often constrained by the lack of targeted and actionable content. This study investigates the impact of the use of large language models (LLMs) to provide tailored conservation suggestions for conservation intentions and their rationale. Through a survey experiment with 1,515 university participants, we compare three virtual nudging scenarios: no nudging, traditional nudging with usage statistics, and LLM-powered nudging with usage statistics and personalized conservation suggestions. The results of statistical analyses and causal forest modeling reveal that nudging led to an increase in conservation intentions among 86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum increase of 18.0% in conservation intentions, surpassing traditional nudging by 88.6%. Furthermore, structural equation modeling results reveal that exposure to LLM-powered nudges enhances self-efficacy and outcome expectations while diminishing dependence on social norms, thereby increasing intrinsic motivation to conserve. These findings highlight the transformative potential of LLMs in promoting individual water and energy conservation, representing a new frontier in the design of sustainable behavioral interventions and resource management.",10.48550/arXiv.2503.11531,"['JournalArticle', 'Review']",Computer Science; Environmental Science
"Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction",2025,"Matus Krajcovic, Peter Demcak, Eduard Kuric","Embodied conversational agents (ECAs) are increasingly more realistic and capable of dynamic conversations. In online surveys, anthropomorphic agents could help address issues like careless responding and satisficing, which originate from the lack of personal engagement and perceived accountability. However, there is a lack of understanding of how ECAs in user experience research may affect participant engagement, satisfaction, and the quality of responses. As a proof of concept, we propose an instrument that enables the incorporation of conversations with a virtual avatar into surveys, using on AI-driven video generation, speech recognition, and Large Language Models. In our between-subjects study, 80 participants (UK, stratified random sample of general population) either talked to a voice-based agent with an animated video avatar, or interacted with a chatbot. Across surveys based on two self-reported psychometric tests, 2,265 conversation responses were obtained. Statistical comparison of results indicates that embodied agents can contribute significantly to more informative, detailed responses, as well as higher yet more time-efficient engagement. Furthermore, qualitative analysis provides valuable insights for causes of no significant change to satisfaction, linked to personal preferences, turn-taking delays and Uncanny Valley reactions. These findings support the pursuit and development of new methods toward human-like agents for the transformation of online surveys into more natural interactions resembling in-person interviews.",10.48550/arXiv.2508.02376,"['JournalArticle', 'Review']",Computer Science; Psychology
Examining the Personalized Applicability of Large Language Models in Business Roles,2025,"Veronika Burjn, . Csap",,10.1109/CogInfoCom66819.2025.11200609,"['Conference', 'Review']",Business; Computer Science
Navigating Consumer Skepticism: Conceptual Model of Distrust Detection and Recognition Technology Adoption Among Online Shop Owners in Indonesia,2024,"Muhamad Rachmat Gunawan, F. Samopa, Ahmad Mukhlason","Background: Consumer skepticism affects trust between online shoppers and shop owners in digital commerce, necessitating effective distrust detection mechanisms. This study presents a conceptual model aimed at promoting the adoption of distrust detection and recognition technology among online shop owners, focusing on technologies powered by Large Language Models (LLMs). The model builds on Taylor's Decomposed Theory of Planned Behavior (DTPB) and incorporates factors relevant to online shop owners' perceptions of consumer distrust. Method: A quantitative research approach was used to survey 100 online shop owners to validate the model empirically. Results: The results indicate a significant effect of Perceived Usefulness (PU) and Social Influence (SI) on intention to adopt the technology. For small businesses such as online shops, PU (an internal factor) and SI (an external factor) can predict owners' intention to adopt distrust detection technology. Implications: This study highlights a simplified method for measuring small business owners' intention to adopt advanced information technologies and suggests business opportunities for e-commerce platforms to strategically position online shop owners.",10.1109/ISCT62336.2024.10791194,['Review'],Business; Computer Science
A systematic study in intelligent software engineering based on Large Language Model,2024,Xiang Li,"The application and research of Large Language Model (LLM) in intelligent software engineering have become a hot topic, offering strong theoretical significance and practical value. Researchers in this field have covered various stages of software engineering. However, there is still a lack of comprehensive and holistic research analysis that unifies the current research findings, which hinders the technological development of this field. Addressing this issue, this paper conducts a comprehensive survey of LLM research in the domain of software engineering. We summarize and extract five specific subfields, including LLM and prompts, code analysis, human-computer interaction, fault prediction, and other software engineering aspects. For these representative research directions, it examines significant research works, traces their research context, and discusses their development trends. This work provides a research foundation and contextual support for subsequent software engineering development based on large models.",10.54254/2755-2721/73/20240408,"['JournalArticle', 'Review']",Computer Science
ResiComp: Loss-Resilient Image Compression via Dual-Functional Masked Visual Token Modeling,2025,"Sixian Wang, Jincheng Dai, Xiaoqi Qin, Ke Yang, Kai Niu, Ping Zhang","Recent advancements in neural image codecs (NICs) are of significant compression performance, but limited attention has been paid to their error resilience. These resulting NICs tend to be sensitive to packet losses, which are prevalent in real-time communications. In this paper, we investigate how to elevate the resilience ability of NICs to combat packet losses. We propose ResiComp, a pioneering neural image compression framework with feature-domain packet loss concealment (PLC). Motivated by the inherent consistency between generation and compression, we advocate merging the tasks of entropy modeling and PLC into a unified framework focused on latent space context modeling. To this end, we take inspiration from the impressive generative capabilities of large language models (LLMs), particularly the recent advances of masked visual token modeling (MVTM). In specific, ResiComp develops a bi-directional masked Transformer to model the contextual dependencies among latents with dual-functionality: 1) iteratively acts as a conditional entropy model to boost compression efficiency; 2) operates latent PLC to improve resilience. During training, we integrate MVTM to mirror the effects of packet loss, enabling a dual-functional Transformer to restore the masked latents by predicting their missing values and conditional probability mass functions. Our ResiComp jointly optimizes compression efficiency and loss resilience. Moreover, ResiComp provides flexible coding modes, allowing for explicitly adjusting the efficiency-resilience trade-off in response to varying Internet or wireless network conditions. Extensive experiments demonstrate that ResiComp can significantly enhance the NICs resilience against packet losses, while exhibits a worthy trade-off between compression efficiency and packet loss resilience. Additionally, packet-level simulations, conducted using diverse network models based on real traces, demonstrate that ResiComp exhibits much better robustness to fluctuating network conditions compared to redundancy-based approaches like VTM + FEC.",10.1109/TCSVT.2025.3539747,['JournalArticle'],Computer Science; Engineering; Mathematics
From Prompting to Partnering: Personalization Features for Human-LLM Interactions,2025,"Si Thu, A. Kocaballi","Large Language Models (LLMs), such as ChatGPT, exhibit advanced capabilities in generating text, images, and videos. However, their effective use remains constrained by challenges in prompt formulation, personalization, and opaque decision-making processes. To investigate these challenges and identify design opportunities, we conducted a two-phase qualitative study. In Phase 1, we performed in-depth interviews with eight everyday LLM users after they engaged in structured tasks using ChatGPT across both familiar and unfamiliar domains. Our findings revealed key user difficulties in constructing effective prompts, iteratively refining AI-generated responses, and assessing response reliability especially in domains beyond users' expertise. Informed by these insights, we designed a high-fidelity prototype incorporating Reflective Prompting, Section Regeneration, Input-Output Mapping, Confidence Indicators, and a Customization Panel. In Phase 2, user testing of the prototype indicated that these interface-level improvements may prove useful for reducing cognitive load, increasing transparency, and fostering more intuitive and collaborative human-AI interactions. Our study contributes to the growing discourse on human-centred AI, advocating for human-LLM interactions that enhance user agency, transparency, and co-creative interaction, ultimately supporting more intuitive, accessible, and trustworthy generative AI systems.",10.48550/arXiv.2503.00681,['JournalArticle'],Computer Science
Sparse Attention Combined with RAG Technology for Financial Data Analysis,2025,"Zhaoyan Zhang, Kaixian Xu, Yu Qiao, Alan Wilson","In response to the challenges of multimodal data integration, real-time information retrieval, model hallucination, and lack of interpretability in financial stock analysis, this paper proposes an innovative financial analysis frameworkFSframe. It aims to address multiple challenges in stock analysis within the financial sector. The framework integrates various technological modules to provide comprehensive and efficient solutions for stock trend prediction and financial question answering tasks. First, FSframe optimizes large language models (LLMs), enhancing their adaptability to financial tasks, and incorporates prompt engineering to mitigate potential hallucination issues during the generation process, thereby improving the accuracy and reliability of the analysis. Secondly, the framework introduces Retrieval-Augmented Generation (RAG) technology, creating a dynamically updated financial knowledge base that enables the model to retrieve and integrate the latest market data, providing real-time external knowledge support for tasks. Furthermore, FSframe adopts a sparse attention mechanism, optimizing the processing efficiency of time-series data by filtering irrelevant information and focusing on key points, while also achieving efficient integration of time-series and textual data. Finally, through its modular design, FSframe organically combines the aforementioned advanced technologies, forming an innovative solution that blends multimodal data processing with real-time analysis, offering strong technical support for intelligent analysis in the financial sector. Validation on large-scale financial datasets (including historical stock prices, financial news, and market announcements) shows that FSframe significantly improves prediction accuracy and real-time responsiveness in stock trend forecasting and financial question answering tasks. Experimental results indicate that FSframe offers significant advantages in multimodal data integration, real-time performance, and interpretability, demonstrating excellent task adaptability and addressing the shortcomings of traditional methods. The FSframe framework not only provides an innovative solution for stock analysis in the financial sector but also opens new pathways for the development of intelligent financial technologies.",10.30564/jcsr.v7i2.8933,['JournalArticle'],Business; Computer Science
AI-Driven Detection and Support for Hidden Addiction Patterns in Remote Workers: A Multimodal Approach,2025,Declan Anthony Dmello 1,"Abstract

 

The widespread adoption of remote work has brought new flexibility to organizations but has also complicated the monitoring of employee well-being, particularly in identifying hidden addiction patterns such as substance abuse, behavioral compulsions, and digital overuse. This study introduces a privacy-preserving, AI-driven framework that leverages lightweight transformer models and multimodal behavioral analytics to detect and support addiction risks within distributed workforces. The proposed system integrates methodologies inspired by the mhGPT (Mental Health GPT) model and incorporates federated learning, differential privacy, and explainable AI to ensure both effectiveness and ethical compliance with GDPR and HIPAA standards. Model development and evaluation primarily utilized synthetically generated datasets and large-scale public datasets, with a limited-response survey informing feature design and scenario construction. Simulated experiments demonstrated high F1-scores in early risk detection and promising engagement rates for a tiered intervention protocol. While these results highlight the frameworks potential, real-world validation and further empirical study are needed to assess practical applicability and address ethical considerations in deployment.

 

Keywords: Remote work, addiction detection, artificial intelligence, multimodal analytics, mental health, privacy",10.55041/ijsrem47910,"['JournalArticle', 'Review']",Computer Science; Psychology
Chinese DeepSeek: Performance of Various Oversampling Techniques on Public Perceptions Using Natural Language Processing,2025,"Anees Ara, Muhammad Mujahid, Amal Al-Rasheed, Shaha T. Al-Otaibi, Tanzila Saba",": DeepSeek Chinese artificial intelligence (AI) open-source model, has gained a lot of attention due to its economical training and efficient inference. DeepSeek, a model trained on large-scale reinforcement learning without supervised fine-tuning as a preliminary step, demonstrates remarkable reasoning capabilities of performing a wide range of tasks. DeepSeek is a prominent AI-driven chatbot that assists individuals in learning and enhances responses by generating insightful solutions to inquiries. Users possess divergent viewpoints regarding advanced models like DeepSeek, posting both their merits and shortcomings across several social media platforms. This research presents a new framework for predicting public sentiment to evaluate perceptions of DeepSeek. To transform the unstructured data into a suitable manner, we initially collect DeepSeek-related tweets from Twitter and subsequently implement various preprocessing methods. Subsequently, we annotated the tweets utilizing the Valence Aware Dictionary and sentiment Reasoning (VADER) methodology and the lexicon-driven TextBlob. Next, we classified the attitudes obtained from the purified data utilizing the proposed hybrid model. The proposed hybrid model consists of long-term, short-term memory (LSTM) and bidirectional gated recurrent units (BiGRU). To strengthen it, we include multi-head attention, regularizer activation, and dropout units to enhance performance. Topic modeling employing KMeans clustering and Latent Dirichlet Allocation (LDA), was utilized to analyze public behavior concerning DeepSeek. The perceptions demonstrate that 82.5% of the people are positive, 15.2% negative, and 2.3% neutral using TextBlob, and 82.8% positive, 16.1% negative, and 1.2% neutral using the VADER analysis. The slight difference in results ensures that both analyses concur with their overall perceptions and may have distinct views of language peculiarities. The results indicate that the proposed model surpassed previous state-of-the-art approaches.",10.32604/cmc.2025.065566,['JournalArticle'],Computer Science
LITO: Learnable Intervention for Truthfulness Optimization,2024,"Farima Fatahi Bayat, Xin Liu, H. V. Jagadish, Lu Wang",,10.48550/arXiv.2405.00301,['JournalArticle'],Computer Science
Comparing and evaluating ChatGPTs performance giving financial advice with Reddit questions and answers,2024,"Sathvik Samant, Aditya Dhar, Shreya Kochar, Aneesha Sreerama, Andrew Wang, Anirudh Sreerama","As artificial intelligence (AI) and particularly Large Language Models (LLMs) rapidly advance, there is a growing interest in AI in the financial industry while understanding its impact on the future of financial advising. To evaluate the performance of such LLMs in the role of a financial advisor, this experiment utilized financial questions asked on the Reddit forum r/Financial Planning. We hypothesized that Chat-GPT would offer commonly observed, yet concise feedback related to typical financial behaviors rather than delivering personalized financial advice. We compared the GPT-4 outputs to actual Reddit comments, assessing the models response content, length, and advice. By evaluating the models advisory competency, this study explored the role of AI in financial forums, its ethical consequences, and potential threat to employment and existing systems. We found that while AI can present accurate information, it failed in its delivery, clarity, and decisiveness. This study further analyzed the implications of GPT-4s performance and its impact on future financial forum systems. More broadly, this study revealed that at its current capabilities GPT-4 does not pose a direct threat to traditional financial forums but has the potential in the future to shift financial forums and advisories to more AI-based systems.",10.59720/23-296,['JournalArticle'],Business; Computer Science; Economics
"""Mirror"" Language AI Models of Depression are Criterion-Contaminated",2025,"Tong Li, Rasiq Hussain, Mehak Gupta, Joshua Oltmanns","A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores (up to R2 of .70). However, many develop these models directly from language responses to depression assessments. These""Mirror models""suffer from""criterion contamination"", which arises when a predicted score depends in part on the predictors themselves. This causes artificial effect size inflation which reduces model generalizability. The present study compares the performance of Mirror models versus""Non-Mirror models"", which are developed from language that does not mirror the assessment they are developed to predict. N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately. Mirror models (using structured diagnostic data) showed very large effect sizes (e.g., R2 = .80). As expected, NonMirror models (using life history data) demonstrated smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror and Non-Mirror model-predicted structured interview depression scores were correlated with self-reported depression symptoms, Mirror and NonMirror performed the same (e.g., r = ~.54), indicating that Mirror models contain bias perhaps due to criterion contamination. Topic modeling identified clusters across Mirror and Non-Mirror models, as well as between true-positive and false-positive predictions. In this head-to-head comparison study, Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. As language AI models for depression continue to evolve, incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment.",10.48550/arXiv.2508.05830,['JournalArticle'],Computer Science; Psychology
We Never Took the Kobayashi Maru Test Until Now. What Do You Think of Our Solutions? Journeys of the Mind Through a No-Win Game,2025,"Kenji Saito, Riki Toriyama, Kokone Noguchi, Tomoyoshi Yamamoto, Jonah Egashira, Rei Tadika","The Kobayashi Maru Test, a fictional no-win scenario from Star Trek, serves as a powerful tool for evaluating leadership qualities under extreme adversity. This raises the question of how individuals' cognitive and emotional states shift before and after experiencing such a scenario. The scenario is particularly intriguing as it explores the interplay between inevitable failure and attempts to rewrite the rules, highlighting how such challenges foster resilience and adaptability. We developed a text-based simulation powered by GPT-4, allowing participants to engage in the Kobayashi Maru Test and attempt various strategies, including rule manipulation to alter the outcome. Their responses were qualitatively analyzed using the SCAT method, revealing psychological transitions akin to the five stages of grief. Our findings suggest that such serious games can effectively elicit profound introspection, making them valuable tools for leadership training. Furthermore, they provide insights for designing future serious games aimed at exploring decision-making under constraints. Moreover, the use of LLMs for simulating and analyzing decision-making presents new possibilities for interactive learning experiences.",10.1109/CoG64752.2025.11114256,"['JournalArticle', 'Conference']",Computer Science; Psychology
"Advances in Transformer-Based Semantic Search: Techniques, Benchmarks, and Future Directions",2025,"Mohammad Kamil, Duygu Cakir","Semantic search has developed quickly as the need for accurate information retrieval has increased in a variety of fields, from expert knowledge systems to web search engines. Conventional search methods that rely solely on keywords frequently fail to understand user intent and contextual hints. This survey focuses on recent advances in Transformer-based models, such as BERT, RoBERTa, T5, and GPT, which leverage self-attention mechanisms and contextual embeddings to deliver heightened precision and recall across diverse domains. Key architectural elements underlying these models are discussed, including dual-encoder and cross-encoder frameworks, and how Dense Passage Retrieval extends their capabilities to large-scale applications is examined. Practical considerations, such as domain adaptation and fine-tuning strategies, are reviewed to highlight their impact on real-world deployment. Benchmark evaluations (e.g., MS MARCO, TREC, and BEIR) are also presented to illustrate performance gains over traditional Information Retrieval methods and explore ongoing challenges involving interpretability, bias, and resource-intensive training. Lastly, emerging trendsmultimodal semantic search, personalized retrieval, and continual learningthat promise to shape the future of AI-driven information retrieval are identified for more efficient and interpretable semantic search.",10.47000/tjmcs.1633092,"['JournalArticle', 'Review']",Computer Science
We Politely Insist: Your LLM Must Learn the Persian Art of Taarof,2025,"Nikta Gohari Sadr, Sahar Heidariasl, Karine Megerdoomian, Laleh Seyyed-Kalantari, Ali Emami","Large language models (LLMs) struggle to navigate culturally specific communication norms, limiting their effectiveness in global contexts. We focus on Persian taarof, a social norm in Iranian interactions, which is a sophisticated system of ritual politeness that emphasizes deference, modesty, and indirectness, yet remains absent from existing cultural benchmarks. We introduce TaarofBench, the first benchmark for evaluating LLM understanding of taarof, comprising 450 role-play scenarios covering 12 common social interaction topics, validated by native speakers. Our evaluation of five frontier LLMs reveals substantial gaps in cultural competence, with accuracy rates 40-48% below native speakers when taarof is culturally appropriate. Performance varies between interaction topics, improves with Persian-language prompts, and exhibits gender-based asymmetries. We also show that responses rated""polite""by standard metrics often violate taarof norms, indicating the limitations of Western politeness frameworks. Through supervised fine-tuning and Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in model alignment with cultural expectations. Our human study with 33 participants (11 native Persian, 11 heritage, and 11 non-Iranian speakers) forms baselines in varying degrees of familiarity with Persian norms. This work lays the foundation for developing diverse and culturally aware LLMs, enabling applications that better navigate complex social interactions.",10.48550/arXiv.2509.01035,['JournalArticle'],Computer Science; Linguistics
My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement for Personalized Implicit Emotion Analysis,2024,"Jian Liao, Yukang Feng, Yujin Zheng, Jun Zhao, Suge Wang, Jianxing Zheng","The subtlety of emotional expressions makes implicit emotion analysis (IEA) particularly sensitive to user-specific characteristics. Current studies personalize emotion analysis by focusing on the author but neglect the impact of the intended reader on implicit emotional feedback. In this paper, we introduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses subjective variability by incorporating reader feedback. In particular, (1) we create reader agents based on large language models to simulate reader feedback, overcoming the issue of ``spiral of silence effect'' and data incompleteness of real reader reaction. (2) We develop a role-aware multi-view graph learning to model the emotion interactive propagation process in scenarios with sparse reader information. (3) We construct two new PIEA datasets covering English and Chinese social media with detailed user metadata, addressing the text-centric limitation of existing datasets. Extensive experiments show that RAPPIE significantly outperforms state-of-the-art baselines, demonstrating the value of incorporating reader feedback in PIEA.",10.18653/v1/2025.acl-long.787,"['JournalArticle', 'Conference']",Computer Science
Data-Driven Solutions and Artificial Intelligence in the Energy Sector: Leveraging Open GHGRP Data for Emissions Forecasting for U.S. Petroleum and Natural Gas Assets,2025,"Jnana Bairy, Ovais Rehman Shah, June Zhang","
 This technical paper explores the application of open-source data from the Environmental Protection Agency's (EPA) Greenhouse Gas Reporting Program (GHGRP) to forecast methane and other greenhouse gas (GHG) emissions across U.S. upstream and downstream assets. It focuses specifically on emissions from onshore and offshore petroleum production operations. By integrating artificial intelligence (AI) techniques, the research aims to develop a robust emissions prediction model to accelerate decarbonization efforts while enhancing sustainability in the energy sector.
 Classical and Advanced AI algorithms and data analytics approaches were applied to interpret emissions data reported in the GHGRP, covering petroleum and natural gas assets from U.S. upstream sector. The approach included machine learning (ML) techniques such as gradient boosting models and deep learning to capture emission patterns and improve prediction accuracy. Natural Language Processing (NLP) techniques were employed to refine data collection processes, extracting insights from regulatory and operational reports. A comprehensive AI-powered framework was designed to identify key emissions sources and forecast potential reductions, providing actionable intelligence for emissions management strategies across operations.
 The study successfully leveraged AI models to forecast GHG emissions across petroleum and natural gas assets. The research identified key areas where emissions were concentrated and predicted future trends with high accuracy. Machine learning models, particularly boosting techniques, outperformed traditional statistical approaches, providing superior results in emissions forecasting at both the equipment and system-wide levels. This methodology highlighted significant sources of emissions within this sector and pinpointed where mitigation efforts could yield the greatest impact.
 Generative AI Large Language Models (LLM) enhanced the capability to process small or large GHG datasets. The hybrid multi-model approach demonstrated a novel use case for processing environmental data, ensuring efficient, accurate analysis across multiple sources. The findings underscore the value of AI in emissions reduction strategies, offering vital insights for energy industry leaders and regulators. The models can be adapted to broader applications, helping the industry reduce its environmental footprint.
 This study introduces a first-of-its-kind approach that utilizes artificial intelligence (AI) and machine learning (ML) to forecast emissions from U.S. production assets, leveraging open GHGRP data from petroleum and natural gas production facilities. It integrates Generative AI, marking a cutting-edge application and a valuable contribution to the petroleum industry. By establishing a repeatable ML forecasting framework and ensemble of models, this applied research helps the energy industry set an industry baseline for emissions, enabling continuous improvements and precise target setting for emissions efficiency and neutrality. This pioneering effort not only complements existing methodologies for natural gas compression, transmission, and storage but also positions the study at the forefront of industry advancements in achieving carbon emissions management through applications of advanced technologies. The use of Generative AI and LLM-based forecasting enhances energy analytics through sophisticated data extraction and analysis, solidifying its role as a novel and innovative industry-leading exercise.",10.2118/224384-ms,nan,Computer Science; Engineering; Environmental Science
Filtering Hallucinations and Omissions in Large Language Models through a Cognitive Architecture,2025,"Md Asaduzzaman, Ioanna Giorgi, G. Masala","While Large Language Models (LLMs) have outpaced recent technological advancements, challenges like hallucinations and omissions persist in all LLMs due to the underlying architecture and model training. Hallucinations refer to instances where the model generates incorrect, fabricated, or ungrounded information. Omissions occur when the model fails to provide certain details or skips relevant information in its response. This paper proposes a novel hybrid methodology to mitigate these phenomena by integrating an LLM (GPT-3.5) with an external brain-inspired cognitive architecture. Unlike classical approaches, our hybrid system leverages mechanisms for longterm memory, structured reasoning, and multi-modal learning and presents further opportunities for improving LLMs with continuous learning, multilingual skills and focus of attention, without ad hoc fine-tuning. The hybrid system was tested and compared with two standalone LLMs (GPT-3.5, Gemini) through simulated open dialogue that mimic daily conversations These tests involved implicit conversational questions or statements on topics like social contexts and basic knowledge, e.g., discussing animals or comparing numbers. Our proposed model reduced hallucinations and omissions compared to the standalone LLMs on the same benchmark dataset. Specifically, reductions were observed in (i) hallucinations: 33.85% over GPT-3.5 and 37.48% over Gemini, (ii) omissions: 29.80% over GPT-3.5 and 27.20% over Gemini; (iii) instruction loss: 8.13% over GPT-3.5 & 4.68% over Gemini.",10.1109/CI-NLPSoMeCompanion65206.2025.10977857,nan,Computer Science
A Survey on Generative AI in Transportation Systems Management and Operation,2025,"Shriyank Somvanshi, Jinli Liu, Subasish Das","The integration of Generative AI (Gen-AI) into Transportation Systems Management and Operations (TSMO) offers transformative potential to address persistent knowledge management challenges. This survey study explores how Gen-AI can revolutionize TSMO through enhanced data integration, automated knowledge extraction, and predictive modeling. By synthesizing diverse datasets, Gen-AI facilitates the creation of unified knowledge repositories, improves real-time decision-making, and supports proactive scenario planning. The study introduces a comprehensive framework for embedding Gen-AI into TSMO workflows, enabling streamlined operational efficiency, cross-agency collaboration, and scalable data-driven strategies. Key applications of Gen-AI in TSMO include adaptive traffic control, crash response planning, and predictive modeling for future traffic scenarios. The study highlights innovative Gen-AI models and techniques, such as generative adversarial networks (GANs), large language models (LLMs), and hybrid augmented intelligence frameworks, which collectively enhance TSMO's capacity for resilience and responsiveness. Despite its transformative potential, Gen-AI adoption faces critical challenges, including ethical considerations, computational constraints, and the need for stakeholder trust. The paper emphasizes the importance of responsible AI development, fairness, and explainability to ensure sustainable adoption. By addressing these barriers, the proposed framework sets the stage for a new era of intelligent, adaptive, and community-centered transportation system management,",10.1109/CAI64502.2025.00148,"['JournalArticle', 'Conference', 'Review']",Computer Science; Engineering
Cellular-based Indoor Localization with Adapted LLM and Label-aware Contrastive Learning,2025,"Ren Ozeki, Haruki Yonekura, Hamada Rizk, Hirozumi Yamaguchi","Accurate indoor positioning is essential for mobile computing, human-computer interaction, and next-generation smart environments, enabling applications in indoor navigation, augmented reality, personalized services, healthcare, and emergency response. Cellular signal fingerprinting has emerged as a widely adopted solution, with deep learning models achieving state-of-the-art performance. However, existing approaches face critical deployment challenges, including labor-intensive fingerprinting, sparse reference points, and missing RSS values caused by environmental interference, hardware variability, and dynamic signal fluctuations. These limitations hinder their scalability, adaptability, and real-world usability in complex indoor environments. To address these challenges, we present GPT2Loc a novel indoor localization framework that integrates LLM with label-aware contrastive learning, improving accuracy while reducing reliance on extensive fingerprinting. LLMs effectively extract meaningful spatial features from incomplete and noisy RSS data, enabling robust localization even in sparsely finger-printed areas. Our label-aware contrastive learning approach further enhances generalization by aligning latent representations with spatial relationships, allowing GPT2Loc to interpolate user locations in unseen areas and mitigate signal inconsistencies.",10.1109/SMARTCOMP65954.2025.00070,"['JournalArticle', 'Conference']",Computer Science; Engineering
"A Critical Analysis of Generative AI: Challenges, Opportunities, and Future Research Directions",2025,"Mueen Uddin, S. Arfeen, Fuhid Alanazi, Saddam Hussain, Tehseen Mazhar, Md. Arafatur Rahman",,10.1007/s11831-025-10355-z,"['JournalArticle', 'Review']",Computer Science
AI Mock Interviewer: Preparing You Towards Smarter Interview Practice,2024,Ijsrem Journal,"In today's fiercely competitive job market, interview performance plays a pivotal role in securing employment. However, candidates often face the challenge of limited access to personalized and realistic interview practice opportunities, hindering their confidence-building efforts. To address this gap, we propose an AI-powered mock interviewer web application that creates an interview environment for candidates and elevates their interview preparation. The web app extracts vital information from user-uploaded resumes using Google's Palm model and Textract, crafting interview questions customized to the user's role and skills. It offers HR and Technical interview rounds. The HR segment assesses soft skills, while the Technical round presents role-specific technical questions, resulting in comprehensive performance reports and personalized feedback for targeted preparation. Notably, the web app employs voice analysis to detect fillers, pauses, and repetitive language, offering practical insights for improved communication. This analysis provides practical insights into communication challenges and offers actionable feedback to improve interview articulation and bolster user confidence. Key Words: Interview Preparation, AI-Powered Mock Interview Simulation, Personalized Feedback, Confidence Building",10.55041/ijsrem28199,['JournalArticle'],Computer Science
Personalized Top-k Set Queries Over Predicted Scores,2025,"Sohrab Namazi Nia, Subhodeep Ghosh, Senjuti Basu Roy, S. Amer-Yahia","This work studies the applicability of expensive external oracles such as large language models in answering top-k queries over predicted scores. Such scores are incurred by user-defined functions to answer personalized queries over multi-modal data. We propose a generic computational framework that handles arbitrary set-based scoring functions, as long as the functions could be decomposed into constructs, each of which sent to an oracle (in our case an LLM) to predict partial scores. At a given point in time, the framework assumes a set of responses and their partial predicted scores, and it maintains a collection of possible sets that are likely to be the true top-k. Since calling oracles is costly, our framework judiciously identifies the next construct, i.e., the next best question to ask the oracle so as to maximize the likelihood of identifying the true top-k. We present a principled probabilistic model that quantifies that likelihood. We study efficiency opportunities in designing algorithms. We run an evaluation with three large scale datasets, scoring functions, and baselines. Experiments indicate the efficacy of our framework, as it achieves an order of magnitude improvement over baselines in requiring LLM calls while ensuring result accuracy. Scalability experiments further indicate that our framework could be used in large-scale applications.",10.48550/arXiv.2502.12998,['JournalArticle'],Computer Science
AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation,2025,"Zefang Liu, Arman Anwar","Incident response (IR) requires fast, coordinated, and well-informed decision-making to contain and mitigate cyber threats. While large language models (LLMs) have shown promise as autonomous agents in simulated IR settings, their reasoning is often limited by a lack of access to external knowledge. In this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that incorporates retrieval-augmented generation (RAG) into multi-agent incident response simulations. Built on the Backdoors&Breaches (B&B) tabletop game environment, AutoBnB-RAG enables agents to issue retrieval queries and incorporate external evidence during collaborative investigations. We introduce two retrieval settings: one grounded in curated technical documentation (RAG-Wiki), and another using narrative-style incident reports (RAG-News). We evaluate performance across eight team structures, including newly introduced argumentative configurations designed to promote critical reasoning. To validate practical utility, we also simulate real-world cyber incidents based on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct complex multi-stage attacks. Our results show that retrieval augmentation improves decision quality and success rates across diverse organizational models. This work demonstrates the value of integrating retrieval mechanisms into LLM-based multi-agent systems for cybersecurity decision-making.",10.48550/arXiv.2508.13118,['JournalArticle'],Computer Science
Mobile Application for the Intelligent Management of Small Shops using the GPT-4 Model,2024,"Gonzalo Sakuda, Jack Leon, Royer Rojas","Small shops and retail stores in Peru face significant challenges in efficient management due to limited access to technological resources, often relying on error-prone manual processes. This study presents the development of a mobile application powered by the GPT-4 model, designed to provide personalized recommendations and optimize key operations such as inventory management and sales forecasting. By leveraging GPT-4's advanced natural language processing capabilities, the application acts as an intelligent assistant, enabling shop owners to make informed decisions, automate routine tasks, and enhance business performance. Key features include product and sales management, dashboards, personalized recommendations, alerts, and other operational tools. A survey conducted with 15 shop owners revealed positive results, with over 66.7% rating the application between 4 and 5 on a Likert scale (15), indicating its effectiveness in improving small shop management with the support of GPT-4.",10.1109/ICALTER65499.2024.10819203,"['Conference', 'Review']",Business; Computer Science
Large Language Model-based CSI Feedback for Massive MIMO Systems,2025,"Seokhyun Jeong, Ji-Sun Moon, Hokyung Kim, Hyunjun Kwon, B. Shim","Recently, large language models (LLMs) have gained much attention in next-generation wireless communications due to their adaptability to the dynamic wireless environment. One application of LLM in wireless communication is channel state information (CSI) feedback enhancement. While existing studies for CSI feedback suffer from generalization and user mobility issues due to the fixed network parameters, LLM can effectively solve the problems by leveraging the generalization capability. In this paper, we propose an LLM-based CSI feedback (LLM-CF) that utilizes the versatility of LLM to achieve CSI feedback performance enhancement in various communication scenarios. One key component of LLM-CF is the wireless configuration prompt (WCP), which explicitly provides LLM with the specifications on wireless scenarios and a detailed guideline for CSI feedback. By following the instructions in WCP, LLM-CF can obtain input characteristics, scenario details, and step-by-step logic for reconstructing CSI. Using this information, LLM-CF can adaptively reconstruct downlink CSI in response to the dynamic wireless environment. From the simulation results, we show that LLM-CF outperforms the conventional CSI feedback techniques by more than 2.5 dB gain in terms of the normalized mean square error (NMSE).",10.1109/ICCWorkshops67674.2025.11162173,"['JournalArticle', 'Conference']",Computer Science; Engineering
EvalNet: Sentiment Analysis and Multimodal Data Fusion for Recruitment Interview Processing,2025,"Minhui Xie, Boyan Liu","Large language models (LLMs) have made strides in various tasks, yet challenges remain in effectively analyzing complex data from multiple sources, particularly in recruitment scenarios. Sentiment analysis in interviews requires comprehension of diverse cues conveyed through audio, visual, and textual modalities. To address this, we introduce EvalNet, a framework designed specifically for recruitment interview processing, enabling a holistic assessment of candidate responses. EvalNet leverages advanced deep learning methodologies to extract features and categorize sentiment in real-time. By integrating multimodal data, EvalNet enhances predictive accuracy significantly compared to traditional methods focused on single modalities. Experiments carried out with a range of recorded interviews illustrate EvalNet's superior performance in sentiment detection and understanding intricate candidate expressions, serving as a valuable tool for recruiters to make evidence-based decisions in recruitment practices.",10.1109/ICAITA67588.2025.11137985,['Conference'],Computer Science
Simulating Field Experiments with Large Language Models,2024,"Yaoyu Chen, Yuheng Hu, Yingda Lu","Prevailing large language models (LLMs) are capable of human responses simulation through its unprecedented content generation and reasoning abilities. However, it is not clear whether and how to leverage LLMs to simulate field experiments. In this paper, we propose and evaluate two prompting strategies: the observer mode that allows a direct prediction on main conclusions and the participant mode that simulates distributions of responses from participants. Using this approach, we examine fifteen well cited field experimental papers published in INFORMS and MISQ, finding encouraging alignments between simulated experimental results and the actual results in certain scenarios. We further identify topics of which LLMs underperform, including gender difference and social norms related research. Additionally, the automatic and standardized workflow proposed in this paper enables the possibility of a large-scale screening of more papers with field experiments. This paper pioneers the utilization of large language models (LLMs) for simulating field experiments, presenting a significant extension to previous work which focused solely on lab environments. By introducing two novel prompting strategies, observer and participant modes, we demonstrate the ability of LLMs to both predict outcomes and replicate participant responses within complex field settings. Our findings indicate a promising alignment with actual experimental results in certain scenarios, achieving a stimulation accuracy of 66% in observer mode. This study expands the scope of potential applications for LLMs and illustrates their utility in assisting researchers prior to engaging in expensive field experiments. Moreover, it sheds light on the boundaries of LLMs when used in simulating field experiments, serving as a cautionary note for researchers considering the integration of LLMs into their experimental toolkit.",10.48550/arXiv.2408.09682,['JournalArticle'],Computer Science; Linguistics
"BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law",2025,"Juvenal Domingos J'unior, Augusto Faria, Eduardo Seiti de Oliveira, Erick de Brito, Matheus Teotonio, Andre Assumpcao, D. Carmo, Roberto A. Lotufo, Jayr Pereira","This paper presents BR-TaxQA-R, a novel dataset designed to support question answering with references in the context of Brazilian personal income tax law. The dataset contains 715 questions from the 2024 official Q\&A document published by Brazil's Internal Revenue Service, enriched with statutory norms and administrative rulings from the Conselho Administrativo de Recursos Fiscais (CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings for searching and GPT-4o-mini for answer generation. We compare different text segmentation strategies and benchmark our system against commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics. Results show that our custom RAG pipeline outperforms commercial systems in Response Relevancy, indicating stronger alignment with user queries, while commercial models achieve higher scores in Factual Correctness and fluency. These findings highlight a trade-off between legally grounded generation and linguistic fluency. Crucially, we argue that human expert evaluation remains essential to ensure the legal validity of AI-generated answers in high-stakes domains such as taxation. BR-TaxQA-R is publicly available at https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.",10.48550/arXiv.2505.15916,['JournalArticle'],Computer Science; Law
Proposing an NLP-Based Model for Multi-Class Classification of Holland Personality,2025,"Evi Kunthi Anggraini, Andry Alamsyah, Nidya Dudija","Aligning job roles with individuals' personalities is a pivotal concern in today's dynamic work environment, where mismatches can impair performance and lead to inefficiencies. Many employees find themselves in roles misaligned with their intrinsic traits, leading to dissatisfaction, disengagement, and high turnover rates. Despite advancements, organizations often rely on traditional assessments like surveys and questionnaire, methods that can be costly and time-consuming. This study introduces a scalable, data-driven solution using text analytics to address personality-job alignment. We evaluate three cutting-edge NLP modelsRoBERTa, BERT, and DistilBERTaimed at classifying Holland personality types through LinkedIn profiles. Our methodology includes data collection, preprocessing, automated labeling using GPT 4.o mini, and model fine-tuning on Google Colab for multiclass classification. We measure performance using accuracy, precision, recall, and F1-score with macro-averaging for balanced evaluation. Results indicate that DistilBERT, when fine-tuned with optimal hyperparameters (learning rate of 5e-5, batch size of 16, and 6 epochs), exceeds its peers, achieving 90.77% accuracy, with 0.91 in precision, recall, and F1-score. These findings demonstrate DistilBERT's capability in identifying personality traits, indicating a shift from traditional methods to more efficient solutions. This research offers insights for talent acquisition and workforce optimization, supporting organizations in understanding personality traits to enhance hiring decisions. Future studies can enhance these findings by using broader datasets and exploring alternative models to refine classification precision.",10.1109/ICERA66156.2025.11087300,"['Conference', 'Review']",Computer Science; Psychology
Leveraging large language models for enhanced client communication in consulting,2025,Suprit Kumar Pattanayak,"This research explores the potential of large language models (LLMs) to revolutionize client communication in management consulting. Through a comprehensive analysis of current LLM capabilities and consulting industry needs, we propose a framework for integrating these AI technologies to enhance consultant-client interactions. Our mixed-methods study combines a systematic literature review, expert interviews, and a quantitative survey of 250 consulting professionals. Results indicate that LLMs can significantly improve communication efficiency, personalization, and knowledge transfer in consulting engagements. Key applications include automated report generation, real-time language translation, and AI-assisted Q&A systems. However, challenges around data privacy, model bias, and maintaining the human touch in consulting relationships must be addressed. This paper contributes to the growing body of research on AI applications in professional services and offers practical guidelines for consulting firms seeking to leverage LLMs responsibly.",10.30574/wjaets.2025.15.2.0607,"['JournalArticle', 'Review']",Business; Computer Science; Linguistics
