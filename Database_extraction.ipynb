{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff231391",
   "metadata": {},
   "source": [
    "# Extraction of studies via Databases & Registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a4bc2ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Standard Packages \n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Iterable, Tuple\n",
    "import unicodedata\n",
    "\n",
    "# API Call Packages\n",
    "import urllib\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98675b6f",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1149dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web of Science API Key: 7c0...\n",
      "Semantic Scholar API Key: eU3...\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# load api keys from .env\n",
    "api_key_WoS = os.getenv('WEB_OF_SCIENCE_API_KEY')\n",
    "api_key_SS = os.getenv('SEMANTIC_SCHOLAR_API_KEY')\n",
    "\n",
    "# Check if API keys are loaded\n",
    "print(f\"Web of Science API Key: {api_key_WoS[:3]}...\")  # Print first 4 characters\n",
    "print(f\"Semantic Scholar API Key: {api_key_SS[:3]}...\")  # Print first 4 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925dea7",
   "metadata": {},
   "source": [
    "## Web of Science API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41b36",
   "metadata": {},
   "source": [
    "**Search and field tags for Web of Science documents**\n",
    "- `sort_field`: Order by field(s). \n",
    "    - Field name and order by clause separated by '+', use A for ASC and D for DESC, \n",
    "    - Example: `PY+D`. Multiple values are separated by comma. \n",
    "    - Supported fields:  * **LD** - Load Date * **PY** - Publication Year * **RS** - Relevance * **TC** - Times Cited  (optional)\n",
    "- `...time_span`: Beginning and end dates must be specified in the yyyy-mm-dd format separated by '+' or ' ', e.g. 2023-01-01+2023-12-31. This parameter is not compatible with the all databases search, i.e. db=WOK is not compatible with this parameter. (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04aeb5c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import WoS API client\n",
    "import clarivate.wos_starter.client\n",
    "from clarivate.wos_starter.client.rest import ApiException\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522fb5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up Web of Science API client\n",
    "BASE_WoS = \"https://api.clarivate.com/apis/wos-starter/v1\"\n",
    "configuration = clarivate.wos_starter.client.Configuration(host = BASE_WoS)\n",
    "configuration.api_key['ClarivateApiKeyAuth'] = api_key_WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0651d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "# -------------- Function to run API query --------------\n",
    "def run_wos_api(\n",
    "        q,                          # Search query in WOS search syntax\n",
    "        db='WOS',                       # Choice of Database\n",
    "        limit=50,                       # Set limit of records on page (1-50) (default to 10)\n",
    "        page=1,                         # Set the result page \n",
    "        sort_field='RS+D',              # Order by Field(s), option: LD, PY, RS, TC\n",
    "        modified_time_span=None,        # Date range in which results were most recently modified.\n",
    "        tc_modified_time_span=None,     # Date range in which times cited counts were modified.\n",
    "        detail=None,                    # Set to returns full data by default, alternative: detail=short\n",
    "        configuration=configuration ):\n",
    "\n",
    "    with clarivate.wos_starter.client.ApiClient(configuration) as api_client:\n",
    "        api_instance = clarivate.wos_starter.client.DocumentsApi(api_client)\n",
    "        try:\n",
    "            api_response = api_instance.documents_get(\n",
    "                q,\n",
    "                db=db,\n",
    "                limit=limit,\n",
    "                page=page,\n",
    "                sort_field=sort_field,\n",
    "                modified_time_span=modified_time_span,\n",
    "                tc_modified_time_span=tc_modified_time_span,\n",
    "                detail=detail\n",
    "            )\n",
    "            return api_response\n",
    "        \n",
    "        except ApiException as e:\n",
    "            print(f\"Exception when calling DocumentsApi->documents_get: {e}\")\n",
    "            return None\n",
    "        \n",
    "# -------------- Function to Fetch --------------\n",
    "# Funciton: Fetch X number of pages\n",
    "def wos_fetch_pages(q: str, limit: int = 50) -> pd.DataFrame:\n",
    "    all_hits = []\n",
    "\n",
    "    for p in range(1, 11):  # pages 1-10\n",
    "        resp = run_wos_api(q, page=p, limit=limit)\n",
    "        if resp is None:\n",
    "            print(f\"[WARN] No response for page {p}\")\n",
    "            continue\n",
    "        hits = getattr(resp, \"hits\", []) or []\n",
    "        all_hits.extend(h.to_dict() for h in hits)\n",
    "\n",
    "    if not all_hits:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_hits)\n",
    "    if \"uid\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"uid\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Funciton: Fetch ALL pages\n",
    "def wos_fetch_all_pages(q: str, limit: int = 50) -> pd.DataFrame:\n",
    "    # Step 1: Fetch the first page to get the total number of records\n",
    "    resp = run_wos_api(q, page=1, limit=limit)\n",
    "    if resp is None:\n",
    "        print(f\"[WARN] No response for the first page of query: {q}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_records = getattr(resp.metadata, \"total\", 0)  # Get the total number of records\n",
    "    if total_records == 0:\n",
    "        print(f\"[WARN] No records found for query: {q}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Step 2: Calculate the number of pages required\n",
    "    total_pages = (total_records + limit - 1) // limit  # equivalent to math.ceil(total_records / limit)\n",
    "\n",
    "    # Step 3: Loop through all pages and collect the records\n",
    "    all_hits = []\n",
    "    for page in range(1, total_pages + 1):\n",
    "        resp = run_wos_api(q, page=page, limit=limit)\n",
    "        if resp is None:\n",
    "            print(f\"[WARN] No response for page {page} of query: {q}\")\n",
    "            continue\n",
    "\n",
    "        hits = getattr(resp, \"hits\", []) or []\n",
    "        all_hits.extend(h.to_dict() for h in hits)\n",
    "\n",
    "    if not all_hits:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Step 4: Convert the results to a DataFrame\n",
    "    df = pd.DataFrame(all_hits)\n",
    "    \n",
    "    # Deduplicate based on 'uid' (unique identifier)\n",
    "    if \"uid\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"uid\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function: Get total record counts for each query\n",
    "def wos_query_totals(wos_queries: dict) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for name, q in wos_queries.items():\n",
    "        resp = run_wos_api(q, page=1, limit=1)\n",
    "        print(f\"Processing query: {name}\")\n",
    "        \n",
    "        if resp is None:\n",
    "            results.append({\"QueryName\": name, \"TotalRecords\": None})\n",
    "            continue\n",
    "\n",
    "        total = getattr(resp.metadata, \"total\", None)\n",
    "        results.append({\"QueryName\": name, \"TotalRecords\": total})\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"TotalRecords\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc332c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define search terms\n",
    "\n",
    "# 1. LLM Block\n",
    "LLM_Block = 'TS=(\"large language model*\" OR \"foundation model*\" OR LLM OR LLMs OR GPT OR LLaMA* OR Mistral OR Mixtral OR Claude* OR Gemini OR PaLM OR Qwen OR DeepSeek OR \"Falcon 180B\" OR \"Phi-3\")'\n",
    "\n",
    "# 2. Survey Block\n",
    "Survey_Block = 'TS=(survey* OR \"survey data\" OR \"survey response*\" OR questionnaire* OR question* OR \"opinion poll*\" OR \"public opinion*\" OR attitude* OR value* OR norm* OR moral* OR \"feeling thermometer*\" OR \"open-ended\" OR \"open ended\" OR nonresponse OR \"non-response\" OR respondent* OR participant* OR interview* OR \"self-report*\" OR \"data collection\")'\n",
    "\n",
    "# 3. Simulation Block (merged A+B)\n",
    "Simulation_BlockA = 'TS=((simulat* OR emulat* OR predict* OR imput* OR \"missing data\" OR nonresponse OR \"non-response\" OR \"item nonresponse\" OR \"unit nonresponse\" OR \"synthetic respondent*\" OR \"synthetic participant*\" OR \"artificial respondent*\" OR \"artificial participant*\" OR \"virtual respondent*\" OR \"virtual participant*\" OR persona* OR \"role play*\") NEAR/5 (survey* OR questionnaire* OR respondent* OR response* OR interview* OR \"self-report*\" OR \"data collection\" OR opinion* OR poll*))'\n",
    "\n",
    "# 3. 2nd version of  Simulation Block (more comprehensive)\n",
    "Simulation_BlockB = 'TS=( ( simulat* OR emulat* OR predict* OR imput* OR \"synthetic data\" OR \"missing data\" OR nonresponse OR \"non-response\" OR \"item nonresponse\" OR \"synthetic respondent*\" OR \"synthetic participant*\" OR \"artificial respondent*\" OR \"artificial participant*\" OR \"virtual respondent*\" OR \"virtual participant*\" OR persona* OR \"role play*\" OR \"as a respondent\" OR \"LLM as respondent\" OR \"model as respondent\" OR proxy OR surrogate OR \"stand-in\" OR \"stand in\" OR replac* OR substitut* OR represent* OR fidelit* OR faithful* OR doppelg* OR (\"Synthetic Voice*\" NEAR/5 (persona* OR respondent* OR survey* OR \"public opinion*\" OR opinion*)) OR (\"representing people\" NEAR/3 (survey* OR respondent* OR persona* OR opinion*)) OR (\"LLM-generated persona*\" OR \"LLM generated persona*\") ) NEAR/5 (survey* OR questionnaire* OR respondent* OR response* OR interview* OR \"self-report*\" OR \"data collection\" OR opinion* OR poll* OR attitude* OR value* OR norm* OR \"public opinion*\") )'\n",
    "\n",
    "# 3. 3rd version of  Simulation Block (fixed phrases)\n",
    "Simulation_BlockC = 'TS=(\"survey simulation\" OR \"simulated participant*\" OR \"simulated respondent*\" OR \"synthetic data\" OR \"synthetic survey data\" OR \"synthetic respondent*\" OR \"synthetic participant*\" OR \"artificial respondent*\" OR \"artificial participant*\" OR \"virtual respondent*\" OR \"virtual participant*\" OR \"LLM as respondent\" OR \"model as respondent\" OR \"as a respondent\" OR \"role play*\" OR persona*)'\n",
    "\n",
    "# 4. Model Training Block (optional)\n",
    "Methods_Block = 'TS=( prompt* OR \"few-shot\" OR \"few-shot learning\" OR \"zero-shot\" OR \"zero-shot learning\" OR \"in-context learning\" OR ICL OR \"chain of thought\" OR \"self-consistency\" OR \"system message\" OR persona OR personas OR \"role prompt*\" OR \"instruction-tun*\" OR \"instruction prompt*\" OR \"fine-tun*\" OR (\"reinforcement learning with human feedback\" OR RLHF) OR (\"reinforcement learning with AI feedback\" OR RLAIF) OR \"temperature parameter\" OR \"temperature setting\" OR \"nucleus sampling\" OR \"top-p sampling\" OR \"active learning\" OR \"transfer learning\" OR \"meta learning\" OR \"meta-learning\" OR \"representation learning\" OR \"continual learning\" OR \"lifelong learning\" )'\n",
    "\n",
    "# Create combinations of search blocks with Exclusion Block\n",
    "LLM_and_Survey = f'{LLM_Block} AND {Survey_Block}'\n",
    "LLM_and_Survey_and_Methods = f'{LLM_Block} AND {Survey_Block} AND {Methods_Block}'\n",
    "LLM_and_SimulationA = f'{LLM_Block} AND {Simulation_BlockA}'\n",
    "LLM_and_SimulationB = f'{LLM_Block} AND {Simulation_BlockB}'\n",
    "LLM_and_SimulationC = f'{LLM_Block} AND {Simulation_BlockC}'\n",
    "LLM_and_Methods = f'{LLM_Block} AND {Methods_Block}'\n",
    "LLM_and_Survey_and_SimulationA = f'{LLM_Block} AND {Survey_Block} AND {Simulation_BlockA}'\n",
    "LLM_and_Survey_and_SimulationB = f'{LLM_Block} AND {Survey_Block} AND {Simulation_BlockB}'\n",
    "LLM_and_Survey_and_SimulationC = f'{LLM_Block} AND {Survey_Block} AND {Simulation_BlockC}'\n",
    "LLM_and_SimulationA_and_Methods = f'{LLM_Block} AND {Simulation_BlockA} AND {Methods_Block}'\n",
    "LLM_and_SimulationB_and_Methods = f'{LLM_Block} AND {Simulation_BlockB} AND {Methods_Block}'\n",
    "LLM_and_SimulationC_and_Methods = f'{LLM_Block} AND {Simulation_BlockC} AND {Methods_Block}'\n",
    "LLMSurvey_or_LLMSimulationA = f'({LLM_and_Survey}) OR ({LLM_and_SimulationA})'\n",
    "LLMSurvey_or_LLMSimulationB = f'({LLM_and_Survey}) OR ({LLM_and_SimulationB})'\n",
    "LLMSurvey_or_LLMSimulationC = f'({LLM_and_Survey}) OR ({LLM_and_SimulationC})'\n",
    "Survey_and_SimulationA = f'{Survey_Block} AND {Simulation_BlockA}'\n",
    "Survey_and_SimulationB = f'{Survey_Block} AND {Simulation_BlockB}'\n",
    "Survey_and_SimulationC = f'{Survey_Block} AND {Simulation_BlockC}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44eefa65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE Set of Queries\n",
    "wos_queries = {\n",
    "    \"LLM\": LLM_Block,\n",
    "    \"Survey\": Survey_Block,\n",
    "    \"SimulationB\": Simulation_BlockB,\n",
    "    \"LLM and Survey\": LLM_and_Survey,\n",
    "    \"LLM and Survey and Methods\": LLM_and_Survey_and_Methods,\n",
    "    \"LLM and SimulationA\": LLM_and_SimulationA,\n",
    "    \"LLM and SimulationB\": LLM_and_SimulationB,\n",
    "    \"LLM and SimulationC\": LLM_and_SimulationC,\n",
    "    \"LLM and Methods\": LLM_and_Methods,\n",
    "    \"LLM and Survey and SimulationA\": LLM_and_Survey_and_SimulationA,\n",
    "    \"LLM and Survey and SimulationB\": LLM_and_Survey_and_SimulationB,\n",
    "    \"LLM and Survey and SimulationC\": LLM_and_Survey_and_SimulationC,\n",
    "    \"LLM and SimulationA and Methods\": LLM_and_SimulationA_and_Methods,\n",
    "    \"LLM and SimulationB and Methods\": LLM_and_SimulationB_and_Methods,\n",
    "    \"LLM and SimulationC and Methods\": LLM_and_SimulationC_and_Methods,\n",
    "    \"LLMSurvey or LLMSimulationA\": LLMSurvey_or_LLMSimulationA,\n",
    "    \"LLMSurvey or LLMSimulationB\": LLMSurvey_or_LLMSimulationB,\n",
    "    \"LLMSurvey or LLMSimulationC\": LLMSurvey_or_LLMSimulationC,\n",
    "    \"Survey and SimulationA\": Survey_and_SimulationA,\n",
    "    \"Survey and SimulationB\": Survey_and_SimulationB,\n",
    "    \"Survey and SimulationC\": Survey_and_SimulationC\n",
    "}\n",
    "\n",
    "wos_queries_subset = {\n",
    "    #\"LLM_and_SimulationA\": LLM_and_SimulationA,\n",
    "    #\"LLM_and_SimulationB\": LLM_and_SimulationB,\n",
    "    #\"LLM_and_Survey_and_SimulationA\": LLM_and_Survey_and_SimulationA,\n",
    "    \"LLM_and_Survey_and_SimulationB\": LLM_and_Survey_and_SimulationB,\n",
    "    #\"LLM_and_SimulationA_and_Methods\": LLM_and_SimulationA_and_Methods,\n",
    "    #\"LLM_and_SimulationB_and_Methods\": LLM_and_SimulationB_and_Methods,\n",
    "    #\"LLMSurvey or LLMSimulationA\": LLMSurvey_or_LLMSimulationA,\n",
    "    #\"LLMSurvey or LLMSimulationB\": LLMSurvey_or_LLMSimulationB,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bb452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: LLM_and_SimulationA\n",
      "Processing query: LLM_and_SimulationB\n",
      "Processing query: LLM_and_Survey_and_SimulationA\n",
      "Processing query: LLM_and_Survey_and_SimulationB\n",
      "Processing query: LLM_and_SimulationA_and_Methods\n",
      "Processing query: LLM_and_SimulationB_and_Methods\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QueryName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TotalRecords",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "bcd390ff-0a88-48f4-8e7a-a72cbaee9b66",
       "rows": [
        [
         "0",
         "LLM_and_SimulationB",
         "2185"
        ],
        [
         "1",
         "LLM_and_Survey_and_SimulationB",
         "1680"
        ],
        [
         "2",
         "LLM_and_SimulationA",
         "829"
        ],
        [
         "3",
         "LLM_and_Survey_and_SimulationA",
         "447"
        ],
        [
         "4",
         "LLM_and_SimulationB_and_Methods",
         "331"
        ],
        [
         "5",
         "LLM_and_SimulationA_and_Methods",
         "171"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryName</th>\n",
       "      <th>TotalRecords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM_and_SimulationB</td>\n",
       "      <td>2185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM_and_Survey_and_SimulationB</td>\n",
       "      <td>1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM_and_SimulationA</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM_and_Survey_and_SimulationA</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM_and_SimulationB_and_Methods</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM_and_SimulationA_and_Methods</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         QueryName  TotalRecords\n",
       "0              LLM_and_SimulationB          2185\n",
       "1   LLM_and_Survey_and_SimulationB          1680\n",
       "2              LLM_and_SimulationA           829\n",
       "3   LLM_and_Survey_and_SimulationA           447\n",
       "4  LLM_and_SimulationB_and_Methods           331\n",
       "5  LLM_and_SimulationA_and_Methods           171"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN: Get Max Total records\n",
    "df_WoS_totals = wos_query_totals(wos_queries_subset)\n",
    "df_WoS_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91faf724",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# RUN Fetch for ALL queries\n",
    "dfs_WoS = {}\n",
    "for name, query in wos_queries.items():\n",
    "    print(f\"\\nFetching WoS results for: {name}\")\n",
    "    df = wos_fetch_all_pages(query, limit=50)\n",
    "    print(f\"{name}: {len(df)} rows\")\n",
    "    dfs_WoS[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "706d3180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching records for query: LLM_and_Survey_and_SimulationB\n"
     ]
    }
   ],
   "source": [
    "# RUN Fetch for SUBSET of queries\n",
    "dfs_WoS_subset = {}\n",
    "for query_name, query in wos_queries_subset.items():\n",
    "    print(f\"Fetching records for query: {query_name}\")\n",
    "    df_results = wos_fetch_all_pages(query)\n",
    "    dfs_WoS_subset[query_name] = df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c28bf0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions to clean and normalize WoS DataFrames\n",
    "def authors_from_names(names_obj):\n",
    "    if isinstance(names_obj, dict):\n",
    "        people = names_obj.get(\"authors\") or []\n",
    "        out = []\n",
    "        for a in people:\n",
    "            if isinstance(a, dict):\n",
    "                dn = a.get(\"displayName\") or a.get(\"wosStandard\") or a.get(\"full_name\") or \"\"\n",
    "                if dn:\n",
    "                    out.append(dn)\n",
    "        return \"; \".join(out)\n",
    "    return \"\"\n",
    "\n",
    "def keywords_from_obj(keywords_obj):\n",
    "    if isinstance(keywords_obj, dict):\n",
    "        ak = keywords_obj.get(\"authorKeywords\")\n",
    "        if isinstance(ak, list):\n",
    "            return \"; \".join([k for k in ak if isinstance(k, str)])\n",
    "        if isinstance(ak, str):\n",
    "            return ak\n",
    "    return \"\"\n",
    "\n",
    "def doi_from_identifiers(ident_obj):\n",
    "    if isinstance(ident_obj, dict):\n",
    "        doi = ident_obj.get(\"doi\")\n",
    "        if doi:\n",
    "            return doi\n",
    "        dois = ident_obj.get(\"dois\")\n",
    "        if isinstance(dois, list) and len(dois) > 0:\n",
    "            return dois[0]\n",
    "    return None\n",
    "\n",
    "def issn_from_identifiers(ident_obj):\n",
    "    if isinstance(ident_obj, dict):\n",
    "        val = ident_obj.get(\"issn\")\n",
    "        issn = val[0] if isinstance(val, list) and val else val\n",
    "        return issn\n",
    "    return None\n",
    "\n",
    "def isbn_from_identifiers(ident_obj):\n",
    "    if isinstance(ident_obj, dict):\n",
    "        val = ident_obj.get(\"isbn\")\n",
    "        isbn = val[0] if isinstance(val, list) and val else val\n",
    "        return isbn\n",
    "    return None\n",
    "\n",
    "def year_from_source(src_obj):\n",
    "    if isinstance(src_obj, dict):\n",
    "        return src_obj.get(\"publishYear\") or src_obj.get(\"publishedYear\")\n",
    "    return None\n",
    "\n",
    "def first_source_type(st_list):\n",
    "    if isinstance(st_list, list) and st_list:\n",
    "        return st_list[0]\n",
    "    return None\n",
    "\n",
    "def clean_wos_df(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        return pd.DataFrame(columns=[\"title\", \"authors\", \"doi\", \"Year\", \"keywords\", \"sourceType\"])\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Compute desired fields\n",
    "    df[\"authors\"] = df[\"names\"].apply(authors_from_names) if \"names\" in df.columns else \"\"\n",
    "    df[\"doi\"] = df[\"identifiers\"].apply(doi_from_identifiers) if \"identifiers\" in df.columns else None\n",
    "    df[\"issn\"] = df[\"identifiers\"].apply(issn_from_identifiers) if \"identifiers\" in df.columns else None\n",
    "    df[\"isbn\"] = df[\"identifiers\"].apply(isbn_from_identifiers) if \"identifiers\" in df.columns else None\n",
    "    df[\"Year\"] = df[\"source\"].apply(year_from_source) if \"source\" in df.columns else None\n",
    "    df[\"keywords\"] = df[\"keywords\"].apply(keywords_from_obj) if \"keywords\" in df.columns else \"\"\n",
    "    df[\"sourceType\"] = df[\"sourceTypes\"].apply(first_source_type) if \"sourceTypes\" in df.columns else None\n",
    "\n",
    "    # Drop intermediate/noisy columns\n",
    "    to_drop = [\"uid\", \"types\", \"sourceTypes\", \"source\", \"names\", \"links\", \"citations\", \"identifiers\"]\n",
    "    df = df.drop(columns=[c for c in to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    # Reorder columns (keep others after the key fields)\n",
    "    key_cols = [c for c in [\"title\", \"authors\", \"doi\", \"issn\", \"isbn\",\n",
    "                            \"Year\", \"keywords\", \"sourceType\"] if c in df.columns]\n",
    "    other_cols = [c for c in df.columns if c not in key_cols]\n",
    "    df = df[key_cols + other_cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df457fdf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# RUN Cleaning \n",
    "dfs_WoS_clean = {name: clean_wos_df(df) for name, df in dfs_WoS_clean.items()}\n",
    "\n",
    "df_WoS_LLM_and_Survey_clean = dfs_WoS_clean[\"LLM and Survey\"]\n",
    "df_WoS_LLM_and_Survey_and_Methods_clean = dfs_WoS_clean[\"LLM and Survey and Methods\"]\n",
    "df_WoS_LLM_and_SimulationA_clean = dfs_WoS_clean[\"LLM and SimulationA\"]\n",
    "df_WoS_LLM_and_SimulationB_clean = dfs_WoS_clean[\"LLM and SimulationB\"]\n",
    "df_WoS_LLM_and_SimulationC_clean = dfs_WoS_clean[\"LLM and SimulationC\"]\n",
    "df_WoS_LLM_and_Methods_clean = dfs_WoS_clean[\"LLM and Methods\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationA_clean = dfs_WoS_clean[\"LLM and Survey and SimulationA\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationB_clean = dfs_WoS_clean[\"LLM and Survey and SimulationB\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationC_clean = dfs_WoS_clean[\"LLM and Survey and SimulationC\"]\n",
    "df_WoS_LLM_and_SimulationA_and_Methods_clean = dfs_WoS_clean[\"LLM and SimulationA and Methods\"]\n",
    "df_WoS_LLM_and_SimulationB_and_Methods_clean = dfs_WoS_clean[\"LLM and SimulationB and Methods\"]\n",
    "df_WoS_LLM_and_SimulationC_and_Methods_clean = dfs_WoS_clean[\"LLM and SimulationC and Methods\"]\n",
    "df_WoS_LLMSurvey_or_LLMSimulationA_clean = dfs_WoS_clean[\"LLMSurvey or LLMSimulationA\"]\n",
    "df_WoS_LLMSurvey_or_LLMSimulationB_clean = dfs_WoS_clean[\"LLMSurvey or LLMSimulationB\"]\n",
    "df_WoS_LLMSurvey_or_LLMSimulationC_clean = dfs_WoS_clean[\"LLMSurvey or LLMSimulationC\"]\n",
    "df_WoS_Survey_and_SimulationA_clean = dfs_WoS_clean[\"Survey and SimulationA\"]\n",
    "df_WoS_Survey_and_SimulationB_clean = dfs_WoS_clean[\"Survey and SimulationB\"]\n",
    "df_WoS_Survey_and_SimulationC_clean = dfs_WoS_clean[\"Survey and SimulationC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1d5c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN Cleaning for Subset\n",
    "dfs_WoS_subset_clean = {name: clean_wos_df(df) for name, df in dfs_WoS_subset.items()}\n",
    "\n",
    "# Bind cleaned dataframes to variables\n",
    "#df_WoS_LLM_and_SimulationA = dfs_WoS_subset_clean[\"LLM_and_SimulationA\"]\n",
    "#df_WoS_LLM_and_SimulationB = dfs_WoS_subset_clean[\"LLM_and_SimulationB\"]\n",
    "#df_WoS_LLM_and_Survey_and_SimulationA = dfs_WoS_subset_clean[\"LLM_and_Survey_and_SimulationA\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationB = dfs_WoS_subset_clean[\"LLM_and_Survey_and_SimulationB\"]\n",
    "#df_WoS_LLM_and_SimulationA_and_Methods = dfs_WoS_subset_clean[\"LLM_and_SimulationA_and_Methods\"]\n",
    "#df_WoS_LLM_and_SimulationB_and_Methods = dfs_WoS_subset_clean[\"LLM_and_SimulationB_and_Methods\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7095cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataframes to CSV files\n",
    "df_WoS_LLM_and_Survey_and_SimulationB.to_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840ea0e",
   "metadata": {},
   "source": [
    "## Semantic Scholar API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ace23e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f41e66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd87858",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Search Parameters\n",
    "FIELDS_SS = [\"paperId\", \"title\", \"year\", \"authors\", \"abstract\", \"url\", \"citationCount\", \"externalIds\", \"publicationTypes\"]\n",
    "YEAR_FILTER = \"2020-\"\n",
    "BULK_SORT = \"citationCount:desc\"\n",
    "MAX_PAPERS_PER_GROUP = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8c2ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Function\n",
    "\n",
    "# -------- Helper Functions --------\n",
    "def author_names(paper_authors):\n",
    "    if not paper_authors:\n",
    "        return \"\"\n",
    "    names = []\n",
    "    for a in paper_authors:\n",
    "        # supports Author objects and dicts\n",
    "        names.append(getattr(a, \"name\", a.get(\"name\") if isinstance(a, dict) else None))\n",
    "    return \", \".join([n for n in names if n])\n",
    "\n",
    "def _safe_get(container, key):\n",
    "    \"\"\"Access dict or object attribute safely.\"\"\"\n",
    "    if container is None:\n",
    "        return None\n",
    "    if isinstance(container, dict):\n",
    "        return container.get(key)\n",
    "    return getattr(container, key, None)\n",
    "\n",
    "def ss_paper_row(p):\n",
    "    ext = getattr(p, \"externalIds\", None)\n",
    "    pv  = getattr(p, \"publicationVenue\", None)\n",
    "\n",
    "    return {\n",
    "        \"paperId\": getattr(p, \"paperId\", None),\n",
    "        \"title\": getattr(p, \"title\", None),\n",
    "        \"year\": getattr(p, \"year\", None),\n",
    "        \"authors\": author_names(getattr(p, \"authors\", None)),\n",
    "        \"abstract\": getattr(p, \"abstract\", None),\n",
    "        \"url\": getattr(p, \"url\", None),\n",
    "        \"citationCount\": getattr(p, \"citationCount\", None),\n",
    "        \"doi\": _safe_get(ext, \"DOI\") or _safe_get(ext, \"doi\"),\n",
    "        \"publicationTypes\": getattr(p, \"publicationTypes\", None),\n",
    "    }\n",
    "\n",
    "# -------- Main Fetch Function --------\n",
    "\n",
    "# Funciton: Fetch all pages\n",
    "def ss_fetch_bulk(tag: str,\n",
    "                       max_papers: int = MAX_PAPERS_PER_GROUP,\n",
    "                       sort: str | None = BULK_SORT) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all pages for a query key via Semantic Scholar bulk search,\n",
    "    returning a DataFrame with:paperId, title, year, authors, abstract, url, citationCount, doi\n",
    "    \"\"\"\n",
    "    if tag not in QUERY_GROUPS:\n",
    "        valid = \", \".join(QUERY_GROUPS.keys())\n",
    "        raise ValueError(f\"Unknown group '{tag}'. Valid keys: {valid}\")\n",
    "\n",
    "    sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "    query = QUERY_GROUPS[tag]\n",
    "\n",
    "    results = sch.search_paper(\n",
    "        query=query,\n",
    "        year=YEAR_FILTER,      \n",
    "        fields=FIELDS_SS,      \n",
    "        bulk=True,             \n",
    "        sort=sort,             \n",
    "    )\n",
    "\n",
    "    est_total = getattr(results, \"total\", None)\n",
    "    print(f\"Estimated total: {est_total if est_total is not None else 'n/a'}\")\n",
    "\n",
    "    rows = []\n",
    "    for i, p in enumerate(results, start=1):\n",
    "        rows.append(ss_paper_row(p))\n",
    "        if i >= max_papers:\n",
    "            break\n",
    "\n",
    "    cols = [\"paperId\", \"title\", \"year\", \"authors\", \"abstract\", \"url\", \"citationCount\", \"doi\", \"publicationTypes\"]\n",
    "    return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "# Function: Get total record counts for each query\n",
    "def ss_query_totals(query_groups: dict) -> pd.DataFrame:\n",
    "    sch = SemanticScholar\n",
    "    sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "    results = []\n",
    "    for name, query in query_groups.items():\n",
    "        res = sch.search_paper(\n",
    "            query=query,\n",
    "            year=YEAR_FILTER,\n",
    "            fields=FIELDS_SS,\n",
    "            bulk=True,\n",
    "            sort=BULK_SORT\n",
    "        )\n",
    "        total = getattr(res, \"total\", None)\n",
    "        results.append({\"QueryName\": name, \"TotalRecords\": total})    \n",
    "    return pd.DataFrame(results).sort_values(\"TotalRecords\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc6c4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Query Blocks\n",
    "LLM_Block = ('( \"large language model\" | \"large language models\" | \"foundation model\" | \"foundation models\" | LLM | LLMs | GPT | \"GPT-3\" | \"GPT-3.5\" | \"GPT-4\" | ChatGPT | LLaMA* | \"Llama 2\" | \"Llama 3\" | Mistral | Mixtral | Claude* | Gemini | PaLM | Qwen | DeepSeek |  \"Falcon 180B\" | \"Phi-3\" | \"AI language model\" | chatbot | \"chat bot\" | \"transformer language model\" | \"transformer-based model\" )')\n",
    "\n",
    "Survey_Block = ('( survey* | \"survey data\" | \"survey response\" | \"survey responses\" | questionnaire* | question* | \"opinion poll\" | \"public opinion\" | attitude* | value* | norm* | moral* | \"feeling thermometer\" | \"open ended\" | \"open-ended\" | nonresponse | \"non-response\" |  respondent* | participant* | interview* | \"self-report\" | \"self-reported\" | \"data collection\")' )\n",
    "\n",
    "Simulation_BlockA = ('((simulat* | emulat* | predict* | imput* | \"missing data\" | nonresponse | \"non-response\" |  \"item nonresponse\" | \"synthetic respondent*\" | \"synthetic participant*\" | \"artificial respondent*\" |  \"artificial participant*\" | \"virtual respondent*\" | \"virtual participant*\" | persona* | \"role play*\") + (survey* | questionnaire* | respondent* | response* | interview* | \"self-report*\" |  \"data collection\" | opinion* | poll*))')\n",
    "\n",
    "Simulation_BlockB = ('( simulat* | emulat* | predict* | imput* | \"synthetic data\" | \"missing data\" | nonresponse | \"non-response\" |  \"item nonresponse\" | \"synthetic respondent\" | \"synthetic respondents\" | \"synthetic participant\" |  \"synthetic participants\" | \"artificial respondent\" | \"artificial participant\" | \"virtual respondent\" | \"virtual participant\" | persona | personas |  \"role play\" | \"role-playing\" | (role + play*) |  \"as a respondent\" | \"LLM as respondent\" | \"model as respondent\" | proxy | surrogate | \"stand-in\" | \"stand in\" | replac* | substitut* | represent* | fidelit* | faithful* | doppelg* | (\"Synthetic Voice persona\"~5 | \"persona Synthetic Voice\"~5 |   \"representing people survey\"~5 | \"survey representing people\"~5 |  \"representing people respondent\"~5 | \"respondent representing people\"~5) )')\n",
    "\n",
    "Simulation_BlockC = ('( \"survey simulation\" | \"simulated participant\" | \"simulated respondent\" |  \"synthetic data\" | \"synthetic survey data\" | \"synthetic respondent\" | \"synthetic participant\" |  \"artificial respondent\" | \"artificial participant\" | \"virtual respondent\" | \"virtual participant\" |  \"LLM as respondent\" | \"model as respondent\" | \"as a respondent\" | \"role play\" | \"role-playing\" | persona | personas)')\n",
    "\n",
    "Methods_Block = ('(prompt* | \"few-shot\" | \"few-shot learning\" | \"zero-shot\" | \"zero-shot learning\" | \"in-context learning\" | ICL | \"chain of thought\" | \"self-consistency\" | \"system message\" | persona | personas | \"role prompt*\" | \"instruction-tun*\" | \"instruction prompt*\" | \"fine-tun*\" | (\"reinforcement learning with human feedback\" | RLHF) | (\"reinforcement learning with AI feedback\" | RLAIF) | \"temperature parameter\" | \"temperature setting\" | \"nucleus sampling\" | \"top-p sampling\" | \"active learning\" | \"transfer learning\" | \"meta learning\" | \"meta-learning\" | \"representation learning\" | \"continual learning\" | \"lifelong learning\")')\n",
    "\n",
    "# Combinations using + for AND and | for OR\n",
    "QUERY_GROUPS = {\n",
    "    # pairs\n",
    "    \"ss_llm_survey\":           f'{LLM_Block} + {Survey_Block}',\n",
    "    \"ss_llm_simA\":             f'{LLM_Block} + {Simulation_BlockA}',\n",
    "    \"ss_llm_simB\":             f'{LLM_Block} + {Simulation_BlockB}',\n",
    "    \"ss_llm_simC\":             f'{LLM_Block} + {Simulation_BlockC}',\n",
    "    #\"ss_llm_methods\":          f'{LLM_Block} + {Methods_Block}',\n",
    "    \"ss_survey_simA\":          f'{Survey_Block} + {Simulation_BlockA}',\n",
    "    \"ss_survey_simB\":          f'{Survey_Block} + {Simulation_BlockB}',\n",
    "    \"ss_survey_simC\":          f'{Survey_Block} + {Simulation_BlockC}',\n",
    "\n",
    "    # triples\n",
    "    #\"ss_llm_survey_methods\":       f'{LLM_Block} + {Survey_Block} + {Methods_Block}',\n",
    "    \"ss_llm_survey_simA\":          f'{LLM_Block} + {Survey_Block} + {Simulation_BlockA}',\n",
    "    \"ss_llm_survey_simB\":          f'{LLM_Block} + {Survey_Block} + {Simulation_BlockB}',\n",
    "    \"ss_llm_survey_simC\":          f'{LLM_Block} + {Survey_Block} + {Simulation_BlockC}',\n",
    "    #\"ss_llm_simA_methods\":         f'{LLM_Block} + {Simulation_BlockA} + {Methods_Block}',\n",
    "    #\"ss_llm_simB_methods\":         f'{LLM_Block} + {Simulation_BlockB} + {Methods_Block}',\n",
    "    #\"ss_llm_simC_methods\":         f'{LLM_Block} + {Simulation_BlockC} + {Methods_Block}',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a84e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QueryName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TotalRecords",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "996effd0-6c42-451c-ba42-9201caa861f6",
       "rows": [
        [
         "0",
         "ss_survey_and_simB",
         "2310601"
        ],
        [
         "1",
         "ss_survey_and_simA",
         "523273"
        ],
        [
         "2",
         "ss_survey_and_simC",
         "412066"
        ],
        [
         "3",
         "ss_llm_and_simB",
         "67238"
        ],
        [
         "4",
         "ss_llm_and_survey",
         "66238"
        ],
        [
         "5",
         "ss_llm_survey_simB",
         "21472"
        ],
        [
         "6",
         "ss_llm_and_simA",
         "9500"
        ],
        [
         "7",
         "ss_llm_and_simC",
         "6701"
        ],
        [
         "8",
         "ss_llm_survey_simA",
         "6014"
        ],
        [
         "9",
         "ss_llm_survey_simC",
         "3302"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryName</th>\n",
       "      <th>TotalRecords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ss_survey_and_simB</td>\n",
       "      <td>2310601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ss_survey_and_simA</td>\n",
       "      <td>523273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ss_survey_and_simC</td>\n",
       "      <td>412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ss_llm_and_simB</td>\n",
       "      <td>67238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ss_llm_and_survey</td>\n",
       "      <td>66238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ss_llm_survey_simB</td>\n",
       "      <td>21472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ss_llm_and_simA</td>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ss_llm_and_simC</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ss_llm_survey_simA</td>\n",
       "      <td>6014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ss_llm_survey_simC</td>\n",
       "      <td>3302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            QueryName  TotalRecords\n",
       "0  ss_survey_and_simB       2310601\n",
       "1  ss_survey_and_simA        523273\n",
       "2  ss_survey_and_simC        412066\n",
       "3     ss_llm_and_simB         67238\n",
       "4   ss_llm_and_survey         66238\n",
       "5  ss_llm_survey_simB         21472\n",
       "6     ss_llm_and_simA          9500\n",
       "7     ss_llm_and_simC          6701\n",
       "8  ss_llm_survey_simA          6014\n",
       "9  ss_llm_survey_simC          3302"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run: Get Max Total records\n",
    "df_SS_totals = ss_query_totals(QUERY_GROUPS)\n",
    "df_SS_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60bf1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total: 21474\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paperId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citationCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "publicationTypes",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "68fcef25-3453-4498-bbc4-b8e0fcc0fbe3",
       "rows": [
        [
         "0",
         "d40c77c010c8dbef6142903a02f2a73a85012d5d",
         "A Survey on Vision Transformer",
         "2020",
         "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, D. Tao",
         "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
         "https://www.semanticscholar.org/paper/d40c77c010c8dbef6142903a02f2a73a85012d5d",
         "2580",
         "10.1109/TPAMI.2022.3152247",
         "['JournalArticle', 'Review']"
        ],
        [
         "1",
         "888728745dbb769e29ed475d4f7661eebe1a71cf",
         "A Survey on Evaluation of Large Language Models",
         "2023",
         "Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, Xingxu Xie",
         "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
         "https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf",
         "2239",
         "10.1145/3641289",
         "['JournalArticle', 'Review']"
        ],
        [
         "2",
         "0671fd553dd670a4e820553a974bc48040ba0819",
         "Reflexion: language agents with verbal reinforcement learning",
         "2023",
         "Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, Shunyu Yao",
         "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.",
         "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
         "1755",
         null,
         "['JournalArticle']"
        ],
        [
         "3",
         "be8db99310602d66bba64bcf41a572c45816fbfc",
         "Let's Verify Step by Step",
         "2023",
         "H. Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, I. Sutskever, K. Cobbe",
         "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
         "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
         "1740",
         "10.48550/arXiv.2305.20050",
         "['JournalArticle']"
        ],
        [
         "4",
         "f4df78183261538e718066331898ee5cad7cad05",
         "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
         "2022",
         "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, M. Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer",
         "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
         "https://www.semanticscholar.org/paper/f4df78183261538e718066331898ee5cad7cad05",
         "1660",
         "10.18653/v1/2022.emnlp-main.759",
         "['JournalArticle', 'Conference']"
        ],
        [
         "5",
         "bd20069f5cac3e63083ecf6479abc1799db33ce0",
         "A Primer in BERTology: What We Know About How BERT Works",
         "2020",
         "Anna Rogers, Olga Kovaleva, Anna Rumshisky",
         "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
         "https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0",
         "1595",
         "10.1162/tacl_a_00349",
         "['JournalArticle', 'Review']"
        ],
        [
         "6",
         "2f3efe44083af91cef562c1a3451eee2f8601d22",
         "WebGPT: Browser-assisted question-answering with human feedback",
         "2021",
         "Reiichiro Nakano, Jacob Hilton, S. Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, W. Saunders, Xu Jiang, K. Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman",
         "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
         "https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22",
         "1460",
         null,
         "['JournalArticle']"
        ],
        [
         "7",
         "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
         "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
         "2023",
         "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu",
         "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
         "https://www.semanticscholar.org/paper/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
         "1371",
         "10.1145/3703155",
         "['JournalArticle', 'Review']"
        ],
        [
         "8",
         "b37b1dc72b1882858f5120f2cd6883134089a6ed",
         "MMBench: Is Your Multi-modal Model an All-around Player?",
         "2023",
         "Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin",
         "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
         "https://www.semanticscholar.org/paper/b37b1dc72b1882858f5120f2cd6883134089a6ed",
         "1340",
         "10.48550/arXiv.2307.06281",
         "['JournalArticle', 'Conference']"
        ],
        [
         "9",
         "002c256d30d6be4b23d365a8de8ae0e67e4c9641",
         "Improving language models by retrieving from trillions of tokens",
         "2021",
         "Sebastian Borgeaud, A. Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, G. Irving, O. Vinyals, Simon Osindero, K. Simonyan, Jack W. Rae, Erich Elsen, L. Sifre",
         "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
         "https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641",
         "1246",
         null,
         "['JournalArticle', 'Conference']"
        ],
        [
         "10",
         "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
         "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
         "2021",
         "Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, D. Roth",
         "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.",
         "https://www.semanticscholar.org/paper/c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
         "1235",
         "10.1145/3605943",
         "['JournalArticle', 'Review']"
        ],
        [
         "11",
         "91deaf9d324c8feafc189da0da03e60a60287bca",
         "Code as Policies: Language Model Programs for Embodied Control",
         "2022",
         "Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, Andy Zeng",
         "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
         "https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca",
         "1126",
         "10.1109/ICRA48891.2023.10160591",
         "['JournalArticle', 'Conference']"
        ],
        [
         "12",
         "afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
         "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
         "2024",
         "Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao",
         "This work presents Depth Anything11While the grammatical soundness of this name may be questionable, we treat it as a whole and pay homage to Segment Anything [26]., a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released here.",
         "https://www.semanticscholar.org/paper/afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
         "1119",
         "10.1109/CVPR52733.2024.00987",
         "['JournalArticle', 'Conference']"
        ],
        [
         "13",
         "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
         "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
         "2023",
         "Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu",
         "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
         "https://www.semanticscholar.org/paper/9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
         "938",
         "10.1109/TKDE.2024.3352100",
         "['JournalArticle', 'Review']"
        ],
        [
         "14",
         "348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
         "Capabilities of GPT-4 on Medical Challenge Problems",
         "2023",
         "Harsha Nori, Nicholas King, S. McKinney, Dean Carignan, E. Horvitz",
         "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.",
         "https://www.semanticscholar.org/paper/348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
         "903",
         "10.48550/arXiv.2303.13375",
         "['JournalArticle']"
        ],
        [
         "15",
         "94972e30504017156ef5b5debc419bf6edc67384",
         "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
         "2023",
         "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang",
         "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.",
         "https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384",
         "868",
         "10.48550/arXiv.2308.02490",
         "['JournalArticle', 'Conference']"
        ],
        [
         "16",
         "ebedc4d7a2356090904baba4104ef0832bc236df",
         "A survey on multimodal large language models",
         "2023",
         "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen",
         "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.",
         "https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",
         "778",
         "10.1093/nsr/nwae403",
         "['Review', 'JournalArticle']"
        ],
        [
         "17",
         "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
         "Red Teaming Language Models with Language Models",
         "2022",
         "Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, G. Irving",
         "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM. We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot’s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",
         "https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64",
         "756",
         "10.18653/v1/2022.emnlp-main.225",
         "['JournalArticle', 'Conference']"
        ],
        [
         "18",
         "db4ab91d5675c37795e719e997a2827d3d83cd45",
         "Towards Reasoning in Large Language Models: A Survey",
         "2022",
         "Jie Huang, K. Chang",
         "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",
         "https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45",
         "740",
         "10.48550/arXiv.2212.10403",
         "['JournalArticle', 'Conference', 'Review']"
        ],
        [
         "19",
         "6f4486c3d8ccd638c2a6bcbfafa01b5a3225bcab",
         "Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence",
         "2023",
         "G. Cooper",
         "The advent of generative artificial intelligence (AI) offers transformative potential in the field of education. The study explores three main areas: (1) How did ChatGPT answer questions related to science education? (2) What are some ways educators could utilise ChatGPT in their science pedagogy? and (3) How has ChatGPT been utilised in this study, and what are my reflections about its use as a research tool? This exploratory research applies a self-study methodology to investigate the technology. Impressively, ChatGPT’s output often aligned with key themes in the research. However, as it currently stands, ChatGPT runs the risk of positioning itself as the ultimate epistemic authority, where a single truth is assumed without a proper grounding in evidence or presented with sufficient qualifications. Key ethical concerns associated with AI include its potential environmental impact, issues related to content moderation, and the risk of copyright infringement. It is important for educators to model responsible use of ChatGPT, prioritise critical thinking, and be clear about expectations. ChatGPT is likely to be a useful tool for educators designing science units, rubrics, and quizzes. Educators should critically evaluate any AI-generated resource and adapt it to their specific teaching contexts. ChatGPT was used as a research tool for assistance with editing and to experiment with making the research narrative clearer. The intention of the paper is to act as a catalyst for a broader conversation about the use of generative AI in science education.",
         "https://www.semanticscholar.org/paper/6f4486c3d8ccd638c2a6bcbfafa01b5a3225bcab",
         "738",
         "10.1007/s10956-023-10039-y",
         null
        ],
        [
         "20",
         "4be7d1524edb0137599a5cc95f72844b85a52fe1",
         "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
         "2022",
         "Tim Dettmers, M. Lewis, Younes Belkada, Luke Zettlemoyer",
         "Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",
         "https://www.semanticscholar.org/paper/4be7d1524edb0137599a5cc95f72844b85a52fe1",
         "730",
         "10.48550/arXiv.2208.07339",
         "['JournalArticle']"
        ],
        [
         "21",
         "008a428e049003fe768068a0f1fa1416af5c4982",
         "Masked Feature Prediction for Self-Supervised Visual Pre-Training",
         "2021",
         "Chen Wei, Haoqi Fan, Saining Xie, Chaoxia Wu, A. Yuille, Christoph Feichtenhofer",
         "We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer based models. Without using extra model weights or supervision, MaskFeat pretrained on unlabeled videos achieves unprecedented results of 86.7% with MViTv2-L on Kinetics-400, 88.3% on Kinetics 600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageN et.",
         "https://www.semanticscholar.org/paper/008a428e049003fe768068a0f1fa1416af5c4982",
         "729",
         "10.1109/CVPR52688.2022.01426",
         "['JournalArticle', 'Conference']"
        ],
        [
         "22",
         "f4e612658bde9db88abfd455b99f181fdc536996",
         "Out of One, Many: Using Language Models to Simulate Human Samples",
         "2022",
         "Lisa P. Argyle, E. Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, D. Wingate",
         "Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.",
         "https://www.semanticscholar.org/paper/f4e612658bde9db88abfd455b99f181fdc536996",
         "724",
         "10.1017/pan.2023.2",
         "['JournalArticle', 'Review']"
        ],
        [
         "23",
         "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
         "Reasoning with Language Model is Planning with World Model",
         "2023",
         "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, D. Wang, Zhiting Hu",
         "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
         "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
         "720",
         "10.48550/arXiv.2305.14992",
         "['JournalArticle', 'Conference']"
        ],
        [
         "24",
         "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
         "Instruction Tuning for Large Language Models: A Survey",
         "2023",
         "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang",
         "This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users'objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
         "https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
         "706",
         "10.48550/arXiv.2308.10792",
         "['JournalArticle', 'Review']"
        ],
        [
         "25",
         "30c0cdc414f68211d5d0514df027cec22e005174",
         "A Survey on In-context Learning",
         "2022",
         "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui",
         "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
         "https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174",
         "686",
         "10.18653/v1/2024.emnlp-main.64",
         "['JournalArticle', 'Conference', 'Review']"
        ],
        [
         "26",
         "e251ba9fe7992fc07a01365a5f8f2b4d9020b875",
         "Artificial intelligence in higher education: the state of the field",
         "2023",
         "H. Crompton, D. Burke",
         "This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles were identified for a full examination. Using a priori, and grounded coding, the data from the 138 articles were extracted, analyzed, and coded. The findings of this study show that in 2021 and 2022, publications rose nearly two to three times the number of previous years. With this rapid rise in the number of AIEd HE publications, new trends have emerged. The findings show that research was conducted in six of the seven continents of the world. The trend has shifted from the US to China leading in the number of publications. Another new trend is in the researcher affiliation as prior studies showed a lack of researchers from departments of education. This has now changed to be the most dominant department. Undergraduate students were the most studied students at 72%. Similar to the findings of other studies, language learning was the most common subject domain. This included writing, reading, and vocabulary acquisition. In examination of who the AIEd was intended for 72% of the studies focused on students, 17% instructors, and 11% managers. In answering the overarching question of how AIEd was used in HE, grounded coding was used. Five usage codes emerged from the data: (1) Assessment/Evaluation, (2) Predicting, (3) AI Assistant, (4) Intelligent Tutoring System (ITS), and (5) Managing Student Learning. This systematic review revealed gaps in the literature to be used as a springboard for future researchers, including new tools, such as Chat GPT. A systematic review examining AIEd in higher education (HE) up to the end of 2022. Unique findings in the switch from US to China in the most studies published. A two to threefold increase in studies published in 2021 and 2022 to prior years. AIEd was used for: Assessment/Evaluation, Predicting, AI Assistant, Intelligent Tutoring System, and Managing Student Learning.",
         "https://www.semanticscholar.org/paper/e251ba9fe7992fc07a01365a5f8f2b4d9020b875",
         "684",
         "10.1186/s41239-023-00392-8",
         "['JournalArticle', 'Review']"
        ],
        [
         "27",
         "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
         "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
         "2023",
         "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto",
         "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction-following abilities. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, DPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.",
         "https://www.semanticscholar.org/paper/cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
         "682",
         "10.48550/arXiv.2305.14387",
         "['JournalArticle']"
        ],
        [
         "28",
         "0cbb518c364067200476a51e5ce7476a4f582770",
         "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation",
         "2023",
         "Rui Chen, Y. Chen, Ningxin Jiao, K. Jia",
         "Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function (BRDF) into the text-to-3D task, and learn the surface material for photorealistic rendering of the generated surface. Our disentangled framework is more compatible with popular graphics engines, supporting relighting, editing, and physical simulation of the generated 3D assets. We conduct thorough experiments that show the advantages of our method over existing ones under different text-to-3D task settings. Project page and source codes: https://fantasia3d.github.io/.",
         "https://www.semanticscholar.org/paper/0cbb518c364067200476a51e5ce7476a4f582770",
         "659",
         "10.1109/ICCV51070.2023.02033",
         "['JournalArticle', 'Conference']"
        ],
        [
         "29",
         "65906e6027246ae9e4ecd18d6e019a24505c842e",
         "Aligning AI With Shared Human Values",
         "2020",
         "Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, J. Li, D. Song, J. Steinhardt",
         "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",
         "https://www.semanticscholar.org/paper/65906e6027246ae9e4ecd18d6e019a24505c842e",
         "651",
         null,
         "['JournalArticle']"
        ],
        [
         "30",
         "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
         "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
         "2023",
         "Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei",
         "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
         "https://www.semanticscholar.org/paper/1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
         "638",
         "10.48550/arXiv.2307.05973",
         "['JournalArticle']"
        ],
        [
         "31",
         "58f8925a8b87054ad0635a6398a7fe24935b1604",
         "Mind2Web: Towards a Generalist Agent for the Web",
         "2023",
         "Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su",
         "We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",
         "https://www.semanticscholar.org/paper/58f8925a8b87054ad0635a6398a7fe24935b1604",
         "623",
         "10.48550/arXiv.2306.06070",
         "['JournalArticle']"
        ],
        [
         "32",
         "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
         "Explainability for Large Language Models: A Survey",
         "2023",
         "Haiyan Zhao, Hanjie Chen, F. Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du",
         "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.",
         "https://www.semanticscholar.org/paper/26089bdfdbca1e6eaaceca71e3116b715bec6d47",
         "608",
         "10.1145/3639372",
         "['JournalArticle', 'Review']"
        ],
        [
         "33",
         "3599a236f285af48782fc30b1341d13ec7320735",
         "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
         "2023",
         "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kaichao Zhang, Cheng Ji, Qi Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, Lichao Sun Michigan State University, B. University, Lehigh University, Macquarie University, Nanyang Technological University, University of California at San Diego, Duke University, U. Chicago, Salesforce Research",
         "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
         "https://www.semanticscholar.org/paper/3599a236f285af48782fc30b1341d13ec7320735",
         "585",
         "10.48550/arXiv.2302.09419",
         "['JournalArticle', 'Review']"
        ],
        [
         "34",
         "8cf01c76b506f6bca5258071ed309fc4430c24d3",
         "The Role of ChatGPT, Generative Language Models, and Artificial Intelligence in Medical Education: A Conversation With ChatGPT and a Call for Papers",
         "2023",
         "G. Eysenbach",
         "ChatGPT is a generative language model tool launched by OpenAI on November 30, 2022, enabling the public to converse with a machine on a broad range of topics. In January 2023, ChatGPT reached over 100 million users, making it the fastest-growing consumer application to date. This interview with ChatGPT is part 2 of a larger interview with ChatGPT. It provides a snapshot of the current capabilities of ChatGPT and illustrates the vast potential for medical education, research, and practice but also hints at current problems and limitations. In this conversation with Gunther Eysenbach, the founder and publisher of JMIR Publications, ChatGPT generated some ideas on how to use chatbots in medical education. It also illustrated its capabilities to generate a virtual patient simulation and quizzes for medical students; critiqued a simulated doctor-patient communication and attempts to summarize a research article (which turned out to be fabricated); commented on methods to detect machine-generated text to ensure academic integrity; generated a curriculum for health professionals to learn about artificial intelligence (AI); and helped to draft a call for papers for a new theme issue to be launched in JMIR Medical Education on ChatGPT. The conversation also highlighted the importance of proper “prompting.” Although the language generator does make occasional mistakes, it admits these when challenged. The well-known disturbing tendency of large language models to hallucinate became evident when ChatGPT fabricated references. The interview provides a glimpse into the capabilities and limitations of ChatGPT and the future of AI-supported medical education. Due to the impact of this new technology on medical education, JMIR Medical Education is launching a call for papers for a new e-collection and theme issue. The initial draft of the call for papers was entirely machine generated by ChatGPT, but will be edited by the human guest editors of the theme issue.",
         "https://www.semanticscholar.org/paper/8cf01c76b506f6bca5258071ed309fc4430c24d3",
         "575",
         "10.2196/46885",
         "['Editorial']"
        ],
        [
         "35",
         "a1f76db91c0debcf93ae9889736bce8470902113",
         "Large Language Models: A Survey",
         "2024",
         "Shervin Minaee, Tomáš Mikolov, Narjes Nikzad, M. Chenaghlu, R. Socher, Xavier Amatriain, Jianfeng Gao",
         "Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",
         "https://www.semanticscholar.org/paper/a1f76db91c0debcf93ae9889736bce8470902113",
         "570",
         "10.48550/arXiv.2402.06196",
         "['JournalArticle', 'Review']"
        ],
        [
         "36",
         "42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
         "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
         "2023",
         "Wen Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Y. Qiao, Jifeng Dai",
         "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.",
         "https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
         "559",
         "10.48550/arXiv.2305.11175",
         "['JournalArticle']"
        ],
        [
         "37",
         "75994ebb52094581dcb7d145795f6bafe6e276bb",
         "Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects",
         "2020",
         "Serge-Lopez Wamba-Taguimdje, S. Wamba, Jean Robert Kala Kamdjoug, C. Wanko",
         "PurposeThe main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.Design/methodology/approachThis study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations’ AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.FindingsAI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes.Research limitations/implicationsAI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations’ performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company’s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.Originality/valueThis article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.",
         "https://www.semanticscholar.org/paper/75994ebb52094581dcb7d145795f6bafe6e276bb",
         "558",
         "10.1108/bpmj-10-2019-0411",
         "['JournalArticle', 'Review']"
        ],
        [
         "38",
         "8f831f341e959955a495730d81996e62c57cc0bd",
         "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
         "2023",
         "Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, K. Chang, Fei Huang, Reynold Cheng, Yongbin Li",
         "Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.",
         "https://www.semanticscholar.org/paper/8f831f341e959955a495730d81996e62c57cc0bd",
         "549",
         "10.48550/arXiv.2305.03111",
         "['JournalArticle']"
        ],
        [
         "39",
         "668075792a7ab40457d92e09da28d35c879271c3",
         "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
         "2025",
         "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Feng Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Haochen Ding, Hao-Xing Hu, Haoming Yang, Hao Zhang, Haotian Yao, Hao-Dong Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jia-Xing Guo, Jianling Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Li-Na Shi, Li-Tao Ye, Long Yu, Meng-Xiao Dong, Neo Y. Zhang, Ningchen Ma, Qi Pan, Qucheng Gong, Shaowei Liu, Shen Ma, Shu-Yan Wei, Sihan Cao, Si-Da Huang, Tao Jiang, Wei-Wei Gao, Weiming Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wen He, Xian-sen Wei, Xian-Xian Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xue-biao Pan, Y. Charles, Yang Li, Yan-Ling Hu, Yangyang Liu, Yanru Chen, Ye-Jia Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Yingbo Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhengxin Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziya Xu, Zonghan Yang",
         "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
         "https://www.semanticscholar.org/paper/668075792a7ab40457d92e09da28d35c879271c3",
         "545",
         "10.48550/arXiv.2501.12599",
         "['JournalArticle']"
        ],
        [
         "40",
         "b486982fa7c68a8a08df1111ba9607119419c488",
         "A Survey on Large Language Models for Recommendation",
         "2023",
         "Likang Wu, Zhilan Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, Enhong Chen",
         "Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.",
         "https://www.semanticscholar.org/paper/b486982fa7c68a8a08df1111ba9607119419c488",
         "522",
         "10.48550/arXiv.2305.19860",
         "['JournalArticle', 'Review']"
        ],
        [
         "41",
         "eb375712bd37250c350ecd3f559e1879e87eb3e5",
         "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
         "2024",
         "Yann Dubois, Bal'azs Galambosi, Percy Liang, Tatsunori Hashimoto",
         "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question:\"What would the preference be if the model's and baseline's output had the same length?\"To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, but we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98.",
         "https://www.semanticscholar.org/paper/eb375712bd37250c350ecd3f559e1879e87eb3e5",
         "502",
         "10.48550/arXiv.2404.04475",
         "['JournalArticle']"
        ],
        [
         "42",
         "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
         "Complexity-Based Prompting for Multi-Step Reasoning",
         "2022",
         "Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, Tushar Khot",
         "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
         "https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a",
         "488",
         "10.48550/arXiv.2210.00720",
         "['JournalArticle']"
        ],
        [
         "43",
         "374dd173491a59a10bbb2b3519ebcfe3649f529d",
         "Teaching Models to Express Their Uncertainty in Words",
         "2022",
         "Stephanie C. Lin, Jacob Hilton, Owain Evans",
         "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
         "https://www.semanticscholar.org/paper/374dd173491a59a10bbb2b3519ebcfe3649f529d",
         "487",
         "10.48550/arXiv.2205.14334",
         "['JournalArticle']"
        ],
        [
         "44",
         "31d2ccff82e313eb5c1620c44bb8322da4a38513",
         "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
         "2024",
         "Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, S. Mondal, Aman Chadha",
         "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
         "https://www.semanticscholar.org/paper/31d2ccff82e313eb5c1620c44bb8322da4a38513",
         "486",
         "10.48550/arXiv.2402.07927",
         "['JournalArticle', 'Review']"
        ],
        [
         "45",
         "0a309c2c47c34f51ef94e8075f11586ad2b1dd2b",
         "Adoption of AI-based chatbots for hospitality and tourism",
         "2020",
         "Rajasshrie Pillai, Brijesh Sivathanu",
         "This study aims to investigate the customers’ behavioral intention and actual usage (AUE) of artificial intelligence (AI)-powered chatbots for hospitality and tourism in India by extending the technology adoption model (TAM) with context-specific variables.,To understand the customers’ behavioral intention and AUE of AI-powered chatbots for tourism, the mixed-method design was used whereby qualitative and quantitative techniques were combined. A total of 36 senior managers and executives from the travel agencies were interviewed and the analysis of interview data was done using NVivo 8.0 software. A total of 1,480 customers were surveyed and the partial least squares structural equation modeling technique was used for data analysis.,As per the results, the predictors of chatbot adoption intention (AIN) are perceived ease of use, perceived usefulness, perceived trust (PTR), perceived intelligence (PNT) and anthropomorphism (ANM). Technological anxiety (TXN) does not influence the chatbot AIN. Stickiness to traditional human travel agents negatively moderates the relation of AIN and AUE of chatbots in tourism and provides deeper insights into manager’s commitment to providing travel planning services using AI-based chatbots.,This research presents unique practical insights to the practitioners, managers and executives in the tourism industry, system designers and developers of AI-based chatbot technologies to understand the antecedents of chatbot adoption by travelers. TXN is a vital concern for the customers; so, designers and developers should ensure that chatbots are easily accessible, have a user-friendly interface, be more human-like and communicate in various native languages with the customers.,This study contributes theoretically by extending the TAM to provide better explanatory power with human–robot interaction context-specific constructs – PTR, PNT, ANM and TXN – to examine the customers’ chatbot AIN. This is the first step in the direction to empirically test and validate a theoretical model for chatbots’ adoption and usage, which is a disruptive technology in the hospitality and tourism sector in an emerging economy such as India.",
         "https://www.semanticscholar.org/paper/0a309c2c47c34f51ef94e8075f11586ad2b1dd2b",
         "483",
         "10.1108/ijchm-04-2020-0259",
         "['Review']"
        ],
        [
         "46",
         "bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e",
         "Transformer models for text-based emotion detection: a review of BERT-based approaches",
         "2021",
         "F. A. Acheampong, Henry Nunoo-Mensah, Wenyu Chen",
         null,
         "https://www.semanticscholar.org/paper/bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e",
         "481",
         "10.1007/s10462-021-09958-2",
         "['JournalArticle', 'Review']"
        ],
        [
         "47",
         "8f070e301979732e0dd73f6aa6170309cf73aa7d",
         "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
         "2024",
         "Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, Xiangliang Zhang",
         "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.",
         "https://www.semanticscholar.org/paper/8f070e301979732e0dd73f6aa6170309cf73aa7d",
         "481",
         "10.48550/arXiv.2402.01680",
         "['JournalArticle', 'Conference', 'Review']"
        ],
        [
         "48",
         "33422275fbb9958f55419620697faf531482699b",
         "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
         "2020",
         "Zhengbao Jiang, J. Araki, Haibo Ding, Graham Neubig",
         "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
         "https://www.semanticscholar.org/paper/33422275fbb9958f55419620697faf531482699b",
         "481",
         "10.1162/tacl_a_00407",
         "['JournalArticle']"
        ],
        [
         "49",
         "123acfbccca0460171b6b06a4012dbb991cde55b",
         "Large Language Models Are Zero-Shot Time Series Forecasters",
         "2023",
         "Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson",
         "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.",
         "https://www.semanticscholar.org/paper/123acfbccca0460171b6b06a4012dbb991cde55b",
         "480",
         "10.48550/arXiv.2310.07820",
         "['JournalArticle']"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 100
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>doi</th>\n",
       "      <th>publicationTypes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d40c77c010c8dbef6142903a02f2a73a85012d5d</td>\n",
       "      <td>A Survey on Vision Transformer</td>\n",
       "      <td>2020</td>\n",
       "      <td>Kai Han, Yunhe Wang, Hanting Chen, Xinghao Che...</td>\n",
       "      <td>Transformer, first applied to the field of nat...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d40c77c0...</td>\n",
       "      <td>2580</td>\n",
       "      <td>10.1109/TPAMI.2022.3152247</td>\n",
       "      <td>[JournalArticle, Review]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>888728745dbb769e29ed475d4f7661eebe1a71cf</td>\n",
       "      <td>A Survey on Evaluation of Large Language Models</td>\n",
       "      <td>2023</td>\n",
       "      <td>Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, ...</td>\n",
       "      <td>Large language models (LLMs) are gaining incre...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/88872874...</td>\n",
       "      <td>2239</td>\n",
       "      <td>10.1145/3641289</td>\n",
       "      <td>[JournalArticle, Review]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0671fd553dd670a4e820553a974bc48040ba0819</td>\n",
       "      <td>Reflexion: language agents with verbal reinfor...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Noah Shinn, Federico Cassano, Beck Labash, A. ...</td>\n",
       "      <td>Large language models (LLMs) have been increas...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0671fd55...</td>\n",
       "      <td>1755</td>\n",
       "      <td>None</td>\n",
       "      <td>[JournalArticle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be8db99310602d66bba64bcf41a572c45816fbfc</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023</td>\n",
       "      <td>H. Lightman, Vineet Kosaraju, Yura Burda, Harr...</td>\n",
       "      <td>In recent years, large language models have gr...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/be8db993...</td>\n",
       "      <td>1740</td>\n",
       "      <td>10.48550/arXiv.2305.20050</td>\n",
       "      <td>[JournalArticle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f4df78183261538e718066331898ee5cad7cad05</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Ma...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Arte...</td>\n",
       "      <td>Large language models (LMs) are able to in-con...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/f4df7818...</td>\n",
       "      <td>1660</td>\n",
       "      <td>10.18653/v1/2022.emnlp-main.759</td>\n",
       "      <td>[JournalArticle, Conference]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>03532123ccffae8d411264320e8a5ae2b6eddea0</td>\n",
       "      <td>Demonstrate-Search-Predict: Composing retrieva...</td>\n",
       "      <td>2022</td>\n",
       "      <td>O. Khattab, Keshav Santhanam, Xiang Lisa Li, D...</td>\n",
       "      <td>Retrieval-augmented in-context learning has em...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/03532123...</td>\n",
       "      <td>305</td>\n",
       "      <td>10.48550/arXiv.2212.14024</td>\n",
       "      <td>[JournalArticle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>af3ab5da98e0807784b57e321ed887a3666a8ab6</td>\n",
       "      <td>Multimodal Foundation Models: From Specialists...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei ...</td>\n",
       "      <td>This paper presents a comprehensive survey of ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/af3ab5da...</td>\n",
       "      <td>300</td>\n",
       "      <td>10.48550/arXiv.2309.10020</td>\n",
       "      <td>[JournalArticle, Review]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6628f9ee35e36cdfdcac8a46cef4dba8d529a83b</td>\n",
       "      <td>Character-LLM: A Trainable Agent for Role-Playing</td>\n",
       "      <td>2023</td>\n",
       "      <td>Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu</td>\n",
       "      <td>Large language models (LLMs) can be used to se...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/6628f9ee...</td>\n",
       "      <td>297</td>\n",
       "      <td>10.48550/arXiv.2310.10158</td>\n",
       "      <td>[JournalArticle, Conference]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1784c987e681d60c634765fe64c8d9c26f73d5ff</td>\n",
       "      <td>SnapKV: LLM Knows What You are Looking for Bef...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yuhong Li, Yingbing Huang, Bowen Yang, Bharat ...</td>\n",
       "      <td>Large Language Models (LLMs) have made remarka...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/1784c987...</td>\n",
       "      <td>294</td>\n",
       "      <td>10.48550/arXiv.2404.14469</td>\n",
       "      <td>[JournalArticle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>c12b80b44d9acfe6cd92fdf965264c4b706c367c</td>\n",
       "      <td>ToolQA: A Dataset for LLM Question Answering w...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun,...</td>\n",
       "      <td>Large Language Models (LLMs) have demonstrated...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/c12b80b4...</td>\n",
       "      <td>293</td>\n",
       "      <td>10.48550/arXiv.2306.13304</td>\n",
       "      <td>[JournalArticle]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     paperId  \\\n",
       "0   d40c77c010c8dbef6142903a02f2a73a85012d5d   \n",
       "1   888728745dbb769e29ed475d4f7661eebe1a71cf   \n",
       "2   0671fd553dd670a4e820553a974bc48040ba0819   \n",
       "3   be8db99310602d66bba64bcf41a572c45816fbfc   \n",
       "4   f4df78183261538e718066331898ee5cad7cad05   \n",
       "..                                       ...   \n",
       "95  03532123ccffae8d411264320e8a5ae2b6eddea0   \n",
       "96  af3ab5da98e0807784b57e321ed887a3666a8ab6   \n",
       "97  6628f9ee35e36cdfdcac8a46cef4dba8d529a83b   \n",
       "98  1784c987e681d60c634765fe64c8d9c26f73d5ff   \n",
       "99  c12b80b44d9acfe6cd92fdf965264c4b706c367c   \n",
       "\n",
       "                                                title  year  \\\n",
       "0                      A Survey on Vision Transformer  2020   \n",
       "1     A Survey on Evaluation of Large Language Models  2023   \n",
       "2   Reflexion: language agents with verbal reinfor...  2023   \n",
       "3                           Let's Verify Step by Step  2023   \n",
       "4   Rethinking the Role of Demonstrations: What Ma...  2022   \n",
       "..                                                ...   ...   \n",
       "95  Demonstrate-Search-Predict: Composing retrieva...  2022   \n",
       "96  Multimodal Foundation Models: From Specialists...  2023   \n",
       "97  Character-LLM: A Trainable Agent for Role-Playing  2023   \n",
       "98  SnapKV: LLM Knows What You are Looking for Bef...  2024   \n",
       "99  ToolQA: A Dataset for LLM Question Answering w...  2023   \n",
       "\n",
       "                                              authors  \\\n",
       "0   Kai Han, Yunhe Wang, Hanting Chen, Xinghao Che...   \n",
       "1   Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, ...   \n",
       "2   Noah Shinn, Federico Cassano, Beck Labash, A. ...   \n",
       "3   H. Lightman, Vineet Kosaraju, Yura Burda, Harr...   \n",
       "4   Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Arte...   \n",
       "..                                                ...   \n",
       "95  O. Khattab, Keshav Santhanam, Xiang Lisa Li, D...   \n",
       "96  Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei ...   \n",
       "97     Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu   \n",
       "98  Yuhong Li, Yingbing Huang, Bowen Yang, Bharat ...   \n",
       "99  Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun,...   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Transformer, first applied to the field of nat...   \n",
       "1   Large language models (LLMs) are gaining incre...   \n",
       "2   Large language models (LLMs) have been increas...   \n",
       "3   In recent years, large language models have gr...   \n",
       "4   Large language models (LMs) are able to in-con...   \n",
       "..                                                ...   \n",
       "95  Retrieval-augmented in-context learning has em...   \n",
       "96  This paper presents a comprehensive survey of ...   \n",
       "97  Large language models (LLMs) can be used to se...   \n",
       "98  Large Language Models (LLMs) have made remarka...   \n",
       "99  Large Language Models (LLMs) have demonstrated...   \n",
       "\n",
       "                                                  url  citationCount  \\\n",
       "0   https://www.semanticscholar.org/paper/d40c77c0...           2580   \n",
       "1   https://www.semanticscholar.org/paper/88872874...           2239   \n",
       "2   https://www.semanticscholar.org/paper/0671fd55...           1755   \n",
       "3   https://www.semanticscholar.org/paper/be8db993...           1740   \n",
       "4   https://www.semanticscholar.org/paper/f4df7818...           1660   \n",
       "..                                                ...            ...   \n",
       "95  https://www.semanticscholar.org/paper/03532123...            305   \n",
       "96  https://www.semanticscholar.org/paper/af3ab5da...            300   \n",
       "97  https://www.semanticscholar.org/paper/6628f9ee...            297   \n",
       "98  https://www.semanticscholar.org/paper/1784c987...            294   \n",
       "99  https://www.semanticscholar.org/paper/c12b80b4...            293   \n",
       "\n",
       "                                doi              publicationTypes  \n",
       "0        10.1109/TPAMI.2022.3152247      [JournalArticle, Review]  \n",
       "1                   10.1145/3641289      [JournalArticle, Review]  \n",
       "2                              None              [JournalArticle]  \n",
       "3         10.48550/arXiv.2305.20050              [JournalArticle]  \n",
       "4   10.18653/v1/2022.emnlp-main.759  [JournalArticle, Conference]  \n",
       "..                              ...                           ...  \n",
       "95        10.48550/arXiv.2212.14024              [JournalArticle]  \n",
       "96        10.48550/arXiv.2309.10020      [JournalArticle, Review]  \n",
       "97        10.48550/arXiv.2310.10158  [JournalArticle, Conference]  \n",
       "98        10.48550/arXiv.2404.14469              [JournalArticle]  \n",
       "99        10.48550/arXiv.2306.13304              [JournalArticle]  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_fetch_bulk(\"ss_llm_survey_simB\", \n",
    "              max_papers = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdd85bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total: 21473\n"
     ]
    }
   ],
   "source": [
    "df_SS_llm_survey_simB = ss_fetch_bulk(\"ss_llm_survey_simB\", max_papers = 21472)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total: 66238\n",
      "Estimated total: 9500\n",
      "Estimated total: 67238\n",
      "Estimated total: 6701\n",
      "Estimated total: 523273\n",
      "Estimated total: 2310601\n",
      "Estimated total: 412066\n",
      "Estimated total: 6014\n",
      "Estimated total: 21472\n",
      "Estimated total: 3302\n"
     ]
    }
   ],
   "source": [
    "# Run Fetch for all defined queries\n",
    "df_SS_llm_survey = ss_fetch_bulk(\"ss_llm_survey\")\n",
    "df_SS_llm_simA = ss_fetch_bulk(\"ss_llm_simA\", max_papers = 9500)\n",
    "df_SS_llm_simB = ss_fetch_bulk(\"ss_llm_simB\", max_papers = 10000)\n",
    "df_SS_llm_simC = ss_fetch_bulk(\"ss_llm_simC\", max_papers = 6701)\n",
    "df_SS_survey_simA = ss_fetch_bulk(\"ss_survey_simA\")\n",
    "df_SS_survey_simB = ss_fetch_bulk(\"ss_survey_simB\")\n",
    "df_SS_survey_simC = ss_fetch_bulk(\"ss_survey_simC\")\n",
    "df_SS_llm_survey_simA = ss_fetch_bulk(\"ss_llm_survey_simA\", max_papers = 6014)\n",
    "df_SS_llm_survey_simB = ss_fetch_bulk(\"ss_llm_survey_simB\", max_papers = 21472)\n",
    "df_SS_llm_survey_simC = ss_fetch_bulk(\"ss_llm_survey_simC\", max_papers = 3302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fetched dataframes to CSV files\n",
    "df_SS_llm_survey_simB.to_csv(\"results_ss/df_SS_llm_survey_simB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6e9ed",
   "metadata": {},
   "source": [
    "## ArXiv API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d45b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41288d7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Search\n",
    "def fetch_results(query, max_results=200, page_size=100):\n",
    "    client = arxiv.Client(\n",
    "        page_size=page_size,      # results per page from API\n",
    "        delay_seconds=3,          # be nice to arXiv\n",
    "        num_retries=3\n",
    "    )\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "    seen = set()\n",
    "    rows = []\n",
    "    for r in client.results(search):\n",
    "        if r.entry_id in seen:\n",
    "            continue\n",
    "        seen.add(r.entry_id)\n",
    "        rows.append({\n",
    "            \"arxiv_id\": r.get_short_id() if hasattr(r, \"get_short_id\") else r.entry_id.split('/')[-1],\n",
    "            \"title\": r.title.strip(),\n",
    "            \"published\": r.published.strftime(\"%Y-%m-%d\") if r.published else \"\",\n",
    "            \"updated\": r.updated.strftime(\"%Y-%m-%d\") if r.updated else \"\",\n",
    "            \"primary_category\": getattr(r, \"primary_category\", \"\"),\n",
    "            \"categories\": \", \".join(getattr(r, \"categories\", []) or []),\n",
    "            \"authors\": \", \".join(a.name for a in r.authors),\n",
    "            \"summary\": r.summary.strip(),\n",
    "            \"pdf_url\": r.pdf_url,\n",
    "            \"abs_url\": r.entry_id,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c725f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "def author_names(paper_authors):\n",
    "    if not paper_authors:\n",
    "        return \"\"\n",
    "    names = []\n",
    "    for a in paper_authors:\n",
    "        # supports Author objects and dicts\n",
    "        names.append(getattr(a, \"name\", a.get(\"name\") if isinstance(a, dict) else None))\n",
    "    return \", \".join([n for n in names if n])\n",
    "\n",
    "def paper_row(p):\n",
    "    return {\n",
    "        \"paperId\": getattr(p, \"paperId\", None),\n",
    "        \"title\": getattr(p, \"title\", None),\n",
    "        \"year\": getattr(p, \"year\", None),\n",
    "        \"authors\": author_names(getattr(p, \"authors\", None)),\n",
    "        \"abstract\": getattr(p, \"abstract\", None),\n",
    "        \"url\": getattr(p, \"url\", None),\n",
    "        \"citationCount\": getattr(p, \"citationCount\", None),\n",
    "    }\n",
    "\n",
    "def fetch_bulk_group(sch: SemanticScholar, query: str,\n",
    "                     year_filter: str, fields: list,\n",
    "                     max_papers: int, sort: str | None = None):\n",
    "    \"\"\"\n",
    "    Runs a bulk search and yields up to max_papers Paper objects.\n",
    "    Prints the API estimated total and progress as it goes.\n",
    "    \"\"\"\n",
    "    results = sch.search_paper(\n",
    "        query=query,\n",
    "        year=year_filter,     # e.g., \"2023-\"\n",
    "        fields=fields,\n",
    "        bulk=True,            # /graph/v1/paper/search/bulk\n",
    "        sort=sort,            # only works with bulk=True\n",
    "    )\n",
    "    est_total = getattr(results, \"total\", None)\n",
    "    print(f\"Estimated total: {est_total if est_total is not None else 'n/a'}\")\n",
    "\n",
    "    count = 0\n",
    "    for p in results:        # iterates across pages automatically\n",
    "        yield p\n",
    "        count += 1\n",
    "        if count >= max_papers:\n",
    "            break\n",
    "\n",
    "def fetch_group_df(sch: SemanticScholar, \n",
    "                   tag: str, \n",
    "                   max_papers_override=None) -> pd.DataFrame:\n",
    "    \"\"\"Fetch a single query group and return a DataFrame.\"\"\"\n",
    "    if tag not in QUERY_GROUPS:\n",
    "        valid = \", \".join(QUERY_GROUPS.keys())\n",
    "        raise ValueError(f\"Unknown group '{tag}'. Valid keys: {valid}\")\n",
    "\n",
    "    query = QUERY_GROUPS[tag]\n",
    "    rows = []\n",
    "    for paper in fetch_bulk_group(\n",
    "        sch,\n",
    "        query=query,\n",
    "        year_filter=YEAR_FILTER,\n",
    "        fields=FIELDS,\n",
    "        max_papers=max_papers_override if max_papers_override is not None else MAX_PAPERS_PER_GROUP,\n",
    "        sort=BULK_SORT,\n",
    "    ):\n",
    "        rows.append(paper_row(paper))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=FIELDS)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main(tag: str | None = None, \n",
    "         max_papers_override=None):\n",
    "\n",
    "    sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "\n",
    "    if tag is not None:\n",
    "        return fetch_group_df(sch, tag, max_papers_override=max_papers_override)\n",
    "\n",
    "    out = {}\n",
    "    for k in QUERY_GROUPS:\n",
    "        out[k] = fetch_group_df(sch, k, max_papers_override=max_papers_override)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12b760",
   "metadata": {},
   "source": [
    "# Measure Precision & Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca95283",
   "metadata": {},
   "source": [
    "## Load Refence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdbfa2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define normalization function (normalize_title)\n",
    "def normalize_title(s: str) -> str:\n",
    "    # Unicode normalize\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "    # Remove punctuation-like characters\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)   # keep letters, numbers, underscore, whitespace\n",
    "    # Collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a901bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold list size: 25\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV of Zotero list of papers\n",
    "gold_df = pd.read_csv(\"data/LLM - Survey Proxies.csv\")\n",
    "\n",
    "gold_df = gold_df[[\"Title\", \"Item Type\", \"Abstract Note\", \n",
    "                   \"Publication Year\", \"Author\", \n",
    "                   \"DOI\", \"ISBN\", \"ISSN\"]].drop_duplicates().reset_index(drop=True)\n",
    "gold_df[\"preprint_flag\"] = gold_df[\"Item Type\"].apply(lambda x: \"preprint\" if x == \"preprint\" else \"non-preprint\")\n",
    "gold_df = gold_df.rename(columns={\"DOI\": \"doi\", \"ISBN\": \"isbn\", \"ISSN\": \"issn\"})\n",
    "gold_df[\"norm_title\"] = gold_df[\"Title\"].apply(normalize_title)\n",
    "\n",
    "print(f\"Gold list size: {len(gold_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286990c-0982-4551-ab17-495e42325387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Gold list size: 22 after removing bad entries\n"
     ]
    }
   ],
   "source": [
    "# Remove known papers that doesn't exist in WoS Dataset\n",
    "bad_titles = [\n",
    "    \"Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study\",\n",
    "    \"Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models\",\n",
    "    \"The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models\"\n",
    "]\n",
    "\n",
    "bad_titles_normalized = [normalize_title(t) for t in bad_titles]\n",
    "\n",
    "gold_df = gold_df[~gold_df[\"norm_title\"].isin(bad_titles_normalized)].reset_index(drop=True) # filter out bad titles\n",
    "\n",
    "print(f\"Cleaned Gold list size: {len(gold_df)} after removing bad entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba22586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles in gold set: 25\n",
      "Unique titles in preprint set: 11\n",
      "Unique titles in journals & proceedings set: 14\n"
     ]
    }
   ],
   "source": [
    "# Separate sets for preprint and non-preprint for 'calc_recall_with_missing' function\n",
    "gold_preprint_set = set(gold_df[gold_df[\"preprint_flag\"] == \"preprint\"][\"norm_title\"])\n",
    "gold_non_preprint_set = set(gold_df[gold_df[\"preprint_flag\"] == \"non-preprint\"][\"norm_title\"])\n",
    "gold_norm_set = set(gold_df[\"norm_title\"])\n",
    "\n",
    "# print the number of unique titles in each set\n",
    "print(f\"Unique titles in gold set: {len(gold_norm_set)}\")\n",
    "print(f\"Unique titles in preprint set: {len(gold_preprint_set)}\")\n",
    "print(f\"Unique titles in journals & proceedings set: {len(gold_non_preprint_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24126d",
   "metadata": {},
   "source": [
    "## Define Recall Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate recall\n",
    "def calc_recall(df, \n",
    "                gold_norm_set, \n",
    "                gold_preprint_set, \n",
    "                gold_non_preprint_set):\n",
    "    df = df.copy()\n",
    "    df[\"norm_title\"] = df[\"title\"].map(normalize_title)\n",
    "    \n",
    "    found_norms = gold_norm_set.intersection(set(df['norm_title']))\n",
    "    recall = len(found_norms) / len(gold_norm_set) if len(gold_norm_set) > 0 else 0\n",
    "\n",
    "    found_preprints = gold_preprint_set.intersection(set(df['norm_title']))\n",
    "    recall_preprint = len(found_preprints) / len(gold_preprint_set) if len(gold_preprint_set) > 0 else 0\n",
    "\n",
    "    found_non_preprints = gold_non_preprint_set.intersection(set(df['norm_title']))\n",
    "    recall_non_preprint = len(found_non_preprints) / len(gold_non_preprint_set) if len(gold_non_preprint_set) > 0 else 0\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Number of Papers Retrieved\": [int(len(df))],\n",
    "        \"Recall (out of 21)\": [f\"{recall:.2%}\"],\n",
    "        \"Recall (journalArticels & other)\": [f\"{recall_non_preprint:.2%}\"],\n",
    "        \"Recall (preprints)\": [f\"{recall_preprint:.2%}\"],\n",
    "    })\n",
    "\n",
    "# Define function to calculate recall w/ missing titles\n",
    "gold_norm_to_orig = {row[\"norm_title\"]: row[\"Title\"] for _, row in gold_df.iterrows()}\n",
    "\n",
    "def calc_recall_with_missing(df, \n",
    "                             gold_norm_set = gold_norm_set, \n",
    "                             gold_preprint_set = gold_preprint_set, \n",
    "                             gold_non_preprint_set = gold_non_preprint_set, \n",
    "                             norm_to_orig = gold_norm_to_orig):\n",
    "    df = df.copy()\n",
    "    if \"title\" not in df.columns:\n",
    "        df[\"title\"] = \"\"\n",
    "    df[\"norm_title\"] = df[\"title\"].map(normalize_title)\n",
    "\n",
    "    df_norms = set(df[\"norm_title\"])\n",
    "\n",
    "    found_norms = gold_norm_set.intersection(df_norms)\n",
    "    found_preprints = gold_preprint_set.intersection(df_norms)\n",
    "    found_non_preprints = gold_non_preprint_set.intersection(df_norms)\n",
    "\n",
    "    recall = len(found_norms) / len(gold_norm_set) if len(gold_norm_set) > 0 else 0\n",
    "    recall_preprint = len(found_preprints) / len(gold_preprint_set) if len(gold_preprint_set) > 0 else 0\n",
    "    recall_non_preprint = len(found_non_preprints) / len(gold_non_preprint_set) if len(gold_non_preprint_set) > 0 else 0\n",
    "\n",
    "    # Missing normalized titles\n",
    "    missing_preprint_norms = gold_preprint_set - df_norms\n",
    "    missing_non_preprint_norms = gold_non_preprint_set - df_norms\n",
    "\n",
    "    # Map back to original titles\n",
    "    missing_preprint_titles = [norm_to_orig.get(n, n) for n in sorted(missing_preprint_norms)]\n",
    "    missing_non_preprint_titles = [norm_to_orig.get(n, n) for n in sorted(missing_non_preprint_norms)]\n",
    "\n",
    "    # return a dataframe with recall stats and missing titles\n",
    "    return pd.DataFrame([{\n",
    "        \"Number of Papers Retrieved\": len(df),\n",
    "        \"Recall (out of 21)\": f\"{recall:.2%}\",\n",
    "        \"Recall (journalArticels & other)\": f\"{recall_non_preprint:.2%}\",\n",
    "        \"Recall (preprints)\": f\"{recall_preprint:.2%}\",\n",
    "        \"Missing Articles\": \"; \".join(missing_non_preprint_titles),\n",
    "        \"Missing Preprint\": \"; \".join(missing_preprint_titles),\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc855c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Recall Function\n",
    "# ---------- Normalizers ----------\n",
    "def _first_token(s: str) -> str:\n",
    "    \"\"\"Return the first nonempty token split on common delimiters.\"\"\"\n",
    "    if not isinstance(s, str): \n",
    "        return \"\"\n",
    "    for tok in re.split(r\"[;,|\\s]+\", s.strip()):\n",
    "        if tok:\n",
    "            return tok\n",
    "    return \"\"\n",
    "\n",
    "def normalize_doi(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        return \"\"\n",
    "    x = _first_token(x).lower()\n",
    "    x = re.sub(r'^(https?://(dx\\.)?doi\\.org/)', '', x)\n",
    "    x = x.replace('\\u200b', '')  # zero-width\n",
    "    return x\n",
    "\n",
    "def normalize_issn(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        return \"\"\n",
    "    s = _first_token(x)\n",
    "    s = re.sub(r'[^0-9xX]', '', s).upper()\n",
    "    if len(s) == 8:\n",
    "        return s[:4] + \"-\" + s[4:]\n",
    "    return s\n",
    "\n",
    "def normalize_isbn(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        return \"\"\n",
    "    s = _first_token(x)\n",
    "    s = re.sub(r'[^0-9xX]', '', s).upper()\n",
    "    return s\n",
    "\n",
    "def canonical_title(s: str) -> str:\n",
    "    if not isinstance(s, str): \n",
    "        return \"\"\n",
    "    # strip diacritics\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    # normalize quotes and dashes/hyphens\n",
    "    s = s.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s = s.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    s = s.lower()\n",
    "    # fix known glued tokens (extend as needed)\n",
    "    s = s.replace(\"financialwellbeing\", \"financial wellbeing\")\n",
    "    # remove punctuation except spaces and alphanumerics\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _ensure_series(df, col):\n",
    "    vals = df[col] if (isinstance(col, str) and col in df.columns) else pd.Series([\"\"] * len(df), index=df.index)\n",
    "    return vals.fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def calc_recall_with_missing(res_df,\n",
    "                             gold_df = gold_df,\n",
    "                             gold_cols=dict(doi=\"doi\", issn=\"issn\", isbn=\"isbn\", title=\"Title\"),\n",
    "                             res_cols=dict(doi=\"doi\", issn=\"issn\", isbn=\"isbn\", title=\"title\")):\n",
    "    g = gold_df.copy()\n",
    "    r = res_df.copy()\n",
    "\n",
    "    # Normalized keys (robust to missing columns)\n",
    "    g[\"_doi\"]  = _ensure_series(g, gold_cols.get(\"doi\")).map(normalize_doi)\n",
    "    r[\"_doi\"]  = _ensure_series(r, res_cols.get(\"doi\")).map(normalize_doi)\n",
    "\n",
    "    g[\"_issn\"] = _ensure_series(g, gold_cols.get(\"issn\")).map(normalize_issn)\n",
    "    r[\"_issn\"] = _ensure_series(r, res_cols.get(\"issn\")).map(normalize_issn)\n",
    "\n",
    "    g[\"_isbn\"] = _ensure_series(g, gold_cols.get(\"isbn\")).map(normalize_isbn)\n",
    "    r[\"_isbn\"] = _ensure_series(r, res_cols.get(\"isbn\")).map(normalize_isbn)\n",
    "\n",
    "    g[\"_tkey\"] = _ensure_series(g, gold_cols.get(\"title\")).map(canonical_title)\n",
    "    r[\"_tkey\"] = _ensure_series(r, res_cols.get(\"title\")).map(canonical_title)\n",
    "\n",
    "    out = g[[gold_cols[\"title\"]]].rename(columns={gold_cols[\"title\"]: \"gold_title\"}).copy()\n",
    "    out[[\"_doi\", \"_issn\", \"_isbn\", \"_tkey\"]] = g[[\"_doi\", \"_issn\", \"_isbn\", \"_tkey\"]]\n",
    "    out[\"preprint_flag\"] = g[\"preprint_flag\"] if \"preprint_flag\" in g.columns else pd.NA\n",
    "    out[\"matched_by\"] = pd.NA\n",
    "    out[\"matched_title\"] = pd.NA\n",
    "\n",
    "    def do_join(key: str, label: str):\n",
    "        nonlocal out, r\n",
    "        pending = out[out[\"matched_by\"].isna()]\n",
    "        pending = pending[pending[key].astype(bool)]\n",
    "        if pending.empty or key not in r.columns:\n",
    "            return\n",
    "        pending = pending.assign(_row_id=pending.index)\n",
    "        if res_cols.get(\"title\") not in r.columns:\n",
    "            return\n",
    "        right = r[[res_cols[\"title\"], key]].drop_duplicates().set_index(key)\n",
    "        merged = pending.join(right, on=key, how=\"left\")\n",
    "        hits = merged[merged[res_cols[\"title\"]].notna()]\n",
    "        if hits.empty:\n",
    "            return\n",
    "        out.loc[hits[\"_row_id\"], \"matched_by\"] = label\n",
    "        out.loc[hits[\"_row_id\"], \"matched_title\"] = hits[res_cols[\"title\"]].values\n",
    "\n",
    "    # Priority order\n",
    "    do_join(\"_doi\",  \"doi\")\n",
    "    do_join(\"_issn\", \"issn\")\n",
    "    do_join(\"_isbn\", \"isbn\")\n",
    "    do_join(\"_tkey\", \"title_exact\")\n",
    "\n",
    "    matched_mask = out[\"matched_title\"].notna()\n",
    "    tp = matched_mask.sum()\n",
    "    fn = (~matched_mask).sum()\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "    pre_mask = out[\"preprint_flag\"].astype(str).str.lower().eq(\"preprint\")\n",
    "    non_pre_mask = out[\"preprint_flag\"].astype(str).str.lower().eq(\"non-preprint\")\n",
    "\n",
    "    recall_preprint = (matched_mask & pre_mask).sum() / pre_mask.sum() if pre_mask.sum() else 0.0\n",
    "    recall_non_preprint = (matched_mask & non_pre_mask).sum() / non_pre_mask.sum() if non_pre_mask.sum() else 0.0\n",
    "\n",
    "    missing_non_pre_titles = out.loc[(~matched_mask) & non_pre_mask, \"gold_title\"].tolist()\n",
    "    missing_pre_titles = out.loc[(~matched_mask) & pre_mask, \"gold_title\"].tolist()\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"Number of Papers Retrieved\": len(res_df),\n",
    "        \"Recall (All)\": f\"{recall:.2%}\",\n",
    "        \"Recall (Journal & Conf. Papers)\": f\"{recall_non_preprint:.2%}\",\n",
    "        \"Recall (Preprints)\": f\"{recall_preprint:.2%}\",\n",
    "        \"Missing Journal & Conf. Papers\": \"; \".join(sorted(missing_non_pre_titles)),\n",
    "        \"Missing Preprints\": \"; \".join(sorted(missing_pre_titles)),\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344af5d5",
   "metadata": {},
   "source": [
    "## Recall Rate - WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39a929",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run recall calculations for Full WoS set\n",
    "wos_recall_results = {\n",
    "    \"LLM and Survey\": calc_recall_with_missing(df_WoS_LLM_and_Survey_clean),\n",
    "    \"LLM and Survey and Methods\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_Methods_clean),\n",
    "    \"LLM and SimulationA\": calc_recall_with_missing(df_WoS_LLM_and_SimulationA_clean),\n",
    "    \"LLM and SimulationB\": calc_recall_with_missing(df_WoS_LLM_and_SimulationB_clean),\n",
    "    \"LLM and SimulationC\": calc_recall_with_missing(df_WoS_LLM_and_SimulationC_clean),\n",
    "    \"LLM and Methods\": calc_recall_with_missing(df_WoS_LLM_and_Methods_clean),\n",
    "    \"LLM and Survey and SimulationA\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationA_clean),\n",
    "    \"LLM and Survey and SimulationB\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationB_clean),\n",
    "    \"LLM and Survey and SimulationC\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationC_clean),\n",
    "    \"LLM and SimulationA and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationA_and_Methods_clean),\n",
    "    \"LLM and SimulationB and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationB_and_Methods_clean),\n",
    "    \"LLM and SimulationC and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationC_and_Methods_clean),\n",
    "    \"LLMSurvey or LLMSimulationA\": calc_recall_with_missing(df_WoS_LLMSurvey_or_LLMSimulationA_clean),\n",
    "    \"LLMSurvey or LLMSimulationB\": calc_recall_with_missing(df_WoS_LLMSurvey_or_LLMSimulationB_clean),\n",
    "    \"LLMSurvey or LLMSimulationC\": calc_recall_with_missing(df_WoS_LLMSurvey_or_LLMSimulationC_clean),\n",
    "    \"Survey and SimulationA\": calc_recall_with_missing(df_WoS_Survey_and_SimulationA_clean),\n",
    "    \"Survey and SimulationB\": calc_recall_with_missing(df_WoS_Survey_and_SimulationB_clean),\n",
    "    \"Survey and SimulationC\": calc_recall_with_missing(df_WoS_Survey_and_SimulationC_clean),\n",
    "}\n",
    "\n",
    "recall_table_WoS = pd.concat(wos_recall_results.values(), \n",
    "                             keys=wos_recall_results.keys()).reset_index(level=1, \n",
    "                             drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "\n",
    "recall_table_WoS = recall_table_WoS.merge(df_WoS_totals.rename(columns={\"QueryName\": \"Query\", \"TotalRecords\": \"Total Records in WoS\"}),\n",
    "                                          on=\"Query\", how=\"left\")\n",
    "\n",
    "recall_table_WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in Preprint Papers gold set: 11\n",
      "Number of records in Journals & Conference Articles gold set: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "72508727-aca3-4f7a-b064-39cc1e21a176",
       "rows": [
        [
         "0",
         "LLM and SimulationA",
         "823",
         "45.45%",
         "90.91%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "1",
         "LLM and SimulationB",
         "2175",
         "50.00%",
         "100.00%",
         "0.00%",
         "",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "2",
         "LLM and Survey and SimulationA",
         "445",
         "45.45%",
         "90.91%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "3",
         "LLM and Survey and SimulationB",
         "1674",
         "50.00%",
         "100.00%",
         "0.00%",
         "",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "4",
         "LLM and SimulationA and Methods",
         "170",
         "27.27%",
         "54.55%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "5",
         "LLM and SimulationB and Methods",
         "330",
         "40.91%",
         "81.82%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Performance and biases of Large Language Models in public opinion simulation",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM and SimulationA</td>\n",
       "      <td>823</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM and SimulationB</td>\n",
       "      <td>2175</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td></td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM and Survey and SimulationA</td>\n",
       "      <td>445</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM and Survey and SimulationB</td>\n",
       "      <td>1674</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td></td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM and SimulationA and Methods</td>\n",
       "      <td>170</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM and SimulationB and Methods</td>\n",
       "      <td>330</td>\n",
       "      <td>40.91%</td>\n",
       "      <td>81.82%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Query  Number of Papers Retrieved Recall (All)  \\\n",
       "0              LLM and SimulationA                         823       45.45%   \n",
       "1              LLM and SimulationB                        2175       50.00%   \n",
       "2   LLM and Survey and SimulationA                         445       45.45%   \n",
       "3   LLM and Survey and SimulationB                        1674       50.00%   \n",
       "4  LLM and SimulationA and Methods                         170       27.27%   \n",
       "5  LLM and SimulationB and Methods                         330       40.91%   \n",
       "\n",
       "  Recall (Journal & Conf. Papers) Recall (Preprints)  \\\n",
       "0                          90.91%              0.00%   \n",
       "1                         100.00%              0.00%   \n",
       "2                          90.91%              0.00%   \n",
       "3                         100.00%              0.00%   \n",
       "4                          54.55%              0.00%   \n",
       "5                          81.82%              0.00%   \n",
       "\n",
       "                      Missing Journal & Conf. Papers  \\\n",
       "0  Knowledge of cultural moral norms in large lan...   \n",
       "1                                                      \n",
       "2  Knowledge of cultural moral norms in large lan...   \n",
       "3                                                      \n",
       "4  Can large language models estimate public opin...   \n",
       "5  Can large language models estimate public opin...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "1  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "2  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "3  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "4  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "5  AI-Augmented Surveys: Leveraging Large Languag...  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run recall calculations for Subset WoS set\n",
    "wos_recall_results_subset = {\n",
    "    \"LLM and SimulationA\": calc_recall_with_missing(df_WoS_LLM_and_SimulationA),\n",
    "    \"LLM and SimulationB\": calc_recall_with_missing(df_WoS_LLM_and_SimulationB),\n",
    "    \"LLM and Survey and SimulationA\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationA),\n",
    "    \"LLM and Survey and SimulationB\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationB),\n",
    "    \"LLM and SimulationA and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationA_and_Methods),\n",
    "    \"LLM and SimulationB and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationB_and_Methods),\n",
    "}\n",
    "\n",
    "recall_table_WoS_subset = pd.concat(wos_recall_results_subset.values(), \n",
    "                                   keys=wos_recall_results_subset.keys()).reset_index(level=1, drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "\n",
    "print(f\"Number of records in Preprint Papers gold set: {len(gold_preprint_set)}\")\n",
    "print(f\"Number of records in Journals & Conference Articles gold set: {len(gold_non_preprint_set)}\")\n",
    "recall_table_WoS_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddbd9d-9f20-40a3-9b6f-f7687c94e02a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Missing Journal Articles between 'LLM and Survey and SimulationA' and 'LLM and SimulationA':\n",
      "\n",
      "ONly Missing in 'LLM and Survey and SimulationA' (0 articles):\n",
      "\n",
      "Only Missing in 'LLM and SimulationA' (0 articles):\n",
      "\n",
      "Missing In both (1 articles):\n",
      " - Knowledge of cultural moral norms in large language models\n"
     ]
    }
   ],
   "source": [
    "# Define function: compare_missing_journal_articles\n",
    "def compare_missing_journal_articles(df_recall, query1, query2):\n",
    "    row1 = df_recall[df_recall[\"Query\"] == query1]\n",
    "    row2 = df_recall[df_recall[\"Query\"] == query2]\n",
    "\n",
    "    # print error for missing queries\n",
    "    if row1.empty:\n",
    "        print(f\"Error: Query '{query1}' not found in the recall table.\")\n",
    "        return None\n",
    "    if row2.empty:\n",
    "        print(f\"Error: Query '{query2}' not found in the recall table.\")\n",
    "        return None\n",
    "    \n",
    "    missing1 = set(row1.iloc[0][\"Missing Journal & Conf. Papers\"].split(\"; \")) if pd.notna(row1.iloc[0][\"Missing Journal & Conf. Papers\"]) else set()\n",
    "    missing2 = set(row2.iloc[0][\"Missing Journal & Conf. Papers\"].split(\"; \")) if pd.notna(row2.iloc[0][\"Missing Journal & Conf. Papers\"]) else set()\n",
    "    \n",
    "    only_in_1 = missing1 - missing2\n",
    "    only_in_2 = missing2 - missing1\n",
    "    in_both = missing1.intersection(missing2)\n",
    "    \n",
    "    # output them in a bullet points like\n",
    "    print(f\"Comparison of Missing Journal Articles between '{query1}' and '{query2}':\\n\")\n",
    "    print(f\"ONly Missing in '{query1}' ({len(only_in_1)} articles):\")\n",
    "    for title in sorted(only_in_1):\n",
    "        print(f\" - {title}\")    \n",
    "    print(f\"\\nOnly Missing in '{query2}' ({len(only_in_2)} articles):\")\n",
    "    for title in sorted(only_in_2):\n",
    "        print(f\" - {title}\")\n",
    "    print(f\"\\nMissing In both ({len(in_both)} articles):\")\n",
    "    for title in sorted(in_both):\n",
    "        print(f\" - {title}\")\n",
    "\n",
    "compare_missing_journal_articles(recall_table_WoS_subset,\n",
    "                                 \"LLM and Survey and SimulationA\", \"LLM and SimulationA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8ae5c",
   "metadata": {},
   "source": [
    "## Recall Rate - S.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c00e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c056f2fc-8707-4978-bc19-45edf4a80112",
       "rows": [
        [
         "0",
         "LLM and Survey",
         "1000",
         "4.00%",
         "7.14%",
         "0.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "1",
         "LLM and SimulationA",
         "9500",
         "76.00%",
         "85.71%",
         "63.64%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models"
        ],
        [
         "2",
         "LLM and SimulationB",
         "10000",
         "36.00%",
         "42.86%",
         "27.27%",
         "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "3",
         "LLM and SimulationC",
         "6701",
         "44.00%",
         "57.14%",
         "27.27%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information"
        ],
        [
         "4",
         "Survey and SimulationA",
         "1000",
         "4.00%",
         "7.14%",
         "0.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "5",
         "Survey and SimulationB",
         "1000",
         "4.00%",
         "7.14%",
         "0.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "6",
         "Survey and SimulationC",
         "1000",
         "0.00%",
         "0.00%",
         "0.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "7",
         "LLM and Survey and SimulationA",
         "6014",
         "76.00%",
         "85.71%",
         "63.64%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models"
        ],
        [
         "8",
         "LLM and Survey and SimulationB",
         "21472",
         "84.00%",
         "92.86%",
         "72.73%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research"
        ],
        [
         "9",
         "LLM and Survey and SimulationC",
         "3302",
         "44.00%",
         "57.14%",
         "27.27%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM and Survey</td>\n",
       "      <td>1000</td>\n",
       "      <td>4.00%</td>\n",
       "      <td>7.14%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM and SimulationA</td>\n",
       "      <td>9500</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>85.71%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM and SimulationB</td>\n",
       "      <td>10000</td>\n",
       "      <td>36.00%</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Algorithmic Fidelity of Large Language Models ...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM and SimulationC</td>\n",
       "      <td>6701</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Survey and SimulationA</td>\n",
       "      <td>1000</td>\n",
       "      <td>4.00%</td>\n",
       "      <td>7.14%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Survey and SimulationB</td>\n",
       "      <td>1000</td>\n",
       "      <td>4.00%</td>\n",
       "      <td>7.14%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Survey and SimulationC</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLM and Survey and SimulationA</td>\n",
       "      <td>6014</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>85.71%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLM and Survey and SimulationB</td>\n",
       "      <td>21472</td>\n",
       "      <td>84.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>72.73%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LLM and Survey and SimulationC</td>\n",
       "      <td>3302</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Query  Number of Papers Retrieved Recall (All)  \\\n",
       "0                  LLM and Survey                        1000        4.00%   \n",
       "1             LLM and SimulationA                        9500       76.00%   \n",
       "2             LLM and SimulationB                       10000       36.00%   \n",
       "3             LLM and SimulationC                        6701       44.00%   \n",
       "4          Survey and SimulationA                        1000        4.00%   \n",
       "5          Survey and SimulationB                        1000        4.00%   \n",
       "6          Survey and SimulationC                        1000        0.00%   \n",
       "7  LLM and Survey and SimulationA                        6014       76.00%   \n",
       "8  LLM and Survey and SimulationB                       21472       84.00%   \n",
       "9  LLM and Survey and SimulationC                        3302       44.00%   \n",
       "\n",
       "  Recall (Journal & Conf. Papers) Recall (Preprints)  \\\n",
       "0                           7.14%              0.00%   \n",
       "1                          85.71%             63.64%   \n",
       "2                          42.86%             27.27%   \n",
       "3                          57.14%             27.27%   \n",
       "4                           7.14%              0.00%   \n",
       "5                           7.14%              0.00%   \n",
       "6                           0.00%              0.00%   \n",
       "7                          85.71%             63.64%   \n",
       "8                          92.86%             72.73%   \n",
       "9                          57.14%             27.27%   \n",
       "\n",
       "                      Missing Journal & Conf. Papers  \\\n",
       "0  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "1  Extracting Affect Aggregates from Longitudinal...   \n",
       "2  Algorithmic Fidelity of Large Language Models ...   \n",
       "3  Extracting Affect Aggregates from Longitudinal...   \n",
       "4  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "5  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "6  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "7  Extracting Affect Aggregates from Longitudinal...   \n",
       "8  The Potential and Challenges of Evaluating Att...   \n",
       "9  Extracting Affect Aggregates from Longitudinal...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "1  Addressing Systematic Non-response Bias with S...  \n",
       "2  Addressing Systematic Non-response Bias with S...  \n",
       "3  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "4  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "5  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "6  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "7  Addressing Systematic Non-response Bias with S...  \n",
       "8  Addressing Systematic Non-response Bias with S...  \n",
       "9  AI-Augmented Surveys: Leveraging Large Languag...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run recall calculations\n",
    "ss_recall_results = {\n",
    "    \"LLM and Survey\": calc_recall_with_missing(df_SS_llm_survey),\n",
    "    \"LLM and SimulationA\": calc_recall_with_missing(df_SS_llm_simA),\n",
    "    \"LLM and SimulationB\": calc_recall_with_missing(df_SS_llm_simB),\n",
    "    \"LLM and SimulationC\": calc_recall_with_missing(df_SS_llm_simC),\n",
    "    \"Survey and SimulationA\": calc_recall_with_missing(df_SS_survey_simA),\n",
    "    \"Survey and SimulationB\": calc_recall_with_missing(df_SS_survey_simB),\n",
    "    \"Survey and SimulationC\": calc_recall_with_missing(df_SS_survey_simC),\n",
    "    \"LLM and Survey and SimulationA\": calc_recall_with_missing(df_SS_llm_survey_simA),\n",
    "    \"LLM and Survey and SimulationB\": calc_recall_with_missing(df_SS_llm_survey_simB),\n",
    "    \"LLM and Survey and SimulationC\": calc_recall_with_missing(df_SS_llm_survey_simC),\n",
    "}\n",
    "\n",
    "recall_table_SS = pd.concat(ss_recall_results.values(), \n",
    "                             keys=ss_recall_results.keys()).reset_index(level=1, drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "recall_table_SS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e0197",
   "metadata": {},
   "source": [
    "## Recall Rate - ArXiV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64af17a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "recall_summary = []\n",
    "for label, dframe in zip(\n",
    "    [\"Simulation Block + Survey Block\", \"LLM Block + Survey Block\", \"All Blocks\"],\n",
    "    [df_sim_survey, df_llm_survey, df_all_blocks]\n",
    "):\n",
    "    stats = calc_recall(dframe, \n",
    "                        gold_norm_set, \n",
    "                        gold_preprint_set, \n",
    "                        gold_non_preprint_set)\n",
    "    \n",
    "    stats[\"Search Block\"] = label\n",
    "    recall_summary.append(stats)\n",
    "\n",
    "recall_table = pd.DataFrame(recall_summary)[[\"Search Block\", \"Number of Papers Retrieved\", \"Recall (out of 21)\", \"Recall (journalArticels & other)\", \"Recall (preprints)\"]]\n",
    "recall_table[\"Number of Papers Retrieved\"] = recall_table[\"Number of Papers Retrieved\"].astype(int)\n",
    "recall_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785080a",
   "metadata": {},
   "source": [
    "## Recall Rate - Elicit A.I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be8852",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load Elicit A.I. Search Results (Elicit prompt 1.csv to Elicit prompt 4.csv) each into their respective dataframe\n",
    "df_Elicit1 = pd.read_csv(\"data/Elicit prompt 1.csv\")\n",
    "df_Elicit2 = pd.read_csv(\"data/Elicit prompt 2.csv\")\n",
    "df_Elicit3 = pd.read_csv(\"data/Elicit prompt 3.csv\")\n",
    "df_Elicit4 = pd.read_csv(\"data/Elicit prompt 4.csv\")\n",
    "\n",
    "# rename all Title to title\n",
    "df_Elicit1 = df_Elicit1.rename(columns={\"Title\": \"title\"})\n",
    "df_Elicit2 = df_Elicit2.rename(columns={\"Title\": \"title\"})\n",
    "df_Elicit3 = df_Elicit3.rename(columns={\"Title\": \"title\"})\n",
    "df_Elicit4 = df_Elicit4.rename(columns={\"Title\": \"title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5098c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys',\n",
       " 'Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias',\n",
       " 'Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design',\n",
       " 'Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models',\n",
       " 'Evaluating the Moral Beliefs Encoded in LLMs',\n",
       " 'Examining the Feasibility of Large Language Models as Survey Respondents',\n",
       " 'Frontiers: Can Large Language Models Capture Human Preferences?',\n",
       " 'Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction',\n",
       " 'Large Language Models Show Human-like Social Desirability Biases in Survey Responses',\n",
       " 'Large Language Models as Subpopulation Representative Models: A Review',\n",
       " 'Large language models display human-like social desirability biases in Big Five personality surveys',\n",
       " 'Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data',\n",
       " 'Out of One, Many: Using Language Models to Simulate Human Samples',\n",
       " 'Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses',\n",
       " 'Questioning the Survey Responses of Large Language Models',\n",
       " 'Synthetic Replacements for Human Survey Data? The Perils of Large Language Models',\n",
       " 'Towards Measuring the Representation of Subjective Global Opinions in Language Models',\n",
       " 'Using GPT for Market Research',\n",
       " 'Whose Opinions Do Language Models Reflect?'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of papers that exist in all four Elicit dataframe df1 to df4\n",
    "common_titles = set(df_Elicit1[\"title\"]).intersection(set(df_Elicit2[\"title\"])).intersection(set(df_Elicit3[\"title\"])).intersection(set(df_Elicit4[\"title\"]))\n",
    "common_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac95fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (out of 21)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (journalArticels & other)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Articles",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprint",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "82bfd0c4-e064-4680-84e2-a670646690f3",
       "rows": [
        [
         "0",
         "Elicit Prompt 1",
         "104",
         "42.86%",
         "27.27%",
         "60.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research"
        ],
        [
         "1",
         "Elicit Prompt 2",
         "104",
         "42.86%",
         "27.27%",
         "60.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research"
        ],
        [
         "2",
         "Elicit Prompt 3",
         "104",
         "42.86%",
         "36.36%",
         "50.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information"
        ],
        [
         "3",
         "Elicit Prompt 4",
         "104",
         "33.33%",
         "18.18%",
         "50.00%",
         "AI–Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgänger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People’s Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (out of 21)</th>\n",
       "      <th>Recall (journalArticels &amp; other)</th>\n",
       "      <th>Recall (preprints)</th>\n",
       "      <th>Missing Articles</th>\n",
       "      <th>Missing Preprint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elicit Prompt 1</td>\n",
       "      <td>104</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>60.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elicit Prompt 2</td>\n",
       "      <td>104</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>60.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elicit Prompt 3</td>\n",
       "      <td>104</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>36.36%</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elicit Prompt 4</td>\n",
       "      <td>104</td>\n",
       "      <td>33.33%</td>\n",
       "      <td>18.18%</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>AI–Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Query  Number of Papers Retrieved Recall (out of 21)  \\\n",
       "0  Elicit Prompt 1                         104             42.86%   \n",
       "1  Elicit Prompt 2                         104             42.86%   \n",
       "2  Elicit Prompt 3                         104             42.86%   \n",
       "3  Elicit Prompt 4                         104             33.33%   \n",
       "\n",
       "  Recall (journalArticels & other) Recall (preprints)  \\\n",
       "0                           27.27%             60.00%   \n",
       "1                           27.27%             60.00%   \n",
       "2                           36.36%             50.00%   \n",
       "3                           18.18%             50.00%   \n",
       "\n",
       "                                    Missing Articles  \\\n",
       "0  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "1  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "2  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "3  AI–Human Hybrids for Marketing Research: Lever...   \n",
       "\n",
       "                                    Missing Preprint  \n",
       "0  Addressing Systematic Non-response Bias with S...  \n",
       "1  Addressing Systematic Non-response Bias with S...  \n",
       "2  Addressing Systematic Non-response Bias with S...  \n",
       "3  Addressing Systematic Non-response Bias with S...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate recall for df_Elicit1, df_Elicit2, df_Elicit3, df_Elicit4 dataframes with missing titles\n",
    "elicit_recall_results = {\n",
    "    \"Elicit Prompt 1\": calc_recall_with_missing(df_Elicit1, gold_norm_set, gold_preprint_set, gold_non_preprint_set, gold_norm_to_orig),\n",
    "    \"Elicit Prompt 2\": calc_recall_with_missing(df_Elicit2, gold_norm_set, gold_preprint_set, gold_non_preprint_set, gold_norm_to_orig),\n",
    "    \"Elicit Prompt 3\": calc_recall_with_missing(df_Elicit3, gold_norm_set, gold_preprint_set, gold_non_preprint_set, gold_norm_to_orig),\n",
    "    \"Elicit Prompt 4\": calc_recall_with_missing(df_Elicit4, gold_norm_set, gold_preprint_set, gold_non_preprint_set, gold_norm_to_orig),\n",
    "} \n",
    "\n",
    "recall_table_Elicit = pd.concat(elicit_recall_results.values(), keys=elicit_recall_results.keys()).reset_index(level=1, drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "recall_table_Elicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4337d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(4, 1, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m     stats[\u001b[33m\"\u001b[39m\u001b[33mSearch Prompt\u001b[39m\u001b[33m\"\u001b[39m] = label\n\u001b[32m     15\u001b[39m     recall_summary_Elicit.append(stats)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m recall_table_Elicit = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecall_summary_Elicit\u001b[49m\u001b[43m)\u001b[49m[[\u001b[33m\"\u001b[39m\u001b[33mSearch Prompt\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     19\u001b[39m                                                            \u001b[33m\"\u001b[39m\u001b[33mNumber of Papers Retrieved\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     20\u001b[39m                                                            \u001b[33m\"\u001b[39m\u001b[33mRecall (out of 21)\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     21\u001b[39m                                                            \u001b[33m\"\u001b[39m\u001b[33mRecall (journalArticels & other)\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     22\u001b[39m                                                            \u001b[33m\"\u001b[39m\u001b[33mRecall (preprints)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMissing Articles\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMissing Preprint\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     24\u001b[39m recall_table_Elicit[\u001b[33m\"\u001b[39m\u001b[33mNumber of Papers Retrieved\u001b[39m\u001b[33m\"\u001b[39m] = recall_table_Elicit[\u001b[33m\"\u001b[39m\u001b[33mNumber of Papers Retrieved\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     25\u001b[39m recall_table_Elicit\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chanho\\Documents\\AIRESIL\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:867\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    859\u001b[39m         mgr = arrays_to_mgr(\n\u001b[32m    860\u001b[39m             arrays,\n\u001b[32m    861\u001b[39m             columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m             typ=manager,\n\u001b[32m    865\u001b[39m         )\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    876\u001b[39m     mgr = dict_to_mgr(\n\u001b[32m    877\u001b[39m         {},\n\u001b[32m    878\u001b[39m         index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    881\u001b[39m         typ=manager,\n\u001b[32m    882\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chanho\\Documents\\AIRESIL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:319\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    314\u001b[39m     values = _ensure_2d(values)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     values = \u001b[43m_prep_ndarraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values.dtype != dtype:\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[32m    323\u001b[39m     values = sanitize_array(\n\u001b[32m    324\u001b[39m         values,\n\u001b[32m    325\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    328\u001b[39m         allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    329\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chanho\\Documents\\AIRESIL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:582\u001b[39m, in \u001b[36m_prep_ndarraylike\u001b[39m\u001b[34m(values, copy)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    580\u001b[39m     values = convert(values)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chanho\\Documents\\AIRESIL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:592\u001b[39m, in \u001b[36m_ensure_2d\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    590\u001b[39m     values = values.reshape((values.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m))\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m values.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mValueError\u001b[39m: Must pass 2-d input. shape=(4, 1, 7)"
     ]
    }
   ],
   "source": [
    "# calculate recall rates for each dataframe and summarize in a table\n",
    "recall_summary_Elicit = []\n",
    "for label, dframe in zip( [\"Elicit Prompt 1\", \"Elicit Prompt 2\", \"Elicit Prompt 3\", \"Elicit Prompt 4\"],\n",
    "                          [df_Elicit1, df_Elicit2, df_Elicit3, df_Elicit4]):\n",
    "    \n",
    "    # calculate recall with missing titles\n",
    "\n",
    "    stats = calc_recall(dframe, \n",
    "                        gold_norm_set, \n",
    "                        gold_preprint_set, \n",
    "                        gold_non_preprint_set)\n",
    "    \n",
    "    stats[\"Search Prompt\"] = label\n",
    "    recall_summary_Elicit.append(stats)\n",
    "\n",
    "\n",
    "recall_table_Elicit = pd.DataFrame(recall_summary_Elicit)[[\"Search Prompt\", \"Number of Papers Retrieved\", \n",
    "                                                           \"Recall (out of 21)\", \"Recall (journalArticels & other)\", \"Recall (preprints)\"]]\n",
    "recall_table_Elicit[\"Number of Papers Retrieved\"] = recall_table_Elicit[\"Number of Papers Retrieved\"].astype(int)\n",
    "recall_table_Elicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae276d18-246f-4814-a02f-a387ddf8fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddbda8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Publication Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Book Authors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Editors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Book Group Authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Author Full Names",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Book Author Full Names",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Group Authors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Source Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Series Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Series Subtitle",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Language",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Document Type",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Location",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Sponsor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Host",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Author Keywords",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Keywords Plus",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Abstract",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Affiliations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Reprint Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Email Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Researcher Ids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ORCIDs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Orgs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Name Preferred",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Text",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cited References",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cited Reference Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Times Cited, WoS Core",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Times Cited, All Databases",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "180 Day Usage Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Since 2013 Usage Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher City",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher Address",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ISSN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eISSN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ISBN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Journal Abbreviation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Journal ISO Abbreviation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication Year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Issue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Part Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Supplement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Special Issue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Meeting Abstract",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Start Page",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "End Page",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Article Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOI Link",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book DOI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Early Access Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number of Pages",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WoS Categories",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Web of Science Index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research Areas",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "IDS Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pubmed Id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Open Access Designations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Highly Cited Status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Hot Paper Status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Date of Export",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "UT (Unique WOS ID)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Web of Science Record",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "10b1f1a9-279f-43b9-a481-a821d441f468",
       "rows": [
        [
         "0",
         "C",
         "Kaiser, C; Kaiser, J; Manewitsch, V; Rau, L; Schallner, R",
         null,
         null,
         "ACM",
         "Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vladimir; Rau, Lea; Schallner, Rene",
         null,
         null,
         "Simulating Human Opinions with Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "1",
         "J",
         "Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunner, A",
         null,
         null,
         null,
         "Ferreira, Gregorio; Amidei, Jacopo; Nieto, Ruben; Kaltenbrunner, Andreas",
         null,
         null,
         "How Well Do Simulated Population Samples with GPT-4 Align with Real Ones? The Case of the Eysenck Personality Questionnaire Revised-Abbreviated Personality Test",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "2",
         "C",
         "Kane, D; Parke, J; Jo, Y; Bak, J",
         null,
         "Bouamor, H; Pino, J; Bali, K",
         null,
         "Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak, JinYeong",
         null,
         null,
         "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "3",
         "J",
         "Arora, N; Chakraborty, I; Nishimura, Y",
         null,
         null,
         null,
         "Arora, Neeraj; Chakraborty, Ishita; Nishimura, Yohei",
         null,
         null,
         "AI-Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "4",
         "J",
         "Antal, M; Beder, N",
         null,
         null,
         null,
         "Antal, Margit; Beder, Norbert",
         null,
         null,
         "Eysenck Personality Questionnaire: A Comparative Study of Humans and Large Language Models Through Repeated Administrations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "5",
         "J",
         "Bisbee, J; Clinton, JD; Dorff, C; Kenkel, B; Larson, JM",
         null,
         null,
         null,
         "Bisbee, James; Clinton, Joshua D.; Dorff, Cassy; Kenkel, Brenton; Larson, Jennifer M.",
         null,
         null,
         "Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "6",
         "J",
         "Liu, HJ; Cao, Y; Wu, X; Qiu, C; Gu, JG; Liu, MF; Hershcovich, D",
         null,
         null,
         null,
         "Liu, Haijiang; Cao, Yong; Wu, Xun; Qiu, Chen; Gu, Jinguang; Liu, Maofu; Hershcovich, Daniel",
         null,
         null,
         "Towards realistic evaluation of cultural value alignment in large language models: Diversity enhancement for survey response simulation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "7",
         "J",
         "Lee, SG; Peng, TQ; Goldberg, MH; Rosenthal, SA; Kotcher, JE; Maibach, EW; Leiserowitz, A",
         null,
         null,
         null,
         "Lee, Sanguk; Peng, Tai-Quan; Goldberg, Matthew H.; Rosenthal, Seth A.; Kotcher, John E.; Maibach, Edward W.; Leiserowitz, Anthony",
         null,
         null,
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "8",
         "J",
         "Boelaert, J; Coavoux, S; Ollion, E; Petev, I; Präg, P",
         null,
         null,
         null,
         "Boelaert, Julien; Coavoux, Samuel; Ollion, Etienne; Petev, Ivaylo; Prag, Patrick",
         null,
         null,
         "Machine Bias. How Do Generative Language Models Answer Opinion Polls?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "9",
         "J",
         "Qu, Y; Wang, J",
         null,
         null,
         null,
         "Qu, Yao; Wang, Jue",
         null,
         null,
         "Performance and biases of Large Language Models in public opinion simulation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "10",
         "C",
         "Gao, S; Gao, BL; Wei, P; Guo, JP; Yuan, M; Han, C; Xu, YY",
         null,
         "Xiao, X; Yao, J",
         null,
         "Gao, Song; Gao, Bolin; Wei, Peng; Guo, Jianpeng; Yuan, Meng; Han, Cheng; Xu, Yueyun",
         null,
         null,
         "Application of foundation models for autonomous driving: a survey of data synthesis",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "11",
         "C",
         "Nguyen, H; Nguyen, V; López-Fierro, S; Ludovise, S; Santagata, R",
         null,
         null,
         "ASSOC COMPUTING MACHINERY",
         "Ha Nguyen; Nguyen, Victoria; Lopez-Fierro, Sariah; Ludovise, Sara; Santagata, Rossella",
         null,
         null,
         "Simulating Climate Change Discussion with Large Language Models: Considerations for Science Communication at Scale",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "12",
         "J",
         "Alonso, SLN; Ozili, PK; Hernández, BMS; Pacheco, LM",
         null,
         null,
         null,
         "Alonso, Sergio Luis Nanez; Ozili, Peterson K.; Hernandez, Beatriz Maria Sastre; Pacheco, Luis Miguel",
         null,
         null,
         "Evaluating the acceptance of CBDCs: experimental research with artificial intelligence (AI) generated synthetic response",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "13",
         "J",
         "Salecha, A; Ireland, ME; Subrahmanya, S; Sedoc, J; Ungar, LH; Eichstaedt, JC",
         null,
         null,
         null,
         "Salecha, Aadesh; Ireland, Molly E.; Subrahmanya, Shashanka; Sedoc, Joao; Ungar, Lyle H.; Eichstaedt, Johannes C.",
         null,
         null,
         "Large language models display human-like social desirability biases in Big Five personality surveys",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "14",
         "J",
         "Lehtonen, E; Buder-Gröndahl, T; Nordhoff, S",
         null,
         null,
         null,
         "Lehtonen, Esko; Buder-Grondahl, Tommi; Nordhoff, Sina",
         null,
         null,
         "Revealing the Influence of Semantic Similarity on Survey Responses: A Synthetic Data Generation Approach",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "15",
         "J",
         "Yao, JC; Zhang, HJ; Ou, J; Zuo, DY; Yang, Z; Dong, ZC",
         null,
         null,
         null,
         "Yao, Junchi; Zhang, Hongjie; Ou, Jie; Zuo, Dingyi; Yang, Zheng; Dong, Zhicheng",
         null,
         null,
         "Social opinions prediction utilizes fusing dynamics equation with LLM-based agents",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "16",
         "C",
         "Hämäläinen, P; Tavast, M; Kunnari, A",
         null,
         null,
         "ACM",
         "Hamalainen, Perttu; Tavast, Mikke; Kunnari, Anton",
         null,
         null,
         "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "17",
         "J",
         "Zhang, S; Xu, J; Alvero, AJ",
         null,
         null,
         null,
         "Zhang, Simone; Xu, Janet; Alvero, A. J.",
         null,
         null,
         "Generative AI Meets Open-Ended Survey Responses: Research Participant Use of AI and Homogenization",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "18",
         "J",
         "Zhang, BY; Chen, T; Wang, X; Li, Q; Zhang, WS; Wang, FY",
         null,
         null,
         null,
         "Zhang, Baoyu; Chen, Tao; Wang, Xiao; Li, Qiang; Zhang, Weishan; Wang, Fei-Yue",
         null,
         null,
         "Decoding Activist Public Opinion in Decentralized Self-Organized Protests Using LLM",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "19",
         "J",
         "von der Heyde, L; Haensch, AC; Wenz, A",
         null,
         null,
         null,
         "von der Heyde, Leah; Haensch, Anna-Carolina; Wenz, Alexander",
         null,
         null,
         "Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "20",
         "J",
         "Gao, C; Lan, XC; Li, N; Yuan, Y; Ding, JT; Zhou, ZL; Xu, FL; Li, Y",
         null,
         null,
         null,
         "Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yuan; Ding, Jingtao; Zhou, Zhilun; Xu, Fengli; Li, Yong",
         null,
         null,
         "Large language models empowered agent-based modeling and simulation: a survey and perspectives",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "21",
         "C",
         "Tan, Z; Li, DW; Wang, S; Beigi, A; Jiang, BH; Bhattacharjee, A; Karami, M; Li, JD; Cheng, L; Liu, H",
         null,
         "Al-Onaizan, Y; Bansal, M; Chen, YN",
         null,
         "Tan, Zhen; Li, Dawei; Wang, Song; Beigi, Alimohammad; Jiang, Bohan; Bhattacharjee, Amrita; Karami, Mansooreh; Li, Jundong; Cheng, Lu; Liu, Huan",
         null,
         null,
         "Large Language Models for Data Annotation and Synthesis: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "22",
         "J",
         "Jung, SG; Salminen, J; Aldous, KK; Jansen, BJ",
         null,
         null,
         null,
         "Jung, Soon-Gyo; Salminen, Joni; Aldous, Kholoud Khalil; Jansen, Bernard J.",
         null,
         null,
         "PersonaCraft: Leveraging language models for data-driven persona development",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "23",
         "J",
         "Cho, S; Kim, J; Kim, JH",
         null,
         null,
         null,
         "Cho, Suhyun; Kim, Jaeyun; Kim, Jang Hyun",
         null,
         null,
         "LLM-Based Doppelganger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "24",
         "C",
         "Kaur, A; Aird, A; Borman, H; Nicastro, A; Leontjeva, A; Pizzato, L; Jermyn, D",
         null,
         null,
         "ACM",
         "Kaur, Arshnoor; Aird, Amanda; Borman, Harris; Nicastro, Andrea; Leontjeva, Anna; Pizzato, Luiz; Jermyn, Dan",
         null,
         null,
         "Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People's FinancialWellbeing",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "25",
         "J",
         "Ding, GZ; Liu, ZR; Li, S; Cao, J; Ye, ZH",
         null,
         null,
         null,
         "Ding, Guozhu; Liu, Zuer; Li, Shan; Cao, Jie; Ye, Zhuohai",
         null,
         null,
         "Impact of mindset types and social community compositions on opinion dynamics: A large language model-based multi-agent simulation study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "26",
         "C",
         "AlKhamissi, B; ElNokrashy, M; AlKhamissi, M; Diab, M",
         null,
         "Ku, LW; Martins, A; Srikumar, V",
         null,
         "AlKhamissi, Badr; ElNokrashy, Muhammad; AlKhamissi, Mai; Diab, Mona",
         null,
         null,
         "Investigating Cultural Alignment of Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "27",
         "J",
         "Zhao, XJ; Wang, H; Dai, CX; Tang, JC; Deng, KX; Zhong, ZH; Kong, FY; Wang, SY; Morikawa, S",
         null,
         null,
         null,
         "Zhao, Xinjie; Wang, Hao; Dai, Chengxiao; Tang, Jiacheng; Deng, Kaixin; Zhong, Zhihua; Kong, Fanying; Wang, Shiyun; Morikawa, So",
         null,
         null,
         "Multi-Stage Simulation of Residents' Disaster Risk Perception and Decision-Making Behavior: An Exploratory Study on Large Language Model-Driven Social-Cognitive Agent Framework",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "28",
         "J",
         "de Winter, JCF; Driessen, T; Dodou, D",
         null,
         null,
         null,
         "de Winter, Joost C. F.; Driessen, Tom; Dodou, Dimitra",
         null,
         null,
         "The use of ChatGPT for personality research: Administering questionnaires using generated personas",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "29",
         "C",
         "Brasoveanu, AMP; Scharl, A; Nixon, LJB; Andonie, R",
         null,
         "Banissi, E; Datia, N; Pires, JM; Ursyn, A; Nazemi, K; Kovalerchuk, B; Andonie, R; Gavrilova, M; Nakayama, M; Nguyen, QV; Mabakane, MS; Rusu, A; Sciarrone, F; Temperini, M; Bouali, F; Venturini, G; Huang, T",
         null,
         "Brasoveanu, Adrian M. P.; Scharl, Arno; Nixon, Lyndon J. B.; Andonie, Razvan",
         null,
         null,
         "Visualizing Large Language Models: A Brief Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "30",
         "J",
         "Rakovics, Z; Rakovics, M",
         null,
         null,
         null,
         "Rakovics, Zsofia; Rakovics, Marton",
         null,
         null,
         "Exploring the potential and limitations of large language models as virtual respondents for social science research",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "31",
         "C",
         "Omata, M; Shimizu, A",
         null,
         "Ardito, C; Lanzilotti, R; Malizia, A; Petrie, H; Piccinno, A; Desolda, G; Inkpen, K",
         null,
         "Omata, Masaki; Shimizu, Atsuki",
         null,
         null,
         "A Proposal for Discreet Auxiliary Figures for Reducing VR Sickness and for Not Obstructing FOV",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "32",
         "J",
         "Leung, HW; Bovy, J",
         null,
         null,
         null,
         "Leung, Henry W.; Bovy, Jo",
         null,
         null,
         "Towards an astronomical foundation model for stars with a transformer-based model",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "33",
         "J",
         "Mburu, TK; Rong, KX; McColley, CJ; Werth, A",
         null,
         null,
         null,
         "Mburu, Ted K.; Rong, Kangxuan; McColley, Campbell J.; Werth, Alexandra",
         null,
         null,
         "Methodological foundations for artificial intelligence-driven survey question generation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "34",
         "C",
         "Li, YH; Wang, SF; Ding, H; Chen, H",
         null,
         null,
         "ACM",
         "Li, Yinheng; Wang, Shaofei; Ding, Han; Chen, Hang",
         null,
         null,
         "Large Language Models in Finance: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "35",
         "J",
         "Timothy, TR",
         null,
         null,
         null,
         "Timothy, Tyrese Raku",
         null,
         null,
         "AI-driven fabrication of healthcare survey data: methods, motivations, and ethical implications",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "36",
         "J",
         "Zhang, KH; Dong, CQ; Guo, YF; Zhou, W; Yu, G; Mi, JN",
         null,
         null,
         null,
         "Zhang, Kaihang; Dong, Changqi; Guo, Yifeng; Zhou, Wuai; Yu, Guang; Mi, Jianing",
         null,
         null,
         "Lagged Stance Interactions and Counter-Spiral of Silence: A Data-Driven Analysis and Agent-Based Modeling of Technical Public Opinion Events",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "37",
         "C",
         "Zhao, XX; Qiu, Y",
         null,
         "Rau, PLP",
         null,
         "Zhao, Xiaoxuan; Qiu, Yue",
         null,
         null,
         "Insight Through Dialogue: A Practical Exploration of AIGC in Cross-cultural Design Research",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "38",
         "J",
         "Zhang, ZC; Wu, HN; Zhang, EL; Zhai, GT; Lin, WS",
         null,
         null,
         null,
         "Zhang, Zicheng; Wu, Haoning; Zhang, Erli; Zhai, Guangtao; Lin, Weisi",
         null,
         null,
         "Q-BENCH+: A Benchmark for Multi-Modal Foundation Models on Low-Level Vision From Single Images to Pairs",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "39",
         "J",
         "Campos, M; Farinhas, A; Zerva, C; Figueiredo, MAT; Martins, AFT",
         null,
         null,
         null,
         "Campos, Margarida; Farinhas, Antonio; Zerva, Chrysoula; Figueiredo, Mario A. T.; Martins, Andre F. T.",
         null,
         null,
         "Conformal Prediction for Natural Language Processing: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "40",
         "J",
         "Wilden, J; Riley, RH",
         null,
         null,
         null,
         "Wilden, J; Riley, RH",
         null,
         null,
         "Personal digital assistant (PDA) use amongst anaesthetists: An Australian survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "41",
         "J",
         "Zheng, JY; Wang, X; Hosio, S; Xu, XX; Lee, LH",
         null,
         null,
         null,
         "Zheng, Jingyao; Wang, Xian; Hosio, Simo; Xu, Xiaoxian; Lee, Lik-Hang",
         null,
         null,
         "LMLPA: Language Model Linguistic Personality Assessment",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "42",
         "C",
         "Cheng, M; Piccardi, T; Yang, DY",
         null,
         "Bouamor, H; Pino, J; Bali, K",
         null,
         "Cheng, Myra; Piccardi, Tiziano; Yang, Diyi",
         null,
         null,
         "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "43",
         "C",
         "Dobre, SC; Popescu, E",
         null,
         null,
         "IEEE",
         "Dobre, Stefania-Carmen; Popescu, Elvira",
         null,
         null,
         "Exploring Students' Perception and Experience with ChatGPT and Critical Thinking in a Higher Education Context",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "44",
         "J",
         "Kaur, A; Budko, A; Liu, K; Eaton, E; Steitz, BD; Johnson, KB",
         null,
         null,
         null,
         "Kaur, Amarpreet; Budko, Alexander; Liu, Katrina; Eaton, Eric; Steitz, Bryan D.; Johnson, Kevin B.",
         null,
         null,
         "Automating Responses to Patient Portal Messages Using Generative AI",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "45",
         "J",
         "Ji, J; Kim, J; Kim, Y",
         null,
         null,
         null,
         "Ji, Junyung; Kim, Jiwoo; Kim, Younghoon",
         null,
         null,
         "Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "46",
         "J",
         "Cisar, P",
         null,
         null,
         null,
         "Cisar, Peter",
         null,
         null,
         "The Place and Role of Honeypot Solutions in Network Intrusion Detection Systems",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "47",
         "J",
         "Goli, A; Singh, A",
         null,
         null,
         null,
         "Goli, Ali; Singh, Amandeep",
         null,
         null,
         "Frontiers: Can Large Language Models Capture Human Preferences?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "48",
         "C",
         "Zhao, RB; Xie, ZW; Zhuang, YP; Li, HX; Yu, PLH",
         null,
         "Kashihara, A; Jiang, B; Rodrigo, MM; Sugay, JO",
         null,
         "Zhao, Ruibin; Xie, Zhiwei; Zhuang, Yipeng; Li, Huixian; Yu, Philip L. H.",
         null,
         null,
         "Enhancing Language Learning Through Multimodal AI-Driven Feedback on Picture Descriptions: An Eye-Tracking Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ],
        [
         "49",
         "J",
         "Hur, JK; Heffner, J; Feng, GW; Joormann, J; Rutledge, RB",
         null,
         null,
         null,
         "Hur, Jihyun K.; Heffner, Joseph; Feng, Gloria W.; Joormann, Jutta; Rutledge, Robb B.",
         null,
         null,
         "Language sentiment predicts changes in depressive symptoms",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0"
        ]
       ],
       "shape": {
        "columns": 72,
        "rows": 124
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Book Authors</th>\n",
       "      <th>Book Editors</th>\n",
       "      <th>Book Group Authors</th>\n",
       "      <th>Author Full Names</th>\n",
       "      <th>Book Author Full Names</th>\n",
       "      <th>Group Authors</th>\n",
       "      <th>title</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Web of Science Index</th>\n",
       "      <th>Research Areas</th>\n",
       "      <th>IDS Number</th>\n",
       "      <th>Pubmed Id</th>\n",
       "      <th>Open Access Designations</th>\n",
       "      <th>Highly Cited Status</th>\n",
       "      <th>Hot Paper Status</th>\n",
       "      <th>Date of Export</th>\n",
       "      <th>UT (Unique WOS ID)</th>\n",
       "      <th>Web of Science Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>Kaiser, C; Kaiser, J; Manewitsch, V; Rau, L; S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simulating Human Opinions with Large Language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Well Do Simulated Population Samples with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>Kane, D; Parke, J; Jo, Y; Bak, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bouamor, H; Pino, J; Bali, K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>From Values to Opinions: Predicting Human Beha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>Arora, N; Chakraborty, I; Nishimura, Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arora, Neeraj; Chakraborty, Ishita; Nishimura,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI-Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J</td>\n",
       "      <td>Antal, M; Beder, N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antal, Margit; Beder, Norbert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eysenck Personality Questionnaire: A Comparati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>J</td>\n",
       "      <td>Moscoso, V; Albernaz, AL; Salomao, RDP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moscoso, Valdenice; Albernaz, Ana Luisa; Salom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Niche modelling for twelve plant species (six ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>J</td>\n",
       "      <td>Domenach, P; Krause, KR; Malmartel, A; Ravaud,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Domenach, Paul; Krause, Karolin R.; Malmartel,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Identifying psychosocial and contextual marker...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>J</td>\n",
       "      <td>Lim, MC; Lukman, KA; Giloi, N; Lim, JF; Salleh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lim, Mei Ching; Lukman, Khamisah Awang; Giloi,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Landscaping Work: Work-related Musculoskeletal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>J</td>\n",
       "      <td>King, RJ; Cordon-Rosales, C; Cox, J; Davies, C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>King, Raymond J.; Cordon-Rosales, Celia; Cox, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Triatoma dimidiata Infestation in Chagas Disea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>J</td>\n",
       "      <td>BOROSON, TA; SALZER, JJ; TROTTER, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOROSON, TA; SALZER, JJ; TROTTER, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A NEW SURVEY FOR LOW-LUMINOSITY EMISSION-LINE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Publication Type                                            Authors  \\\n",
       "0                  C  Kaiser, C; Kaiser, J; Manewitsch, V; Rau, L; S...   \n",
       "1                  J  Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...   \n",
       "2                  C                   Kane, D; Parke, J; Jo, Y; Bak, J   \n",
       "3                  J             Arora, N; Chakraborty, I; Nishimura, Y   \n",
       "4                  J                                 Antal, M; Beder, N   \n",
       "..               ...                                                ...   \n",
       "119                J             Moscoso, V; Albernaz, AL; Salomao, RDP   \n",
       "120                J  Domenach, P; Krause, KR; Malmartel, A; Ravaud,...   \n",
       "121                J  Lim, MC; Lukman, KA; Giloi, N; Lim, JF; Salleh...   \n",
       "122                J  King, RJ; Cordon-Rosales, C; Cox, J; Davies, C...   \n",
       "123                J                BOROSON, TA; SALZER, JJ; TROTTER, A   \n",
       "\n",
       "     Book Authors                  Book Editors Book Group Authors  \\\n",
       "0             NaN                           NaN                ACM   \n",
       "1             NaN                           NaN                NaN   \n",
       "2             NaN  Bouamor, H; Pino, J; Bali, K                NaN   \n",
       "3             NaN                           NaN                NaN   \n",
       "4             NaN                           NaN                NaN   \n",
       "..            ...                           ...                ...   \n",
       "119           NaN                           NaN                NaN   \n",
       "120           NaN                           NaN                NaN   \n",
       "121           NaN                           NaN                NaN   \n",
       "122           NaN                           NaN                NaN   \n",
       "123           NaN                           NaN                NaN   \n",
       "\n",
       "                                     Author Full Names  \\\n",
       "0    Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vl...   \n",
       "1    Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...   \n",
       "2    Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...   \n",
       "3    Arora, Neeraj; Chakraborty, Ishita; Nishimura,...   \n",
       "4                        Antal, Margit; Beder, Norbert   \n",
       "..                                                 ...   \n",
       "119  Moscoso, Valdenice; Albernaz, Ana Luisa; Salom...   \n",
       "120  Domenach, Paul; Krause, Karolin R.; Malmartel,...   \n",
       "121  Lim, Mei Ching; Lukman, Khamisah Awang; Giloi,...   \n",
       "122  King, Raymond J.; Cordon-Rosales, Celia; Cox, ...   \n",
       "123                BOROSON, TA; SALZER, JJ; TROTTER, A   \n",
       "\n",
       "     Book Author Full Names  Group Authors  \\\n",
       "0                       NaN            NaN   \n",
       "1                       NaN            NaN   \n",
       "2                       NaN            NaN   \n",
       "3                       NaN            NaN   \n",
       "4                       NaN            NaN   \n",
       "..                      ...            ...   \n",
       "119                     NaN            NaN   \n",
       "120                     NaN            NaN   \n",
       "121                     NaN            NaN   \n",
       "122                     NaN            NaN   \n",
       "123                     NaN            NaN   \n",
       "\n",
       "                                                 title  Source Title  ...  \\\n",
       "0    Simulating Human Opinions with Large Language ...           NaN  ...   \n",
       "1    How Well Do Simulated Population Samples with ...           NaN  ...   \n",
       "2    From Values to Opinions: Predicting Human Beha...           NaN  ...   \n",
       "3    AI-Human Hybrids for Marketing Research: Lever...           NaN  ...   \n",
       "4    Eysenck Personality Questionnaire: A Comparati...           NaN  ...   \n",
       "..                                                 ...           ...  ...   \n",
       "119  Niche modelling for twelve plant species (six ...           NaN  ...   \n",
       "120  Identifying psychosocial and contextual marker...           NaN  ...   \n",
       "121  Landscaping Work: Work-related Musculoskeletal...           NaN  ...   \n",
       "122  Triatoma dimidiata Infestation in Chagas Disea...           NaN  ...   \n",
       "123  A NEW SURVEY FOR LOW-LUMINOSITY EMISSION-LINE ...           NaN  ...   \n",
       "\n",
       "     Web of Science Index  Research Areas  IDS Number  Pubmed Id  \\\n",
       "0                     NaN             NaN         NaN        NaN   \n",
       "1                     NaN             NaN         NaN        NaN   \n",
       "2                     NaN             NaN         NaN        NaN   \n",
       "3                     NaN             NaN         NaN        NaN   \n",
       "4                     NaN             NaN         NaN        NaN   \n",
       "..                    ...             ...         ...        ...   \n",
       "119                   NaN             NaN         NaN        NaN   \n",
       "120                   NaN             NaN         NaN        NaN   \n",
       "121                   NaN             NaN         NaN        NaN   \n",
       "122                   NaN             NaN         NaN        NaN   \n",
       "123                   NaN             NaN         NaN        NaN   \n",
       "\n",
       "     Open Access Designations  Highly Cited Status  Hot Paper Status  \\\n",
       "0                         NaN                  NaN               NaN   \n",
       "1                         NaN                  NaN               NaN   \n",
       "2                         NaN                  NaN               NaN   \n",
       "3                         NaN                  NaN               NaN   \n",
       "4                         NaN                  NaN               NaN   \n",
       "..                        ...                  ...               ...   \n",
       "119                       NaN                  NaN               NaN   \n",
       "120                       NaN                  NaN               NaN   \n",
       "121                       NaN                  NaN               NaN   \n",
       "122                       NaN                  NaN               NaN   \n",
       "123                       NaN                  NaN               NaN   \n",
       "\n",
       "     Date of Export  UT (Unique WOS ID)  Web of Science Record  \n",
       "0               NaN                 NaN                      0  \n",
       "1               NaN                 NaN                      0  \n",
       "2               NaN                 NaN                      0  \n",
       "3               NaN                 NaN                      0  \n",
       "4               NaN                 NaN                      0  \n",
       "..              ...                 ...                    ...  \n",
       "119             NaN                 NaN                      0  \n",
       "120             NaN                 NaN                      0  \n",
       "121             NaN                 NaN                      0  \n",
       "122             NaN                 NaN                      0  \n",
       "123             NaN                 NaN                      0  \n",
       "\n",
       "[124 rows x 72 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web of Science Search Results\n",
    "df_results_wos = pd.read_excel(\"data/savedrecs (2).xls\")\n",
    "df_results_wos = df_results_wos.rename(columns={\"Article Title\": \"title\"})\n",
    "\n",
    "# Semantic Scholar Search Results\n",
    "df_SS_g7\n",
    "\n",
    "df_results_wos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317cbea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Publication Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Book Authors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Editors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Book Group Authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Author Full Names",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Book Author Full Names",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Group Authors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "title_WoS",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Source Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Series Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Series Subtitle",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Language",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Document Type",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Location",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Sponsor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Host",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Author Keywords",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Keywords Plus",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Abstract",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Affiliations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Reprint Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Email Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Researcher Ids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ORCIDs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Orgs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Name Preferred",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Text",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cited References",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cited Reference Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Times Cited, WoS Core",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Times Cited, All Databases",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "180 Day Usage Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Since 2013 Usage Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher City",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher Address",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ISSN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eISSN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ISBN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Journal Abbreviation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Journal ISO Abbreviation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication Year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Issue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Part Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Supplement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Special Issue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Meeting Abstract",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Start Page",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "End Page",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Article Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOI Link",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book DOI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Early Access Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number of Pages",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WoS Categories",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Web of Science Index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research Areas",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "IDS Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pubmed Id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Open Access Designations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Highly Cited Status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Hot Paper Status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Date of Export",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "UT (Unique WOS ID)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Web of Science Record",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "norm_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "paperId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title_SS",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citationCount",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "6eaad3e7-3f5d-45d9-96c5-285484d05a60",
       "rows": [
        [
         "0",
         "J",
         "Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunner, A",
         null,
         null,
         null,
         "Ferreira, Gregorio; Amidei, Jacopo; Nieto, Ruben; Kaltenbrunner, Andreas",
         null,
         null,
         "How Well Do Simulated Population Samples with GPT-4 Align with Real Ones? The Case of the Eysenck Personality Questionnaire Revised-Abbreviated Personality Test",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "how well do simulated population samples with gpt 4 align with real ones the case of the eysenck personality questionnaire revised abbreviated personality test",
         "25f383b7a807392696073801959dcc1c1aadd2bb",
         "How Well Do Simulated Population Samples with GPT-4 Align with Real Ones? The Case of the Eysenck Personality Questionnaire Revised-Abbreviated Personality Test",
         "2025",
         "Gregorio Ferreira, Jacopo Amidei, Rubén Nieto, Andreas Kaltenbrunner",
         "Background: Advances in artificial intelligence have enabled the simulation of human-like behaviors, raising the possibility of using large language models (LLMs) to generate synthetic population samples for research purposes, which may be particularly useful in health and social sciences. Methods: This paper explores the potential of LLMs to simulate population samples mirroring real ones, as well as the feasibility of using personality questionnaires to assess the personality of LLMs. To advance in that direction, 2 experiments were conducted with GPT-4o using the Eysenck Personality Questionnaire Revised-Abbreviated (EPQR-A) in 6 languages: Spanish, English, Slovak, Hebrew, Portuguese, and Turkish. Results: We find that GPT-4o exhibits distinct personality traits, which vary based on parameter settings and the language of the questionnaire. While the model shows promising trends in reflecting certain personality traits and differences across gender and academic fields, discrepancies between the synthetic populations’ responses and those from real populations remain. Conclusions: These inconsistencies suggest that creating fully reliable synthetic population samples for questionnaire testing is still an open challenge. Further research is required to better align synthetic and real population behaviors.",
         "https://www.semanticscholar.org/paper/25f383b7a807392696073801959dcc1c1aadd2bb",
         "0"
        ],
        [
         "1",
         "C",
         "Kane, D; Parke, J; Jo, Y; Bak, J",
         null,
         "Bouamor, H; Pino, J; Bali, K",
         null,
         "Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak, JinYeong",
         null,
         null,
         "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "from values to opinions predicting human behaviors and stances using value injected large language models",
         "52e963c40a5083d5403cebf4d4782271aaa06994",
         "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
         "2023",
         "Dongjun Kang, Joonsuk Park, Yohan Jo, Jinyeong Bak",
         "Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods -- argument generation and question answering -- designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.",
         "https://www.semanticscholar.org/paper/52e963c40a5083d5403cebf4d4782271aaa06994",
         "4"
        ],
        [
         "2",
         "J",
         "Bisbee, J; Clinton, JD; Dorff, C; Kenkel, B; Larson, JM",
         null,
         null,
         null,
         "Bisbee, James; Clinton, Joshua D.; Dorff, Cassy; Kenkel, Brenton; Larson, Jennifer M.",
         null,
         null,
         "Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "synthetic replacements for human survey data the perils of large language models",
         "58d735a54d3aba79ad3bffbfa2433d8e5ee27313",
         "Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         "2024",
         "James Bisbee, Joshua D. Clinton, C. Dorff, Brenton Kenkel, Jennifer M. Larson",
         "\n Large language models (LLMs) offer new research possibilities for social scientists, but their potential as “synthetic data” is still largely unknown. In this paper, we investigate how accurately the popular LLM ChatGPT can recover public opinion, prompting the LLM to adopt different “personas” and then provide feeling thermometer scores for 11 sociopolitical groups. The average scores generated by ChatGPT correspond closely to the averages in our baseline survey, the 2016–2020 American National Election Study (ANES). Nevertheless, sampling by ChatGPT is not reliable for statistical inference: there is less variation in responses than in the real surveys, and regression coefficients often differ significantly from equivalent estimates obtained using ANES data. We also document how the distribution of synthetic responses varies with minor changes in prompt wording, and we show how the same prompt yields significantly different results over a 3-month period. Altogether, our findings raise serious concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.",
         "https://www.semanticscholar.org/paper/58d735a54d3aba79ad3bffbfa2433d8e5ee27313",
         "74"
        ],
        [
         "3",
         "J",
         "Liu, HJ; Cao, Y; Wu, X; Qiu, C; Gu, JG; Liu, MF; Hershcovich, D",
         null,
         null,
         null,
         "Liu, Haijiang; Cao, Yong; Wu, Xun; Qiu, Chen; Gu, Jinguang; Liu, Maofu; Hershcovich, Daniel",
         null,
         null,
         "Towards realistic evaluation of cultural value alignment in large language models: Diversity enhancement for survey response simulation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "towards realistic evaluation of cultural value alignment in large language models diversity enhancement for survey response simulation",
         "3ab59b3d4a4b2e89f7eda93a950eeaa77b37332e",
         "Towards realistic evaluation of cultural value alignment in large language models: Diversity enhancement for survey response simulation",
         "2025",
         "Haijiang Liu, Yong Cao, Xun Wu, Chen Qiu, Jinguang Gu, Maofu Liu, Daniel Hershcovich",
         null,
         "https://www.semanticscholar.org/paper/3ab59b3d4a4b2e89f7eda93a950eeaa77b37332e",
         "2"
        ],
        [
         "4",
         "J",
         "Boelaert, J; Coavoux, S; Ollion, E; Petev, I; Präg, P",
         null,
         null,
         null,
         "Boelaert, Julien; Coavoux, Samuel; Ollion, Etienne; Petev, Ivaylo; Prag, Patrick",
         null,
         null,
         "Machine Bias. How Do Generative Language Models Answer Opinion Polls?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "machine bias how do generative language models answer opinion polls",
         "45f9ea8d0dc1a7e6c56ff6e1f23c8e632687d2a7",
         "Machine Bias. How Do Generative Language Models Answer Opinion Polls?",
         "2025",
         "J. Boelaert, Samuel Coavoux, Étienne Ollion, Ivaylo Petev, P. Präg",
         "Generative artificial intelligence (AI) is increasingly presented as a potential substitute for humans, including as research subjects. However, there is no scientific consensus on how closely these in silico clones can emulate survey respondents. While some defend the use of these “synthetic users,” others point toward social biases in the responses provided by large language models (LLMs). In this article, we demonstrate that these critics are right to be wary of using generative AI to emulate respondents, but probably not for the right reasons. Our results show (i) that to date, models cannot replace research subjects for opinion or attitudinal research; (ii) that they display a strong bias and a low variance on each topic; and (iii) that this bias randomly varies from one topic to the next. We label this pattern “machine bias,” a concept we define, and whose consequences for LLM-based research we further explore.",
         "https://www.semanticscholar.org/paper/45f9ea8d0dc1a7e6c56ff6e1f23c8e632687d2a7",
         "9"
        ],
        [
         "5",
         "J",
         "Qu, Y; Wang, J",
         null,
         null,
         null,
         "Qu, Yao; Wang, Jue",
         null,
         null,
         "Performance and biases of Large Language Models in public opinion simulation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "performance and biases of large language models in public opinion simulation",
         "e6d14d140c4faaf8f3d9f47e61cc5c6091bccf1e",
         "Performance and Biases of Large Language Models in Public Opinion Simulation",
         "2024",
         "Yao Qu, Jue Wang",
         null,
         "https://www.semanticscholar.org/paper/e6d14d140c4faaf8f3d9f47e61cc5c6091bccf1e",
         "46"
        ],
        [
         "6",
         "C",
         "Nguyen, H; Nguyen, V; López-Fierro, S; Ludovise, S; Santagata, R",
         null,
         null,
         "ASSOC COMPUTING MACHINERY",
         "Ha Nguyen; Nguyen, Victoria; Lopez-Fierro, Sariah; Ludovise, Sara; Santagata, Rossella",
         null,
         null,
         "Simulating Climate Change Discussion with Large Language Models: Considerations for Science Communication at Scale",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "simulating climate change discussion with large language models considerations for science communication at scale",
         "dd95064d28ee5d123a6a284422bbba3d443f0416",
         "Simulating Climate Change Discussion with Large Language Models: Considerations for Science Communication at Scale",
         "2024",
         "Ha Nguyen, Victoria Nguyen, Saríah López-Fierro, Sara Ludovise, R. Santagata",
         "Large language models (LLMs) have shown promise in simulating public opinions on social issues. These models can be leveraged in educational simulations that allow students to acquire information and feedback from multiple perspectives. In this research, we investigate the potential of using LLMs (specifically GPT-4) to generate open-ended responses about climate change within a science communication simulation. We prompt GPT-4 to role-play as different personas with various demographics (race/ethnicity, gender, age, income, political affiliations, and ability status) and levels of concern about climate change. We find that GPT-4 is capable of representing multifaceted perspectives around climate change's impact and solutions. However, the model may exaggerate narratives for certain personas based on political affiliations, gender, and concern levels. Such exaggeration may lead to homogeneous narratives that do not fully represent the simulated personas. Our findings highlight the affordances and challenges of applying LLMs to simulating public opinions and enriching educational experiences.",
         "https://www.semanticscholar.org/paper/dd95064d28ee5d123a6a284422bbba3d443f0416",
         "16"
        ],
        [
         "7",
         "J",
         "Salecha, A; Ireland, ME; Subrahmanya, S; Sedoc, J; Ungar, LH; Eichstaedt, JC",
         null,
         null,
         null,
         "Salecha, Aadesh; Ireland, Molly E.; Subrahmanya, Shashanka; Sedoc, Joao; Ungar, Lyle H.; Eichstaedt, Johannes C.",
         null,
         null,
         "Large language models display human-like social desirability biases in Big Five personality surveys",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "large language models display human like social desirability biases in big five personality surveys",
         "8253104f5b1481d8557380d2dc5dab03ff9a7716",
         "Large language models display human-like social desirability biases in Big Five personality surveys",
         "2024",
         "Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, João Sedoc, Pallavi V. Kulkarni, J. Eichstaedt",
         "Abstract Large language models (LLMs) are becoming more widely used to simulate human participants and so understanding their biases is important. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e. increased extraversion, decreased neuroticism, etc.). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4’s survey responses changing by 1.20 (human) SD and Llama 3’s by 0.98 SD, which are very large effects. This bias remains after question order randomization and paraphrasing. Reverse coding the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on this use of LLMs as proxies for human participants.",
         "https://www.semanticscholar.org/paper/8253104f5b1481d8557380d2dc5dab03ff9a7716",
         "25"
        ],
        [
         "8",
         "J",
         "Yao, JC; Zhang, HJ; Ou, J; Zuo, DY; Yang, Z; Dong, ZC",
         null,
         null,
         null,
         "Yao, Junchi; Zhang, Hongjie; Ou, Jie; Zuo, Dingyi; Yang, Zheng; Dong, Zhicheng",
         null,
         null,
         "Social opinions prediction utilizes fusing dynamics equation with LLM-based agents",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "social opinions prediction utilizes fusing dynamics equation with llm based agents",
         "392de716c8f6610f080ba655e885935c20ac6c73",
         "Social opinions prediction utilizes fusing dynamics equation with LLM-based agents",
         "2024",
         "Junchi Yao, Hongjie Zhang, Jie Ou, Dingyi Zuo, Zheng Yang, Zhicheng Dong",
         "In the context where social media emerges as a pivotal platform for social movements and shaping public opinion, accurately simulating and predicting the dynamics of user opinions is of significant importance. Such insights are vital for understanding social phenomena, informing policy decisions, and guiding public opinion. Unfortunately, traditional algorithms based on idealized models and disregarding social data often fail to capture the complexity and nuance of real-world social interactions. This study proposes the Fusing Dynamics Equation-Large Language Model (FDE-LLM) algorithm. This innovative approach aligns the actions and evolution of opinions in Large Language Models (LLMs) with the real-world data on social networks. The FDE-LLM divides users into two roles: opinion leaders and followers. Opinion leaders use LLM for role-playing and employ Cellular Automata(CA) to constrain opinion changes. In contrast, opinion followers are integrated into a dynamic system that combines the CA model with the Susceptible-Infectious-Recovered (SIR) model. This innovative design significantly improves the accuracy of the simulation. Our experiments utilized four real-world datasets from Weibo. The result demonstrates that the FDE-LLM significantly outperforms traditional Agent-Based Modeling (ABM) algorithms and LLM-based algorithms. Additionally, our algorithm accurately simulates the decay and recovery of opinions over time, underscoring LLMs potential to revolutionize the understanding of social media dynamics.",
         "https://www.semanticscholar.org/paper/392de716c8f6610f080ba655e885935c20ac6c73",
         "5"
        ],
        [
         "9",
         "C",
         "Hämäläinen, P; Tavast, M; Kunnari, A",
         null,
         null,
         "ACM",
         "Hamalainen, Perttu; Tavast, Mikke; Kunnari, Anton",
         null,
         null,
         "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "evaluating large language models in generating synthetic hci research data a case study",
         "0ffd57884d7957f6b5634b9fa24843dc3759668f",
         "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
         "2023",
         "Perttu Hämäläinen, Mikke Tavast, Anton Kunnari",
         "Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.",
         "https://www.semanticscholar.org/paper/0ffd57884d7957f6b5634b9fa24843dc3759668f",
         "218"
        ],
        [
         "10",
         "J",
         "Zhang, S; Xu, J; Alvero, AJ",
         null,
         null,
         null,
         "Zhang, Simone; Xu, Janet; Alvero, A. J.",
         null,
         null,
         "Generative AI Meets Open-Ended Survey Responses: Research Participant Use of AI and Homogenization",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "generative ai meets open ended survey responses research participant use of ai and homogenization",
         "8398dfa4d015b3784654a77b3913b62a1f68eed8",
         "Generative AI Meets Open-Ended Survey Responses: Research Participant Use of AI and Homogenization",
         "2025",
         "Simone Zhang, Janet Xu, AJ Alvero",
         "The growing popularity of generative artificial intelligence (AI) tools presents new challenges for data quality in online surveys and experiments. This study examines participants’ use of large language models to answer open-ended survey questions and describes empirical tendencies in human versus large language model (LLM)-generated text responses. In an original survey of research participants recruited from a popular online platform for sourcing social science research subjects, 34 percent reported using LLMs to help them answer open-ended survey questions. Simulations comparing human-written responses from three pre-ChatGPT studies with LLM-generated text reveal that LLM responses are more homogeneous and positive, particularly when they describe social groups in sensitive questions. These homogenization patterns may mask important underlying social variation in attitudes and beliefs among human subjects, raising concerns about data validity. Our findings shed light on the scope and potential consequences of participants’ LLM use in online research.",
         "https://www.semanticscholar.org/paper/8398dfa4d015b3784654a77b3913b62a1f68eed8",
         "10"
        ],
        [
         "11",
         "J",
         "Zhang, BY; Chen, T; Wang, X; Li, Q; Zhang, WS; Wang, FY",
         null,
         null,
         null,
         "Zhang, Baoyu; Chen, Tao; Wang, Xiao; Li, Qiang; Zhang, Weishan; Wang, Fei-Yue",
         null,
         null,
         "Decoding Activist Public Opinion in Decentralized Self-Organized Protests Using LLM",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "decoding activist public opinion in decentralized self organized protests using llm",
         "4949ff97cf5e7c31ed9a057cbbde4b95d3ddd1f8",
         "Decoding Activist Public Opinion in Decentralized Self-Organized Protests Using LLM",
         "2024",
         "Baoyu Zhang, Tao Chen, Xiao Wang, Qiang Li, Weishan Zhang, Fei‐Yue Wang",
         "Based on an investigation of online public opinion on the Nahel Merzouk protests in France, an approach for analyzing and predicting public opinion on protests based on large language model (LLM) is proposed, revealing the impact of emerging social media on the protests. We demonstrate that protests generate public opinion on social media with some lag, but that comment sentiment and expression are consistent with protest trends. As the protests unfolded, we analyzed the evolution of public sentiment. We constructed the prompt based on historical data to predict the protests using the p-tuning and Lora approach to fine-tune LLM. In addition, we discuss how to use blockchain technology to optimize distributed, self-organizing protests and reduce the potential for disinformation and violent conflict.",
         "https://www.semanticscholar.org/paper/4949ff97cf5e7c31ed9a057cbbde4b95d3ddd1f8",
         "2"
        ],
        [
         "12",
         "J",
         "Gao, C; Lan, XC; Li, N; Yuan, Y; Ding, JT; Zhou, ZL; Xu, FL; Li, Y",
         null,
         null,
         null,
         "Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yuan; Ding, Jingtao; Zhou, Zhilun; Xu, Fengli; Li, Yong",
         null,
         null,
         "Large language models empowered agent-based modeling and simulation: a survey and perspectives",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "large language models empowered agent based modeling and simulation a survey and perspectives",
         "592ac35991e583fc37c26ee6659d2deb85142ad9",
         "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
         "2023",
         "Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li",
         "Agent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.",
         "https://www.semanticscholar.org/paper/592ac35991e583fc37c26ee6659d2deb85142ad9",
         "182"
        ],
        [
         "13",
         "C",
         "AlKhamissi, B; ElNokrashy, M; AlKhamissi, M; Diab, M",
         null,
         "Ku, LW; Martins, A; Srikumar, V",
         null,
         "AlKhamissi, Badr; ElNokrashy, Muhammad; AlKhamissi, Mai; Diab, Mona",
         null,
         null,
         "Investigating Cultural Alignment of Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "investigating cultural alignment of large language models",
         "b1890367317f0657c08ed96be4c474035b34b485",
         "Investigating Cultural Alignment of Large Language Models",
         "2024",
         "Badr AlKhamissi, Muhammad N. ElNokrashy, Mai AlKhamissi, Mona Diab",
         "The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.",
         "https://www.semanticscholar.org/paper/b1890367317f0657c08ed96be4c474035b34b485",
         "78"
        ],
        [
         "14",
         "J",
         "Zhao, XJ; Wang, H; Dai, CX; Tang, JC; Deng, KX; Zhong, ZH; Kong, FY; Wang, SY; Morikawa, S",
         null,
         null,
         null,
         "Zhao, Xinjie; Wang, Hao; Dai, Chengxiao; Tang, Jiacheng; Deng, Kaixin; Zhong, Zhihua; Kong, Fanying; Wang, Shiyun; Morikawa, So",
         null,
         null,
         "Multi-Stage Simulation of Residents' Disaster Risk Perception and Decision-Making Behavior: An Exploratory Study on Large Language Model-Driven Social-Cognitive Agent Framework",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "multi stage simulation of residents disaster risk perception and decision making behavior an exploratory study on large language model driven social cognitive agent framework",
         "6d2077c8f4864103780b160501cd207005d045c8",
         "Multi-Stage Simulation of Residents’ Disaster Risk Perception and Decision-Making Behavior: An Exploratory Study on Large Language Model-Driven Social–Cognitive Agent Framework",
         "2025",
         "Xinjie Zhao, Hao Wang, Chengxiao Dai, Jiacheng Tang, Kaixin Deng, Zhihua Zhong, Fanying Kong, Shiyun Wang, So Morikawa",
         "The escalating frequency and complexity of natural disasters highlight the urgent need for deeper insights into how individuals and communities perceive and respond to risk information. Yet, conventional research methods—such as surveys, laboratory experiments, and field observations—often struggle with limited sample sizes, external validity concerns, and difficulties in controlling for confounding variables. These constraints hinder our ability to develop comprehensive models that capture the dynamic, context-sensitive nature of disaster decision-making. To address these challenges, we present a novel multi-stage simulation framework that integrates Large Language Model (LLM)-driven social–cognitive agents with well-established theoretical perspectives from psychology, sociology, and decision science. This framework enables the simulation of three critical phases—information perception, cognitive processing, and decision-making—providing a granular analysis of how demographic attributes, situational factors, and social influences interact to shape behavior under uncertain and evolving disaster conditions. A case study focusing on pre-disaster preventive measures demonstrates its effectiveness. By aligning agent demographics with real-world survey data across 5864 simulated scenarios, we reveal nuanced behavioral patterns closely mirroring human responses, underscoring the potential to overcome longstanding methodological limitations and offer improved ecological validity and flexibility to explore diverse disaster environments and policy interventions. While acknowledging the current constraints, such as the need for enhanced emotional modeling and multimodal inputs, our framework lays a foundation for more nuanced, empirically grounded analyses of risk perception and response patterns. By seamlessly blending theory, advanced LLM capabilities, and empirical alignment strategies, this research not only advances the state of computational social simulation but also provides valuable guidance for developing more context-sensitive and targeted disaster management strategies.",
         "https://www.semanticscholar.org/paper/6d2077c8f4864103780b160501cd207005d045c8",
         "1"
        ],
        [
         "15",
         "J",
         "de Winter, JCF; Driessen, T; Dodou, D",
         null,
         null,
         null,
         "de Winter, Joost C. F.; Driessen, Tom; Dodou, Dimitra",
         null,
         null,
         "The use of ChatGPT for personality research: Administering questionnaires using generated personas",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "the use of chatgpt for personality research administering questionnaires using generated personas",
         "781703bc7e4fb90766824ee808097171afa223b3",
         "The use of ChatGPT for personality research: Administering questionnaires using generated personas",
         "2024",
         "J. D. de Winter, Tom Driessen, Dimitra Dodou",
         null,
         "https://www.semanticscholar.org/paper/781703bc7e4fb90766824ee808097171afa223b3",
         "19"
        ],
        [
         "16",
         "J",
         "Rakovics, Z; Rakovics, M",
         null,
         null,
         null,
         "Rakovics, Zsofia; Rakovics, Marton",
         null,
         null,
         "Exploring the potential and limitations of large language models as virtual respondents for social science research",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "exploring the potential and limitations of large language models as virtual respondents for social science research",
         "6cd94c4fdfd1ddf59b5385919851de2662e412fe",
         "Exploring the potential and limitations of large language models as virtual respondents for social science research",
         "2024",
         "Zsófia Rakovics, Márton Rakovics",
         "Social and linguistic differences encoded in various textual content available on the internet represent certain features of modern societies. For any scientific research which is interested in social differences mediated by language, the advent of large language models (LLMs) has brought new opportunities. LLMs could be used to extract information about different groups of society and utilized as data providers by acting as virtual respondents generating answers as such. \nUsing LLMs (GPT-variants, Llama2, and Mixtral), we generated virtual answers for politics and democracy related attitude questions of the European Social Survey (10th wave) and statistically compared the results of the simulated responses to the real ones. We explored different prompting techniques and the effect of different types and richness of contextual information provided to the models. Our results suggest that the tested LLMs generate highly realistic answers and are good at invoking the needed patterns from limited contextual information given to them if a couple of relevant examples are provided, but struggle in a zero-shot setting. \nA critical methodological analysis is inevitable when considering the potential use of data generated by LLMs for scientific research, the exploration of known biases and reflection on social reality not represented on the internet are essential.",
         "https://www.semanticscholar.org/paper/6cd94c4fdfd1ddf59b5385919851de2662e412fe",
         "1"
        ],
        [
         "17",
         "J",
         "Leung, HW; Bovy, J",
         null,
         null,
         null,
         "Leung, Henry W.; Bovy, Jo",
         null,
         null,
         "Towards an astronomical foundation model for stars with a transformer-based model",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "towards an astronomical foundation model for stars with a transformer based model",
         "264cb7a7dbee1303ff9e0ebe2dac78646271a2fb",
         "Towards an astronomical foundation model for stars with a Transformer-based model",
         "2023",
         "Henry W. Leung, J. Bovy",
         "Rapid strides are currently being made in the field of artificial intelligence using Transformer-based models like Large Language Models (LLMs). The potential of these methods for creating a single, large, versatile model in astronomy has not yet been explored. In this work, we propose a framework for data-driven astronomy that uses the same core techniques and architecture as used by LLMs. Using a variety of observations and labels of stars as an example, we build a Transformer-based model and train it in a self-supervised manner with cross-survey data sets to perform a variety of inference tasks. In particular, we demonstrate that a $\\textit{single}$ model can perform both discriminative and generative tasks even if the model was not trained or fine-tuned to do any specific task. For example, on the discriminative task of deriving stellar parameters from Gaia XP spectra, we achieve an accuracy of 47 K in $T_\\mathrm{eff}$, 0.11 dex in $\\log{g}$, and 0.07 dex in $[\\mathrm{M/H}]$, outperforming an expert $\\texttt{XGBoost}$ model in the same setting. But the same model can also generate XP spectra from stellar parameters, inpaint unobserved spectral regions, extract empirical stellar loci, and even determine the interstellar extinction curve. Our framework demonstrates that building and training a $\\textit{single}$ foundation model without fine-tuning using data and parameters from multiple surveys to predict unmeasured observations and parameters is well within reach. Such\"Large Astronomy Models\"trained on large quantities of observational data will play a large role in the analysis of current and future large surveys.",
         "https://www.semanticscholar.org/paper/264cb7a7dbee1303ff9e0ebe2dac78646271a2fb",
         "24"
        ],
        [
         "18",
         "J",
         "Mburu, TK; Rong, KX; McColley, CJ; Werth, A",
         null,
         null,
         null,
         "Mburu, Ted K.; Rong, Kangxuan; McColley, Campbell J.; Werth, Alexandra",
         null,
         null,
         "Methodological foundations for artificial intelligence-driven survey question generation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "methodological foundations for artificial intelligence driven survey question generation",
         "54b63afefc315b5f051f4a19fe413ef6c544c9fd",
         "Methodological foundations for artificial intelligence‐driven survey question generation",
         "2025",
         "Ted K. Mburu, Kangxuan Rong, Campbell J. McColley, Alexandra Werth",
         "This study investigates the use of large language models to create adaptive, contextually relevant survey questions, aiming to enhance data quality in educational research without limiting scalability.We provide step‐by‐step methods to develop a dynamic survey instrument, driven by artificial intelligence (AI), and introduce the Synthetic Question–Response Analysis (SQRA) framework, a methodology designed to help evaluate AI‐generated questions before deployment with human participants.We examine the questions generated by our survey instrument, as well as compare AI‐to‐AI, generated through our SQRA framework, with AI‐to‐human interactions. Activity theory provides a theoretical lens to examine the dynamic interactions between AI and participants, highlighting the mutual influence within the survey tool.We found that AI‐generated questions were contextually relevant and adaptable, successfully incorporating course‐specific references. However, issues such as redundant phrasing, double‐barreled questions, and jargon affected the clarity of the questions. Although the SQRA framework exhibited limitations in replicating human response variability, its iterative refinement process proved effective in improving question quality, reinforcing the utility of this approach for enhancing AI‐driven surveys.While AI‐driven question generation can enhance the scalability and personalization of open‐ended survey prompts, more research is needed to establish best practices for high‐quality educational research. The SQRA framework demonstrated practical utility for prompt refinement and initial validation of AI‐generated survey content, but it is not capable of replicating human responses. We highlight the importance of iterative prompt engineering, ethical considerations, and the need for methodological advancements in the development of trustworthy AI‐driven survey instruments for educational research.",
         "https://www.semanticscholar.org/paper/54b63afefc315b5f051f4a19fe413ef6c544c9fd",
         "1"
        ],
        [
         "19",
         "J",
         "Zhang, KH; Dong, CQ; Guo, YF; Zhou, W; Yu, G; Mi, JN",
         null,
         null,
         null,
         "Zhang, Kaihang; Dong, Changqi; Guo, Yifeng; Zhou, Wuai; Yu, Guang; Mi, Jianing",
         null,
         null,
         "Lagged Stance Interactions and Counter-Spiral of Silence: A Data-Driven Analysis and Agent-Based Modeling of Technical Public Opinion Events",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "lagged stance interactions and counter spiral of silence a data driven analysis and agent based modeling of technical public opinion events",
         "bc270ab2e00f78691c8fd16548809e3609dedee2",
         "Lagged Stance Interactions and Counter-Spiral of Silence: A Data-Driven Analysis and Agent-Based Modeling of Technical Public Opinion Events",
         "2025",
         "Kaihang Zhang, Changqi Dong, Yifeng Guo, Wuai Zhou, Guang Yu, Jianing Mi",
         "Understanding the dynamics of public opinion formation in digital environments is crucial for managing technological communications effectively. This study investigates stance interactions and opinion reversal phenomena in technical discourse through analysis of the Manus AI controversy that generated approximately 36,932 social media interactions during March 2025. Employing an integrated methodology combining Large Language Model (LLM)-enhanced stance detection with agent-based modeling (ABM), we reveal distinctive patterns challenging traditional public opinion theories. Our cross-correlation analysis identifies significant lagged interaction effects between skeptical and supportive stances, demonstrating how critical expressions trigger amplified counter-responses rather than inducing silence. Unlike prior conceptualizations of counter-silencing that emphasize ideological resistance or echo chambers, our notion of the “counter-spiral of silence” specifically highlights lagged emotional responses and reactive amplification triggered by minority expressions in digital technical discourse. We delineate its boundary conditions as arising under high emotional salience, asymmetrical expertise, and platform structures that enable real-time feedback. The agent-based simulation reproduces empirical patterns, revealing how emotional contagion and network clustering mechanisms generate “counter-spiral of silence” phenomena where challenges to dominant positions ultimately strengthen rather than weaken those positions. These findings illuminate how cognitive asymmetries between public expectations and industry realities create distinctive discourse patterns in technical contexts, offering insights for managing technology communication and predicting public response trajectories in rapidly evolving digital environments.",
         "https://www.semanticscholar.org/paper/bc270ab2e00f78691c8fd16548809e3609dedee2",
         "0"
        ],
        [
         "20",
         "J",
         "Campos, M; Farinhas, A; Zerva, C; Figueiredo, MAT; Martins, AFT",
         null,
         null,
         null,
         "Campos, Margarida; Farinhas, Antonio; Zerva, Chrysoula; Figueiredo, Mario A. T.; Martins, Andre F. T.",
         null,
         null,
         "Conformal Prediction for Natural Language Processing: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "conformal prediction for natural language processing a survey",
         "346fdbda3ecf4775819fced0cfed78357bee8128",
         "Conformal Prediction for Natural Language Processing: A Survey",
         "2024",
         "Margarida M. Campos, António Farinhas, Chrysoula Zerva, M'ario A.T. Figueiredo, Andr'e F. T. Martins",
         "Abstract The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as Hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.",
         "https://www.semanticscholar.org/paper/346fdbda3ecf4775819fced0cfed78357bee8128",
         "22"
        ],
        [
         "21",
         "C",
         "Cheng, M; Piccardi, T; Yang, DY",
         null,
         "Bouamor, H; Pino, J; Bali, K",
         null,
         "Cheng, Myra; Piccardi, Tiziano; Yang, Diyi",
         null,
         null,
         "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "compost characterizing and evaluating caricature in llm simulations",
         "7a4fe2f003241ad97bf1778e527cb0306fa90da2",
         "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
         "2023",
         "Myra Cheng, Tiziano Piccardi, Diyi Yang",
         "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",
         "https://www.semanticscholar.org/paper/7a4fe2f003241ad97bf1778e527cb0306fa90da2",
         "89"
        ],
        [
         "22",
         "J",
         "Kaur, A; Budko, A; Liu, K; Eaton, E; Steitz, BD; Johnson, KB",
         null,
         null,
         null,
         "Kaur, Amarpreet; Budko, Alexander; Liu, Katrina; Eaton, Eric; Steitz, Bryan D.; Johnson, Kevin B.",
         null,
         null,
         "Automating Responses to Patient Portal Messages Using Generative AI",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "automating responses to patient portal messages using generative ai",
         "caa3c4eb1ede700f03eef5b4f25a18b08c88d832",
         "Automating Responses to Patient Portal Messages Using Generative AI",
         "2024",
         "Amarpreet Kaur, Alexander Budko, Katrina Liu, Eric Eaton, Bryan D. Steitz, Kevin B. Johnson",
         "Background: Patient portals serve as vital bridges between patients and providers, playing an increasing role in healthcare communication. The rising volume and complexity of these messages is exacerbating physician and nursing burnout. Recent studies have demonstrated that AI chatbots can generate message responses that are viewed favorably by healthcare professionals; however, these studies have not included the diverse range of messages typically found in patient portals. Our goal is to investigate the quality of GPT-generated message responses across the spectrum of message types within a patient portal. Methods: We used novel prompt engineering techniques to craft synthetic responses tailored to adult primary care patients. We enrolled a sample of primary care providers in a cross-sectional study to compare authentic with synthetic patient portal message responses, generated by GPT-4. The survey assessed each messages empathy, relevance, medical accuracy, and readability on a scale from 0 to 5. Respondents were asked to identify messages that were GPT-generated vs. provider-generated. Mean scores for all metrics were computed for subsequent analysis. Results: A total of 49 health care providers participated in the survey (59% completion rate), comprising 16 physicians and 32 advanced practice providers (APPs). When presented with GPT vs. authentic message response pairs, participants correctly identified GPT-generated responses 73% of the time and correctly identified authentic responses 50% of the time. In comparison to messages generated by physicians, GPT-4 generated messages exhibited higher mean scores for empathy (3.57 vs. 3.07, p < 0.001), relevance (3.94 vs. 3.81, p = 0.08) accuracy (4.05 vs. 3.95, p= 0.12) and readability (4.5 vs. 4.13, p < 0.001). Limitations: The study is a single-site, single-specialty study, limited due to the use of synthetic data. Conclusion: Our findings affirm the potential of GPT-generated patient portal message responses to achieve comparable levels of empathy, relevance, and readability to those found in typical responses according to the health care providers and indicates promising prospects for their integration in the healthcare sector. Additional studies should be done within provider workflows and with careful evaluation of patient attitudes and concerns related to the ethics as well as the quality of generated patient portal message responses in all settings.",
         "https://www.semanticscholar.org/paper/caa3c4eb1ede700f03eef5b4f25a18b08c88d832",
         "7"
        ],
        [
         "23",
         "J",
         "Ji, J; Kim, J; Kim, Y",
         null,
         null,
         null,
         "Ji, Junyung; Kim, Jiwoo; Kim, Younghoon",
         null,
         null,
         "Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "predicting missing values in survey data using prompt engineering for addressing item non response",
         "bdeeaf207e2563f39ad27a9d9511d8573b5bff95",
         "Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response",
         "2024",
         "Junyung Ji, Jiwoo Kim, Younghoon Kim",
         "Survey data play a crucial role in various research fields, including economics, education, and healthcare, by providing insights into human behavior and opinions. However, item non-response, where respondents fail to answer specific questions, presents a significant challenge by creating incomplete datasets that undermine data integrity and can hinder or even prevent accurate analysis. Traditional methods for addressing missing data, such as statistical imputation techniques and deep learning models, often fall short when dealing with the rich linguistic content of survey data. These approaches are also hampered by high time complexity for training and the need for extensive preprocessing or feature selection. In this paper, we introduce an approach that leverages Large Language Models (LLMs) through prompt engineering for predicting item non-responses in survey data. Our method combines the strengths of both traditional imputation techniques and deep learning methods with the advanced linguistic understanding of LLMs. By integrating respondent similarities, question relevance, and linguistic semantics, our approach enhances the accuracy and comprehensiveness of survey data analysis. The proposed method bypasses the need for complex preprocessing and additional training, making it adaptable, scalable, and capable of generating explainable predictions in natural language. We evaluated the effectiveness of our LLM-based approach through a series of experiments, demonstrating its competitive performance against established methods such as Multivariate Imputation by Chained Equations (MICE), MissForest, and deep learning models like TabTransformer. The results show that our approach not only matches but, in some cases, exceeds the performance of these methods while significantly reducing the time required for data processing.",
         "https://www.semanticscholar.org/paper/bdeeaf207e2563f39ad27a9d9511d8573b5bff95",
         "2"
        ],
        [
         "24",
         "J",
         "Goli, A; Singh, A",
         null,
         null,
         null,
         "Goli, Ali; Singh, Amandeep",
         null,
         null,
         "Frontiers: Can Large Language Models Capture Human Preferences?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "frontiers can large language models capture human preferences",
         "0377c4c20d86e3a23cb5c22d89b3cb488c31a564",
         "Frontiers: Can Large Language Models Capture Human Preferences?",
         "2024",
         "Ali Goli, Amandeep Singh",
         "This paper examines the potential of large language models to mimic human survey respondents and to derive their preferences.",
         "https://www.semanticscholar.org/paper/0377c4c20d86e3a23cb5c22d89b3cb488c31a564",
         "31"
        ],
        [
         "25",
         "C",
         "Liu, YH; Chen, XY; Zhang, XQ; Gao, X; Zhang, J; Yan, R",
         null,
         "Larson, K",
         null,
         "Liu, Yuhan; Chen, Xiuying; Zhang, Xiaoqing; Gao, Xing; Zhang, Ji; Yan, Rui",
         null,
         null,
         "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "from skepticism to acceptance simulating the attitude dynamics toward fake news",
         "1bd4b8be136072c8f56114f2f8479aaed2ad6d9b",
         "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News",
         "2024",
         "Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, Rui Yan",
         "In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our simulation results uncover patterns in fake news propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of LLMs in combating fake news.",
         "https://www.semanticscholar.org/paper/1bd4b8be136072c8f56114f2f8479aaed2ad6d9b",
         "44"
        ],
        [
         "26",
         "J",
         "Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunner, A",
         null,
         null,
         null,
         "Ferreira, Gregorio; Amidei, Jacopo; Nieto, Ruben; Kaltenbrunner, Andreas",
         null,
         null,
         "Matching GPT-simulated Populations with Real Ones in Psychological Studies-The Case of the EPQR-A Personality Test",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "matching gpt simulated populations with real ones in psychological studies the case of the epqr a personality test",
         "e186d394a49ab2866a8b1248a99e6f1637a238fe",
         "Matching GPT-simulated Populations with Real Ones in Psychological Studies—The Case of the EPQR-A Personality Test",
         "2025",
         "Gregorio Ferreira, Jacopo Amidei, Rubén Nieto, Andreas Kaltenbrunner",
         "This article analyzes how well OpenAI’s LLM GPT-4 can emulate different personalities and simulate populations to answer psychological questionnaires similarly to real population samples. For this purpose, we performed different experiments with the Eysenck Personality Questionnaire-Revised Abbreviated (EPQR-A) in three different languages (Spanish, English, and Slovak). The EPQR-A measures personality on four scales: extraversion (E: sociability), neuroticism (N: emotional stability), psychoticism (P: tendency to break social rules, and not having empathy), and lying (L: social desirability). We perform a comparative analysis of the answers of synthetic populations with those of two real population samples of Spanish students as well as the unconditioned baseline personality of GPT. Furthermore, the impact of time (what year the questionnaire is answered), questionnaire language, and student age and gender are analyzed. To our knowledge, this is the first time the EPQR-A test has been used to assess the GPT´s personality and the impact of different language versions and time are measured. Our analysis reveals that GPT-4 exhibits an extroverted, emotionally stable personality with low psychoticism levels and high social desirability. GPT-4 replicates some differences observed in real populations in terms of gender but only partially replicates the results for real populations.",
         "https://www.semanticscholar.org/paper/e186d394a49ab2866a8b1248a99e6f1637a238fe",
         "2"
        ],
        [
         "27",
         "C",
         "Scarlatos, A; Baker, RS; Lan, A",
         null,
         null,
         "ACM",
         "Scarlatos, Alexander; Baker, Ryan S.; Lan, Andrew",
         null,
         null,
         "Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "exploring knowledge tracing in tutor student dialogues using llms",
         "06e9ec37cc25980544d0a78b5aa4893dafc65fd3",
         "Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs",
         "2024",
         "Alexander Scarlatos, Ryan S. Baker, Andrew Lan",
         "Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components/skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLM’s accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work.",
         "https://www.semanticscholar.org/paper/06e9ec37cc25980544d0a78b5aa4893dafc65fd3",
         "14"
        ],
        [
         "28",
         "C",
         "Steinmacher, I; Penney, JM; Felizardo, KR; Garcia, AF; Gerosa, MA",
         null,
         null,
         "ACM",
         "Steinmacher, Igor; Penney, Jacob Mcauley; Felizardo, Katia Romero; Garcia, Alessandro F.; Gerosa, Marco A.",
         null,
         null,
         "Can ChatGPT emulate humans in software engineering surveys?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "can chatgpt emulate humans in software engineering surveys",
         "2a77ac8d1c36ccf9f222cf0ae16e251ac6b13e86",
         "Can ChatGPT emulate humans in software engineering surveys?",
         "2024",
         "Igor Steinmacher, Jacob Penney, K. Felizardo, Alessandro F. Garcia, M. Gerosa",
         "Context: There is a growing belief in the literature that large language models (LLMs), such as ChatGPT, can mimic human behavior in surveys. Gap: While the literature has shown promising results in social sciences and market research, there is scant evidence of its effectiveness in technical fields like software engineering. Objective: Inspired by previous work, this paper explores ChatGPT’s ability to replicate findings from prior software engineering research. Given the frequent use of surveys in this field, if LLMs can accurately emulate human responses, this technique could address common methodological challenges like recruitment difficulties, representational shortcomings, and respondent fatigue. Method: We prompted ChatGPT to reflect the behavior of a ‘mega-persona’ representing the demographic distribution of interest. We replicated surveys from 2019 to 2023 from leading SE conferences, examining ChatGPT’s proficiency in mimicking responses from diverse demographics. Results: Our findings reveal that ChatGPT can successfully replicate the outcomes of some studies, but in others, the results were not significantly better than a random baseline. Conclusions: This paper reports our results so far and discusses the challenges and potential research opportunities in leveraging LLMs for representing humans in software engineering surveys.",
         "https://www.semanticscholar.org/paper/2a77ac8d1c36ccf9f222cf0ae16e251ac6b13e86",
         "7"
        ],
        [
         "29",
         "J",
         "Wahidur, RSM; Tashdeed, I; Kaur, M; Lee, HN",
         null,
         null,
         null,
         "Wahidur, Rahman S. M.; Tashdeed, Ishmam; Kaur, Manjit; Lee, Heung-No",
         null,
         null,
         "Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "enhancing zero shot crypto sentiment with fine tuned language model and prompt engineering",
         "bcdc44ef48ffadbdaa3bd5cacfe9ddb9b9f48750",
         "Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering",
         "2023",
         "Rahman S M Wahidur, Ishmam Tashdeed, Manjit Kaur, Heung-No Lee",
         "Blockchain technology has revolutionized the financial landscape, witnessing widespread adoption of cryptocurrencies due to their decentralized and transparent nature. As sentiments expressed on social media platforms wield substantial influence over cryptocurrency market dynamics, sentiment analysis has emerged as a crucial tool for gauging public opinion and predicting market trends. This paper explores fine-tuning techniques for large language models to enhance sentiment analysis performance. Experimental results demonstrate a significant average zero-shot performance gain of 40% on unseen tasks after fine-tuning, highlighting its potential. Additionally, the impact of instruction-based fine-tuning on models of varying scales is examined, revealing that larger models benefit from instruction tuning, achieving the highest average accuracy score of 75.16%. In contrast, smaller-scale models may experience reduced generalization due to complete model capacity utilization. To gain deeper insight into instruction effectiveness, the paper presents experimental investigations under different instruction tuning setups. Results show the model achieves an average accuracy score of 72.38% for short and simple instructions, outperforming long and complex instructions by over 12%. Finally, the paper explores the relationship between fine-tuning corpus size and model performance, identifying an optimal corpus size of 6,000 data points for the highest performance across different models. Microsoft’s MiniLM, a distilled version of BERT, excels in efficient data use and performance optimization, while Google’s FLAN-T5 demonstrates consistent and reliable performance across diverse datasets.",
         "https://www.semanticscholar.org/paper/bcdc44ef48ffadbdaa3bd5cacfe9ddb9b9f48750",
         "17"
        ],
        [
         "30",
         "C",
         "Hwang, E; Majumder, BP; Tandon, N",
         null,
         "Bouamor, H; Pino, J; Bali K",
         null,
         "Hwang, EunJeong; Majumder, Bodhisattwa Prasad; Tandon, Niket",
         null,
         null,
         "Aligning Language Models to User Opinions",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "aligning language models to user opinions",
         "5db0f55332839c408e3049cea1a6ad48fefba70c",
         "Aligning Language Models to User Opinions",
         "2023",
         "EunJeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon",
         "An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by Pew Research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. In addition to the typical approach of prompting LLMs with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately.",
         "https://www.semanticscholar.org/paper/5db0f55332839c408e3049cea1a6ad48fefba70c",
         "82"
        ],
        [
         "31",
         "C",
         "Min, Y; Jeong, JW",
         null,
         "Eck, U; Sra, M; Stefanucci, J; Sugimoto, M; Tatzgern, M; Williams, I",
         null,
         "Min, Yewon; Jeong, Jin-Woo",
         null,
         null,
         "Public Speaking Q&A Practice with LLM-Generated Personas in Virtual Reality",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "public speaking q a practice with llm generated personas in virtual reality",
         "3b36ec5f47076620dc7735f8aba55d9c3c3e6b32",
         "Public Speaking Q&A Practice with LLM-Generated Personas in Virtual Reality",
         "2024",
         "Yewon Min, Jin-Woo Jeong",
         "This paper introduces a novel VR-based Q&A practice system that harnesses the power of Large Language Models (LLMs). We support Q&A practice for upcoming public speaking by providing an immersive VR training environment populated with LLM-generated audiences, each capable of posing diverse and realistic questions based on different personas. We conducted a pilot user study involving 20 participants who engaged in VR-based Q&A practice sessions. The sessions featured a variety of questions regarding presentation material provided by the participants, all of which were generated by LLM-based personas. Through post-surveys and interviews, we evaluated the effectiveness of the proposed method. The participants valued the system for engagement and focus while also identifying several areas for improvement. Our study demonstrated the potential of integrating VR and LLMs to create a powerful, immersive tool for Q&A practice.",
         "https://www.semanticscholar.org/paper/3b36ec5f47076620dc7735f8aba55d9c3c3e6b32",
         "5"
        ],
        [
         "32",
         "C",
         "Liu, A; Diab, M; Fried, D",
         null,
         "Martins, A; Srikumar, V; Ku, LW",
         null,
         "Liu, Andy; Diab, Mona; Fried, Daniel",
         null,
         null,
         "Evaluating Large Language Model Biases in Persona-Steered Generation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "evaluating large language model biases in persona steered generation",
         "ae03f10729959435ecefc0e90cba4cbe8438a10b",
         "Evaluating Large Language Model Biases in Persona-Steered Generation",
         "2024",
         "Andy Liu, Mona T. Diab, Daniel Fried",
         "The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have. People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas. We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending. We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas. We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases. Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints.",
         "https://www.semanticscholar.org/paper/ae03f10729959435ecefc0e90cba4cbe8438a10b",
         "49"
        ],
        [
         "33",
         "J",
         "Mishra, T; Sutanto, E; Rossanti, R; Pant, N; Ashraf, A; Raut, A; Uwabareze, G; Oluwatomiwa, A; Zeeshan, B",
         null,
         null,
         null,
         "Mishra, Tanisha; Sutanto, Edward; Rossanti, Rini; Pant, Nayana; Ashraf, Anum; Raut, Akshay; Uwabareze, Germaine; Oluwatomiwa, Ajayi; Zeeshan, Bushra",
         null,
         null,
         "Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers",
         "0c1e396f7f23d34ebad01dac29de6898f18ae63e",
         "Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers",
         "2024",
         "Tanisha Mishra, Edward Sutanto, Rini Rossanti, Nayana Pant, Anum Ashraf, Akshay Raut, Germaine Uwabareze, Ajayi Oluwatomiwa, Bushra Zeeshan",
         "With breakthroughs in Natural Language Processing and Artificial Intelligence (AI), the usage of Large Language Models (LLMs) in academic research has increased tremendously. Models such as Generative Pre-trained Transformer (GPT) are used by researchers in literature review, abstract screening, and manuscript drafting. However, these models also present the attendant challenge of providing ethically questionable scientific information. Our study provides a snapshot of global researchers’ perception of current trends and future impacts of LLMs in research. Using a cross-sectional design, we surveyed 226 medical and paramedical researchers from 59 countries across 65 specialties, trained in the Global Clinical Scholars’ Research Training certificate program of Harvard Medical School between 2020 and 2024. Majority (57.5%) of these participants practiced in an academic setting with a median of 7 (2,18) PubMed Indexed published articles. 198 respondents (87.6%) were aware of LLMs and those who were aware had higher number of publications (p < 0.001). 18.7% of the respondents who were aware (n = 37) had previously used LLMs in publications especially for grammatical errors and formatting (64.9%); however, most (40.5%) did not acknowledge its use in their papers. 50.8% of aware respondents (n = 95) predicted an overall positive future impact of LLMs while 32.6% were unsure of its scope. 52% of aware respondents (n = 102) believed that LLMs would have a major impact in areas such as grammatical errors and formatting (66.3%), revision and editing (57.2%), writing (57.2%) and literature review (54.2%). 58.1% of aware respondents were opined that journals should allow for use of AI in research and 78.3% believed that regulations should be put in place to avoid its abuse. Seeing the perception of researchers towards LLMs and the significant association between awareness of LLMs and number of published works, we emphasize the importance of developing comprehensive guidelines and ethical framework to govern the use of AI in academic research and address the current challenges. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-024-81370-6.",
         "https://www.semanticscholar.org/paper/0c1e396f7f23d34ebad01dac29de6898f18ae63e",
         "14"
        ],
        [
         "34",
         "J",
         "Teferra, BG; Perivolaris, A; Hsiang, WN; Sidharta, CK; Rueda, A; Parkington, K; Wu, YQ; Soni, A; Samavi, R; Jetly, R; Zhang, YB; Cao, B; Rambhatla, S; Krishnan, S; Bhat, V",
         null,
         null,
         null,
         "Teferra, Bazen Gashaw; Perivolaris, Argyrios; Hsiang, Wei-Ni; Sidharta, Christian Kevin; Rueda, Alice; Parkington, Karisa; Wu, Yuqi; Soni, Achint; Samavi, Reza; Jetly, Rakesh; Zhang, Yanbo; Cao, Bo; Rambhatla, Sirisha; Krishnan, Sri; Bhat, Venkat",
         null,
         null,
         "Leveraging large language models for automated depression screening",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "leveraging large language models for automated depression screening",
         "d0deb3ffd586a886b435d67c1b3aad89eeeaf358",
         "Leveraging large language models for automated depression screening",
         "2025",
         "Bazen Gashaw Teferra, Argyrios Perivolaris, Wei-Ni Hsiang, Christian Kevin Sidharta, Alice Rueda, Karisa B. Parkington, Yuqi Wu, Achint Soni, Reza Samavi, Rakesh Jetly, Yanbo Zhang, Bo Cao, Sirisha Rambhatla, Sri Krishnan, Venkat Bhat",
         "Mental health diagnoses possess unique challenges that often lead to nuanced difficulties in managing an individual’s well-being and daily functioning. Self-report questionnaires are a common practice in clinical settings to help mitigate the challenges involved in mental health disorder screening. However, these questionnaires rely on an individual’s subjective response which can be influenced by various factors. Despite the advancements of Large Language Models (LLMs), quantifying self-reported experiences with natural language processing has resulted in imperfect accuracy. This project aims to demonstrate the effectiveness of zero-shot learning LLMs for screening and assessing item scales for depression using LLMs. The DAIC-WOZ is a publicly available mental health dataset that contains textual data from clinical interviews and self-report questionnaires with relevant mental health disorder labels. The RISEN prompt engineering framework was utilized to evaluate LLMs’ effectiveness in predicting depression symptoms based on individual PHQ-8 items. Various LLMs, including GPT models, Llama3_8B, Cohere, and Gemini were assessed based on performance. The GPT models, especially GPT-4o, were consistently better than other LLMs (Llama3_8B, Cohere, Gemini) across all eight items of the PHQ-8 scale in accuracy (M = 75.9%), and F1 score (0.74). GPT models were able to predict PHQ-8 items related to emotional and cognitive states. Llama 3_8B demonstrated superior detection of anhedonia-related symptoms and the Cohere LLM’s strength was identifying and predicting psychomotor activity symptoms. This study provides a novel outlook on the potential of LLMs for predicting self-reported questionnaire scores from textual interview data. The promising preliminary performance of the various models indicates there is potential that these models could effectively assist in the screening of depression. Further research is needed to establish a framework for which LLM can be used for specific mental health symptoms and other disorders. As well, analysis of additional datasets while fine-tuning models should be explored.",
         "https://www.semanticscholar.org/paper/d0deb3ffd586a886b435d67c1b3aad89eeeaf358",
         "0"
        ],
        [
         "35",
         "J",
         "Bachmann, F; van der Weijden, D; Heitz, L; Sarasua, C; Bernstein, A",
         null,
         null,
         null,
         "Bachmann, Fynn; van der Weijden, Daan; Heitz, Lucien; Sarasua, Cristina; Bernstein, Abraham",
         null,
         null,
         "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "adaptive political surveys and gpt 4 tackling the cold start problem with simulated user interactions",
         "1d1097d378555393b73b492995121ab880fff142",
         "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
         "2025",
         "Fynn Bachmann, Daan van der Weijden, Lucien Heitz, Cristina Sarasua, Abraham Bernstein",
         "Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an “oracle” questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.",
         "https://www.semanticscholar.org/paper/1d1097d378555393b73b492995121ab880fff142",
         "1"
        ],
        [
         "36",
         "C",
         "Mancera, J; Terán, L",
         null,
         "Liao, HC; Cid, DD; Macadar, MA; Bernardini, F",
         null,
         "Mancera, Jose; Teran, Luis",
         null,
         null,
         "From GenAI to Political Profiling Avatars: A Data-Driven Approach to Crafting Virtual Experts for Voting Advice Applications",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "from genai to political profiling avatars a data driven approach to crafting virtual experts for voting advice applications",
         "3084188f92330fa837a87b6ca422b28ae1ca713c",
         "From GenAI to Political Profiling Avatars: A Data-Driven Approach to Crafting Virtual Experts for Voting Advice Applications",
         "2024",
         "José Alberto Mancera Andrade, Luis Terán",
         "Voting advice applications (VAAs) are pivotal web-based tools that guide citizens to align with political parties and candidates that match their preferences. Traditional methods for creating candidate profiles predominantly rely on questionnaire responses, a time-intensive and costly process. To address these challenges, we introduce a data-centric methodology utilizing generative artificial intelligence (GenAI), culminating in creating political avatars. These political avatars are engineered using cutting-edge large language models (LLMs), including GPT-4 and Bard. They are adept at processing and interpreting data primarily sourced from Twitter and leveraging bespoke, self-trained datasets. Integrating advanced AI technology with diverse data sources equips political avatars with unprecedented analytical and predictive capabilities, setting a new standard in political analysis. Unlike traditional methods, political avatars are adept at emulating the responses of real politicians or experts, showcasing a remarkable capacity to interact with VAA surveys. This novel approach presents the potential to either compete with or enhance the insights traditionally obtained from human experts. Another critical aspect of our study is comparing political avatars and previous research employing question-answering (QA) models based on advanced natural language processing (NLP) techniques for political profiling. This comparative analysis reveals that Political Avatars offer a significantly more robust solution for profile construction. While QA models provide structured responses based on specific queries, political avatars bring an element of dynamism and depth, capable of generating nuanced, context-aware responses. This shift from static, questionnaire-based profiling to dynamic, AI-driven avatars marks a substantial leap in political analysis. Generative AI in crafting Political Avatars introduces a transformative element to data analysis. This approach facilitates a layered and more sophisticated interpretation of political stances, moving beyond the limitations of traditional profiling methods. By employing political avatars, our methodology not only streamlines the profiling process but also enriches the quality of insights derived, paving the way for a more nuanced understanding of the political landscape.",
         "https://www.semanticscholar.org/paper/3084188f92330fa837a87b6ca422b28ae1ca713c",
         "1"
        ],
        [
         "37",
         "C",
         "Kaate, I; Salminen, J; Jung, SG; Xuan, TTT; Häyhänen, E; Azem, JY; Jansen, BJ",
         null,
         null,
         "ACM",
         "Kaate, Ilkka; Salminen, Joni; Jung, Soon-Gyo; Trang Thi Thu Xuan; Hayhanen, Essi; Azem, Jinan Y.; Jansen, Bernard J.",
         null,
         null,
         "You Always Get an Answer: Analyzing Users' Interaction with AI-Generated Personas Given Unanswerable Questions and Risk of Hallucination",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "you always get an answer analyzing users interaction with ai generated personas given unanswerable questions and risk of hallucination",
         "48ebe4b39ff2dce8d6c026f6cd2cbea8406a9a6e",
         "“You Always Get an Answer”: Analyzing Users’ Interaction with AI-Generated Personas Given Unanswerable Questions and Risk of Hallucination",
         "2025",
         "Ilkka Kaate, Joni O. Salminen, Soon-gyo Jung, T. Xuan, Essi Häyhänen, Jinan Y. Azem, Bernard J. Jansen",
         "We investigated the presence and acceptance of hallucinations (i.e., accidental misinformation) of an AI-generated persona system that leverages large language models for persona creation from survey data in a 54-user within-subjects experiment. After interacting with the personas, users were given a task to ask the personas a series of questions, including an unanswerable question, meaning the personas lacked the data to answer the question. The AI-generated persona system provided a plausible but incorrect answer half (52%) of the time, and more than half of the time (57%), the users accepted the incorrect answer, and the rest of the time, users answered the unanswerable question correctly (no answer). We found that when the AI-generated persona hallucinated, the user was significantly more likely to answer the unanswerable question incorrectly. Also, for genders separately, when the AI-generated persona hallucinated, it was significantly more likely for the female user and the male users to answer the unanswerable question incorrectly. We identified four themes in the AI-generated persona's answers and found that users perceive AI-generated persona's answers as long and unclear for the unanswerable question. Findings imply that personas leveraging LLMs require guardrails to ensure that personas clearly state the possibility of data restrictions and hallucinations when asked unanswerable questions.",
         "https://www.semanticscholar.org/paper/48ebe4b39ff2dce8d6c026f6cd2cbea8406a9a6e",
         "3"
        ],
        [
         "38",
         "J",
         "Hadar-Shoval, D; Asraf, K; Mizrachi, Y; Haber, Y; Elyoseph, Z",
         null,
         null,
         null,
         "Hadar-Shoval, Dorit; Asraf, Kfir; Mizrachi, Yonathan; Haber, Yuval; Elyoseph, Zohar",
         null,
         null,
         "Assessing the Alignment of Large Language Models With Human Values for Mental Health Integration: Cross-Sectional Study Using Schwartz's Theory of Basic Values",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "assessing the alignment of large language models with human values for mental health integration cross sectional study using schwartz s theory of basic values",
         "3a455a00d01fcd45d7797f296eb5b5db331ff7b1",
         "Assessing the Alignment of Large Language Models With Human Values for Mental Health Integration: Cross-Sectional Study Using Schwartz’s Theory of Basic Values",
         "2024",
         "D. Hadar-Shoval, K. Asraf, Yonathan Mizrachi, Yuval Haber, Zohar Elyoseph",
         "Background Large language models (LLMs) hold potential for mental health applications. However, their opaque alignment processes may embed biases that shape problematic perspectives. Evaluating the values embedded within LLMs that guide their decision-making have ethical importance. Schwartz’s theory of basic values (STBV) provides a framework for quantifying cultural value orientations and has shown utility for examining values in mental health contexts, including cultural, diagnostic, and therapist-client dynamics. Objective This study aimed to (1) evaluate whether the STBV can measure value-like constructs within leading LLMs and (2) determine whether LLMs exhibit distinct value-like patterns from humans and each other. Methods In total, 4 LLMs (Bard, Claude 2, Generative Pretrained Transformer [GPT]-3.5, GPT-4) were anthropomorphized and instructed to complete the Portrait Values Questionnaire—Revised (PVQ-RR) to assess value-like constructs. Their responses over 10 trials were analyzed for reliability and validity. To benchmark the LLMs’ value profiles, their results were compared to published data from a diverse sample of 53,472 individuals across 49 nations who had completed the PVQ-RR. This allowed us to assess whether the LLMs diverged from established human value patterns across cultural groups. Value profiles were also compared between models via statistical tests. Results The PVQ-RR showed good reliability and validity for quantifying value-like infrastructure within the LLMs. However, substantial divergence emerged between the LLMs’ value profiles and population data. The models lacked consensus and exhibited distinct motivational biases, reflecting opaque alignment processes. For example, all models prioritized universalism and self-direction, while de-emphasizing achievement, power, and security relative to humans. Successful discriminant analysis differentiated the 4 LLMs’ distinct value profiles. Further examination found the biased value profiles strongly predicted the LLMs’ responses when presented with mental health dilemmas requiring choosing between opposing values. This provided further validation for the models embedding distinct motivational value-like constructs that shape their decision-making. Conclusions This study leveraged the STBV to map the motivational value-like infrastructure underpinning leading LLMs. Although the study demonstrated the STBV can effectively characterize value-like infrastructure within LLMs, substantial divergence from human values raises ethical concerns about aligning these models with mental health applications. The biases toward certain cultural value sets pose risks if integrated without proper safeguards. For example, prioritizing universalism could promote unconditional acceptance even when clinically unwise. Furthermore, the differences between the LLMs underscore the need to standardize alignment processes to capture true cultural diversity. Thus, any responsible integration of LLMs into mental health care must account for their embedded biases and motivation mismatches to ensure equitable delivery across diverse populations. Achieving this will require transparency and refinement of alignment techniques to instill comprehensive human values.",
         "https://www.semanticscholar.org/paper/3a455a00d01fcd45d7797f296eb5b5db331ff7b1",
         "36"
        ],
        [
         "39",
         "J",
         "Amirova, A; Fteropoulli, T; Ahmed, N; Cowie, MR; Leibo, JZ",
         null,
         null,
         null,
         "Amirova, Aliya; Fteropoulli, Theodora; Ahmed, Nafiso; Cowie, Martin R.; Leibo, Joel Z.",
         null,
         null,
         "Framework-based qualitative analysis of free responses of Large Language Models: Algorithmic fidelity",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "framework based qualitative analysis of free responses of large language models algorithmic fidelity",
         "ba5aefce80edc7da110f53fd071f4fbd6b5195b9",
         "Framework-based qualitative analysis of free responses of Large Language Models: Algorithmic fidelity",
         "2023",
         "A. Amirova, T. Fteropoulli, Nafiso Ahmed, Martin R. Cowie, Joel Z. Leibo",
         "Today, with the advent of Large-scale generative Language Models (LLMs) it is now possible to simulate free responses to interview questions such as those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial “silicon participants” generated by LLMs may be productively studied using qualitative analysis methods in such a way as to generate insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a validity concept capturing the degree to which LLM-generated outputs mirror human sub-populations’ beliefs and attitudes. By definition, high algorithmic fidelity suggests that latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews with “silicon participants” matching specific demographic characteristics one-for-one with a set of human participants. Using framework-based qualitative analysis, we showed the key themes obtained from both human and silicon participants were strikingly similar. However, when we analyzed the structure and tone of the interviews we found even more striking differences. We also found evidence of a hyper-accuracy distortion. We conclude that the LLM we tested (GPT-3.5) does not have sufficient algorithmic fidelity to expect in silico research on it to generalize to real human populations. However, rapid advances in artificial intelligence raise the possibility that algorithmic fidelity may improve in the future. Thus we stress the need to establish epistemic norms now around how to assess the validity of LLM-based qualitative research, especially concerning the need to ensure the representation of heterogeneous lived experiences.",
         "https://www.semanticscholar.org/paper/ba5aefce80edc7da110f53fd071f4fbd6b5195b9",
         "14"
        ],
        [
         "40",
         "J",
         "Sumner, J; Wang, YC; Tan, SY; Chew, EHH; Yip, AW",
         null,
         null,
         null,
         "Sumner, Jennifer; Wang, Yuchen; Tan, Si Ying; Chew, Emily Hwee Hoon; Yip, Alexander Wenjun",
         null,
         null,
         "Perspectives and Experiences With Large Language Models in Health Care: Survey Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "perspectives and experiences with large language models in health care survey study",
         "db593a47eec5f6dbd9f28e88d0e4d7450cc6376b",
         "Perspectives and Experiences With Large Language Models in Health Care: Survey Study",
         "2025",
         "Jennifer Sumner, Yuchen Wang, Si Ying Tan, Emily Hwee Hoon Chew, Alexander Wenjun Yip",
         "Background Large language models (LLMs) are transforming how data is used, including within the health care sector. However, frameworks including the Unified Theory of Acceptance and Use of Technology highlight the importance of understanding the factors that influence technology use for successful implementation. Objective This study aimed to (1) investigate users’ uptake, perceptions, and experiences regarding LLMs in health care and (2) contextualize survey responses by demographics and professional profiles. Methods An electronic survey was administered to elicit stakeholder perspectives of LLMs (health care providers and support functions), their experiences with LLMs, and their potential impact on functional roles. Survey domains included: demographics (6 questions), user experiences of LLMs (8 questions), motivations for using LLMs (6 questions), and perceived impact on functional roles (4 questions). The survey was launched electronically, targeting health care providers or support staff, health care students, and academics in health-related fields. Respondents were adults (>18 years) aware of LLMs. Results Responses were received from 1083 individuals, of which 845 were analyzable. Of the 845 respondents, 221 had yet to use an LLM. Nonusers were more likely to be health care workers (P<.001), older (P<.001), and female (P<.01). Users primarily adopted LLMs for speed, convenience, and productivity. While 75% (470/624) agreed that the user experience was positive, 46% (294/624) found the generated content unhelpful. Regression analysis showed that the experience with LLMs is more likely to be positive if the user is male (odds ratio [OR] 1.62, CI 1.06-2.48), and increasing age was associated with a reduced likelihood of reporting LLM output as useful (OR 0.98, CI 0.96-0.99). Nonusers compared to LLM users were less likely to report LLMs meeting unmet needs (45%, 99/221 vs 65%, 407/624; OR 0.48, CI 0.35-0.65), and males were more likely to report that LLMs do address unmet needs (OR 1.64, CI 1.18-2.28). Furthermore, nonusers compared to LLM users were less likely to agree that LLMs will improve functional roles (63%, 140/221 vs 75%, 469/624; OR 0.60, CI 0.43-0.85). Free-text opinions highlighted concerns regarding autonomy, outperformance, and reduced demand for care. Respondents also predicted changes to human interactions, including fewer but higher quality interactions and a change in consumer needs as LLMs become more common, which would require provider adaptation. Conclusions Despite the reported benefits of LLMs, nonusers—primarily health care workers, older individuals, and females—appeared more hesitant to adopt these tools. These findings underscore the need for targeted education and support to address adoption barriers and ensure the successful integration of LLMs in health care. Anticipated role changes, evolving human interactions, and the risk of the digital divide further emphasize the need for careful implementation and ongoing evaluation of LLMs in health care to ensure equity and sustainability.",
         "https://www.semanticscholar.org/paper/db593a47eec5f6dbd9f28e88d0e4d7450cc6376b",
         "1"
        ],
        [
         "41",
         "J",
         "Rädel-Ablass, K; Schliz, K; Schlick, C; Meindl, B; Pahr-Hosbach, S; Schwendemann, H; Rupp, S; Roddewig, M; Miersch, C",
         null,
         null,
         null,
         "Raedel-Ablass, Katharina; Schliz, Klaus; Schlick, Cornelia; Meindl, Benjamin; Pahr-Hosbach, Sandra; Schwendemann, Hanna; Rupp, Stephanie; Roddewig, Marion; Miersch, Claudia",
         null,
         null,
         "Teaching opportunities for anamnesis interviews through AI based teaching role plays: a survey with online learning students from health study programs",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "teaching opportunities for anamnesis interviews through ai based teaching role plays a survey with online learning students from health study programs",
         "d46b4155182de3a385b8f0b323ca54a201e1c60f",
         "Teaching opportunities for anamnesis interviews through AI based teaching role plays: a survey with online learning students from health study programs",
         "2025",
         "Katharina Rädel-Ablass, Klaus Schliz, Cornelia Schlick, Benjamin Meindl, Sandra Pahr-Hosbach, Hanna Schwendemann, Stephanie Rupp, Marion Roddewig, Claudia Miersch",
         "Background This study presents a novel approach to educational role-playing through an AI-based bot, leveraging GPT-4 to simulate anamnesis interviews in various learning scenarios. Developed collaboratively by an interdisciplinary team of university lecturers and AI experts, the bot provides a platform for students of different health study programs to engage in complex patient-health professional conversations, offering an alternative to traditional role plays with actors or real patients. Methods This study utilized a GPT-4 based digital teaching assistant, implemented through a proprietary chatbot design platform, to train anamnesis interviews in virtual settings with students from different online health care study programs. Students’ satisfaction, virtual patient’s accuracy, its realism, and quality were evaluated with a quantitative survey. Results The evaluation of the bot focused on student feedback, highlighting a preference for the AI-driven method due to its immersive and interactive nature. Preliminary results show that students consistently rate the language ability of the AI model positively. More than 80% of students rated the professional and content-related precision of the virtual patient as good to excellent. Even as a text-based chatbot, the vast majority of students see a fairly close to very close relationship to a real anamnesis interview. The results further indicate that students even prefer this training approach to traditional in-person role-plays. Conclusions The study underscores the bot’s potential as a versatile tool for enriching learning experiences across multiple health disciplines, signaling a meaningful shift in educational practices towards the integration of AI technologies. Supplementary Information The online version contains supplementary material available at 10.1186/s12909-025-06756-0.",
         "https://www.semanticscholar.org/paper/d46b4155182de3a385b8f0b323ca54a201e1c60f",
         "8"
        ],
        [
         "42",
         "J",
         "Lau, C; Zhu, XD; Chan, WY",
         null,
         null,
         null,
         "Lau, Clinton; Zhu, Xiaodan; Chan, Wai-Yip",
         null,
         null,
         "Automatic depression severity assessment with deep learning using parameter-efficient tuning",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "automatic depression severity assessment with deep learning using parameter efficient tuning",
         "527288d9ded807883d756f1b3503bc39e79e0a06",
         "Automatic depression severity assessment with deep learning using parameter-efficient tuning",
         "2023",
         "Clinton Lau, Xiaodan Zhu, Wai-Yip Chan",
         "Introduction To assist mental health care providers with the assessment of depression, research to develop a standardized, accessible, and non-invasive technique has garnered considerable attention. Our study focuses on the application of deep learning models for automatic assessment of depression severity based on clinical interview transcriptions. Despite the recent success of deep learning, the lack of large-scale high-quality datasets is a major performance bottleneck for many mental health applications. Methods A novel approach is proposed to address the data scarcity problem for depression assessment. It leverages both pretrained large language models and parameter-efficient tuning techniques. The approach is built upon adapting a small set of tunable parameters, known as prefix vectors, to guide a pretrained model towards predicting the Patient Health Questionnaire (PHQ)-8 score of a person. Experiments were conducted on the Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) benchmark dataset with 189 subjects, partitioned into training, development, and test sets. Model learning was done on the training set. Prediction performance mean and standard deviation of each model, with five randomly-initialized runs, were reported on the development set. Finally, optimized models were evaluated on the test set. Results The proposed model with prefix vectors outperformed all previously published methods, including models which utilized multiple types of data modalities, and achieved the best reported performance on the test set of DAIC-WOZ with a root mean square error of 4.67 and a mean absolute error of 3.80 on the PHQ-8 scale. Compared to conventionally fine-tuned baseline models, prefix-enhanced models were less prone to overfitting by using far fewer training parameters (<6% relatively). Discussion While transfer learning through pretrained large language models can provide a good starting point for downstream learning, prefix vectors can further adapt the pretrained models effectively to the depression assessment task by only adjusting a small number of parameters. The improvement is in part due to the fine-grain flexibility of prefix vector size in adjusting the model's learning capacity. Our results provide evidence that prefix-tuning can be a useful approach in developing tools for automatic depression assessment.",
         "https://www.semanticscholar.org/paper/527288d9ded807883d756f1b3503bc39e79e0a06",
         "20"
        ],
        [
         "43",
         "J",
         "Heston, TF",
         null,
         null,
         null,
         "Heston, Thomas F.",
         null,
         null,
         "Safety of Large Language Models in Addressing Depression",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "safety of large language models in addressing depression",
         "1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
         "Safety of Large Language Models in Addressing Depression",
         "2023",
         "T. F. Heston",
         "Background Generative artificial intelligence (AI) models, exemplified by systems such as ChatGPT, Bard, and Anthropic, are currently under intense investigation for their potential to address existing gaps in mental health support. One implementation of these large language models involves the development of mental health-focused conversational agents, which utilize pre-structured prompts to facilitate user interaction without requiring specialized knowledge in prompt engineering. However, uncertainties persist regarding the safety and efficacy of these agents in recognizing severe depression and suicidal tendencies. Given the well-established correlation between the severity of depression and the risk of suicide, improperly calibrated conversational agents may inadequately identify and respond to crises. Consequently, it is crucial to investigate whether publicly accessible repositories of mental health-focused conversational agents can consistently and safely address crisis scenarios before considering their adoption in clinical settings. This study assesses the safety of publicly available ChatGPT-3.5 conversational agents by evaluating their responses to a patient simulation indicating worsening depression and suicidality. Methodology This study evaluated ChatGPT-3.5 conversational agents on a publicly available repository specifically designed for mental health counseling. Each conversational agent was evaluated twice by a highly structured patient simulation. First, the simulation indicated escalating suicide risk based on the Patient Health Questionnaire (PHQ-9). For the second patient simulation, the escalating risk was presented in a more generalized manner not associated with an existing risk scale to assess the more generalized ability of the conversational agent to recognize suicidality. Each simulation recorded the exact point at which the conversational agent recommended human support. Then, the simulation continued until the conversational agent stopped entirely and shut down completely, insisting on human intervention. Results All 25 agents available on the public repository FlowGPT.com were evaluated. The point at which the conversational agents referred to a human occurred around the mid-point of the simulation, and definitive shutdown predominantly only happened at the highest risk levels. For the PHQ-9 simulation, the average initial referral and shutdown aligned with PHQ-9 scores of 12 (moderate depression) and 25 (severe depression). Few agents included crisis resources - only two referenced suicide hotlines. Despite the conversational agents insisting on human intervention, 22 out of 25 agents would eventually resume the dialogue if the simulation reverted to a lower risk level. Conclusions Current generative AI-based conversational agents are slow to escalate mental health risk scenarios, postponing referral to a human to potentially dangerous levels. More rigorous testing and oversight of conversational agents are needed before deployment in mental healthcare settings. Additionally, further investigation should explore if sustained engagement worsens outcomes and whether enhanced accessibility outweighs the risks of improper escalation. Advancing AI safety in mental health remains imperative as these technologies continue rapidly advancing.",
         "https://www.semanticscholar.org/paper/1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
         "43"
        ],
        [
         "44",
         "J",
         "Hanss, K; Sarma, K; Glowinski, AL; Krystal, A; Saunders, R; Halls, A; Gorrell, S; Reilly, E",
         null,
         null,
         null,
         "Hanss, Kaitlin; Sarma, Karthik, V; Glowinski, Anne L.; Krystal, Andrew; Saunders, Ramotse; Halls, Andrew; Gorrell, Sasha; Reilly, Erin",
         null,
         null,
         "Assessing the Accuracy and Reliability of Large Language Models in Psychiatry Using Standardized Multiple-Choice Questions: Cross-Sectional Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "assessing the accuracy and reliability of large language models in psychiatry using standardized multiple choice questions cross sectional study",
         "faf47d35147d836bcc03bffda819db2426febac8",
         "Assessing the Accuracy and Reliability of Large Language Models in Psychiatry Using Standardized Multiple-Choice Questions: Cross-Sectional Study",
         "2025",
         "Kaitlin Hanss, Karthik V Sarma, Anne L Glowinski, Andrew Krystal, Ramotse Saunders, Andrew Halls, S. Gorrell, E. Reilly",
         "Background Large language models (LLMs), such as OpenAI’s GPT-3.5, GPT-4, and GPT-4o, have garnered early and significant enthusiasm for their potential applications within mental health, ranging from documentation support to chat-bot therapy. Understanding the accuracy and reliability of the psychiatric “knowledge” stored within the parameters of these models and developing measures of confidence in their responses (ie, the likelihood that an LLM response is accurate) are crucial for the safe and effective integration of these tools into mental health settings. Objective This study aimed to assess the accuracy, reliability, and predictors of accuracy of GPT-3.5 (175 billion parameters), GPT-4 (approximately 1.8 trillion parameters), and GPT-4o (an optimized version of GPT-4 with unknown parameters) with standardized psychiatry multiple-choice questions (MCQs). Methods A cross-sectional study was conducted where 3 commonly available, commercial LLMs (GPT-3.5, GPT-4, and GPT-4o) were tested for their ability to provide answers to single-answer MCQs (N=150) extracted from the Psychiatry Test Preparation and Review Manual. Each model generated answers to every MCQ 10 times. We evaluated the accuracy and reliability of the answers and sought predictors of answer accuracy. Our primary outcome was the proportion of questions answered correctly by each LLM (accuracy). Secondary measures were (1) response consistency to MCQs across 10 trials (reliability), (2) the correlation between MCQ answer accuracy and response consistency, and (3) the correlation between MCQ answer accuracy and model self-reported confidence. Results On the first attempt, GPT-3.5 answered 58.0% (87/150) of MCQs correctly, while GPT-4 and GPT-4o answered 84.0% (126/150) and 87.3% (131/150) correctly, respectively. GPT-4 and GPT-4o showed no difference in performance (P=.51), but they significantly outperformed GPT-3.5 (P<.001). GPT-3.5 exhibited less response consistency on average compared to the other models (P<.001). MCQ response consistency was positively correlated with MCQ accuracy across all models (r=0.340, 0.682, and 0.590 for GPT-3.5, GPT-4, and GPT-4o, respectively; all P<.001), whereas model self-reported confidence showed no correlation with accuracy in the models, except for GPT-3.5, where self-reported confidence was weakly inversely correlated with accuracy (P<.001). Conclusions To our knowledge, this is the first comprehensive evaluation of the general psychiatric knowledge encoded in commercially available LLMs and the first study to assess their reliability and identify predictors of response accuracy within medical domains. The findings suggest that GPT-4 and GPT-4o encode accurate and reliable general psychiatric knowledge and that methods, such as repeated prompting, may provide a measure of LLM response confidence. This work supports the potential of LLMs in mental health settings and motivates further research to assess their performance in more open-ended clinical contexts.",
         "https://www.semanticscholar.org/paper/faf47d35147d836bcc03bffda819db2426febac8",
         "2"
        ],
        [
         "45",
         "J",
         "Alsalamah, SA; AlSalamah, S; Alsalamah, HA; Sheerah, HA; Luther, K; Lu, CT",
         null,
         null,
         null,
         "Alsalamah, Sara A.; AlSalamah, Shada; Alsalamah, Hessah A.; Sheerah, Haytham A.; Luther, Kurt; Lu, Chang-Tien",
         null,
         null,
         "Virtual healthcare bot (VHC-Bot): a Person-centered AI chatbot for transforming patient care and healthcare workforce dynamics",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "virtual healthcare bot vhc bot a person centered ai chatbot for transforming patient care and healthcare workforce dynamics",
         "4408852e43dcff9da71f1053f6aeefbea10db450",
         "Virtual healthcare bot (VHC-Bot): a Person-centered AI chatbot for transforming patient care and healthcare workforce dynamics",
         "2025",
         "Sara A. Alsalamah, Shada Alsalamah, Hessah A. Alsalamah, Haytham A. Sheerah, Kurt Luther, Chang-Tien Lu",
         null,
         "https://www.semanticscholar.org/paper/4408852e43dcff9da71f1053f6aeefbea10db450",
         "0"
        ],
        [
         "46",
         "J",
         "Nguyen, MH; Sedoc, J; Taylor, CO",
         null,
         null,
         null,
         "Nguyen, Michelle Hoang; Sedoc, Joao; Taylor, Casey Overby",
         null,
         null,
         "Usability, Engagement, and Report Usefulness of Chatbot-Based Family Health History Data Collection: Mixed Methods Analysis",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "usability engagement and report usefulness of chatbot based family health history data collection mixed methods analysis",
         "cddaccfd097ba11b991829930f95d0abd6a9dade",
         "Usability, Engagement, and Report Usefulness of Chatbot-Based Family Health History Data Collection: Mixed Methods Analysis",
         "2024",
         "M. Nguyen, João Sedoc, C. O. Taylor",
         "Background Family health history (FHx) is an important predictor of a person’s genetic risk but is not collected by many adults in the United States. Objective This study aims to test and compare the usability, engagement, and report usefulness of 2 web-based methods to collect FHx. Methods This mixed methods study compared FHx data collection using a flow-based chatbot (KIT; the curious interactive test) and a form-based method. KIT’s design was optimized to reduce user burden. We recruited and randomized individuals from 2 crowdsourced platforms to 1 of the 2 FHx methods. All participants were asked to complete a questionnaire to assess the method’s usability, the usefulness of a report summarizing their experience, user-desired chatbot enhancements, and general user experience. Engagement was studied using log data collected by the methods. We used qualitative findings from analyzing free-text comments to supplement the primary quantitative results. Results Participants randomized to KIT reported higher usability than those randomized to the form, with a mean System Usability Scale score of 80.2 versus 61.9 (P<.001), respectively. The engagement analysis reflected design differences in the onboarding process. KIT users spent less time entering FHx information and reported more conditions than form users (mean 5.90 vs 7.97 min; P=.04; and mean 7.8 vs 10.1 conditions; P=.04). Both KIT and form users somewhat agreed that the report was useful (Likert scale ratings of 4.08 and 4.29, respectively). Among desired enhancements, personalization was the highest-rated feature (188/205, 91.7% rated medium- to high-priority). Qualitative analyses revealed positive and negative characteristics of both KIT and the form-based method. Among respondents randomized to KIT, most indicated it was easy to use and navigate and that they could respond to and understand user prompts. Negative comments addressed KIT’s personality, conversational pace, and ability to manage errors. For KIT and form respondents, qualitative results revealed common themes, including a desire for more information about conditions and a mutual appreciation for the multiple-choice button response format. Respondents also said they wanted to report health information beyond KIT’s prompts (eg, personal health history) and for KIT to provide more personalized responses. Conclusions We showed that KIT provided a usable way to collect FHx. We also identified design considerations to improve chatbot-based FHx data collection: First, the final report summarizing the FHx collection experience should be enhanced to provide more value for patients. Second, the onboarding chatbot prompt may impact data quality and should be carefully considered. Finally, we highlighted several areas that could be improved by moving from a flow-based chatbot to a large language model implementation strategy.",
         "https://www.semanticscholar.org/paper/cddaccfd097ba11b991829930f95d0abd6a9dade",
         "2"
        ],
        [
         "47",
         "J",
         "Safrai, M; Azaria, A",
         null,
         null,
         null,
         "Safrai, Myriam; Azaria, Amos",
         null,
         null,
         "Does small talk with a medical provider affect ChatGPT's medical counsel? Performance of ChatGPT on USMLE with and without distractions",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "does small talk with a medical provider affect chatgpt s medical counsel performance of chatgpt on usmle with and without distractions",
         "dc8ed1d85c4674181746ffe8f08e86e00d0054ff",
         "Does small talk with a medical provider affect ChatGPT's medical counsel? Performance of ChatGPT on USMLE with and without distractions.",
         "2024",
         "Myriam Safrai, A. Azaria",
         "Efforts are being made to improve the time effectiveness of healthcare providers. Artificial intelligence tools can help transcript and summarize physician-patient encounters and produce medical notes and medical recommendations. However, in addition to medical information, discussion between healthcare and patients includes small talk and other information irrelevant to medical concerns. As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given. Therefore, this study aims to investigate the impact of medical data mixed with small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3 questions were used as a model for relevant medical data. We use both multiple-choice and open-ended questions. First, we gathered small talk sentences from human participants using the Mechanical Turk platform. Second, both sets of USLME questions were arranged in a pattern where each sentence from the original questions was followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both sets of questions with and without the small talk sentences. Finally, a board-certified physician analyzed the answers by ChatGPT and compared them to the formal correct answer. The analysis results demonstrate that the ability of ChatGPT-3.5 to answer correctly was impaired when small talk was added to medical data (66.8% vs. 56.6%; p = 0.025). Specifically, for multiple-choice questions (72.1% vs. 68.9%; p = 0.67) and for the open questions (61.5% vs. 44.3%; p = 0.01), respectively. In contrast, small talk phrases did not impair ChatGPT-4 ability in both types of questions (83.6% and 66.2%, respectively). According to these results, ChatGPT-4 seems more accurate than the earlier 3.5 version, and it appears that small talk does not impair its capability to provide medical recommendations. Our results are an important first step in understanding the potential and limitations of utilizing ChatGPT and other LLMs for physician-patient interactions, which include casual conversations.",
         "https://www.semanticscholar.org/paper/dc8ed1d85c4674181746ffe8f08e86e00d0054ff",
         "1"
        ],
        [
         "48",
         "J",
         "Patsia, O; Giannopoulos, A; Giannakis, I",
         null,
         null,
         null,
         "Patsia, Ourania; Giannopoulos, Antonios; Giannakis, Iraklis",
         null,
         null,
         "GPR Full-Waveform Inversion With Deep-Learning Forward Modeling: A Case Study From Non-Destructive Testing",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "gpr full waveform inversion with deep learning forward modeling a case study from non destructive testing",
         "0be15269dffa995e0c2859ed9c354ad4e916632b",
         "GPR Full-Waveform Inversion With Deep-Learning Forward Modeling: A Case Study From Non-Destructive Testing",
         "2023",
         "O. Patsia, A. Giannopoulos, I. Giannakis",
         "Numerical modeling of ground penetrating radars (GPRs), such as the finite-difference time-domain (FDTD) method, has been extensively used to enhance the interpretation of GPR data and as a key component of full-waveform inversion (FWI). A major drawback of numerical solvers, especially within the context of FWI, is that they are still computationally expensive requiring often unattainable computational resources and access to high-performance computing (HPC). In this work, we present a near real-time deep-learning forward solver for GPR data that can generate entire B-scans, given certain model parameters as inputs. The machine-learning (ML) model is tuned for reinforced concrete slab scenarios, but the same rationale can be applied in a straightforward manner to other applications as well. The training was performed using entirely synthetic data, where a 3-D digital twin based on the 2000-MHz “palm” antenna from Geophysical Survey Systems, Inc. (GSSI) was included in FDTD simulations for the training set. The accuracy of the deep-learning solver is demonstrated with both synthetic and real data from reinforced concrete slabs. The predicted ML responses were in very good agreement with FDTD, showing a high degree of accuracy. The ML solver is then used as part of an FWI algorithm to characterize the concrete slab and estimate the depth and radius of the buried rebars. Coupled FWI with an ML-based forward solver results in significantly less execution times compared to conventional FWI using numerical solvers. The high accuracy of the proposed FWI, combined with the efficiency and speed of the ML-based forward solver, make the proposed scheme an ideal tool for characterizing concrete structures in nondestructive testing.",
         "https://www.semanticscholar.org/paper/0be15269dffa995e0c2859ed9c354ad4e916632b",
         "15"
        ],
        [
         "49",
         "J",
         "Putek, J; Szepietowski, JC",
         null,
         null,
         null,
         "Putek, Justyna; Szepietowski, Jacek C.",
         null,
         null,
         "Alexithymia in people with tattoos",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0",
         "alexithymia in people with tattoos",
         "ecaff1dd1b14f06daf698e6eaa4a5926928bc27a",
         "Alexithymia in people with tattoos",
         "2024",
         "Justyna Putek, J. Szepietowski",
         "Introduction Tattoos are a form of body modifications. Alexithymia is a complex personality structure that includes emotional and cognitive deficits such as difficulty in recognizing and describing feelings. Aim To assess the prevalence of alexithymia among tattooed individuals. Moreover, we aimed to check if the type of tattoo is related to alexithymia. Material and methods This was a cross-sectional survey, conducted on 403 individuals from Poland. 200 of them had tattoos and were assigned to the study group, and 203 of them had no tattoos and were assigned to the control group. Results Most respondents (24%) had one tattoo. Most tattoos (29.8%) were situated on forearms and palms, had a plant motif (21.8%) and were done to express personality of the respondents (20.7%). In the research group 80 (19.9%) respondents were classified as alexithymic ones. Out of them, 47 (11.7%) individuals had tattoos and 33 (8.9%) belonged to the non-tattooed group. 35 (17.5%) respondents with non-verbal tattoos were screened as alexithymic while 12 (6%) individuals with verbal, personal tattoos were classified as alexithymic ones (p < 0.05). Twenty-two (11%) respondents who did their tattoos for psychological reasons and 25 (12.5%) individuals who did their tattoos for aesthetic reasons were classified as alexithymic (NS). Conclusions Subjects with tattoos should be regarded as a group with increased prevalence of alexithymia. Individuals with non-verbal tattoos had a higher tendency to be screened as alexithymic as alexithymic ones. Motivation for getting the tattoo does not seem to have a significant impact on the prevalence of alexithymia.",
         "https://www.semanticscholar.org/paper/ecaff1dd1b14f06daf698e6eaa4a5926928bc27a",
         "0"
        ]
       ],
       "shape": {
        "columns": 80,
        "rows": 51
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Book Authors</th>\n",
       "      <th>Book Editors</th>\n",
       "      <th>Book Group Authors</th>\n",
       "      <th>Author Full Names</th>\n",
       "      <th>Book Author Full Names</th>\n",
       "      <th>Group Authors</th>\n",
       "      <th>title_WoS</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>...</th>\n",
       "      <th>UT (Unique WOS ID)</th>\n",
       "      <th>Web of Science Record</th>\n",
       "      <th>norm_title</th>\n",
       "      <th>paperId</th>\n",
       "      <th>title_SS</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>citationCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Well Do Simulated Population Samples with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>how well do simulated population samples with ...</td>\n",
       "      <td>25f383b7a807392696073801959dcc1c1aadd2bb</td>\n",
       "      <td>How Well Do Simulated Population Samples with ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Gregorio Ferreira, Jacopo Amidei, Rubén Nieto,...</td>\n",
       "      <td>Background: Advances in artificial intelligenc...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/25f383b7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>Kane, D; Parke, J; Jo, Y; Bak, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bouamor, H; Pino, J; Bali, K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>From Values to Opinions: Predicting Human Beha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>from values to opinions predicting human behav...</td>\n",
       "      <td>52e963c40a5083d5403cebf4d4782271aaa06994</td>\n",
       "      <td>From Values to Opinions: Predicting Human Beha...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Dongjun Kang, Joonsuk Park, Yohan Jo, Jinyeong...</td>\n",
       "      <td>Being able to predict people's opinions on iss...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/52e963c4...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>Bisbee, J; Clinton, JD; Dorff, C; Kenkel, B; L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bisbee, James; Clinton, Joshua D.; Dorff, Cass...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Synthetic Replacements for Human Survey Data? ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>synthetic replacements for human survey data t...</td>\n",
       "      <td>58d735a54d3aba79ad3bffbfa2433d8e5ee27313</td>\n",
       "      <td>Synthetic Replacements for Human Survey Data? ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>James Bisbee, Joshua D. Clinton, C. Dorff, Bre...</td>\n",
       "      <td>\\n Large language models (LLMs) offer new rese...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/58d735a5...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>Liu, HJ; Cao, Y; Wu, X; Qiu, C; Gu, JG; Liu, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liu, Haijiang; Cao, Yong; Wu, Xun; Qiu, Chen; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Towards realistic evaluation of cultural value...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>towards realistic evaluation of cultural value...</td>\n",
       "      <td>3ab59b3d4a4b2e89f7eda93a950eeaa77b37332e</td>\n",
       "      <td>Towards realistic evaluation of cultural value...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Haijiang Liu, Yong Cao, Xun Wu, Chen Qiu, Jing...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.semanticscholar.org/paper/3ab59b3d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J</td>\n",
       "      <td>Boelaert, J; Coavoux, S; Ollion, E; Petev, I; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Boelaert, Julien; Coavoux, Samuel; Ollion, Eti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine Bias. How Do Generative Language Model...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>machine bias how do generative language models...</td>\n",
       "      <td>45f9ea8d0dc1a7e6c56ff6e1f23c8e632687d2a7</td>\n",
       "      <td>Machine Bias. How Do Generative Language Model...</td>\n",
       "      <td>2025</td>\n",
       "      <td>J. Boelaert, Samuel Coavoux, Étienne Ollion, I...</td>\n",
       "      <td>Generative artificial intelligence (AI) is inc...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/45f9ea8d...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>J</td>\n",
       "      <td>Qu, Y; Wang, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Qu, Yao; Wang, Jue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Performance and biases of Large Language Model...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>performance and biases of large language model...</td>\n",
       "      <td>e6d14d140c4faaf8f3d9f47e61cc5c6091bccf1e</td>\n",
       "      <td>Performance and Biases of Large Language Model...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yao Qu, Jue Wang</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.semanticscholar.org/paper/e6d14d14...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>Nguyen, H; Nguyen, V; López-Fierro, S; Ludovis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASSOC COMPUTING MACHINERY</td>\n",
       "      <td>Ha Nguyen; Nguyen, Victoria; Lopez-Fierro, Sar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simulating Climate Change Discussion with Larg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>simulating climate change discussion with larg...</td>\n",
       "      <td>dd95064d28ee5d123a6a284422bbba3d443f0416</td>\n",
       "      <td>Simulating Climate Change Discussion with Larg...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Ha Nguyen, Victoria Nguyen, Saríah López-Fierr...</td>\n",
       "      <td>Large language models (LLMs) have shown promis...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dd95064d...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>J</td>\n",
       "      <td>Salecha, A; Ireland, ME; Subrahmanya, S; Sedoc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salecha, Aadesh; Ireland, Molly E.; Subrahmany...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large language models display human-like socia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>large language models display human like socia...</td>\n",
       "      <td>8253104f5b1481d8557380d2dc5dab03ff9a7716</td>\n",
       "      <td>Large language models display human-like socia...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Aadesh Salecha, Molly E. Ireland, Shashanka Su...</td>\n",
       "      <td>Abstract Large language models (LLMs) are beco...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/8253104f...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J</td>\n",
       "      <td>Yao, JC; Zhang, HJ; Ou, J; Zuo, DY; Yang, Z; D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yao, Junchi; Zhang, Hongjie; Ou, Jie; Zuo, Din...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Social opinions prediction utilizes fusing dyn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>social opinions prediction utilizes fusing dyn...</td>\n",
       "      <td>392de716c8f6610f080ba655e885935c20ac6c73</td>\n",
       "      <td>Social opinions prediction utilizes fusing dyn...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Junchi Yao, Hongjie Zhang, Jie Ou, Dingyi Zuo,...</td>\n",
       "      <td>In the context where social media emerges as a...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/392de716...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C</td>\n",
       "      <td>Hämäläinen, P; Tavast, M; Kunnari, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>Hamalainen, Perttu; Tavast, Mikke; Kunnari, Anton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Evaluating Large Language Models in Generating...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>evaluating large language models in generating...</td>\n",
       "      <td>0ffd57884d7957f6b5634b9fa24843dc3759668f</td>\n",
       "      <td>Evaluating Large Language Models in Generating...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Perttu Hämäläinen, Mikke Tavast, Anton Kunnari</td>\n",
       "      <td>Collecting data is one of the bottlenecks of H...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0ffd5788...</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>J</td>\n",
       "      <td>Zhang, S; Xu, J; Alvero, AJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zhang, Simone; Xu, Janet; Alvero, A. J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generative AI Meets Open-Ended Survey Response...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>generative ai meets open ended survey response...</td>\n",
       "      <td>8398dfa4d015b3784654a77b3913b62a1f68eed8</td>\n",
       "      <td>Generative AI Meets Open-Ended Survey Response...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Simone Zhang, Janet Xu, AJ Alvero</td>\n",
       "      <td>The growing popularity of generative artificia...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/8398dfa4...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>J</td>\n",
       "      <td>Zhang, BY; Chen, T; Wang, X; Li, Q; Zhang, WS;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zhang, Baoyu; Chen, Tao; Wang, Xiao; Li, Qiang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decoding Activist Public Opinion in Decentrali...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>decoding activist public opinion in decentrali...</td>\n",
       "      <td>4949ff97cf5e7c31ed9a057cbbde4b95d3ddd1f8</td>\n",
       "      <td>Decoding Activist Public Opinion in Decentrali...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Baoyu Zhang, Tao Chen, Xiao Wang, Qiang Li, We...</td>\n",
       "      <td>Based on an investigation of online public opi...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4949ff97...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>J</td>\n",
       "      <td>Gao, C; Lan, XC; Li, N; Yuan, Y; Ding, JT; Zho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large language models empowered agent-based mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>large language models empowered agent based mo...</td>\n",
       "      <td>592ac35991e583fc37c26ee6659d2deb85142ad9</td>\n",
       "      <td>Large Language Models Empowered Agent-based Mo...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, J...</td>\n",
       "      <td>Agent-based modeling and simulation have evolv...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/592ac359...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C</td>\n",
       "      <td>AlKhamissi, B; ElNokrashy, M; AlKhamissi, M; D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ku, LW; Martins, A; Srikumar, V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AlKhamissi, Badr; ElNokrashy, Muhammad; AlKham...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Investigating Cultural Alignment of Large Lang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>investigating cultural alignment of large lang...</td>\n",
       "      <td>b1890367317f0657c08ed96be4c474035b34b485</td>\n",
       "      <td>Investigating Cultural Alignment of Large Lang...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Badr AlKhamissi, Muhammad N. ElNokrashy, Mai A...</td>\n",
       "      <td>The intricate relationship between language an...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/b1890367...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>J</td>\n",
       "      <td>Zhao, XJ; Wang, H; Dai, CX; Tang, JC; Deng, KX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zhao, Xinjie; Wang, Hao; Dai, Chengxiao; Tang,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multi-Stage Simulation of Residents' Disaster ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>multi stage simulation of residents disaster r...</td>\n",
       "      <td>6d2077c8f4864103780b160501cd207005d045c8</td>\n",
       "      <td>Multi-Stage Simulation of Residents’ Disaster ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Xinjie Zhao, Hao Wang, Chengxiao Dai, Jiacheng...</td>\n",
       "      <td>The escalating frequency and complexity of nat...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/6d2077c8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>J</td>\n",
       "      <td>de Winter, JCF; Driessen, T; Dodou, D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de Winter, Joost C. F.; Driessen, Tom; Dodou, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The use of ChatGPT for personality research: A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>the use of chatgpt for personality research ad...</td>\n",
       "      <td>781703bc7e4fb90766824ee808097171afa223b3</td>\n",
       "      <td>The use of ChatGPT for personality research: A...</td>\n",
       "      <td>2024</td>\n",
       "      <td>J. D. de Winter, Tom Driessen, Dimitra Dodou</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.semanticscholar.org/paper/781703bc...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>J</td>\n",
       "      <td>Rakovics, Z; Rakovics, M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rakovics, Zsofia; Rakovics, Marton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exploring the potential and limitations of lar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>exploring the potential and limitations of lar...</td>\n",
       "      <td>6cd94c4fdfd1ddf59b5385919851de2662e412fe</td>\n",
       "      <td>Exploring the potential and limitations of lar...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Zsófia Rakovics, Márton Rakovics</td>\n",
       "      <td>Social and linguistic differences encoded in v...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/6cd94c4f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>J</td>\n",
       "      <td>Leung, HW; Bovy, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leung, Henry W.; Bovy, Jo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Towards an astronomical foundation model for s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>towards an astronomical foundation model for s...</td>\n",
       "      <td>264cb7a7dbee1303ff9e0ebe2dac78646271a2fb</td>\n",
       "      <td>Towards an astronomical foundation model for s...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Henry W. Leung, J. Bovy</td>\n",
       "      <td>Rapid strides are currently being made in the ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/264cb7a7...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>J</td>\n",
       "      <td>Mburu, TK; Rong, KX; McColley, CJ; Werth, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mburu, Ted K.; Rong, Kangxuan; McColley, Campb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Methodological foundations for artificial inte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>methodological foundations for artificial inte...</td>\n",
       "      <td>54b63afefc315b5f051f4a19fe413ef6c544c9fd</td>\n",
       "      <td>Methodological foundations for artificial inte...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Ted K. Mburu, Kangxuan Rong, Campbell J. McCol...</td>\n",
       "      <td>This study investigates the use of large langu...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/54b63afe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>J</td>\n",
       "      <td>Zhang, KH; Dong, CQ; Guo, YF; Zhou, W; Yu, G; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zhang, Kaihang; Dong, Changqi; Guo, Yifeng; Zh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lagged Stance Interactions and Counter-Spiral ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>lagged stance interactions and counter spiral ...</td>\n",
       "      <td>bc270ab2e00f78691c8fd16548809e3609dedee2</td>\n",
       "      <td>Lagged Stance Interactions and Counter-Spiral ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Kaihang Zhang, Changqi Dong, Yifeng Guo, Wuai ...</td>\n",
       "      <td>Understanding the dynamics of public opinion f...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/bc270ab2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>J</td>\n",
       "      <td>Campos, M; Farinhas, A; Zerva, C; Figueiredo, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campos, Margarida; Farinhas, Antonio; Zerva, C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conformal Prediction for Natural Language Proc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>conformal prediction for natural language proc...</td>\n",
       "      <td>346fdbda3ecf4775819fced0cfed78357bee8128</td>\n",
       "      <td>Conformal Prediction for Natural Language Proc...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Margarida M. Campos, António Farinhas, Chrysou...</td>\n",
       "      <td>Abstract The rapid proliferation of large lang...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/346fdbda...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>C</td>\n",
       "      <td>Cheng, M; Piccardi, T; Yang, DY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bouamor, H; Pino, J; Bali, K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cheng, Myra; Piccardi, Tiziano; Yang, Diyi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CoMPosT: Characterizing and Evaluating Caricat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>compost characterizing and evaluating caricatu...</td>\n",
       "      <td>7a4fe2f003241ad97bf1778e527cb0306fa90da2</td>\n",
       "      <td>CoMPosT: Characterizing and Evaluating Caricat...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Myra Cheng, Tiziano Piccardi, Diyi Yang</td>\n",
       "      <td>Recent work has aimed to capture nuances of hu...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/7a4fe2f0...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>J</td>\n",
       "      <td>Kaur, A; Budko, A; Liu, K; Eaton, E; Steitz, B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kaur, Amarpreet; Budko, Alexander; Liu, Katrin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Automating Responses to Patient Portal Message...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>automating responses to patient portal message...</td>\n",
       "      <td>caa3c4eb1ede700f03eef5b4f25a18b08c88d832</td>\n",
       "      <td>Automating Responses to Patient Portal Message...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Amarpreet Kaur, Alexander Budko, Katrina Liu, ...</td>\n",
       "      <td>Background: Patient portals serve as vital bri...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/caa3c4eb...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>J</td>\n",
       "      <td>Ji, J; Kim, J; Kim, Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ji, Junyung; Kim, Jiwoo; Kim, Younghoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Predicting Missing Values in Survey Data Using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>predicting missing values in survey data using...</td>\n",
       "      <td>bdeeaf207e2563f39ad27a9d9511d8573b5bff95</td>\n",
       "      <td>Predicting Missing Values in Survey Data Using...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Junyung Ji, Jiwoo Kim, Younghoon Kim</td>\n",
       "      <td>Survey data play a crucial role in various res...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/bdeeaf20...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>J</td>\n",
       "      <td>Goli, A; Singh, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Goli, Ali; Singh, Amandeep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Frontiers: Can Large Language Models Capture H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>frontiers can large language models capture hu...</td>\n",
       "      <td>0377c4c20d86e3a23cb5c22d89b3cb488c31a564</td>\n",
       "      <td>Frontiers: Can Large Language Models Capture H...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Ali Goli, Amandeep Singh</td>\n",
       "      <td>This paper examines the potential of large lan...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0377c4c2...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>C</td>\n",
       "      <td>Liu, YH; Chen, XY; Zhang, XQ; Gao, X; Zhang, J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Larson, K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liu, Yuhan; Chen, Xiuying; Zhang, Xiaoqing; Ga...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>From Skepticism to Acceptance: Simulating the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>from skepticism to acceptance simulating the a...</td>\n",
       "      <td>1bd4b8be136072c8f56114f2f8479aaed2ad6d9b</td>\n",
       "      <td>From Skepticism to Acceptance: Simulating the ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing ...</td>\n",
       "      <td>In the digital era, the rapid propagation of f...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/1bd4b8be...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>J</td>\n",
       "      <td>Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matching GPT-simulated Populations with Real O...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>matching gpt simulated populations with real o...</td>\n",
       "      <td>e186d394a49ab2866a8b1248a99e6f1637a238fe</td>\n",
       "      <td>Matching GPT-simulated Populations with Real O...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Gregorio Ferreira, Jacopo Amidei, Rubén Nieto,...</td>\n",
       "      <td>This article analyzes how well OpenAI’s LLM GP...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/e186d394...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C</td>\n",
       "      <td>Scarlatos, A; Baker, RS; Lan, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>Scarlatos, Alexander; Baker, Ryan S.; Lan, Andrew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exploring Knowledge Tracing in Tutor-Student D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>exploring knowledge tracing in tutor student d...</td>\n",
       "      <td>06e9ec37cc25980544d0a78b5aa4893dafc65fd3</td>\n",
       "      <td>Exploring Knowledge Tracing in Tutor-Student D...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Alexander Scarlatos, Ryan S. Baker, Andrew Lan</td>\n",
       "      <td>Recent advances in large language models (LLMs...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/06e9ec37...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>C</td>\n",
       "      <td>Steinmacher, I; Penney, JM; Felizardo, KR; Gar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>Steinmacher, Igor; Penney, Jacob Mcauley; Feli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can ChatGPT emulate humans in software enginee...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>can chatgpt emulate humans in software enginee...</td>\n",
       "      <td>2a77ac8d1c36ccf9f222cf0ae16e251ac6b13e86</td>\n",
       "      <td>Can ChatGPT emulate humans in software enginee...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Igor Steinmacher, Jacob Penney, K. Felizardo, ...</td>\n",
       "      <td>Context: There is a growing belief in the lite...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/2a77ac8d...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>J</td>\n",
       "      <td>Wahidur, RSM; Tashdeed, I; Kaur, M; Lee, HN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wahidur, Rahman S. M.; Tashdeed, Ishmam; Kaur,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enhancing Zero-Shot Crypto Sentiment With Fine...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>enhancing zero shot crypto sentiment with fine...</td>\n",
       "      <td>bcdc44ef48ffadbdaa3bd5cacfe9ddb9b9f48750</td>\n",
       "      <td>Enhancing Zero-Shot Crypto Sentiment With Fine...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Rahman S M Wahidur, Ishmam Tashdeed, Manjit Ka...</td>\n",
       "      <td>Blockchain technology has revolutionized the f...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/bcdc44ef...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>C</td>\n",
       "      <td>Hwang, E; Majumder, BP; Tandon, N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bouamor, H; Pino, J; Bali K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hwang, EunJeong; Majumder, Bodhisattwa Prasad;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aligning Language Models to User Opinions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>aligning language models to user opinions</td>\n",
       "      <td>5db0f55332839c408e3049cea1a6ad48fefba70c</td>\n",
       "      <td>Aligning Language Models to User Opinions</td>\n",
       "      <td>2023</td>\n",
       "      <td>EunJeong Hwang, Bodhisattwa Prasad Majumder, N...</td>\n",
       "      <td>An important aspect of developing LLMs that in...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/5db0f553...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>C</td>\n",
       "      <td>Min, Y; Jeong, JW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eck, U; Sra, M; Stefanucci, J; Sugimoto, M; Ta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Min, Yewon; Jeong, Jin-Woo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Public Speaking Q&amp;A Practice with LLM-Generate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>public speaking q a practice with llm generate...</td>\n",
       "      <td>3b36ec5f47076620dc7735f8aba55d9c3c3e6b32</td>\n",
       "      <td>Public Speaking Q&amp;A Practice with LLM-Generate...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Yewon Min, Jin-Woo Jeong</td>\n",
       "      <td>This paper introduces a novel VR-based Q&amp;A pra...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/3b36ec5f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>C</td>\n",
       "      <td>Liu, A; Diab, M; Fried, D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Martins, A; Srikumar, V; Ku, LW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liu, Andy; Diab, Mona; Fried, Daniel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Evaluating Large Language Model Biases in Pers...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>evaluating large language model biases in pers...</td>\n",
       "      <td>ae03f10729959435ecefc0e90cba4cbe8438a10b</td>\n",
       "      <td>Evaluating Large Language Model Biases in Pers...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Andy Liu, Mona T. Diab, Daniel Fried</td>\n",
       "      <td>The task of persona-steered text generation re...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/ae03f107...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>J</td>\n",
       "      <td>Mishra, T; Sutanto, E; Rossanti, R; Pant, N; A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mishra, Tanisha; Sutanto, Edward; Rossanti, Ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Use of large language models as artificial int...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>use of large language models as artificial int...</td>\n",
       "      <td>0c1e396f7f23d34ebad01dac29de6898f18ae63e</td>\n",
       "      <td>Use of large language models as artificial int...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Tanisha Mishra, Edward Sutanto, Rini Rossanti,...</td>\n",
       "      <td>With breakthroughs in Natural Language Process...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0c1e396f...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>J</td>\n",
       "      <td>Teferra, BG; Perivolaris, A; Hsiang, WN; Sidha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teferra, Bazen Gashaw; Perivolaris, Argyrios; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leveraging large language models for automated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>leveraging large language models for automated...</td>\n",
       "      <td>d0deb3ffd586a886b435d67c1b3aad89eeeaf358</td>\n",
       "      <td>Leveraging large language models for automated...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Bazen Gashaw Teferra, Argyrios Perivolaris, We...</td>\n",
       "      <td>Mental health diagnoses possess unique challen...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d0deb3ff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>J</td>\n",
       "      <td>Bachmann, F; van der Weijden, D; Heitz, L; Sar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bachmann, Fynn; van der Weijden, Daan; Heitz, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adaptive political surveys and GPT-4: Tackling...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>adaptive political surveys and gpt 4 tackling ...</td>\n",
       "      <td>1d1097d378555393b73b492995121ab880fff142</td>\n",
       "      <td>Adaptive political surveys and GPT-4: Tackling...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fynn Bachmann, Daan van der Weijden, Lucien He...</td>\n",
       "      <td>Adaptive questionnaires dynamically select the...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/1d1097d3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>C</td>\n",
       "      <td>Mancera, J; Terán, L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liao, HC; Cid, DD; Macadar, MA; Bernardini, F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mancera, Jose; Teran, Luis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>From GenAI to Political Profiling Avatars: A D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>from genai to political profiling avatars a da...</td>\n",
       "      <td>3084188f92330fa837a87b6ca422b28ae1ca713c</td>\n",
       "      <td>From GenAI to Political Profiling Avatars: A D...</td>\n",
       "      <td>2024</td>\n",
       "      <td>José Alberto Mancera Andrade, Luis Terán</td>\n",
       "      <td>Voting advice applications (VAAs) are pivotal ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/3084188f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>C</td>\n",
       "      <td>Kaate, I; Salminen, J; Jung, SG; Xuan, TTT; Hä...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>Kaate, Ilkka; Salminen, Joni; Jung, Soon-Gyo; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You Always Get an Answer: Analyzing Users' Int...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>you always get an answer analyzing users inter...</td>\n",
       "      <td>48ebe4b39ff2dce8d6c026f6cd2cbea8406a9a6e</td>\n",
       "      <td>“You Always Get an Answer”: Analyzing Users’ I...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Ilkka Kaate, Joni O. Salminen, Soon-gyo Jung, ...</td>\n",
       "      <td>We investigated the presence and acceptance of...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/48ebe4b3...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>J</td>\n",
       "      <td>Hadar-Shoval, D; Asraf, K; Mizrachi, Y; Haber,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hadar-Shoval, Dorit; Asraf, Kfir; Mizrachi, Yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the Alignment of Large Language Mode...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>assessing the alignment of large language mode...</td>\n",
       "      <td>3a455a00d01fcd45d7797f296eb5b5db331ff7b1</td>\n",
       "      <td>Assessing the Alignment of Large Language Mode...</td>\n",
       "      <td>2024</td>\n",
       "      <td>D. Hadar-Shoval, K. Asraf, Yonathan Mizrachi, ...</td>\n",
       "      <td>Background Large language models (LLMs) hold p...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/3a455a00...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>J</td>\n",
       "      <td>Amirova, A; Fteropoulli, T; Ahmed, N; Cowie, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amirova, Aliya; Fteropoulli, Theodora; Ahmed, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Framework-based qualitative analysis of free r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>framework based qualitative analysis of free r...</td>\n",
       "      <td>ba5aefce80edc7da110f53fd071f4fbd6b5195b9</td>\n",
       "      <td>Framework-based qualitative analysis of free r...</td>\n",
       "      <td>2023</td>\n",
       "      <td>A. Amirova, T. Fteropoulli, Nafiso Ahmed, Mart...</td>\n",
       "      <td>Today, with the advent of Large-scale generati...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/ba5aefce...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>J</td>\n",
       "      <td>Sumner, J; Wang, YC; Tan, SY; Chew, EHH; Yip, AW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sumner, Jennifer; Wang, Yuchen; Tan, Si Ying; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perspectives and Experiences With Large Langua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>perspectives and experiences with large langua...</td>\n",
       "      <td>db593a47eec5f6dbd9f28e88d0e4d7450cc6376b</td>\n",
       "      <td>Perspectives and Experiences With Large Langua...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Jennifer Sumner, Yuchen Wang, Si Ying Tan, Emi...</td>\n",
       "      <td>Background Large language models (LLMs) are tr...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/db593a47...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>J</td>\n",
       "      <td>Rädel-Ablass, K; Schliz, K; Schlick, C; Meindl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Raedel-Ablass, Katharina; Schliz, Klaus; Schli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Teaching opportunities for anamnesis interview...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>teaching opportunities for anamnesis interview...</td>\n",
       "      <td>d46b4155182de3a385b8f0b323ca54a201e1c60f</td>\n",
       "      <td>Teaching opportunities for anamnesis interview...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Katharina Rädel-Ablass, Klaus Schliz, Cornelia...</td>\n",
       "      <td>Background This study presents a novel approac...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d46b4155...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>J</td>\n",
       "      <td>Lau, C; Zhu, XD; Chan, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lau, Clinton; Zhu, Xiaodan; Chan, Wai-Yip</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Automatic depression severity assessment with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>automatic depression severity assessment with ...</td>\n",
       "      <td>527288d9ded807883d756f1b3503bc39e79e0a06</td>\n",
       "      <td>Automatic depression severity assessment with ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Clinton Lau, Xiaodan Zhu, Wai-Yip Chan</td>\n",
       "      <td>Introduction To assist mental health care prov...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/527288d9...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>J</td>\n",
       "      <td>Heston, TF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heston, Thomas F.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Safety of Large Language Models in Addressing ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>safety of large language models in addressing ...</td>\n",
       "      <td>1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f</td>\n",
       "      <td>Safety of Large Language Models in Addressing ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>T. F. Heston</td>\n",
       "      <td>Background Generative artificial intelligence ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/1554d7e7...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>J</td>\n",
       "      <td>Hanss, K; Sarma, K; Glowinski, AL; Krystal, A;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hanss, Kaitlin; Sarma, Karthik, V; Glowinski, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the Accuracy and Reliability of Larg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>assessing the accuracy and reliability of larg...</td>\n",
       "      <td>faf47d35147d836bcc03bffda819db2426febac8</td>\n",
       "      <td>Assessing the Accuracy and Reliability of Larg...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Kaitlin Hanss, Karthik V Sarma, Anne L Glowins...</td>\n",
       "      <td>Background Large language models (LLMs), such ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/faf47d35...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>J</td>\n",
       "      <td>Alsalamah, SA; AlSalamah, S; Alsalamah, HA; Sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alsalamah, Sara A.; AlSalamah, Shada; Alsalama...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virtual healthcare bot (VHC-Bot): a Person-cen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>virtual healthcare bot vhc bot a person center...</td>\n",
       "      <td>4408852e43dcff9da71f1053f6aeefbea10db450</td>\n",
       "      <td>Virtual healthcare bot (VHC-Bot): a Person-cen...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Sara A. Alsalamah, Shada Alsalamah, Hessah A. ...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4408852e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>J</td>\n",
       "      <td>Nguyen, MH; Sedoc, J; Taylor, CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nguyen, Michelle Hoang; Sedoc, Joao; Taylor, C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usability, Engagement, and Report Usefulness o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>usability engagement and report usefulness of ...</td>\n",
       "      <td>cddaccfd097ba11b991829930f95d0abd6a9dade</td>\n",
       "      <td>Usability, Engagement, and Report Usefulness o...</td>\n",
       "      <td>2024</td>\n",
       "      <td>M. Nguyen, João Sedoc, C. O. Taylor</td>\n",
       "      <td>Background Family health history (FHx) is an i...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/cddaccfd...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>J</td>\n",
       "      <td>Safrai, M; Azaria, A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Safrai, Myriam; Azaria, Amos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Does small talk with a medical provider affect...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>does small talk with a medical provider affect...</td>\n",
       "      <td>dc8ed1d85c4674181746ffe8f08e86e00d0054ff</td>\n",
       "      <td>Does small talk with a medical provider affect...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Myriam Safrai, A. Azaria</td>\n",
       "      <td>Efforts are being made to improve the time eff...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dc8ed1d8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>J</td>\n",
       "      <td>Patsia, O; Giannopoulos, A; Giannakis, I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patsia, Ourania; Giannopoulos, Antonios; Giann...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPR Full-Waveform Inversion With Deep-Learning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>gpr full waveform inversion with deep learning...</td>\n",
       "      <td>0be15269dffa995e0c2859ed9c354ad4e916632b</td>\n",
       "      <td>GPR Full-Waveform Inversion With Deep-Learning...</td>\n",
       "      <td>2023</td>\n",
       "      <td>O. Patsia, A. Giannopoulos, I. Giannakis</td>\n",
       "      <td>Numerical modeling of ground penetrating radar...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0be15269...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>J</td>\n",
       "      <td>Putek, J; Szepietowski, JC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Putek, Justyna; Szepietowski, Jacek C.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexithymia in people with tattoos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>alexithymia in people with tattoos</td>\n",
       "      <td>ecaff1dd1b14f06daf698e6eaa4a5926928bc27a</td>\n",
       "      <td>Alexithymia in people with tattoos</td>\n",
       "      <td>2024</td>\n",
       "      <td>Justyna Putek, J. Szepietowski</td>\n",
       "      <td>Introduction Tattoos are a form of body modifi...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/ecaff1dd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>J</td>\n",
       "      <td>Nadi, M; Aboghazleh, R; Dabbas, WF; Ibrahim, B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nadi, Mustafa; Aboghazleh, Refat; Dabbas, Wale...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carotico-Clinoid Ligament Ossification: Unprov...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>carotico clinoid ligament ossification unprove...</td>\n",
       "      <td>242d9d92f6e2c91acb66e975ecf888c797c3676f</td>\n",
       "      <td>Carotico-Clinoid Ligament Ossification: Unprov...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Mustafa Nadi, R. Aboghazleh, Waleed F Dabbas, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.semanticscholar.org/paper/242d9d92...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Publication Type                                            Authors  \\\n",
       "0                 J  Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...   \n",
       "1                 C                   Kane, D; Parke, J; Jo, Y; Bak, J   \n",
       "2                 J  Bisbee, J; Clinton, JD; Dorff, C; Kenkel, B; L...   \n",
       "3                 J  Liu, HJ; Cao, Y; Wu, X; Qiu, C; Gu, JG; Liu, M...   \n",
       "4                 J  Boelaert, J; Coavoux, S; Ollion, E; Petev, I; ...   \n",
       "5                 J                                     Qu, Y; Wang, J   \n",
       "6                 C  Nguyen, H; Nguyen, V; López-Fierro, S; Ludovis...   \n",
       "7                 J  Salecha, A; Ireland, ME; Subrahmanya, S; Sedoc...   \n",
       "8                 J  Yao, JC; Zhang, HJ; Ou, J; Zuo, DY; Yang, Z; D...   \n",
       "9                 C               Hämäläinen, P; Tavast, M; Kunnari, A   \n",
       "10                J                        Zhang, S; Xu, J; Alvero, AJ   \n",
       "11                J  Zhang, BY; Chen, T; Wang, X; Li, Q; Zhang, WS;...   \n",
       "12                J  Gao, C; Lan, XC; Li, N; Yuan, Y; Ding, JT; Zho...   \n",
       "13                C  AlKhamissi, B; ElNokrashy, M; AlKhamissi, M; D...   \n",
       "14                J  Zhao, XJ; Wang, H; Dai, CX; Tang, JC; Deng, KX...   \n",
       "15                J              de Winter, JCF; Driessen, T; Dodou, D   \n",
       "16                J                           Rakovics, Z; Rakovics, M   \n",
       "17                J                                 Leung, HW; Bovy, J   \n",
       "18                J        Mburu, TK; Rong, KX; McColley, CJ; Werth, A   \n",
       "19                J  Zhang, KH; Dong, CQ; Guo, YF; Zhou, W; Yu, G; ...   \n",
       "20                J  Campos, M; Farinhas, A; Zerva, C; Figueiredo, ...   \n",
       "21                C                    Cheng, M; Piccardi, T; Yang, DY   \n",
       "22                J  Kaur, A; Budko, A; Liu, K; Eaton, E; Steitz, B...   \n",
       "23                J                              Ji, J; Kim, J; Kim, Y   \n",
       "24                J                                  Goli, A; Singh, A   \n",
       "25                C  Liu, YH; Chen, XY; Zhang, XQ; Gao, X; Zhang, J...   \n",
       "26                J  Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...   \n",
       "27                C                    Scarlatos, A; Baker, RS; Lan, A   \n",
       "28                C  Steinmacher, I; Penney, JM; Felizardo, KR; Gar...   \n",
       "29                J        Wahidur, RSM; Tashdeed, I; Kaur, M; Lee, HN   \n",
       "30                C                  Hwang, E; Majumder, BP; Tandon, N   \n",
       "31                C                                  Min, Y; Jeong, JW   \n",
       "32                C                          Liu, A; Diab, M; Fried, D   \n",
       "33                J  Mishra, T; Sutanto, E; Rossanti, R; Pant, N; A...   \n",
       "34                J  Teferra, BG; Perivolaris, A; Hsiang, WN; Sidha...   \n",
       "35                J  Bachmann, F; van der Weijden, D; Heitz, L; Sar...   \n",
       "36                C                               Mancera, J; Terán, L   \n",
       "37                C  Kaate, I; Salminen, J; Jung, SG; Xuan, TTT; Hä...   \n",
       "38                J  Hadar-Shoval, D; Asraf, K; Mizrachi, Y; Haber,...   \n",
       "39                J  Amirova, A; Fteropoulli, T; Ahmed, N; Cowie, M...   \n",
       "40                J   Sumner, J; Wang, YC; Tan, SY; Chew, EHH; Yip, AW   \n",
       "41                J  Rädel-Ablass, K; Schliz, K; Schlick, C; Meindl...   \n",
       "42                J                          Lau, C; Zhu, XD; Chan, WY   \n",
       "43                J                                         Heston, TF   \n",
       "44                J  Hanss, K; Sarma, K; Glowinski, AL; Krystal, A;...   \n",
       "45                J  Alsalamah, SA; AlSalamah, S; Alsalamah, HA; Sh...   \n",
       "46                J                   Nguyen, MH; Sedoc, J; Taylor, CO   \n",
       "47                J                               Safrai, M; Azaria, A   \n",
       "48                J           Patsia, O; Giannopoulos, A; Giannakis, I   \n",
       "49                J                         Putek, J; Szepietowski, JC   \n",
       "50                J  Nadi, M; Aboghazleh, R; Dabbas, WF; Ibrahim, B...   \n",
       "\n",
       "    Book Authors                                       Book Editors  \\\n",
       "0            NaN                                                NaN   \n",
       "1            NaN                       Bouamor, H; Pino, J; Bali, K   \n",
       "2            NaN                                                NaN   \n",
       "3            NaN                                                NaN   \n",
       "4            NaN                                                NaN   \n",
       "5            NaN                                                NaN   \n",
       "6            NaN                                                NaN   \n",
       "7            NaN                                                NaN   \n",
       "8            NaN                                                NaN   \n",
       "9            NaN                                                NaN   \n",
       "10           NaN                                                NaN   \n",
       "11           NaN                                                NaN   \n",
       "12           NaN                                                NaN   \n",
       "13           NaN                    Ku, LW; Martins, A; Srikumar, V   \n",
       "14           NaN                                                NaN   \n",
       "15           NaN                                                NaN   \n",
       "16           NaN                                                NaN   \n",
       "17           NaN                                                NaN   \n",
       "18           NaN                                                NaN   \n",
       "19           NaN                                                NaN   \n",
       "20           NaN                                                NaN   \n",
       "21           NaN                       Bouamor, H; Pino, J; Bali, K   \n",
       "22           NaN                                                NaN   \n",
       "23           NaN                                                NaN   \n",
       "24           NaN                                                NaN   \n",
       "25           NaN                                          Larson, K   \n",
       "26           NaN                                                NaN   \n",
       "27           NaN                                                NaN   \n",
       "28           NaN                                                NaN   \n",
       "29           NaN                                                NaN   \n",
       "30           NaN                        Bouamor, H; Pino, J; Bali K   \n",
       "31           NaN  Eck, U; Sra, M; Stefanucci, J; Sugimoto, M; Ta...   \n",
       "32           NaN                    Martins, A; Srikumar, V; Ku, LW   \n",
       "33           NaN                                                NaN   \n",
       "34           NaN                                                NaN   \n",
       "35           NaN                                                NaN   \n",
       "36           NaN      Liao, HC; Cid, DD; Macadar, MA; Bernardini, F   \n",
       "37           NaN                                                NaN   \n",
       "38           NaN                                                NaN   \n",
       "39           NaN                                                NaN   \n",
       "40           NaN                                                NaN   \n",
       "41           NaN                                                NaN   \n",
       "42           NaN                                                NaN   \n",
       "43           NaN                                                NaN   \n",
       "44           NaN                                                NaN   \n",
       "45           NaN                                                NaN   \n",
       "46           NaN                                                NaN   \n",
       "47           NaN                                                NaN   \n",
       "48           NaN                                                NaN   \n",
       "49           NaN                                                NaN   \n",
       "50           NaN                                                NaN   \n",
       "\n",
       "           Book Group Authors  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "3                         NaN   \n",
       "4                         NaN   \n",
       "5                         NaN   \n",
       "6   ASSOC COMPUTING MACHINERY   \n",
       "7                         NaN   \n",
       "8                         NaN   \n",
       "9                         ACM   \n",
       "10                        NaN   \n",
       "11                        NaN   \n",
       "12                        NaN   \n",
       "13                        NaN   \n",
       "14                        NaN   \n",
       "15                        NaN   \n",
       "16                        NaN   \n",
       "17                        NaN   \n",
       "18                        NaN   \n",
       "19                        NaN   \n",
       "20                        NaN   \n",
       "21                        NaN   \n",
       "22                        NaN   \n",
       "23                        NaN   \n",
       "24                        NaN   \n",
       "25                        NaN   \n",
       "26                        NaN   \n",
       "27                        ACM   \n",
       "28                        ACM   \n",
       "29                        NaN   \n",
       "30                        NaN   \n",
       "31                        NaN   \n",
       "32                        NaN   \n",
       "33                        NaN   \n",
       "34                        NaN   \n",
       "35                        NaN   \n",
       "36                        NaN   \n",
       "37                        ACM   \n",
       "38                        NaN   \n",
       "39                        NaN   \n",
       "40                        NaN   \n",
       "41                        NaN   \n",
       "42                        NaN   \n",
       "43                        NaN   \n",
       "44                        NaN   \n",
       "45                        NaN   \n",
       "46                        NaN   \n",
       "47                        NaN   \n",
       "48                        NaN   \n",
       "49                        NaN   \n",
       "50                        NaN   \n",
       "\n",
       "                                    Author Full Names  Book Author Full Names  \\\n",
       "0   Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...                     NaN   \n",
       "1   Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...                     NaN   \n",
       "2   Bisbee, James; Clinton, Joshua D.; Dorff, Cass...                     NaN   \n",
       "3   Liu, Haijiang; Cao, Yong; Wu, Xun; Qiu, Chen; ...                     NaN   \n",
       "4   Boelaert, Julien; Coavoux, Samuel; Ollion, Eti...                     NaN   \n",
       "5                                  Qu, Yao; Wang, Jue                     NaN   \n",
       "6   Ha Nguyen; Nguyen, Victoria; Lopez-Fierro, Sar...                     NaN   \n",
       "7   Salecha, Aadesh; Ireland, Molly E.; Subrahmany...                     NaN   \n",
       "8   Yao, Junchi; Zhang, Hongjie; Ou, Jie; Zuo, Din...                     NaN   \n",
       "9   Hamalainen, Perttu; Tavast, Mikke; Kunnari, Anton                     NaN   \n",
       "10            Zhang, Simone; Xu, Janet; Alvero, A. J.                     NaN   \n",
       "11  Zhang, Baoyu; Chen, Tao; Wang, Xiao; Li, Qiang...                     NaN   \n",
       "12  Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yua...                     NaN   \n",
       "13  AlKhamissi, Badr; ElNokrashy, Muhammad; AlKham...                     NaN   \n",
       "14  Zhao, Xinjie; Wang, Hao; Dai, Chengxiao; Tang,...                     NaN   \n",
       "15  de Winter, Joost C. F.; Driessen, Tom; Dodou, ...                     NaN   \n",
       "16                 Rakovics, Zsofia; Rakovics, Marton                     NaN   \n",
       "17                          Leung, Henry W.; Bovy, Jo                     NaN   \n",
       "18  Mburu, Ted K.; Rong, Kangxuan; McColley, Campb...                     NaN   \n",
       "19  Zhang, Kaihang; Dong, Changqi; Guo, Yifeng; Zh...                     NaN   \n",
       "20  Campos, Margarida; Farinhas, Antonio; Zerva, C...                     NaN   \n",
       "21         Cheng, Myra; Piccardi, Tiziano; Yang, Diyi                     NaN   \n",
       "22  Kaur, Amarpreet; Budko, Alexander; Liu, Katrin...                     NaN   \n",
       "23            Ji, Junyung; Kim, Jiwoo; Kim, Younghoon                     NaN   \n",
       "24                         Goli, Ali; Singh, Amandeep                     NaN   \n",
       "25  Liu, Yuhan; Chen, Xiuying; Zhang, Xiaoqing; Ga...                     NaN   \n",
       "26  Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...                     NaN   \n",
       "27  Scarlatos, Alexander; Baker, Ryan S.; Lan, Andrew                     NaN   \n",
       "28  Steinmacher, Igor; Penney, Jacob Mcauley; Feli...                     NaN   \n",
       "29  Wahidur, Rahman S. M.; Tashdeed, Ishmam; Kaur,...                     NaN   \n",
       "30  Hwang, EunJeong; Majumder, Bodhisattwa Prasad;...                     NaN   \n",
       "31                         Min, Yewon; Jeong, Jin-Woo                     NaN   \n",
       "32               Liu, Andy; Diab, Mona; Fried, Daniel                     NaN   \n",
       "33  Mishra, Tanisha; Sutanto, Edward; Rossanti, Ri...                     NaN   \n",
       "34  Teferra, Bazen Gashaw; Perivolaris, Argyrios; ...                     NaN   \n",
       "35  Bachmann, Fynn; van der Weijden, Daan; Heitz, ...                     NaN   \n",
       "36                         Mancera, Jose; Teran, Luis                     NaN   \n",
       "37  Kaate, Ilkka; Salminen, Joni; Jung, Soon-Gyo; ...                     NaN   \n",
       "38  Hadar-Shoval, Dorit; Asraf, Kfir; Mizrachi, Yo...                     NaN   \n",
       "39  Amirova, Aliya; Fteropoulli, Theodora; Ahmed, ...                     NaN   \n",
       "40  Sumner, Jennifer; Wang, Yuchen; Tan, Si Ying; ...                     NaN   \n",
       "41  Raedel-Ablass, Katharina; Schliz, Klaus; Schli...                     NaN   \n",
       "42          Lau, Clinton; Zhu, Xiaodan; Chan, Wai-Yip                     NaN   \n",
       "43                                  Heston, Thomas F.                     NaN   \n",
       "44  Hanss, Kaitlin; Sarma, Karthik, V; Glowinski, ...                     NaN   \n",
       "45  Alsalamah, Sara A.; AlSalamah, Shada; Alsalama...                     NaN   \n",
       "46  Nguyen, Michelle Hoang; Sedoc, Joao; Taylor, C...                     NaN   \n",
       "47                       Safrai, Myriam; Azaria, Amos                     NaN   \n",
       "48  Patsia, Ourania; Giannopoulos, Antonios; Giann...                     NaN   \n",
       "49             Putek, Justyna; Szepietowski, Jacek C.                     NaN   \n",
       "50  Nadi, Mustafa; Aboghazleh, Refat; Dabbas, Wale...                     NaN   \n",
       "\n",
       "    Group Authors                                          title_WoS  \\\n",
       "0             NaN  How Well Do Simulated Population Samples with ...   \n",
       "1             NaN  From Values to Opinions: Predicting Human Beha...   \n",
       "2             NaN  Synthetic Replacements for Human Survey Data? ...   \n",
       "3             NaN  Towards realistic evaluation of cultural value...   \n",
       "4             NaN  Machine Bias. How Do Generative Language Model...   \n",
       "5             NaN  Performance and biases of Large Language Model...   \n",
       "6             NaN  Simulating Climate Change Discussion with Larg...   \n",
       "7             NaN  Large language models display human-like socia...   \n",
       "8             NaN  Social opinions prediction utilizes fusing dyn...   \n",
       "9             NaN  Evaluating Large Language Models in Generating...   \n",
       "10            NaN  Generative AI Meets Open-Ended Survey Response...   \n",
       "11            NaN  Decoding Activist Public Opinion in Decentrali...   \n",
       "12            NaN  Large language models empowered agent-based mo...   \n",
       "13            NaN  Investigating Cultural Alignment of Large Lang...   \n",
       "14            NaN  Multi-Stage Simulation of Residents' Disaster ...   \n",
       "15            NaN  The use of ChatGPT for personality research: A...   \n",
       "16            NaN  Exploring the potential and limitations of lar...   \n",
       "17            NaN  Towards an astronomical foundation model for s...   \n",
       "18            NaN  Methodological foundations for artificial inte...   \n",
       "19            NaN  Lagged Stance Interactions and Counter-Spiral ...   \n",
       "20            NaN  Conformal Prediction for Natural Language Proc...   \n",
       "21            NaN  CoMPosT: Characterizing and Evaluating Caricat...   \n",
       "22            NaN  Automating Responses to Patient Portal Message...   \n",
       "23            NaN  Predicting Missing Values in Survey Data Using...   \n",
       "24            NaN  Frontiers: Can Large Language Models Capture H...   \n",
       "25            NaN  From Skepticism to Acceptance: Simulating the ...   \n",
       "26            NaN  Matching GPT-simulated Populations with Real O...   \n",
       "27            NaN  Exploring Knowledge Tracing in Tutor-Student D...   \n",
       "28            NaN  Can ChatGPT emulate humans in software enginee...   \n",
       "29            NaN  Enhancing Zero-Shot Crypto Sentiment With Fine...   \n",
       "30            NaN          Aligning Language Models to User Opinions   \n",
       "31            NaN  Public Speaking Q&A Practice with LLM-Generate...   \n",
       "32            NaN  Evaluating Large Language Model Biases in Pers...   \n",
       "33            NaN  Use of large language models as artificial int...   \n",
       "34            NaN  Leveraging large language models for automated...   \n",
       "35            NaN  Adaptive political surveys and GPT-4: Tackling...   \n",
       "36            NaN  From GenAI to Political Profiling Avatars: A D...   \n",
       "37            NaN  You Always Get an Answer: Analyzing Users' Int...   \n",
       "38            NaN  Assessing the Alignment of Large Language Mode...   \n",
       "39            NaN  Framework-based qualitative analysis of free r...   \n",
       "40            NaN  Perspectives and Experiences With Large Langua...   \n",
       "41            NaN  Teaching opportunities for anamnesis interview...   \n",
       "42            NaN  Automatic depression severity assessment with ...   \n",
       "43            NaN  Safety of Large Language Models in Addressing ...   \n",
       "44            NaN  Assessing the Accuracy and Reliability of Larg...   \n",
       "45            NaN  Virtual healthcare bot (VHC-Bot): a Person-cen...   \n",
       "46            NaN  Usability, Engagement, and Report Usefulness o...   \n",
       "47            NaN  Does small talk with a medical provider affect...   \n",
       "48            NaN  GPR Full-Waveform Inversion With Deep-Learning...   \n",
       "49            NaN                 Alexithymia in people with tattoos   \n",
       "50            NaN  Carotico-Clinoid Ligament Ossification: Unprov...   \n",
       "\n",
       "    Source Title  ...  UT (Unique WOS ID)  Web of Science Record  \\\n",
       "0            NaN  ...                 NaN                      0   \n",
       "1            NaN  ...                 NaN                      0   \n",
       "2            NaN  ...                 NaN                      0   \n",
       "3            NaN  ...                 NaN                      0   \n",
       "4            NaN  ...                 NaN                      0   \n",
       "5            NaN  ...                 NaN                      0   \n",
       "6            NaN  ...                 NaN                      0   \n",
       "7            NaN  ...                 NaN                      0   \n",
       "8            NaN  ...                 NaN                      0   \n",
       "9            NaN  ...                 NaN                      0   \n",
       "10           NaN  ...                 NaN                      0   \n",
       "11           NaN  ...                 NaN                      0   \n",
       "12           NaN  ...                 NaN                      0   \n",
       "13           NaN  ...                 NaN                      0   \n",
       "14           NaN  ...                 NaN                      0   \n",
       "15           NaN  ...                 NaN                      0   \n",
       "16           NaN  ...                 NaN                      0   \n",
       "17           NaN  ...                 NaN                      0   \n",
       "18           NaN  ...                 NaN                      0   \n",
       "19           NaN  ...                 NaN                      0   \n",
       "20           NaN  ...                 NaN                      0   \n",
       "21           NaN  ...                 NaN                      0   \n",
       "22           NaN  ...                 NaN                      0   \n",
       "23           NaN  ...                 NaN                      0   \n",
       "24           NaN  ...                 NaN                      0   \n",
       "25           NaN  ...                 NaN                      0   \n",
       "26           NaN  ...                 NaN                      0   \n",
       "27           NaN  ...                 NaN                      0   \n",
       "28           NaN  ...                 NaN                      0   \n",
       "29           NaN  ...                 NaN                      0   \n",
       "30           NaN  ...                 NaN                      0   \n",
       "31           NaN  ...                 NaN                      0   \n",
       "32           NaN  ...                 NaN                      0   \n",
       "33           NaN  ...                 NaN                      0   \n",
       "34           NaN  ...                 NaN                      0   \n",
       "35           NaN  ...                 NaN                      0   \n",
       "36           NaN  ...                 NaN                      0   \n",
       "37           NaN  ...                 NaN                      0   \n",
       "38           NaN  ...                 NaN                      0   \n",
       "39           NaN  ...                 NaN                      0   \n",
       "40           NaN  ...                 NaN                      0   \n",
       "41           NaN  ...                 NaN                      0   \n",
       "42           NaN  ...                 NaN                      0   \n",
       "43           NaN  ...                 NaN                      0   \n",
       "44           NaN  ...                 NaN                      0   \n",
       "45           NaN  ...                 NaN                      0   \n",
       "46           NaN  ...                 NaN                      0   \n",
       "47           NaN  ...                 NaN                      0   \n",
       "48           NaN  ...                 NaN                      0   \n",
       "49           NaN  ...                 NaN                      0   \n",
       "50           NaN  ...                 NaN                      0   \n",
       "\n",
       "                                           norm_title  \\\n",
       "0   how well do simulated population samples with ...   \n",
       "1   from values to opinions predicting human behav...   \n",
       "2   synthetic replacements for human survey data t...   \n",
       "3   towards realistic evaluation of cultural value...   \n",
       "4   machine bias how do generative language models...   \n",
       "5   performance and biases of large language model...   \n",
       "6   simulating climate change discussion with larg...   \n",
       "7   large language models display human like socia...   \n",
       "8   social opinions prediction utilizes fusing dyn...   \n",
       "9   evaluating large language models in generating...   \n",
       "10  generative ai meets open ended survey response...   \n",
       "11  decoding activist public opinion in decentrali...   \n",
       "12  large language models empowered agent based mo...   \n",
       "13  investigating cultural alignment of large lang...   \n",
       "14  multi stage simulation of residents disaster r...   \n",
       "15  the use of chatgpt for personality research ad...   \n",
       "16  exploring the potential and limitations of lar...   \n",
       "17  towards an astronomical foundation model for s...   \n",
       "18  methodological foundations for artificial inte...   \n",
       "19  lagged stance interactions and counter spiral ...   \n",
       "20  conformal prediction for natural language proc...   \n",
       "21  compost characterizing and evaluating caricatu...   \n",
       "22  automating responses to patient portal message...   \n",
       "23  predicting missing values in survey data using...   \n",
       "24  frontiers can large language models capture hu...   \n",
       "25  from skepticism to acceptance simulating the a...   \n",
       "26  matching gpt simulated populations with real o...   \n",
       "27  exploring knowledge tracing in tutor student d...   \n",
       "28  can chatgpt emulate humans in software enginee...   \n",
       "29  enhancing zero shot crypto sentiment with fine...   \n",
       "30          aligning language models to user opinions   \n",
       "31  public speaking q a practice with llm generate...   \n",
       "32  evaluating large language model biases in pers...   \n",
       "33  use of large language models as artificial int...   \n",
       "34  leveraging large language models for automated...   \n",
       "35  adaptive political surveys and gpt 4 tackling ...   \n",
       "36  from genai to political profiling avatars a da...   \n",
       "37  you always get an answer analyzing users inter...   \n",
       "38  assessing the alignment of large language mode...   \n",
       "39  framework based qualitative analysis of free r...   \n",
       "40  perspectives and experiences with large langua...   \n",
       "41  teaching opportunities for anamnesis interview...   \n",
       "42  automatic depression severity assessment with ...   \n",
       "43  safety of large language models in addressing ...   \n",
       "44  assessing the accuracy and reliability of larg...   \n",
       "45  virtual healthcare bot vhc bot a person center...   \n",
       "46  usability engagement and report usefulness of ...   \n",
       "47  does small talk with a medical provider affect...   \n",
       "48  gpr full waveform inversion with deep learning...   \n",
       "49                 alexithymia in people with tattoos   \n",
       "50  carotico clinoid ligament ossification unprove...   \n",
       "\n",
       "                                     paperId  \\\n",
       "0   25f383b7a807392696073801959dcc1c1aadd2bb   \n",
       "1   52e963c40a5083d5403cebf4d4782271aaa06994   \n",
       "2   58d735a54d3aba79ad3bffbfa2433d8e5ee27313   \n",
       "3   3ab59b3d4a4b2e89f7eda93a950eeaa77b37332e   \n",
       "4   45f9ea8d0dc1a7e6c56ff6e1f23c8e632687d2a7   \n",
       "5   e6d14d140c4faaf8f3d9f47e61cc5c6091bccf1e   \n",
       "6   dd95064d28ee5d123a6a284422bbba3d443f0416   \n",
       "7   8253104f5b1481d8557380d2dc5dab03ff9a7716   \n",
       "8   392de716c8f6610f080ba655e885935c20ac6c73   \n",
       "9   0ffd57884d7957f6b5634b9fa24843dc3759668f   \n",
       "10  8398dfa4d015b3784654a77b3913b62a1f68eed8   \n",
       "11  4949ff97cf5e7c31ed9a057cbbde4b95d3ddd1f8   \n",
       "12  592ac35991e583fc37c26ee6659d2deb85142ad9   \n",
       "13  b1890367317f0657c08ed96be4c474035b34b485   \n",
       "14  6d2077c8f4864103780b160501cd207005d045c8   \n",
       "15  781703bc7e4fb90766824ee808097171afa223b3   \n",
       "16  6cd94c4fdfd1ddf59b5385919851de2662e412fe   \n",
       "17  264cb7a7dbee1303ff9e0ebe2dac78646271a2fb   \n",
       "18  54b63afefc315b5f051f4a19fe413ef6c544c9fd   \n",
       "19  bc270ab2e00f78691c8fd16548809e3609dedee2   \n",
       "20  346fdbda3ecf4775819fced0cfed78357bee8128   \n",
       "21  7a4fe2f003241ad97bf1778e527cb0306fa90da2   \n",
       "22  caa3c4eb1ede700f03eef5b4f25a18b08c88d832   \n",
       "23  bdeeaf207e2563f39ad27a9d9511d8573b5bff95   \n",
       "24  0377c4c20d86e3a23cb5c22d89b3cb488c31a564   \n",
       "25  1bd4b8be136072c8f56114f2f8479aaed2ad6d9b   \n",
       "26  e186d394a49ab2866a8b1248a99e6f1637a238fe   \n",
       "27  06e9ec37cc25980544d0a78b5aa4893dafc65fd3   \n",
       "28  2a77ac8d1c36ccf9f222cf0ae16e251ac6b13e86   \n",
       "29  bcdc44ef48ffadbdaa3bd5cacfe9ddb9b9f48750   \n",
       "30  5db0f55332839c408e3049cea1a6ad48fefba70c   \n",
       "31  3b36ec5f47076620dc7735f8aba55d9c3c3e6b32   \n",
       "32  ae03f10729959435ecefc0e90cba4cbe8438a10b   \n",
       "33  0c1e396f7f23d34ebad01dac29de6898f18ae63e   \n",
       "34  d0deb3ffd586a886b435d67c1b3aad89eeeaf358   \n",
       "35  1d1097d378555393b73b492995121ab880fff142   \n",
       "36  3084188f92330fa837a87b6ca422b28ae1ca713c   \n",
       "37  48ebe4b39ff2dce8d6c026f6cd2cbea8406a9a6e   \n",
       "38  3a455a00d01fcd45d7797f296eb5b5db331ff7b1   \n",
       "39  ba5aefce80edc7da110f53fd071f4fbd6b5195b9   \n",
       "40  db593a47eec5f6dbd9f28e88d0e4d7450cc6376b   \n",
       "41  d46b4155182de3a385b8f0b323ca54a201e1c60f   \n",
       "42  527288d9ded807883d756f1b3503bc39e79e0a06   \n",
       "43  1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f   \n",
       "44  faf47d35147d836bcc03bffda819db2426febac8   \n",
       "45  4408852e43dcff9da71f1053f6aeefbea10db450   \n",
       "46  cddaccfd097ba11b991829930f95d0abd6a9dade   \n",
       "47  dc8ed1d85c4674181746ffe8f08e86e00d0054ff   \n",
       "48  0be15269dffa995e0c2859ed9c354ad4e916632b   \n",
       "49  ecaff1dd1b14f06daf698e6eaa4a5926928bc27a   \n",
       "50  242d9d92f6e2c91acb66e975ecf888c797c3676f   \n",
       "\n",
       "                                             title_SS  year  \\\n",
       "0   How Well Do Simulated Population Samples with ...  2025   \n",
       "1   From Values to Opinions: Predicting Human Beha...  2023   \n",
       "2   Synthetic Replacements for Human Survey Data? ...  2024   \n",
       "3   Towards realistic evaluation of cultural value...  2025   \n",
       "4   Machine Bias. How Do Generative Language Model...  2025   \n",
       "5   Performance and Biases of Large Language Model...  2024   \n",
       "6   Simulating Climate Change Discussion with Larg...  2024   \n",
       "7   Large language models display human-like socia...  2024   \n",
       "8   Social opinions prediction utilizes fusing dyn...  2024   \n",
       "9   Evaluating Large Language Models in Generating...  2023   \n",
       "10  Generative AI Meets Open-Ended Survey Response...  2025   \n",
       "11  Decoding Activist Public Opinion in Decentrali...  2024   \n",
       "12  Large Language Models Empowered Agent-based Mo...  2023   \n",
       "13  Investigating Cultural Alignment of Large Lang...  2024   \n",
       "14  Multi-Stage Simulation of Residents’ Disaster ...  2025   \n",
       "15  The use of ChatGPT for personality research: A...  2024   \n",
       "16  Exploring the potential and limitations of lar...  2024   \n",
       "17  Towards an astronomical foundation model for s...  2023   \n",
       "18  Methodological foundations for artificial inte...  2025   \n",
       "19  Lagged Stance Interactions and Counter-Spiral ...  2025   \n",
       "20  Conformal Prediction for Natural Language Proc...  2024   \n",
       "21  CoMPosT: Characterizing and Evaluating Caricat...  2023   \n",
       "22  Automating Responses to Patient Portal Message...  2024   \n",
       "23  Predicting Missing Values in Survey Data Using...  2024   \n",
       "24  Frontiers: Can Large Language Models Capture H...  2024   \n",
       "25  From Skepticism to Acceptance: Simulating the ...  2024   \n",
       "26  Matching GPT-simulated Populations with Real O...  2025   \n",
       "27  Exploring Knowledge Tracing in Tutor-Student D...  2024   \n",
       "28  Can ChatGPT emulate humans in software enginee...  2024   \n",
       "29  Enhancing Zero-Shot Crypto Sentiment With Fine...  2023   \n",
       "30          Aligning Language Models to User Opinions  2023   \n",
       "31  Public Speaking Q&A Practice with LLM-Generate...  2024   \n",
       "32  Evaluating Large Language Model Biases in Pers...  2024   \n",
       "33  Use of large language models as artificial int...  2024   \n",
       "34  Leveraging large language models for automated...  2025   \n",
       "35  Adaptive political surveys and GPT-4: Tackling...  2025   \n",
       "36  From GenAI to Political Profiling Avatars: A D...  2024   \n",
       "37  “You Always Get an Answer”: Analyzing Users’ I...  2025   \n",
       "38  Assessing the Alignment of Large Language Mode...  2024   \n",
       "39  Framework-based qualitative analysis of free r...  2023   \n",
       "40  Perspectives and Experiences With Large Langua...  2025   \n",
       "41  Teaching opportunities for anamnesis interview...  2025   \n",
       "42  Automatic depression severity assessment with ...  2023   \n",
       "43  Safety of Large Language Models in Addressing ...  2023   \n",
       "44  Assessing the Accuracy and Reliability of Larg...  2025   \n",
       "45  Virtual healthcare bot (VHC-Bot): a Person-cen...  2025   \n",
       "46  Usability, Engagement, and Report Usefulness o...  2024   \n",
       "47  Does small talk with a medical provider affect...  2024   \n",
       "48  GPR Full-Waveform Inversion With Deep-Learning...  2023   \n",
       "49                 Alexithymia in people with tattoos  2024   \n",
       "50  Carotico-Clinoid Ligament Ossification: Unprov...  2025   \n",
       "\n",
       "                                              authors  \\\n",
       "0   Gregorio Ferreira, Jacopo Amidei, Rubén Nieto,...   \n",
       "1   Dongjun Kang, Joonsuk Park, Yohan Jo, Jinyeong...   \n",
       "2   James Bisbee, Joshua D. Clinton, C. Dorff, Bre...   \n",
       "3   Haijiang Liu, Yong Cao, Xun Wu, Chen Qiu, Jing...   \n",
       "4   J. Boelaert, Samuel Coavoux, Étienne Ollion, I...   \n",
       "5                                    Yao Qu, Jue Wang   \n",
       "6   Ha Nguyen, Victoria Nguyen, Saríah López-Fierr...   \n",
       "7   Aadesh Salecha, Molly E. Ireland, Shashanka Su...   \n",
       "8   Junchi Yao, Hongjie Zhang, Jie Ou, Dingyi Zuo,...   \n",
       "9      Perttu Hämäläinen, Mikke Tavast, Anton Kunnari   \n",
       "10                  Simone Zhang, Janet Xu, AJ Alvero   \n",
       "11  Baoyu Zhang, Tao Chen, Xiao Wang, Qiang Li, We...   \n",
       "12  Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, J...   \n",
       "13  Badr AlKhamissi, Muhammad N. ElNokrashy, Mai A...   \n",
       "14  Xinjie Zhao, Hao Wang, Chengxiao Dai, Jiacheng...   \n",
       "15       J. D. de Winter, Tom Driessen, Dimitra Dodou   \n",
       "16                   Zsófia Rakovics, Márton Rakovics   \n",
       "17                            Henry W. Leung, J. Bovy   \n",
       "18  Ted K. Mburu, Kangxuan Rong, Campbell J. McCol...   \n",
       "19  Kaihang Zhang, Changqi Dong, Yifeng Guo, Wuai ...   \n",
       "20  Margarida M. Campos, António Farinhas, Chrysou...   \n",
       "21            Myra Cheng, Tiziano Piccardi, Diyi Yang   \n",
       "22  Amarpreet Kaur, Alexander Budko, Katrina Liu, ...   \n",
       "23               Junyung Ji, Jiwoo Kim, Younghoon Kim   \n",
       "24                           Ali Goli, Amandeep Singh   \n",
       "25  Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing ...   \n",
       "26  Gregorio Ferreira, Jacopo Amidei, Rubén Nieto,...   \n",
       "27     Alexander Scarlatos, Ryan S. Baker, Andrew Lan   \n",
       "28  Igor Steinmacher, Jacob Penney, K. Felizardo, ...   \n",
       "29  Rahman S M Wahidur, Ishmam Tashdeed, Manjit Ka...   \n",
       "30  EunJeong Hwang, Bodhisattwa Prasad Majumder, N...   \n",
       "31                           Yewon Min, Jin-Woo Jeong   \n",
       "32               Andy Liu, Mona T. Diab, Daniel Fried   \n",
       "33  Tanisha Mishra, Edward Sutanto, Rini Rossanti,...   \n",
       "34  Bazen Gashaw Teferra, Argyrios Perivolaris, We...   \n",
       "35  Fynn Bachmann, Daan van der Weijden, Lucien He...   \n",
       "36           José Alberto Mancera Andrade, Luis Terán   \n",
       "37  Ilkka Kaate, Joni O. Salminen, Soon-gyo Jung, ...   \n",
       "38  D. Hadar-Shoval, K. Asraf, Yonathan Mizrachi, ...   \n",
       "39  A. Amirova, T. Fteropoulli, Nafiso Ahmed, Mart...   \n",
       "40  Jennifer Sumner, Yuchen Wang, Si Ying Tan, Emi...   \n",
       "41  Katharina Rädel-Ablass, Klaus Schliz, Cornelia...   \n",
       "42             Clinton Lau, Xiaodan Zhu, Wai-Yip Chan   \n",
       "43                                       T. F. Heston   \n",
       "44  Kaitlin Hanss, Karthik V Sarma, Anne L Glowins...   \n",
       "45  Sara A. Alsalamah, Shada Alsalamah, Hessah A. ...   \n",
       "46                M. Nguyen, João Sedoc, C. O. Taylor   \n",
       "47                           Myriam Safrai, A. Azaria   \n",
       "48           O. Patsia, A. Giannopoulos, I. Giannakis   \n",
       "49                     Justyna Putek, J. Szepietowski   \n",
       "50  Mustafa Nadi, R. Aboghazleh, Waleed F Dabbas, ...   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Background: Advances in artificial intelligenc...   \n",
       "1   Being able to predict people's opinions on iss...   \n",
       "2   \\n Large language models (LLMs) offer new rese...   \n",
       "3                                                None   \n",
       "4   Generative artificial intelligence (AI) is inc...   \n",
       "5                                                None   \n",
       "6   Large language models (LLMs) have shown promis...   \n",
       "7   Abstract Large language models (LLMs) are beco...   \n",
       "8   In the context where social media emerges as a...   \n",
       "9   Collecting data is one of the bottlenecks of H...   \n",
       "10  The growing popularity of generative artificia...   \n",
       "11  Based on an investigation of online public opi...   \n",
       "12  Agent-based modeling and simulation have evolv...   \n",
       "13  The intricate relationship between language an...   \n",
       "14  The escalating frequency and complexity of nat...   \n",
       "15                                               None   \n",
       "16  Social and linguistic differences encoded in v...   \n",
       "17  Rapid strides are currently being made in the ...   \n",
       "18  This study investigates the use of large langu...   \n",
       "19  Understanding the dynamics of public opinion f...   \n",
       "20  Abstract The rapid proliferation of large lang...   \n",
       "21  Recent work has aimed to capture nuances of hu...   \n",
       "22  Background: Patient portals serve as vital bri...   \n",
       "23  Survey data play a crucial role in various res...   \n",
       "24  This paper examines the potential of large lan...   \n",
       "25  In the digital era, the rapid propagation of f...   \n",
       "26  This article analyzes how well OpenAI’s LLM GP...   \n",
       "27  Recent advances in large language models (LLMs...   \n",
       "28  Context: There is a growing belief in the lite...   \n",
       "29  Blockchain technology has revolutionized the f...   \n",
       "30  An important aspect of developing LLMs that in...   \n",
       "31  This paper introduces a novel VR-based Q&A pra...   \n",
       "32  The task of persona-steered text generation re...   \n",
       "33  With breakthroughs in Natural Language Process...   \n",
       "34  Mental health diagnoses possess unique challen...   \n",
       "35  Adaptive questionnaires dynamically select the...   \n",
       "36  Voting advice applications (VAAs) are pivotal ...   \n",
       "37  We investigated the presence and acceptance of...   \n",
       "38  Background Large language models (LLMs) hold p...   \n",
       "39  Today, with the advent of Large-scale generati...   \n",
       "40  Background Large language models (LLMs) are tr...   \n",
       "41  Background This study presents a novel approac...   \n",
       "42  Introduction To assist mental health care prov...   \n",
       "43  Background Generative artificial intelligence ...   \n",
       "44  Background Large language models (LLMs), such ...   \n",
       "45                                               None   \n",
       "46  Background Family health history (FHx) is an i...   \n",
       "47  Efforts are being made to improve the time eff...   \n",
       "48  Numerical modeling of ground penetrating radar...   \n",
       "49  Introduction Tattoos are a form of body modifi...   \n",
       "50                                               None   \n",
       "\n",
       "                                                  url  citationCount  \n",
       "0   https://www.semanticscholar.org/paper/25f383b7...              0  \n",
       "1   https://www.semanticscholar.org/paper/52e963c4...              4  \n",
       "2   https://www.semanticscholar.org/paper/58d735a5...             74  \n",
       "3   https://www.semanticscholar.org/paper/3ab59b3d...              2  \n",
       "4   https://www.semanticscholar.org/paper/45f9ea8d...              9  \n",
       "5   https://www.semanticscholar.org/paper/e6d14d14...             46  \n",
       "6   https://www.semanticscholar.org/paper/dd95064d...             16  \n",
       "7   https://www.semanticscholar.org/paper/8253104f...             25  \n",
       "8   https://www.semanticscholar.org/paper/392de716...              5  \n",
       "9   https://www.semanticscholar.org/paper/0ffd5788...            218  \n",
       "10  https://www.semanticscholar.org/paper/8398dfa4...             10  \n",
       "11  https://www.semanticscholar.org/paper/4949ff97...              2  \n",
       "12  https://www.semanticscholar.org/paper/592ac359...            182  \n",
       "13  https://www.semanticscholar.org/paper/b1890367...             78  \n",
       "14  https://www.semanticscholar.org/paper/6d2077c8...              1  \n",
       "15  https://www.semanticscholar.org/paper/781703bc...             19  \n",
       "16  https://www.semanticscholar.org/paper/6cd94c4f...              1  \n",
       "17  https://www.semanticscholar.org/paper/264cb7a7...             24  \n",
       "18  https://www.semanticscholar.org/paper/54b63afe...              1  \n",
       "19  https://www.semanticscholar.org/paper/bc270ab2...              0  \n",
       "20  https://www.semanticscholar.org/paper/346fdbda...             22  \n",
       "21  https://www.semanticscholar.org/paper/7a4fe2f0...             89  \n",
       "22  https://www.semanticscholar.org/paper/caa3c4eb...              7  \n",
       "23  https://www.semanticscholar.org/paper/bdeeaf20...              2  \n",
       "24  https://www.semanticscholar.org/paper/0377c4c2...             31  \n",
       "25  https://www.semanticscholar.org/paper/1bd4b8be...             44  \n",
       "26  https://www.semanticscholar.org/paper/e186d394...              2  \n",
       "27  https://www.semanticscholar.org/paper/06e9ec37...             14  \n",
       "28  https://www.semanticscholar.org/paper/2a77ac8d...              7  \n",
       "29  https://www.semanticscholar.org/paper/bcdc44ef...             17  \n",
       "30  https://www.semanticscholar.org/paper/5db0f553...             82  \n",
       "31  https://www.semanticscholar.org/paper/3b36ec5f...              5  \n",
       "32  https://www.semanticscholar.org/paper/ae03f107...             49  \n",
       "33  https://www.semanticscholar.org/paper/0c1e396f...             14  \n",
       "34  https://www.semanticscholar.org/paper/d0deb3ff...              0  \n",
       "35  https://www.semanticscholar.org/paper/1d1097d3...              1  \n",
       "36  https://www.semanticscholar.org/paper/3084188f...              1  \n",
       "37  https://www.semanticscholar.org/paper/48ebe4b3...              3  \n",
       "38  https://www.semanticscholar.org/paper/3a455a00...             36  \n",
       "39  https://www.semanticscholar.org/paper/ba5aefce...             14  \n",
       "40  https://www.semanticscholar.org/paper/db593a47...              1  \n",
       "41  https://www.semanticscholar.org/paper/d46b4155...              8  \n",
       "42  https://www.semanticscholar.org/paper/527288d9...             20  \n",
       "43  https://www.semanticscholar.org/paper/1554d7e7...             43  \n",
       "44  https://www.semanticscholar.org/paper/faf47d35...              2  \n",
       "45  https://www.semanticscholar.org/paper/4408852e...              0  \n",
       "46  https://www.semanticscholar.org/paper/cddaccfd...              2  \n",
       "47  https://www.semanticscholar.org/paper/dc8ed1d8...              1  \n",
       "48  https://www.semanticscholar.org/paper/0be15269...             15  \n",
       "49  https://www.semanticscholar.org/paper/ecaff1dd...              0  \n",
       "50  https://www.semanticscholar.org/paper/242d9d92...              0  \n",
       "\n",
       "[51 rows x 80 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize titles for matching\n",
    "df_results_wos['norm_title'] = df_results_wos['title'].map(normalize_title)\n",
    "df_SS_g7['norm_title'] = df_SS_g7['title'].map(normalize_title)\n",
    "\n",
    "# Find intersection of normalized titles\n",
    "common_norm_titles = set(df_results_wos['norm_title']).intersection(set(df_SS_g7['norm_title']))\n",
    "\n",
    "# Filter both dataframes to only those with common titles\n",
    "df_common = df_results_wos[df_results_wos['norm_title'].isin(common_norm_titles)].copy()\n",
    "df_common = df_common.merge(\n",
    "    df_SS_g7[df_SS_g7['norm_title'].isin(common_norm_titles)],\n",
    "    on='norm_title',\n",
    "    suffixes=('_WoS', '_SS')\n",
    ")\n",
    "\n",
    "df_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae5cfb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Publication Type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Book Authors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Editors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Book Group Authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Author Full Names",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Book Author Full Names",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Group Authors",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Source Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Series Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book Series Subtitle",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Language",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Document Type",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Title",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Location",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Sponsor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Conference Host",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Author Keywords",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Keywords Plus",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Abstract",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Affiliations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Reprint Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Email Addresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Researcher Ids",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ORCIDs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Orgs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Name Preferred",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Funding Text",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cited References",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Cited Reference Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Times Cited, WoS Core",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Times Cited, All Databases",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "180 Day Usage Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Since 2013 Usage Count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher City",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publisher Address",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ISSN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eISSN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ISBN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Journal Abbreviation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Journal ISO Abbreviation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication Year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Issue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Part Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Supplement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Special Issue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Meeting Abstract",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Start Page",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "End Page",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Article Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOI Link",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Book DOI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Early Access Date",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number of Pages",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WoS Categories",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Web of Science Index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research Areas",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "IDS Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pubmed Id",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Open Access Designations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Highly Cited Status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Hot Paper Status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Date of Export",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "UT (Unique WOS ID)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Web of Science Record",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "norm_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "paperId",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "citationCount",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7dabac1e-7039-4a7f-83cb-5c2781c819df",
       "rows": [
        [
         "0",
         "C",
         "Kaiser, C; Kaiser, J; Manewitsch, V; Rau, L; Schallner, R",
         null,
         null,
         "ACM",
         "Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vladimir; Rau, Lea; Schallner, Rene",
         null,
         null,
         "Simulating Human Opinions with Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "simulating human opinions with large language models",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "J",
         "Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunner, A",
         null,
         null,
         null,
         "Ferreira, Gregorio; Amidei, Jacopo; Nieto, Ruben; Kaltenbrunner, Andreas",
         null,
         null,
         "How Well Do Simulated Population Samples with GPT-4 Align with Real Ones? The Case of the Eysenck Personality Questionnaire Revised-Abbreviated Personality Test",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "how well do simulated population samples with gpt 4 align with real ones the case of the eysenck personality questionnaire revised abbreviated personality test",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "C",
         "Kane, D; Parke, J; Jo, Y; Bak, J",
         null,
         "Bouamor, H; Pino, J; Bali, K",
         null,
         "Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak, JinYeong",
         null,
         null,
         "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "from values to opinions predicting human behaviors and stances using value injected large language models",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "J",
         "Arora, N; Chakraborty, I; Nishimura, Y",
         null,
         null,
         null,
         "Arora, Neeraj; Chakraborty, Ishita; Nishimura, Yohei",
         null,
         null,
         "AI-Human Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "ai human hybrids for marketing research leveraging large language models llms as collaborators",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "J",
         "Antal, M; Beder, N",
         null,
         null,
         null,
         "Antal, Margit; Beder, Norbert",
         null,
         null,
         "Eysenck Personality Questionnaire: A Comparative Study of Humans and Large Language Models Through Repeated Administrations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "eysenck personality questionnaire a comparative study of humans and large language models through repeated administrations",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "J",
         "Bisbee, J; Clinton, JD; Dorff, C; Kenkel, B; Larson, JM",
         null,
         null,
         null,
         "Bisbee, James; Clinton, Joshua D.; Dorff, Cassy; Kenkel, Brenton; Larson, Jennifer M.",
         null,
         null,
         "Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "synthetic replacements for human survey data the perils of large language models",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "J",
         "Liu, HJ; Cao, Y; Wu, X; Qiu, C; Gu, JG; Liu, MF; Hershcovich, D",
         null,
         null,
         null,
         "Liu, Haijiang; Cao, Yong; Wu, Xun; Qiu, Chen; Gu, Jinguang; Liu, Maofu; Hershcovich, Daniel",
         null,
         null,
         "Towards realistic evaluation of cultural value alignment in large language models: Diversity enhancement for survey response simulation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "towards realistic evaluation of cultural value alignment in large language models diversity enhancement for survey response simulation",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "J",
         "Lee, SG; Peng, TQ; Goldberg, MH; Rosenthal, SA; Kotcher, JE; Maibach, EW; Leiserowitz, A",
         null,
         null,
         null,
         "Lee, Sanguk; Peng, Tai-Quan; Goldberg, Matthew H.; Rosenthal, Seth A.; Kotcher, John E.; Maibach, Edward W.; Leiserowitz, Anthony",
         null,
         null,
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "can large language models estimate public opinion about global warming an empirical assessment of algorithmic fidelity and bias",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "J",
         "Boelaert, J; Coavoux, S; Ollion, E; Petev, I; Präg, P",
         null,
         null,
         null,
         "Boelaert, Julien; Coavoux, Samuel; Ollion, Etienne; Petev, Ivaylo; Prag, Patrick",
         null,
         null,
         "Machine Bias. How Do Generative Language Models Answer Opinion Polls?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "machine bias how do generative language models answer opinion polls",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "J",
         "Qu, Y; Wang, J",
         null,
         null,
         null,
         "Qu, Yao; Wang, Jue",
         null,
         null,
         "Performance and biases of Large Language Models in public opinion simulation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "performance and biases of large language models in public opinion simulation",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "C",
         "Gao, S; Gao, BL; Wei, P; Guo, JP; Yuan, M; Han, C; Xu, YY",
         null,
         "Xiao, X; Yao, J",
         null,
         "Gao, Song; Gao, Bolin; Wei, Peng; Guo, Jianpeng; Yuan, Meng; Han, Cheng; Xu, Yueyun",
         null,
         null,
         "Application of foundation models for autonomous driving: a survey of data synthesis",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "application of foundation models for autonomous driving a survey of data synthesis",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "C",
         "Nguyen, H; Nguyen, V; López-Fierro, S; Ludovise, S; Santagata, R",
         null,
         null,
         "ASSOC COMPUTING MACHINERY",
         "Ha Nguyen; Nguyen, Victoria; Lopez-Fierro, Sariah; Ludovise, Sara; Santagata, Rossella",
         null,
         null,
         "Simulating Climate Change Discussion with Large Language Models: Considerations for Science Communication at Scale",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "simulating climate change discussion with large language models considerations for science communication at scale",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "J",
         "Alonso, SLN; Ozili, PK; Hernández, BMS; Pacheco, LM",
         null,
         null,
         null,
         "Alonso, Sergio Luis Nanez; Ozili, Peterson K.; Hernandez, Beatriz Maria Sastre; Pacheco, Luis Miguel",
         null,
         null,
         "Evaluating the acceptance of CBDCs: experimental research with artificial intelligence (AI) generated synthetic response",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "evaluating the acceptance of cbdcs experimental research with artificial intelligence ai generated synthetic response",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "J",
         "Salecha, A; Ireland, ME; Subrahmanya, S; Sedoc, J; Ungar, LH; Eichstaedt, JC",
         null,
         null,
         null,
         "Salecha, Aadesh; Ireland, Molly E.; Subrahmanya, Shashanka; Sedoc, Joao; Ungar, Lyle H.; Eichstaedt, Johannes C.",
         null,
         null,
         "Large language models display human-like social desirability biases in Big Five personality surveys",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "large language models display human like social desirability biases in big five personality surveys",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "J",
         "Lehtonen, E; Buder-Gröndahl, T; Nordhoff, S",
         null,
         null,
         null,
         "Lehtonen, Esko; Buder-Grondahl, Tommi; Nordhoff, Sina",
         null,
         null,
         "Revealing the Influence of Semantic Similarity on Survey Responses: A Synthetic Data Generation Approach",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "revealing the influence of semantic similarity on survey responses a synthetic data generation approach",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "J",
         "Yao, JC; Zhang, HJ; Ou, J; Zuo, DY; Yang, Z; Dong, ZC",
         null,
         null,
         null,
         "Yao, Junchi; Zhang, Hongjie; Ou, Jie; Zuo, Dingyi; Yang, Zheng; Dong, Zhicheng",
         null,
         null,
         "Social opinions prediction utilizes fusing dynamics equation with LLM-based agents",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "social opinions prediction utilizes fusing dynamics equation with llm based agents",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "C",
         "Hämäläinen, P; Tavast, M; Kunnari, A",
         null,
         null,
         "ACM",
         "Hamalainen, Perttu; Tavast, Mikke; Kunnari, Anton",
         null,
         null,
         "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "evaluating large language models in generating synthetic hci research data a case study",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "J",
         "Zhang, S; Xu, J; Alvero, AJ",
         null,
         null,
         null,
         "Zhang, Simone; Xu, Janet; Alvero, A. J.",
         null,
         null,
         "Generative AI Meets Open-Ended Survey Responses: Research Participant Use of AI and Homogenization",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "generative ai meets open ended survey responses research participant use of ai and homogenization",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "J",
         "Zhang, BY; Chen, T; Wang, X; Li, Q; Zhang, WS; Wang, FY",
         null,
         null,
         null,
         "Zhang, Baoyu; Chen, Tao; Wang, Xiao; Li, Qiang; Zhang, Weishan; Wang, Fei-Yue",
         null,
         null,
         "Decoding Activist Public Opinion in Decentralized Self-Organized Protests Using LLM",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "decoding activist public opinion in decentralized self organized protests using llm",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "J",
         "von der Heyde, L; Haensch, AC; Wenz, A",
         null,
         null,
         null,
         "von der Heyde, Leah; Haensch, Anna-Carolina; Wenz, Alexander",
         null,
         null,
         "Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "vox populi vox ai using large language models to estimate german vote choice",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "J",
         "Gao, C; Lan, XC; Li, N; Yuan, Y; Ding, JT; Zhou, ZL; Xu, FL; Li, Y",
         null,
         null,
         null,
         "Gao, Chen; Lan, Xiaochong; Li, Nian; Yuan, Yuan; Ding, Jingtao; Zhou, Zhilun; Xu, Fengli; Li, Yong",
         null,
         null,
         "Large language models empowered agent-based modeling and simulation: a survey and perspectives",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "large language models empowered agent based modeling and simulation a survey and perspectives",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "C",
         "Tan, Z; Li, DW; Wang, S; Beigi, A; Jiang, BH; Bhattacharjee, A; Karami, M; Li, JD; Cheng, L; Liu, H",
         null,
         "Al-Onaizan, Y; Bansal, M; Chen, YN",
         null,
         "Tan, Zhen; Li, Dawei; Wang, Song; Beigi, Alimohammad; Jiang, Bohan; Bhattacharjee, Amrita; Karami, Mansooreh; Li, Jundong; Cheng, Lu; Liu, Huan",
         null,
         null,
         "Large Language Models for Data Annotation and Synthesis: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "large language models for data annotation and synthesis a survey",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "J",
         "Jung, SG; Salminen, J; Aldous, KK; Jansen, BJ",
         null,
         null,
         null,
         "Jung, Soon-Gyo; Salminen, Joni; Aldous, Kholoud Khalil; Jansen, Bernard J.",
         null,
         null,
         "PersonaCraft: Leveraging language models for data-driven persona development",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "personacraft leveraging language models for data driven persona development",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "J",
         "Cho, S; Kim, J; Kim, JH",
         null,
         null,
         null,
         "Cho, Suhyun; Kim, Jaeyun; Kim, Jang Hyun",
         null,
         null,
         "LLM-Based Doppelganger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "llm based doppelganger models leveraging synthetic data for human like responses in survey simulations",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "C",
         "Kaur, A; Aird, A; Borman, H; Nicastro, A; Leontjeva, A; Pizzato, L; Jermyn, D",
         null,
         null,
         "ACM",
         "Kaur, Arshnoor; Aird, Amanda; Borman, Harris; Nicastro, Andrea; Leontjeva, Anna; Pizzato, Luiz; Jermyn, Dan",
         null,
         null,
         "Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People's FinancialWellbeing",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "synthetic voices evaluating the fidelity of llm generated personas in representing people s financialwellbeing",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "J",
         "Ding, GZ; Liu, ZR; Li, S; Cao, J; Ye, ZH",
         null,
         null,
         null,
         "Ding, Guozhu; Liu, Zuer; Li, Shan; Cao, Jie; Ye, Zhuohai",
         null,
         null,
         "Impact of mindset types and social community compositions on opinion dynamics: A large language model-based multi-agent simulation study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "impact of mindset types and social community compositions on opinion dynamics a large language model based multi agent simulation study",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "C",
         "AlKhamissi, B; ElNokrashy, M; AlKhamissi, M; Diab, M",
         null,
         "Ku, LW; Martins, A; Srikumar, V",
         null,
         "AlKhamissi, Badr; ElNokrashy, Muhammad; AlKhamissi, Mai; Diab, Mona",
         null,
         null,
         "Investigating Cultural Alignment of Large Language Models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "investigating cultural alignment of large language models",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "J",
         "Zhao, XJ; Wang, H; Dai, CX; Tang, JC; Deng, KX; Zhong, ZH; Kong, FY; Wang, SY; Morikawa, S",
         null,
         null,
         null,
         "Zhao, Xinjie; Wang, Hao; Dai, Chengxiao; Tang, Jiacheng; Deng, Kaixin; Zhong, Zhihua; Kong, Fanying; Wang, Shiyun; Morikawa, So",
         null,
         null,
         "Multi-Stage Simulation of Residents' Disaster Risk Perception and Decision-Making Behavior: An Exploratory Study on Large Language Model-Driven Social-Cognitive Agent Framework",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "multi stage simulation of residents disaster risk perception and decision making behavior an exploratory study on large language model driven social cognitive agent framework",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "J",
         "de Winter, JCF; Driessen, T; Dodou, D",
         null,
         null,
         null,
         "de Winter, Joost C. F.; Driessen, Tom; Dodou, Dimitra",
         null,
         null,
         "The use of ChatGPT for personality research: Administering questionnaires using generated personas",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "the use of chatgpt for personality research administering questionnaires using generated personas",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "C",
         "Brasoveanu, AMP; Scharl, A; Nixon, LJB; Andonie, R",
         null,
         "Banissi, E; Datia, N; Pires, JM; Ursyn, A; Nazemi, K; Kovalerchuk, B; Andonie, R; Gavrilova, M; Nakayama, M; Nguyen, QV; Mabakane, MS; Rusu, A; Sciarrone, F; Temperini, M; Bouali, F; Venturini, G; Huang, T",
         null,
         "Brasoveanu, Adrian M. P.; Scharl, Arno; Nixon, Lyndon J. B.; Andonie, Razvan",
         null,
         null,
         "Visualizing Large Language Models: A Brief Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "visualizing large language models a brief survey",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "J",
         "Rakovics, Z; Rakovics, M",
         null,
         null,
         null,
         "Rakovics, Zsofia; Rakovics, Marton",
         null,
         null,
         "Exploring the potential and limitations of large language models as virtual respondents for social science research",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "exploring the potential and limitations of large language models as virtual respondents for social science research",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "C",
         "Omata, M; Shimizu, A",
         null,
         "Ardito, C; Lanzilotti, R; Malizia, A; Petrie, H; Piccinno, A; Desolda, G; Inkpen, K",
         null,
         "Omata, Masaki; Shimizu, Atsuki",
         null,
         null,
         "A Proposal for Discreet Auxiliary Figures for Reducing VR Sickness and for Not Obstructing FOV",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "a proposal for discreet auxiliary figures for reducing vr sickness and for not obstructing fov",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "J",
         "Leung, HW; Bovy, J",
         null,
         null,
         null,
         "Leung, Henry W.; Bovy, Jo",
         null,
         null,
         "Towards an astronomical foundation model for stars with a transformer-based model",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "towards an astronomical foundation model for stars with a transformer based model",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "33",
         "J",
         "Mburu, TK; Rong, KX; McColley, CJ; Werth, A",
         null,
         null,
         null,
         "Mburu, Ted K.; Rong, Kangxuan; McColley, Campbell J.; Werth, Alexandra",
         null,
         null,
         "Methodological foundations for artificial intelligence-driven survey question generation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "methodological foundations for artificial intelligence driven survey question generation",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "34",
         "C",
         "Li, YH; Wang, SF; Ding, H; Chen, H",
         null,
         null,
         "ACM",
         "Li, Yinheng; Wang, Shaofei; Ding, Han; Chen, Hang",
         null,
         null,
         "Large Language Models in Finance: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "large language models in finance a survey",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "J",
         "Timothy, TR",
         null,
         null,
         null,
         "Timothy, Tyrese Raku",
         null,
         null,
         "AI-driven fabrication of healthcare survey data: methods, motivations, and ethical implications",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "ai driven fabrication of healthcare survey data methods motivations and ethical implications",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "J",
         "Zhang, KH; Dong, CQ; Guo, YF; Zhou, W; Yu, G; Mi, JN",
         null,
         null,
         null,
         "Zhang, Kaihang; Dong, Changqi; Guo, Yifeng; Zhou, Wuai; Yu, Guang; Mi, Jianing",
         null,
         null,
         "Lagged Stance Interactions and Counter-Spiral of Silence: A Data-Driven Analysis and Agent-Based Modeling of Technical Public Opinion Events",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "lagged stance interactions and counter spiral of silence a data driven analysis and agent based modeling of technical public opinion events",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "C",
         "Zhao, XX; Qiu, Y",
         null,
         "Rau, PLP",
         null,
         "Zhao, Xiaoxuan; Qiu, Yue",
         null,
         null,
         "Insight Through Dialogue: A Practical Exploration of AIGC in Cross-cultural Design Research",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "insight through dialogue a practical exploration of aigc in cross cultural design research",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "J",
         "Zhang, ZC; Wu, HN; Zhang, EL; Zhai, GT; Lin, WS",
         null,
         null,
         null,
         "Zhang, Zicheng; Wu, Haoning; Zhang, Erli; Zhai, Guangtao; Lin, Weisi",
         null,
         null,
         "Q-BENCH+: A Benchmark for Multi-Modal Foundation Models on Low-Level Vision From Single Images to Pairs",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "q bench a benchmark for multi modal foundation models on low level vision from single images to pairs",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "J",
         "Campos, M; Farinhas, A; Zerva, C; Figueiredo, MAT; Martins, AFT",
         null,
         null,
         null,
         "Campos, Margarida; Farinhas, Antonio; Zerva, Chrysoula; Figueiredo, Mario A. T.; Martins, Andre F. T.",
         null,
         null,
         "Conformal Prediction for Natural Language Processing: A Survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "conformal prediction for natural language processing a survey",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "J",
         "Wilden, J; Riley, RH",
         null,
         null,
         null,
         "Wilden, J; Riley, RH",
         null,
         null,
         "Personal digital assistant (PDA) use amongst anaesthetists: An Australian survey",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "personal digital assistant pda use amongst anaesthetists an australian survey",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "J",
         "Zheng, JY; Wang, X; Hosio, S; Xu, XX; Lee, LH",
         null,
         null,
         null,
         "Zheng, Jingyao; Wang, Xian; Hosio, Simo; Xu, Xiaoxian; Lee, Lik-Hang",
         null,
         null,
         "LMLPA: Language Model Linguistic Personality Assessment",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "lmlpa language model linguistic personality assessment",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "C",
         "Cheng, M; Piccardi, T; Yang, DY",
         null,
         "Bouamor, H; Pino, J; Bali, K",
         null,
         "Cheng, Myra; Piccardi, Tiziano; Yang, Diyi",
         null,
         null,
         "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "compost characterizing and evaluating caricature in llm simulations",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "C",
         "Dobre, SC; Popescu, E",
         null,
         null,
         "IEEE",
         "Dobre, Stefania-Carmen; Popescu, Elvira",
         null,
         null,
         "Exploring Students' Perception and Experience with ChatGPT and Critical Thinking in a Higher Education Context",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "exploring students perception and experience with chatgpt and critical thinking in a higher education context",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "J",
         "Kaur, A; Budko, A; Liu, K; Eaton, E; Steitz, BD; Johnson, KB",
         null,
         null,
         null,
         "Kaur, Amarpreet; Budko, Alexander; Liu, Katrina; Eaton, Eric; Steitz, Bryan D.; Johnson, Kevin B.",
         null,
         null,
         "Automating Responses to Patient Portal Messages Using Generative AI",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "automating responses to patient portal messages using generative ai",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "45",
         "J",
         "Ji, J; Kim, J; Kim, Y",
         null,
         null,
         null,
         "Ji, Junyung; Kim, Jiwoo; Kim, Younghoon",
         null,
         null,
         "Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "predicting missing values in survey data using prompt engineering for addressing item non response",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "J",
         "Cisar, P",
         null,
         null,
         null,
         "Cisar, Peter",
         null,
         null,
         "The Place and Role of Honeypot Solutions in Network Intrusion Detection Systems",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "the place and role of honeypot solutions in network intrusion detection systems",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "J",
         "Goli, A; Singh, A",
         null,
         null,
         null,
         "Goli, Ali; Singh, Amandeep",
         null,
         null,
         "Frontiers: Can Large Language Models Capture Human Preferences?",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "frontiers can large language models capture human preferences",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "C",
         "Zhao, RB; Xie, ZW; Zhuang, YP; Li, HX; Yu, PLH",
         null,
         "Kashihara, A; Jiang, B; Rodrigo, MM; Sugay, JO",
         null,
         "Zhao, Ruibin; Xie, Zhiwei; Zhuang, Yipeng; Li, Huixian; Yu, Philip L. H.",
         null,
         null,
         "Enhancing Language Learning Through Multimodal AI-Driven Feedback on Picture Descriptions: An Eye-Tracking Study",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "enhancing language learning through multimodal ai driven feedback on picture descriptions an eye tracking study",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "J",
         "Hur, JK; Heffner, J; Feng, GW; Joormann, J; Rutledge, RB",
         null,
         null,
         null,
         "Hur, Jihyun K.; Heffner, Joseph; Feng, Gloria W.; Joormann, Jutta; Rutledge, Robb B.",
         null,
         null,
         "Language sentiment predicts changes in depressive symptoms",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "language sentiment predicts changes in depressive symptoms",
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 79,
        "rows": 2919
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Book Authors</th>\n",
       "      <th>Book Editors</th>\n",
       "      <th>Book Group Authors</th>\n",
       "      <th>Author Full Names</th>\n",
       "      <th>Book Author Full Names</th>\n",
       "      <th>Group Authors</th>\n",
       "      <th>title</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Date of Export</th>\n",
       "      <th>UT (Unique WOS ID)</th>\n",
       "      <th>Web of Science Record</th>\n",
       "      <th>norm_title</th>\n",
       "      <th>paperId</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>citationCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>Kaiser, C; Kaiser, J; Manewitsch, V; Rau, L; S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACM</td>\n",
       "      <td>Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simulating Human Opinions with Large Language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>simulating human opinions with large language ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Well Do Simulated Population Samples with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>how well do simulated population samples with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>Kane, D; Parke, J; Jo, Y; Bak, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bouamor, H; Pino, J; Bali, K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>From Values to Opinions: Predicting Human Beha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>from values to opinions predicting human behav...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>Arora, N; Chakraborty, I; Nishimura, Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arora, Neeraj; Chakraborty, Ishita; Nishimura,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AI-Human Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ai human hybrids for marketing research levera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J</td>\n",
       "      <td>Antal, M; Beder, N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antal, Margit; Beder, Norbert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eysenck Personality Questionnaire: A Comparati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>eysenck personality questionnaire a comparativ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generating Interpretations of Policy Announcem...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generating interpretations of policy announcem...</td>\n",
       "      <td>00837a426339a384df537eaaac69e52480c8e8b5</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Andreas Marfurt, Ashley Thornton, David Sylvan...</td>\n",
       "      <td>Recent advances in language modeling have focu...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/00837a42...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Demystifying diagnosis: an efficient deep lear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>demystifying diagnosis an efficient deep learn...</td>\n",
       "      <td>0081eedf01655a7c541e52c8fb6a04b8da18e9f4</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>Ahmed Alzahrani, Muhammad Ali Raza, Muhammad Z...</td>\n",
       "      <td>As per a WHO survey conducted in 2023, more th...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0081eedf...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usability Testing of ChatGPT Website as a Medi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>usability testing of chatgpt website as a medi...</td>\n",
       "      <td>00798a978fa3f62624668109bb414bb4add1ff32</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Harry Ma'ruf, Bayu Rima Aditya, Elis Hernawati...</td>\n",
       "      <td>This study aims to determine the level of usab...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/00798a97...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Artificial Intelligence for Urban Safety: A Ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artificial intelligence for urban safety a cas...</td>\n",
       "      <td>0043df60e07f3c5f6d8aece33aa999f036c35c00</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>Alessandro Marceddu, Massimo Miccoli, Alessand...</td>\n",
       "      <td>Abstract. This study explores the application ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0043df60...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Future Trends in AI: Data Management and Analy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>future trends in ai data management and analys...</td>\n",
       "      <td>003f273d1bda250dbd24b4dcf043dfabf558dfc9</td>\n",
       "      <td>2025.0</td>\n",
       "      <td></td>\n",
       "      <td>Introduction: This study offers a thorough ana...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/003f273d...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Publication Type                                            Authors  \\\n",
       "0                   C  Kaiser, C; Kaiser, J; Manewitsch, V; Rau, L; S...   \n",
       "1                   J  Ferreira, G; Amidei, J; Nieto, R; Kaltenbrunne...   \n",
       "2                   C                   Kane, D; Parke, J; Jo, Y; Bak, J   \n",
       "3                   J             Arora, N; Chakraborty, I; Nishimura, Y   \n",
       "4                   J                                 Antal, M; Beder, N   \n",
       "...               ...                                                ...   \n",
       "2914              NaN                                                NaN   \n",
       "2915              NaN                                                NaN   \n",
       "2916              NaN                                                NaN   \n",
       "2917              NaN                                                NaN   \n",
       "2918              NaN                                                NaN   \n",
       "\n",
       "      Book Authors                  Book Editors Book Group Authors  \\\n",
       "0              NaN                           NaN                ACM   \n",
       "1              NaN                           NaN                NaN   \n",
       "2              NaN  Bouamor, H; Pino, J; Bali, K                NaN   \n",
       "3              NaN                           NaN                NaN   \n",
       "4              NaN                           NaN                NaN   \n",
       "...            ...                           ...                ...   \n",
       "2914           NaN                           NaN                NaN   \n",
       "2915           NaN                           NaN                NaN   \n",
       "2916           NaN                           NaN                NaN   \n",
       "2917           NaN                           NaN                NaN   \n",
       "2918           NaN                           NaN                NaN   \n",
       "\n",
       "                                      Author Full Names  \\\n",
       "0     Kaiser, Carolin; Kaiser, Jakob; Manewitsch, Vl...   \n",
       "1     Ferreira, Gregorio; Amidei, Jacopo; Nieto, Rub...   \n",
       "2     Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...   \n",
       "3     Arora, Neeraj; Chakraborty, Ishita; Nishimura,...   \n",
       "4                         Antal, Margit; Beder, Norbert   \n",
       "...                                                 ...   \n",
       "2914                                                NaN   \n",
       "2915                                                NaN   \n",
       "2916                                                NaN   \n",
       "2917                                                NaN   \n",
       "2918                                                NaN   \n",
       "\n",
       "      Book Author Full Names  Group Authors  \\\n",
       "0                        NaN            NaN   \n",
       "1                        NaN            NaN   \n",
       "2                        NaN            NaN   \n",
       "3                        NaN            NaN   \n",
       "4                        NaN            NaN   \n",
       "...                      ...            ...   \n",
       "2914                     NaN            NaN   \n",
       "2915                     NaN            NaN   \n",
       "2916                     NaN            NaN   \n",
       "2917                     NaN            NaN   \n",
       "2918                     NaN            NaN   \n",
       "\n",
       "                                                  title  Source Title  ...  \\\n",
       "0     Simulating Human Opinions with Large Language ...           NaN  ...   \n",
       "1     How Well Do Simulated Population Samples with ...           NaN  ...   \n",
       "2     From Values to Opinions: Predicting Human Beha...           NaN  ...   \n",
       "3     AI-Human Hybrids for Marketing Research: Lever...           NaN  ...   \n",
       "4     Eysenck Personality Questionnaire: A Comparati...           NaN  ...   \n",
       "...                                                 ...           ...  ...   \n",
       "2914  Generating Interpretations of Policy Announcem...           NaN  ...   \n",
       "2915  Demystifying diagnosis: an efficient deep lear...           NaN  ...   \n",
       "2916  Usability Testing of ChatGPT Website as a Medi...           NaN  ...   \n",
       "2917  Artificial Intelligence for Urban Safety: A Ca...           NaN  ...   \n",
       "2918  Future Trends in AI: Data Management and Analy...           NaN  ...   \n",
       "\n",
       "      Date of Export  UT (Unique WOS ID)  Web of Science Record  \\\n",
       "0                NaN                 NaN                    0.0   \n",
       "1                NaN                 NaN                    0.0   \n",
       "2                NaN                 NaN                    0.0   \n",
       "3                NaN                 NaN                    0.0   \n",
       "4                NaN                 NaN                    0.0   \n",
       "...              ...                 ...                    ...   \n",
       "2914             NaN                 NaN                    NaN   \n",
       "2915             NaN                 NaN                    NaN   \n",
       "2916             NaN                 NaN                    NaN   \n",
       "2917             NaN                 NaN                    NaN   \n",
       "2918             NaN                 NaN                    NaN   \n",
       "\n",
       "                                             norm_title  \\\n",
       "0     simulating human opinions with large language ...   \n",
       "1     how well do simulated population samples with ...   \n",
       "2     from values to opinions predicting human behav...   \n",
       "3     ai human hybrids for marketing research levera...   \n",
       "4     eysenck personality questionnaire a comparativ...   \n",
       "...                                                 ...   \n",
       "2914  generating interpretations of policy announcem...   \n",
       "2915  demystifying diagnosis an efficient deep learn...   \n",
       "2916  usability testing of chatgpt website as a medi...   \n",
       "2917  artificial intelligence for urban safety a cas...   \n",
       "2918  future trends in ai data management and analys...   \n",
       "\n",
       "                                       paperId    year  \\\n",
       "0                                          NaN     NaN   \n",
       "1                                          NaN     NaN   \n",
       "2                                          NaN     NaN   \n",
       "3                                          NaN     NaN   \n",
       "4                                          NaN     NaN   \n",
       "...                                        ...     ...   \n",
       "2914  00837a426339a384df537eaaac69e52480c8e8b5  2024.0   \n",
       "2915  0081eedf01655a7c541e52c8fb6a04b8da18e9f4  2025.0   \n",
       "2916  00798a978fa3f62624668109bb414bb4add1ff32  2023.0   \n",
       "2917  0043df60e07f3c5f6d8aece33aa999f036c35c00  2024.0   \n",
       "2918  003f273d1bda250dbd24b4dcf043dfabf558dfc9  2025.0   \n",
       "\n",
       "                                                authors  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "2914  Andreas Marfurt, Ashley Thornton, David Sylvan...   \n",
       "2915  Ahmed Alzahrani, Muhammad Ali Raza, Muhammad Z...   \n",
       "2916  Harry Ma'ruf, Bayu Rima Aditya, Elis Hernawati...   \n",
       "2917  Alessandro Marceddu, Massimo Miccoli, Alessand...   \n",
       "2918                                                      \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "2914  Recent advances in language modeling have focu...   \n",
       "2915  As per a WHO survey conducted in 2023, more th...   \n",
       "2916  This study aims to determine the level of usab...   \n",
       "2917  Abstract. This study explores the application ...   \n",
       "2918  Introduction: This study offers a thorough ana...   \n",
       "\n",
       "                                                    url  citationCount  \n",
       "0                                                   NaN            NaN  \n",
       "1                                                   NaN            NaN  \n",
       "2                                                   NaN            NaN  \n",
       "3                                                   NaN            NaN  \n",
       "4                                                   NaN            NaN  \n",
       "...                                                 ...            ...  \n",
       "2914  https://www.semanticscholar.org/paper/00837a42...            0.0  \n",
       "2915  https://www.semanticscholar.org/paper/0081eedf...            0.0  \n",
       "2916  https://www.semanticscholar.org/paper/00798a97...            0.0  \n",
       "2917  https://www.semanticscholar.org/paper/0043df60...            0.0  \n",
       "2918  https://www.semanticscholar.org/paper/003f273d...            0.0  \n",
       "\n",
       "[2919 rows x 79 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new dataframe that combines papers from both df_results_wos and df_SS_g7 but remove duplicates based on 'norm_title' column\n",
    "df_combined = pd.concat([df_results_wos, df_SS_g7], ignore_index=True).drop_duplicates(subset=['norm_title']).reset_index(drop=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f42c3",
   "metadata": {},
   "source": [
    "# Screening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a17eeaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1c2aeadf-e07b-4ef4-9aea-75ae7b3271fa",
       "rows": [
        [
         "2000",
         "6"
        ],
        [
         "2001",
         "13"
        ],
        [
         "2002",
         "4"
        ],
        [
         "2003",
         "17"
        ],
        [
         "2004",
         "14"
        ],
        [
         "2005",
         "17"
        ],
        [
         "2006",
         "17"
        ],
        [
         "2007",
         "20"
        ],
        [
         "2008",
         "23"
        ],
        [
         "2009",
         "16"
        ],
        [
         "2010",
         "26"
        ],
        [
         "2011",
         "40"
        ],
        [
         "2012",
         "45"
        ],
        [
         "2013",
         "24"
        ],
        [
         "2014",
         "38"
        ],
        [
         "2015",
         "39"
        ],
        [
         "2016",
         "43"
        ],
        [
         "2017",
         "44"
        ],
        [
         "2018",
         "57"
        ],
        [
         "2019",
         "63"
        ],
        [
         "2020",
         "56"
        ],
        [
         "2021",
         "84"
        ],
        [
         "2022",
         "93"
        ],
        [
         "2023",
         "109"
        ],
        [
         "2024",
         "299"
        ],
        [
         "2025",
         "408"
        ],
        [
         "2026",
         "4"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 27
       }
      },
      "text/plain": [
       "Year\n",
       "2000      6\n",
       "2001     13\n",
       "2002      4\n",
       "2003     17\n",
       "2004     14\n",
       "2005     17\n",
       "2006     17\n",
       "2007     20\n",
       "2008     23\n",
       "2009     16\n",
       "2010     26\n",
       "2011     40\n",
       "2012     45\n",
       "2013     24\n",
       "2014     38\n",
       "2015     39\n",
       "2016     43\n",
       "2017     44\n",
       "2018     57\n",
       "2019     63\n",
       "2020     56\n",
       "2021     84\n",
       "2022     93\n",
       "2023    109\n",
       "2024    299\n",
       "2025    408\n",
       "2026      4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "359ebdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_WoS_final = df_WoS_LLM_and_Survey_and_SimulationB\n",
    "df_SS_final = df_SS_llm_survey_simB\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 7), sharex=True)\n",
    "\n",
    "sns.histplot(df_WoS_final['Year'], color='blue', ax=axes[0], \n",
    "             kde=False, stat='count', bins=range(2020, 2027), alpha=0.7)\n",
    "axes[0].set_title('Distribution of Publication Year (2020 to 2025)')\n",
    "axes[0].set_ylabel('Number of Papers')\n",
    "axes[0].legend(['Web of Science'])\n",
    "\n",
    "sns.histplot(df_SS_final['year'], color='orange', ax=axes[1], \n",
    "             kde=False, stat='count', bins=range(2020, 2027), alpha=0.7)\n",
    "axes[1].set_title('')\n",
    "axes[1].set_xlabel('Publication Year')\n",
    "axes[1].set_ylabel('Number of Papers')\n",
    "axes[1].legend(['Semantic Scholar'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429e31b",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "518f4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'authors', 'doi', 'issn', 'isbn', 'Year', 'keywords',\n",
      "       'sourceType'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "issn",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "isbn",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sourceType",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "dd17eb79-71a7-48a0-9e64-55ceac22585c",
       "rows": [
        [
         "0",
         "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
         "Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak, JinYeong",
         null,
         null,
         null,
         "2023",
         "",
         "Proceedings Paper"
        ],
        [
         "1",
         "Simulating Public Opinion: Comparing Distributional and Individual-Level Predictions from LLMs and Random Forests",
         "Miranda, Fernando; Balbi, Pedro Paulo",
         "10.3390/e27090923",
         null,
         null,
         "2025",
         "LLM; random forests; Jensen-Shannon divergence; public opinion simulation; social sciences",
         "Article"
        ],
        [
         "2",
         "Predictive value of PET-CT imaging versus AGO-scoring in patients planned for cytoreductive surgery in recurrent ovarian cancer",
         "Lenhard, S. M.; Burges, A.; Johnson, T. R. C.; Kirschenhofer, A.; Bruns, C.; Linke, R.; Friese, K.",
         "10.1016/j.ejogrb.2008.05.006",
         "0301-2115",
         null,
         "2008",
         "Ovarian cancer; Relapse; Positron emission tomography computed tomography; Tumor marker; Cytoreductive surgery",
         "Article"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>doi</th>\n",
       "      <th>issn</th>\n",
       "      <th>isbn</th>\n",
       "      <th>Year</th>\n",
       "      <th>keywords</th>\n",
       "      <th>sourceType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Values to Opinions: Predicting Human Beha...</td>\n",
       "      <td>Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>Proceedings Paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simulating Public Opinion: Comparing Distribut...</td>\n",
       "      <td>Miranda, Fernando; Balbi, Pedro Paulo</td>\n",
       "      <td>10.3390/e27090923</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025</td>\n",
       "      <td>LLM; random forests; Jensen-Shannon divergence...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Predictive value of PET-CT imaging versus AGO-...</td>\n",
       "      <td>Lenhard, S. M.; Burges, A.; Johnson, T. R. C.;...</td>\n",
       "      <td>10.1016/j.ejogrb.2008.05.006</td>\n",
       "      <td>0301-2115</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Ovarian cancer; Relapse; Positron emission tom...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  From Values to Opinions: Predicting Human Beha...   \n",
       "1  Simulating Public Opinion: Comparing Distribut...   \n",
       "2  Predictive value of PET-CT imaging versus AGO-...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Kane, Dongjun; Parke, JoonSuk; Jo, Yohan; Bak,...   \n",
       "1              Miranda, Fernando; Balbi, Pedro Paulo   \n",
       "2  Lenhard, S. M.; Burges, A.; Johnson, T. R. C.;...   \n",
       "\n",
       "                            doi       issn  isbn  Year  \\\n",
       "0                          None       None  None  2023   \n",
       "1             10.3390/e27090923       None  None  2025   \n",
       "2  10.1016/j.ejogrb.2008.05.006  0301-2115  None  2008   \n",
       "\n",
       "                                            keywords         sourceType  \n",
       "0                                                     Proceedings Paper  \n",
       "1  LLM; random forests; Jensen-Shannon divergence...            Article  \n",
       "2  Ovarian cancer; Relapse; Positron emission tom...            Article  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the columns of df_WoS_LLM_and_Survey_and_SimulationB\n",
    "print(df_WoS_LLM_and_Survey_and_SimulationB.columns)\n",
    "df_WoS_LLM_and_Survey_and_SimulationB.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c8bffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['paperId', 'title', 'year', 'authors', 'abstract', 'url',\n",
      "       'citationCount', 'doi'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paperId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citationCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "3561cfad-9dc0-449b-8099-c7cf79c789e9",
       "rows": [
        [
         "0",
         "d40c77c010c8dbef6142903a02f2a73a85012d5d",
         "A Survey on Vision Transformer",
         "2020",
         "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, D. Tao",
         "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
         "https://www.semanticscholar.org/paper/d40c77c010c8dbef6142903a02f2a73a85012d5d",
         "2580",
         "10.1109/TPAMI.2022.3152247"
        ],
        [
         "1",
         "888728745dbb769e29ed475d4f7661eebe1a71cf",
         "A Survey on Evaluation of Large Language Models",
         "2023",
         "Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, Xingxu Xie",
         "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
         "https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf",
         "2239",
         "10.1145/3641289"
        ],
        [
         "2",
         "0671fd553dd670a4e820553a974bc48040ba0819",
         "Reflexion: language agents with verbal reinforcement learning",
         "2023",
         "Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, Shunyu Yao",
         "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.",
         "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
         "1755",
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d40c77c010c8dbef6142903a02f2a73a85012d5d</td>\n",
       "      <td>A Survey on Vision Transformer</td>\n",
       "      <td>2020</td>\n",
       "      <td>Kai Han, Yunhe Wang, Hanting Chen, Xinghao Che...</td>\n",
       "      <td>Transformer, first applied to the field of nat...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d40c77c0...</td>\n",
       "      <td>2580</td>\n",
       "      <td>10.1109/TPAMI.2022.3152247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>888728745dbb769e29ed475d4f7661eebe1a71cf</td>\n",
       "      <td>A Survey on Evaluation of Large Language Models</td>\n",
       "      <td>2023</td>\n",
       "      <td>Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, ...</td>\n",
       "      <td>Large language models (LLMs) are gaining incre...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/88872874...</td>\n",
       "      <td>2239</td>\n",
       "      <td>10.1145/3641289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0671fd553dd670a4e820553a974bc48040ba0819</td>\n",
       "      <td>Reflexion: language agents with verbal reinfor...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Noah Shinn, Federico Cassano, Beck Labash, A. ...</td>\n",
       "      <td>Large language models (LLMs) have been increas...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0671fd55...</td>\n",
       "      <td>1755</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  d40c77c010c8dbef6142903a02f2a73a85012d5d   \n",
       "1  888728745dbb769e29ed475d4f7661eebe1a71cf   \n",
       "2  0671fd553dd670a4e820553a974bc48040ba0819   \n",
       "\n",
       "                                               title  year  \\\n",
       "0                     A Survey on Vision Transformer  2020   \n",
       "1    A Survey on Evaluation of Large Language Models  2023   \n",
       "2  Reflexion: language agents with verbal reinfor...  2023   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Kai Han, Yunhe Wang, Hanting Chen, Xinghao Che...   \n",
       "1  Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, ...   \n",
       "2  Noah Shinn, Federico Cassano, Beck Labash, A. ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Transformer, first applied to the field of nat...   \n",
       "1  Large language models (LLMs) are gaining incre...   \n",
       "2  Large language models (LLMs) have been increas...   \n",
       "\n",
       "                                                 url  citationCount  \\\n",
       "0  https://www.semanticscholar.org/paper/d40c77c0...           2580   \n",
       "1  https://www.semanticscholar.org/paper/88872874...           2239   \n",
       "2  https://www.semanticscholar.org/paper/0671fd55...           1755   \n",
       "\n",
       "                          doi  \n",
       "0  10.1109/TPAMI.2022.3152247  \n",
       "1             10.1145/3641289  \n",
       "2                        None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_SS_llm_survey_simB.columns)\n",
    "df_SS_llm_survey_simB.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9442e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paperId",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "citationCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "issn",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "isbn",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sourceType",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "7ef12619-8d00-41d9-a108-d79d3c3ee821",
       "rows": [
        [
         "0",
         "d40c77c010c8dbef6142903a02f2a73a85012d5d",
         "A Survey on Vision Transformer",
         "2020.0",
         "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, D. Tao",
         "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
         "https://www.semanticscholar.org/paper/d40c77c010c8dbef6142903a02f2a73a85012d5d",
         "2580.0",
         "10.1109/TPAMI.2022.3152247",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "888728745dbb769e29ed475d4f7661eebe1a71cf",
         "A Survey on Evaluation of Large Language Models",
         "2023.0",
         "Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, Xingxu Xie",
         "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
         "https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf",
         "2239.0",
         "10.1145/3641289",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "0671fd553dd670a4e820553a974bc48040ba0819",
         "Reflexion: language agents with verbal reinforcement learning",
         "2023.0",
         "Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, Shunyu Yao",
         "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.",
         "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
         "1755.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "be8db99310602d66bba64bcf41a572c45816fbfc",
         "Let's Verify Step by Step",
         "2023.0",
         "H. Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, I. Sutskever, K. Cobbe",
         "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
         "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
         "1739.0",
         "10.48550/arXiv.2305.20050",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "f4df78183261538e718066331898ee5cad7cad05",
         "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
         "2022.0",
         "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, M. Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer",
         "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
         "https://www.semanticscholar.org/paper/f4df78183261538e718066331898ee5cad7cad05",
         "1660.0",
         "10.18653/v1/2022.emnlp-main.759",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "bd20069f5cac3e63083ecf6479abc1799db33ce0",
         "A Primer in BERTology: What We Know About How BERT Works",
         "2020.0",
         "Anna Rogers, Olga Kovaleva, Anna Rumshisky",
         "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
         "https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0",
         "1595.0",
         "10.1162/tacl_a_00349",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "2f3efe44083af91cef562c1a3451eee2f8601d22",
         "WebGPT: Browser-assisted question-answering with human feedback",
         "2021.0",
         "Reiichiro Nakano, Jacob Hilton, S. Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, W. Saunders, Xu Jiang, K. Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman",
         "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
         "https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22",
         "1459.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
         "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
         "2023.0",
         "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu",
         "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
         "https://www.semanticscholar.org/paper/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
         "1371.0",
         "10.1145/3703155",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "b37b1dc72b1882858f5120f2cd6883134089a6ed",
         "MMBench: Is Your Multi-modal Model an All-around Player?",
         "2023.0",
         "Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin",
         "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
         "https://www.semanticscholar.org/paper/b37b1dc72b1882858f5120f2cd6883134089a6ed",
         "1340.0",
         "10.48550/arXiv.2307.06281",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "002c256d30d6be4b23d365a8de8ae0e67e4c9641",
         "Improving language models by retrieving from trillions of tokens",
         "2021.0",
         "Sebastian Borgeaud, A. Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, G. Irving, O. Vinyals, Simon Osindero, K. Simonyan, Jack W. Rae, Erich Elsen, L. Sifre",
         "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
         "https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641",
         "1245.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
         "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
         "2021.0",
         "Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, D. Roth",
         "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.",
         "https://www.semanticscholar.org/paper/c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
         "1235.0",
         "10.1145/3605943",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "91deaf9d324c8feafc189da0da03e60a60287bca",
         "Code as Policies: Language Model Programs for Embodied Control",
         "2022.0",
         "Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, Andy Zeng",
         "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
         "https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca",
         "1125.0",
         "10.1109/ICRA48891.2023.10160591",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
         "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
         "2024.0",
         "Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao",
         "This work presents Depth Anything11While the grammatical soundness of this name may be questionable, we treat it as a whole and pay homage to Segment Anything [26]., a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released here.",
         "https://www.semanticscholar.org/paper/afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
         "1119.0",
         "10.1109/CVPR52733.2024.00987",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
         "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
         "2023.0",
         "Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu",
         "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia, and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and, simultaneously, leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely: 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
         "https://www.semanticscholar.org/paper/9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
         "938.0",
         "10.1109/TKDE.2024.3352100",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
         "Capabilities of GPT-4 on Medical Challenge Problems",
         "2023.0",
         "Harsha Nori, Nicholas King, S. McKinney, Dean Carignan, E. Horvitz",
         "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.",
         "https://www.semanticscholar.org/paper/348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
         "903.0",
         "10.48550/arXiv.2303.13375",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "94972e30504017156ef5b5debc419bf6edc67384",
         "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
         "2023.0",
         "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang",
         "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.",
         "https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384",
         "868.0",
         "10.48550/arXiv.2308.02490",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "ebedc4d7a2356090904baba4104ef0832bc236df",
         "A survey on multimodal large language models",
         "2023.0",
         "Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen",
         "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.",
         "https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",
         "778.0",
         "10.1093/nsr/nwae403",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
         "Red Teaming Language Models with Language Models",
         "2022.0",
         "Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, G. Irving",
         "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM. We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot’s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",
         "https://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64",
         "756.0",
         "10.18653/v1/2022.emnlp-main.225",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "db4ab91d5675c37795e719e997a2827d3d83cd45",
         "Towards Reasoning in Large Language Models: A Survey",
         "2022.0",
         "Jie Huang, K. Chang",
         "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",
         "https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45",
         "740.0",
         "10.48550/arXiv.2212.10403",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "6f4486c3d8ccd638c2a6bcbfafa01b5a3225bcab",
         "Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence",
         "2023.0",
         "G. Cooper",
         "The advent of generative artificial intelligence (AI) offers transformative potential in the field of education. The study explores three main areas: (1) How did ChatGPT answer questions related to science education? (2) What are some ways educators could utilise ChatGPT in their science pedagogy? and (3) How has ChatGPT been utilised in this study, and what are my reflections about its use as a research tool? This exploratory research applies a self-study methodology to investigate the technology. Impressively, ChatGPT’s output often aligned with key themes in the research. However, as it currently stands, ChatGPT runs the risk of positioning itself as the ultimate epistemic authority, where a single truth is assumed without a proper grounding in evidence or presented with sufficient qualifications. Key ethical concerns associated with AI include its potential environmental impact, issues related to content moderation, and the risk of copyright infringement. It is important for educators to model responsible use of ChatGPT, prioritise critical thinking, and be clear about expectations. ChatGPT is likely to be a useful tool for educators designing science units, rubrics, and quizzes. Educators should critically evaluate any AI-generated resource and adapt it to their specific teaching contexts. ChatGPT was used as a research tool for assistance with editing and to experiment with making the research narrative clearer. The intention of the paper is to act as a catalyst for a broader conversation about the use of generative AI in science education.",
         "https://www.semanticscholar.org/paper/6f4486c3d8ccd638c2a6bcbfafa01b5a3225bcab",
         "738.0",
         "10.1007/s10956-023-10039-y",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "4be7d1524edb0137599a5cc95f72844b85a52fe1",
         "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
         "2022.0",
         "Tim Dettmers, M. Lewis, Younes Belkada, Luke Zettlemoyer",
         "Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",
         "https://www.semanticscholar.org/paper/4be7d1524edb0137599a5cc95f72844b85a52fe1",
         "730.0",
         "10.48550/arXiv.2208.07339",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "008a428e049003fe768068a0f1fa1416af5c4982",
         "Masked Feature Prediction for Self-Supervised Visual Pre-Training",
         "2021.0",
         "Chen Wei, Haoqi Fan, Saining Xie, Chaoxia Wu, A. Yuille, Christoph Feichtenhofer",
         "We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer based models. Without using extra model weights or supervision, MaskFeat pretrained on unlabeled videos achieves unprecedented results of 86.7% with MViTv2-L on Kinetics-400, 88.3% on Kinetics 600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageN et.",
         "https://www.semanticscholar.org/paper/008a428e049003fe768068a0f1fa1416af5c4982",
         "729.0",
         "10.1109/CVPR52688.2022.01426",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "f4e612658bde9db88abfd455b99f181fdc536996",
         "Out of One, Many: Using Language Models to Simulate Human Samples",
         "2022.0",
         "Lisa P. Argyle, E. Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, D. Wingate",
         "Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.",
         "https://www.semanticscholar.org/paper/f4e612658bde9db88abfd455b99f181fdc536996",
         "724.0",
         "10.1017/pan.2023.2",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
         "Reasoning with Language Model is Planning with World Model",
         "2023.0",
         "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, D. Wang, Zhiting Hu",
         "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
         "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
         "720.0",
         "10.48550/arXiv.2305.14992",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
         "Instruction Tuning for Large Language Models: A Survey",
         "2023.0",
         "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang",
         "This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users'objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
         "https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",
         "706.0",
         "10.48550/arXiv.2308.10792",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "30c0cdc414f68211d5d0514df027cec22e005174",
         "A Survey on In-context Learning",
         "2022.0",
         "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui",
         "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
         "https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174",
         "686.0",
         "10.18653/v1/2024.emnlp-main.64",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "e251ba9fe7992fc07a01365a5f8f2b4d9020b875",
         "Artificial intelligence in higher education: the state of the field",
         "2023.0",
         "H. Crompton, D. Burke",
         "This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles were identified for a full examination. Using a priori, and grounded coding, the data from the 138 articles were extracted, analyzed, and coded. The findings of this study show that in 2021 and 2022, publications rose nearly two to three times the number of previous years. With this rapid rise in the number of AIEd HE publications, new trends have emerged. The findings show that research was conducted in six of the seven continents of the world. The trend has shifted from the US to China leading in the number of publications. Another new trend is in the researcher affiliation as prior studies showed a lack of researchers from departments of education. This has now changed to be the most dominant department. Undergraduate students were the most studied students at 72%. Similar to the findings of other studies, language learning was the most common subject domain. This included writing, reading, and vocabulary acquisition. In examination of who the AIEd was intended for 72% of the studies focused on students, 17% instructors, and 11% managers. In answering the overarching question of how AIEd was used in HE, grounded coding was used. Five usage codes emerged from the data: (1) Assessment/Evaluation, (2) Predicting, (3) AI Assistant, (4) Intelligent Tutoring System (ITS), and (5) Managing Student Learning. This systematic review revealed gaps in the literature to be used as a springboard for future researchers, including new tools, such as Chat GPT. A systematic review examining AIEd in higher education (HE) up to the end of 2022. Unique findings in the switch from US to China in the most studies published. A two to threefold increase in studies published in 2021 and 2022 to prior years. AIEd was used for: Assessment/Evaluation, Predicting, AI Assistant, Intelligent Tutoring System, and Managing Student Learning.",
         "https://www.semanticscholar.org/paper/e251ba9fe7992fc07a01365a5f8f2b4d9020b875",
         "682.0",
         "10.1186/s41239-023-00392-8",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
         "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
         "2023.0",
         "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto",
         "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction-following abilities. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, DPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.",
         "https://www.semanticscholar.org/paper/cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
         "682.0",
         "10.48550/arXiv.2305.14387",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "0cbb518c364067200476a51e5ce7476a4f582770",
         "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation",
         "2023.0",
         "Rui Chen, Y. Chen, Ningxin Jiao, K. Jia",
         "Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function (BRDF) into the text-to-3D task, and learn the surface material for photorealistic rendering of the generated surface. Our disentangled framework is more compatible with popular graphics engines, supporting relighting, editing, and physical simulation of the generated 3D assets. We conduct thorough experiments that show the advantages of our method over existing ones under different text-to-3D task settings. Project page and source codes: https://fantasia3d.github.io/.",
         "https://www.semanticscholar.org/paper/0cbb518c364067200476a51e5ce7476a4f582770",
         "659.0",
         "10.1109/ICCV51070.2023.02033",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "65906e6027246ae9e4ecd18d6e019a24505c842e",
         "Aligning AI With Shared Human Values",
         "2020.0",
         "Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, J. Li, D. Song, J. Steinhardt",
         "We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",
         "https://www.semanticscholar.org/paper/65906e6027246ae9e4ecd18d6e019a24505c842e",
         "651.0",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
         "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
         "2023.0",
         "Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei",
         "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
         "https://www.semanticscholar.org/paper/1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
         "638.0",
         "10.48550/arXiv.2307.05973",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "58f8925a8b87054ad0635a6398a7fe24935b1604",
         "Mind2Web: Towards a Generalist Agent for the Web",
         "2023.0",
         "Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su",
         "We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",
         "https://www.semanticscholar.org/paper/58f8925a8b87054ad0635a6398a7fe24935b1604",
         "623.0",
         "10.48550/arXiv.2306.06070",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
         "Explainability for Large Language Models: A Survey",
         "2023.0",
         "Haiyan Zhao, Hanjie Chen, F. Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du",
         "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.",
         "https://www.semanticscholar.org/paper/26089bdfdbca1e6eaaceca71e3116b715bec6d47",
         "608.0",
         "10.1145/3639372",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "33",
         "3599a236f285af48782fc30b1341d13ec7320735",
         "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
         "2023.0",
         "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kaichao Zhang, Cheng Ji, Qi Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, Lichao Sun Michigan State University, B. University, Lehigh University, Macquarie University, Nanyang Technological University, University of California at San Diego, Duke University, U. Chicago, Salesforce Research",
         "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
         "https://www.semanticscholar.org/paper/3599a236f285af48782fc30b1341d13ec7320735",
         "585.0",
         "10.48550/arXiv.2302.09419",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "34",
         "8cf01c76b506f6bca5258071ed309fc4430c24d3",
         "The Role of ChatGPT, Generative Language Models, and Artificial Intelligence in Medical Education: A Conversation With ChatGPT and a Call for Papers",
         "2023.0",
         "G. Eysenbach",
         "ChatGPT is a generative language model tool launched by OpenAI on November 30, 2022, enabling the public to converse with a machine on a broad range of topics. In January 2023, ChatGPT reached over 100 million users, making it the fastest-growing consumer application to date. This interview with ChatGPT is part 2 of a larger interview with ChatGPT. It provides a snapshot of the current capabilities of ChatGPT and illustrates the vast potential for medical education, research, and practice but also hints at current problems and limitations. In this conversation with Gunther Eysenbach, the founder and publisher of JMIR Publications, ChatGPT generated some ideas on how to use chatbots in medical education. It also illustrated its capabilities to generate a virtual patient simulation and quizzes for medical students; critiqued a simulated doctor-patient communication and attempts to summarize a research article (which turned out to be fabricated); commented on methods to detect machine-generated text to ensure academic integrity; generated a curriculum for health professionals to learn about artificial intelligence (AI); and helped to draft a call for papers for a new theme issue to be launched in JMIR Medical Education on ChatGPT. The conversation also highlighted the importance of proper “prompting.” Although the language generator does make occasional mistakes, it admits these when challenged. The well-known disturbing tendency of large language models to hallucinate became evident when ChatGPT fabricated references. The interview provides a glimpse into the capabilities and limitations of ChatGPT and the future of AI-supported medical education. Due to the impact of this new technology on medical education, JMIR Medical Education is launching a call for papers for a new e-collection and theme issue. The initial draft of the call for papers was entirely machine generated by ChatGPT, but will be edited by the human guest editors of the theme issue.",
         "https://www.semanticscholar.org/paper/8cf01c76b506f6bca5258071ed309fc4430c24d3",
         "575.0",
         "10.2196/46885",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "a1f76db91c0debcf93ae9889736bce8470902113",
         "Large Language Models: A Survey",
         "2024.0",
         "Shervin Minaee, Tomáš Mikolov, Narjes Nikzad, M. Chenaghlu, R. Socher, Xavier Amatriain, Jianfeng Gao",
         "Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",
         "https://www.semanticscholar.org/paper/a1f76db91c0debcf93ae9889736bce8470902113",
         "570.0",
         "10.48550/arXiv.2402.06196",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
         "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
         "2023.0",
         "Wen Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Y. Qiao, Jifeng Dai",
         "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.",
         "https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",
         "559.0",
         "10.48550/arXiv.2305.11175",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "75994ebb52094581dcb7d145795f6bafe6e276bb",
         "Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects",
         "2020.0",
         "Serge-Lopez Wamba-Taguimdje, S. Wamba, Jean Robert Kala Kamdjoug, C. Wanko",
         "PurposeThe main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.Design/methodology/approachThis study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations’ AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.FindingsAI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes.Research limitations/implicationsAI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations’ performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company’s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.Originality/valueThis article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.",
         "https://www.semanticscholar.org/paper/75994ebb52094581dcb7d145795f6bafe6e276bb",
         "558.0",
         "10.1108/bpmj-10-2019-0411",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "8f831f341e959955a495730d81996e62c57cc0bd",
         "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
         "2023.0",
         "Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, K. Chang, Fei Huang, Reynold Cheng, Yongbin Li",
         "Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.",
         "https://www.semanticscholar.org/paper/8f831f341e959955a495730d81996e62c57cc0bd",
         "549.0",
         "10.48550/arXiv.2305.03111",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "668075792a7ab40457d92e09da28d35c879271c3",
         "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
         "2025.0",
         "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Feng Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Haochen Ding, Hao-Xing Hu, Haoming Yang, Hao Zhang, Haotian Yao, Hao-Dong Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jia-Xing Guo, Jianling Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Li-Na Shi, Li-Tao Ye, Long Yu, Meng-Xiao Dong, Neo Y. Zhang, Ningchen Ma, Qi Pan, Qucheng Gong, Shaowei Liu, Shen Ma, Shu-Yan Wei, Sihan Cao, Si-Da Huang, Tao Jiang, Wei-Wei Gao, Weiming Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wen He, Xian-sen Wei, Xian-Xian Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xue-biao Pan, Y. Charles, Yang Li, Yan-Ling Hu, Yangyang Liu, Yanru Chen, Ye-Jia Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Yingbo Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhengxin Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziya Xu, Zonghan Yang",
         "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
         "https://www.semanticscholar.org/paper/668075792a7ab40457d92e09da28d35c879271c3",
         "545.0",
         "10.48550/arXiv.2501.12599",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "b486982fa7c68a8a08df1111ba9607119419c488",
         "A Survey on Large Language Models for Recommendation",
         "2023.0",
         "Likang Wu, Zhilan Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, Enhong Chen",
         "Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.",
         "https://www.semanticscholar.org/paper/b486982fa7c68a8a08df1111ba9607119419c488",
         "522.0",
         "10.48550/arXiv.2305.19860",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "eb375712bd37250c350ecd3f559e1879e87eb3e5",
         "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
         "2024.0",
         "Yann Dubois, Bal'azs Galambosi, Percy Liang, Tatsunori Hashimoto",
         "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question:\"What would the preference be if the model's and baseline's output had the same length?\"To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, but we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98.",
         "https://www.semanticscholar.org/paper/eb375712bd37250c350ecd3f559e1879e87eb3e5",
         "502.0",
         "10.48550/arXiv.2404.04475",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
         "Complexity-Based Prompting for Multi-Step Reasoning",
         "2022.0",
         "Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, Tushar Khot",
         "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
         "https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a",
         "487.0",
         "10.48550/arXiv.2210.00720",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "374dd173491a59a10bbb2b3519ebcfe3649f529d",
         "Teaching Models to Express Their Uncertainty in Words",
         "2022.0",
         "Stephanie C. Lin, Jacob Hilton, Owain Evans",
         "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
         "https://www.semanticscholar.org/paper/374dd173491a59a10bbb2b3519ebcfe3649f529d",
         "486.0",
         "10.48550/arXiv.2205.14334",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "31d2ccff82e313eb5c1620c44bb8322da4a38513",
         "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
         "2024.0",
         "Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, S. Mondal, Aman Chadha",
         "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
         "https://www.semanticscholar.org/paper/31d2ccff82e313eb5c1620c44bb8322da4a38513",
         "486.0",
         "10.48550/arXiv.2402.07927",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "45",
         "0a309c2c47c34f51ef94e8075f11586ad2b1dd2b",
         "Adoption of AI-based chatbots for hospitality and tourism",
         "2020.0",
         "Rajasshrie Pillai, Brijesh Sivathanu",
         "This study aims to investigate the customers’ behavioral intention and actual usage (AUE) of artificial intelligence (AI)-powered chatbots for hospitality and tourism in India by extending the technology adoption model (TAM) with context-specific variables.,To understand the customers’ behavioral intention and AUE of AI-powered chatbots for tourism, the mixed-method design was used whereby qualitative and quantitative techniques were combined. A total of 36 senior managers and executives from the travel agencies were interviewed and the analysis of interview data was done using NVivo 8.0 software. A total of 1,480 customers were surveyed and the partial least squares structural equation modeling technique was used for data analysis.,As per the results, the predictors of chatbot adoption intention (AIN) are perceived ease of use, perceived usefulness, perceived trust (PTR), perceived intelligence (PNT) and anthropomorphism (ANM). Technological anxiety (TXN) does not influence the chatbot AIN. Stickiness to traditional human travel agents negatively moderates the relation of AIN and AUE of chatbots in tourism and provides deeper insights into manager’s commitment to providing travel planning services using AI-based chatbots.,This research presents unique practical insights to the practitioners, managers and executives in the tourism industry, system designers and developers of AI-based chatbot technologies to understand the antecedents of chatbot adoption by travelers. TXN is a vital concern for the customers; so, designers and developers should ensure that chatbots are easily accessible, have a user-friendly interface, be more human-like and communicate in various native languages with the customers.,This study contributes theoretically by extending the TAM to provide better explanatory power with human–robot interaction context-specific constructs – PTR, PNT, ANM and TXN – to examine the customers’ chatbot AIN. This is the first step in the direction to empirically test and validate a theoretical model for chatbots’ adoption and usage, which is a disruptive technology in the hospitality and tourism sector in an emerging economy such as India.",
         "https://www.semanticscholar.org/paper/0a309c2c47c34f51ef94e8075f11586ad2b1dd2b",
         "483.0",
         "10.1108/ijchm-04-2020-0259",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e",
         "Transformer models for text-based emotion detection: a review of BERT-based approaches",
         "2021.0",
         "F. A. Acheampong, Henry Nunoo-Mensah, Wenyu Chen",
         null,
         "https://www.semanticscholar.org/paper/bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e",
         "481.0",
         "10.1007/s10462-021-09958-2",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "8f070e301979732e0dd73f6aa6170309cf73aa7d",
         "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
         "2024.0",
         "Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, Xiangliang Zhang",
         "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.",
         "https://www.semanticscholar.org/paper/8f070e301979732e0dd73f6aa6170309cf73aa7d",
         "481.0",
         "10.48550/arXiv.2402.01680",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "33422275fbb9958f55419620697faf531482699b",
         "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
         "2020.0",
         "Zhengbao Jiang, J. Araki, Haibo Ding, Graham Neubig",
         "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
         "https://www.semanticscholar.org/paper/33422275fbb9958f55419620697faf531482699b",
         "481.0",
         "10.1162/tacl_a_00407",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "123acfbccca0460171b6b06a4012dbb991cde55b",
         "Large Language Models Are Zero-Shot Time Series Forecasters",
         "2023.0",
         "Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson",
         "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.",
         "https://www.semanticscholar.org/paper/123acfbccca0460171b6b06a4012dbb991cde55b",
         "479.0",
         "10.48550/arXiv.2310.07820",
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 23152
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>doi</th>\n",
       "      <th>issn</th>\n",
       "      <th>isbn</th>\n",
       "      <th>Year</th>\n",
       "      <th>keywords</th>\n",
       "      <th>sourceType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d40c77c010c8dbef6142903a02f2a73a85012d5d</td>\n",
       "      <td>A Survey on Vision Transformer</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Kai Han, Yunhe Wang, Hanting Chen, Xinghao Che...</td>\n",
       "      <td>Transformer, first applied to the field of nat...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d40c77c0...</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>10.1109/TPAMI.2022.3152247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>888728745dbb769e29ed475d4f7661eebe1a71cf</td>\n",
       "      <td>A Survey on Evaluation of Large Language Models</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, ...</td>\n",
       "      <td>Large language models (LLMs) are gaining incre...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/88872874...</td>\n",
       "      <td>2239.0</td>\n",
       "      <td>10.1145/3641289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0671fd553dd670a4e820553a974bc48040ba0819</td>\n",
       "      <td>Reflexion: language agents with verbal reinfor...</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Noah Shinn, Federico Cassano, Beck Labash, A. ...</td>\n",
       "      <td>Large language models (LLMs) have been increas...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0671fd55...</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be8db99310602d66bba64bcf41a572c45816fbfc</td>\n",
       "      <td>Let's Verify Step by Step</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>H. Lightman, Vineet Kosaraju, Yura Burda, Harr...</td>\n",
       "      <td>In recent years, large language models have gr...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/be8db993...</td>\n",
       "      <td>1739.0</td>\n",
       "      <td>10.48550/arXiv.2305.20050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f4df78183261538e718066331898ee5cad7cad05</td>\n",
       "      <td>Rethinking the Role of Demonstrations: What Ma...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Arte...</td>\n",
       "      <td>Large language models (LMs) are able to in-con...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/f4df7818...</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.18653/v1/2022.emnlp-main.759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23147</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Secure in Diversity? Transborder Ethnicity, Tr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Smith, David J.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1093/jogss/ogae033</td>\n",
       "      <td>2057-3170</td>\n",
       "      <td>None</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>ontological security; minority rights; Europea...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Phase transitions for the long-time behavior o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Greven, A.; Den Hollander, F.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1214/009117906000001060</td>\n",
       "      <td>0091-1798</td>\n",
       "      <td>None</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>interacting diffusions; phase transitions; lar...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Hope the Russians Love Their Children Too\": R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Smetana, Michal; Onderco, Michal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1093/jogss/ogaf012</td>\n",
       "      <td>2057-3170</td>\n",
       "      <td>None</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>nuclear taboo; survey experiment; public opini...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23150</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Detailed review of transgenic rodent mutation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lambert, LB; Singer, TM; Boucher, SE; Douglas, GR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.mrrev.2005.04.002</td>\n",
       "      <td>1383-5742</td>\n",
       "      <td>None</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>genetic toxicology; mutation assay; Transgenic...</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23151</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Barriers to public engagement with biodiversit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gregg, Emily A.; Garrard, Georgia E.; Bekessy,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1111/cobi.70078</td>\n",
       "      <td>0888-8892</td>\n",
       "      <td>None</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>audiences; behavior; communications; community...</td>\n",
       "      <td>Article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23152 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        paperId  \\\n",
       "0      d40c77c010c8dbef6142903a02f2a73a85012d5d   \n",
       "1      888728745dbb769e29ed475d4f7661eebe1a71cf   \n",
       "2      0671fd553dd670a4e820553a974bc48040ba0819   \n",
       "3      be8db99310602d66bba64bcf41a572c45816fbfc   \n",
       "4      f4df78183261538e718066331898ee5cad7cad05   \n",
       "...                                         ...   \n",
       "23147                                       NaN   \n",
       "23148                                       NaN   \n",
       "23149                                       NaN   \n",
       "23150                                       NaN   \n",
       "23151                                       NaN   \n",
       "\n",
       "                                                   title    year  \\\n",
       "0                         A Survey on Vision Transformer  2020.0   \n",
       "1        A Survey on Evaluation of Large Language Models  2023.0   \n",
       "2      Reflexion: language agents with verbal reinfor...  2023.0   \n",
       "3                              Let's Verify Step by Step  2023.0   \n",
       "4      Rethinking the Role of Demonstrations: What Ma...  2022.0   \n",
       "...                                                  ...     ...   \n",
       "23147  Secure in Diversity? Transborder Ethnicity, Tr...     NaN   \n",
       "23148  Phase transitions for the long-time behavior o...     NaN   \n",
       "23149  \"Hope the Russians Love Their Children Too\": R...     NaN   \n",
       "23150  Detailed review of transgenic rodent mutation ...     NaN   \n",
       "23151  Barriers to public engagement with biodiversit...     NaN   \n",
       "\n",
       "                                                 authors  \\\n",
       "0      Kai Han, Yunhe Wang, Hanting Chen, Xinghao Che...   \n",
       "1      Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, ...   \n",
       "2      Noah Shinn, Federico Cassano, Beck Labash, A. ...   \n",
       "3      H. Lightman, Vineet Kosaraju, Yura Burda, Harr...   \n",
       "4      Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Arte...   \n",
       "...                                                  ...   \n",
       "23147                                    Smith, David J.   \n",
       "23148                      Greven, A.; Den Hollander, F.   \n",
       "23149                   Smetana, Michal; Onderco, Michal   \n",
       "23150  Lambert, LB; Singer, TM; Boucher, SE; Douglas, GR   \n",
       "23151  Gregg, Emily A.; Garrard, Georgia E.; Bekessy,...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      Transformer, first applied to the field of nat...   \n",
       "1      Large language models (LLMs) are gaining incre...   \n",
       "2      Large language models (LLMs) have been increas...   \n",
       "3      In recent years, large language models have gr...   \n",
       "4      Large language models (LMs) are able to in-con...   \n",
       "...                                                  ...   \n",
       "23147                                                NaN   \n",
       "23148                                                NaN   \n",
       "23149                                                NaN   \n",
       "23150                                                NaN   \n",
       "23151                                                NaN   \n",
       "\n",
       "                                                     url  citationCount  \\\n",
       "0      https://www.semanticscholar.org/paper/d40c77c0...         2580.0   \n",
       "1      https://www.semanticscholar.org/paper/88872874...         2239.0   \n",
       "2      https://www.semanticscholar.org/paper/0671fd55...         1755.0   \n",
       "3      https://www.semanticscholar.org/paper/be8db993...         1739.0   \n",
       "4      https://www.semanticscholar.org/paper/f4df7818...         1660.0   \n",
       "...                                                  ...            ...   \n",
       "23147                                                NaN            NaN   \n",
       "23148                                                NaN            NaN   \n",
       "23149                                                NaN            NaN   \n",
       "23150                                                NaN            NaN   \n",
       "23151                                                NaN            NaN   \n",
       "\n",
       "                                   doi       issn  isbn    Year  \\\n",
       "0           10.1109/TPAMI.2022.3152247        NaN   NaN     NaN   \n",
       "1                      10.1145/3641289        NaN   NaN     NaN   \n",
       "2                                 None        NaN   NaN     NaN   \n",
       "3            10.48550/arXiv.2305.20050        NaN   NaN     NaN   \n",
       "4      10.18653/v1/2022.emnlp-main.759        NaN   NaN     NaN   \n",
       "...                                ...        ...   ...     ...   \n",
       "23147            10.1093/jogss/ogae033  2057-3170  None  2024.0   \n",
       "23148       10.1214/009117906000001060  0091-1798  None  2007.0   \n",
       "23149            10.1093/jogss/ogaf012  2057-3170  None  2025.0   \n",
       "23150      10.1016/j.mrrev.2005.04.002  1383-5742  None  2005.0   \n",
       "23151               10.1111/cobi.70078  0888-8892  None  2025.0   \n",
       "\n",
       "                                                keywords sourceType  \n",
       "0                                                    NaN        NaN  \n",
       "1                                                    NaN        NaN  \n",
       "2                                                    NaN        NaN  \n",
       "3                                                    NaN        NaN  \n",
       "4                                                    NaN        NaN  \n",
       "...                                                  ...        ...  \n",
       "23147  ontological security; minority rights; Europea...    Article  \n",
       "23148  interacting diffusions; phase transitions; lar...    Article  \n",
       "23149  nuclear taboo; survey experiment; public opini...    Article  \n",
       "23150  genetic toxicology; mutation assay; Transgenic...     Review  \n",
       "23151  audiences; behavior; communications; community...    Article  \n",
       "\n",
       "[23152 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of df_WoS_LLM_and_Survey_and_SimulationB`: 'title', 'authors', 'doi', 'issn', 'isbn', 'Year', 'keywords',        'sourceType'\n",
    "# columns of df_SS_llm_survey_simB: 'paperId', 'title', 'year', 'authors', 'abstract', 'url', 'citationCount', 'doi'\n",
    "\n",
    "# combine df_SS_llm_survey_simB and df_WoS_LLM_and_Survey_and_SimulationB\n",
    "df_combined_final = pd.concat([df_SS_llm_survey_simB, df_WoS_LLM_and_Survey_and_SimulationB], ignore_index=True)\n",
    "df_combined_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
