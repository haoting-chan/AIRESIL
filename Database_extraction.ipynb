{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff231391",
   "metadata": {},
   "source": [
    "# Extraction of studies via Databases & Registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bc2ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Standard Packages \n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Iterable, Tuple\n",
    "import unicodedata\n",
    "\n",
    "# API Call Packages\n",
    "import urllib\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98675b6f",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5b1149dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web of Science API Key: 7c0...\n",
      "Semantic Scholar API Key: eU3...\n",
      "Perplexity API Key: pplx...\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# load api keys from .env\n",
    "api_key_WoS = os.getenv('WEB_OF_SCIENCE_API_KEY')\n",
    "api_key_SS = os.getenv('SEMANTIC_SCHOLAR_API_KEY')\n",
    "api_key_Perplexity = os.getenv('PERPLEXITY_API_KEY')\n",
    "\n",
    "# Check if API keys are loaded\n",
    "print(f\"Web of Science API Key: {api_key_WoS[:3]}...\")  \n",
    "print(f\"Semantic Scholar API Key: {api_key_SS[:3]}...\")  \n",
    "print(f\"Perplexity API Key: {api_key_Perplexity[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925dea7",
   "metadata": {},
   "source": [
    "## Web of Science API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41b36",
   "metadata": {},
   "source": [
    "**Search and field tags for Web of Science documents**\n",
    "- `sort_field`: Order by field(s). \n",
    "    - Field name and order by clause separated by '+', use A for ASC and D for DESC, \n",
    "    - Example: `PY+D`. Multiple values are separated by comma. \n",
    "    - Supported fields:  * **LD** - Load Date * **PY** - Publication Year * **RS** - Relevance * **TC** - Times Cited  (optional)\n",
    "- `...time_span`: Beginning and end dates must be specified in the yyyy-mm-dd format separated by '+' or ' ', e.g. 2023-01-01+2023-12-31. This parameter is not compatible with the all databases search, i.e. db=WOK is not compatible with this parameter. (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04aeb5c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import WoS API client\n",
    "import clarivate.wos_starter.client\n",
    "from clarivate.wos_starter.client.rest import ApiException\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "522fb5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up Web of Science API client\n",
    "BASE_WoS = \"https://api.clarivate.com/apis/wos-starter/v1\"\n",
    "configuration = clarivate.wos_starter.client.Configuration(host = BASE_WoS)\n",
    "configuration.api_key['ClarivateApiKeyAuth'] = api_key_WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c0651d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "# -------------- Function to run API query --------------\n",
    "def run_wos_api(\n",
    "        q,                          # Search query in WOS search syntax\n",
    "        db='WOS',                       # Choice of Database\n",
    "        limit=50,                       # Set limit of records on page (1-50) (default to 10)\n",
    "        page=1,                         # Set the result page \n",
    "        sort_field='RS+D',              # Order by Field(s), option: LD, PY, RS, TC\n",
    "        modified_time_span=None,        # Date range in which results were most recently modified.\n",
    "        tc_modified_time_span=None,     # Date range in which times cited counts were modified.\n",
    "        detail=None,                    # Set to returns full data by default, alternative: detail=short\n",
    "        configuration=configuration ):\n",
    "\n",
    "    with clarivate.wos_starter.client.ApiClient(configuration) as api_client:\n",
    "        api_instance = clarivate.wos_starter.client.DocumentsApi(api_client)\n",
    "        try:\n",
    "            api_response = api_instance.documents_get(\n",
    "                q,\n",
    "                db=db,\n",
    "                limit=limit,\n",
    "                page=page,\n",
    "                sort_field=sort_field,\n",
    "                modified_time_span=modified_time_span,\n",
    "                tc_modified_time_span=tc_modified_time_span,\n",
    "                detail=detail\n",
    "            )\n",
    "            return api_response\n",
    "        \n",
    "        except ApiException as e:\n",
    "            print(f\"Exception when calling DocumentsApi->documents_get: {e}\")\n",
    "            return None\n",
    "        \n",
    "# -------------- Function to Fetch --------------\n",
    "# Funciton: Fetch X number of pages\n",
    "def wos_fetch_pages(q: str, limit: int = 50) -> pd.DataFrame:\n",
    "    all_hits = []\n",
    "\n",
    "    for p in range(1, 11):  # pages 1-10\n",
    "        resp = run_wos_api(q, page=p, limit=limit)\n",
    "        if resp is None:\n",
    "            print(f\"[WARN] No response for page {p}\")\n",
    "            continue\n",
    "        hits = getattr(resp, \"hits\", []) or []\n",
    "        all_hits.extend(h.to_dict() for h in hits)\n",
    "\n",
    "    if not all_hits:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_hits)\n",
    "    if \"uid\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"uid\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Funciton: Fetch ALL pages\n",
    "def wos_fetch_all_pages(q: str, limit: int = 50) -> pd.DataFrame:\n",
    "    # Step 1: Fetch the first page to get the total number of records\n",
    "    resp = run_wos_api(q, page=1, limit=limit)\n",
    "    if resp is None:\n",
    "        print(f\"[WARN] No response for the first page of query: {q}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_records = getattr(resp.metadata, \"total\", 0)  # Get the total number of records\n",
    "    if total_records == 0:\n",
    "        print(f\"[WARN] No records found for query: {q}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Step 2: Calculate the number of pages required\n",
    "    total_pages = (total_records + limit - 1) // limit  # equivalent to math.ceil(total_records / limit)\n",
    "\n",
    "    # Step 3: Loop through all pages and collect the records\n",
    "    all_hits = []\n",
    "    for page in range(1, total_pages + 1):\n",
    "        resp = run_wos_api(q, page=page, limit=limit)\n",
    "        if resp is None:\n",
    "            print(f\"[WARN] No response for page {page} of query: {q}\")\n",
    "            continue\n",
    "\n",
    "        hits = getattr(resp, \"hits\", []) or []\n",
    "        all_hits.extend(h.to_dict() for h in hits)\n",
    "\n",
    "    if not all_hits:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Step 4: Convert the results to a DataFrame\n",
    "    df = pd.DataFrame(all_hits)\n",
    "    \n",
    "    # Deduplicate based on 'uid' (unique identifier)\n",
    "    if \"uid\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"uid\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function: Get total record counts for each query\n",
    "def wos_query_totals(wos_queries: dict) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for name, q in wos_queries.items():\n",
    "        resp = run_wos_api(q, page=1, limit=1)\n",
    "        print(f\"Processing query: {name}\")\n",
    "        \n",
    "        if resp is None:\n",
    "            results.append({\"QueryName\": name, \"TotalRecords\": None})\n",
    "            continue\n",
    "\n",
    "        total = getattr(resp.metadata, \"total\", None)\n",
    "        results.append({\"QueryName\": name, \"TotalRecords\": total})\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"TotalRecords\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc332c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define search terms\n",
    "\n",
    "# 1. LLM Block\n",
    "LLM_Block = 'TS=(\"large language model*\" OR \"foundation model*\" OR LLM OR LLMs OR GPT OR LLaMA* OR Mistral OR Mixtral OR Claude* OR Gemini OR PaLM OR Qwen OR DeepSeek OR \"Falcon 180B\" OR \"Phi-3\")'\n",
    "\n",
    "# 2. Survey Block\n",
    "Survey_Block = 'TS=(survey* OR \"survey data\" OR \"survey response*\" OR questionnaire* OR question* OR \"opinion poll*\" OR \"public opinion*\" OR attitude* OR value* OR norm* OR moral* OR \"feeling thermometer*\" OR \"open-ended\" OR \"open ended\" OR nonresponse OR \"non-response\" OR respondent* OR participant* OR interview* OR \"self-report*\" OR \"data collection\")'\n",
    "\n",
    "# 3. Simulation Block \n",
    "Simulation_BlockA = 'TS=((simulat* OR emulat* OR predict* OR imput* OR \"missing data\" OR nonresponse OR \"non-response\" OR \"item nonresponse\" OR \"unit nonresponse\" OR \"synthetic respondent*\" OR \"synthetic participant*\" OR \"artificial respondent*\" OR \"artificial participant*\" OR \"virtual respondent*\" OR \"virtual participant*\" OR persona* OR \"role play*\") NEAR/5 (survey* OR questionnaire* OR respondent* OR response* OR interview* OR \"self-report*\" OR \"data collection\" OR opinion* OR poll*))'\n",
    "\n",
    "Simulation_BlockB = 'TS=( ( simulat* OR emulat* OR predict* OR imput* OR \"synthetic data\" OR \"missing data\" OR nonresponse OR \"non-response\" OR \"item nonresponse\" OR \"synthetic respondent*\" OR \"synthetic participant*\" OR \"artificial respondent*\" OR \"artificial participant*\" OR \"virtual respondent*\" OR \"virtual participant*\" OR persona* OR \"role play*\" OR \"as a respondent\" OR \"LLM as respondent\" OR \"model as respondent\" OR proxy OR surrogate OR \"stand-in\" OR \"stand in\" OR replac* OR substitut* OR represent* OR fidelit* OR faithful* OR doppelg* OR (\"Synthetic Voice*\" NEAR/5 (persona* OR respondent* OR survey* OR \"public opinion*\" OR opinion*)) OR (\"representing people\" NEAR/3 (survey* OR respondent* OR persona* OR opinion*)) OR (\"LLM-generated persona*\" OR \"LLM generated persona*\") ) NEAR/5 (survey* OR questionnaire* OR respondent* OR response* OR interview* OR \"self-report*\" OR \"data collection\" OR opinion* OR poll* OR attitude* OR value* OR norm* OR \"public opinion*\") )'\n",
    "\n",
    "Simulation_BlockC = 'TS=(\"survey simulation\" OR \"simulated participant*\" OR \"simulated respondent*\" OR \"synthetic data\" OR \"synthetic survey data\" OR \"synthetic respondent*\" OR \"synthetic participant*\" OR \"artificial respondent*\" OR \"artificial participant*\" OR \"virtual respondent*\" OR \"virtual participant*\" OR \"LLM as respondent\" OR \"model as respondent\" OR \"as a respondent\" OR \"role play*\" OR persona*)'\n",
    "\n",
    "# 4. Model Training Block (optional)\n",
    "Methods_Block = 'TS=( prompt* OR \"few-shot\" OR \"few-shot learning\" OR \"zero-shot\" OR \"zero-shot learning\" OR \"in-context learning\" OR ICL OR \"chain of thought\" OR \"self-consistency\" OR \"system message\" OR persona OR personas OR \"role prompt*\" OR \"instruction-tun*\" OR \"instruction prompt*\" OR \"fine-tun*\" OR (\"reinforcement learning with human feedback\" OR RLHF) OR (\"reinforcement learning with AI feedback\" OR RLAIF) OR \"temperature parameter\" OR \"temperature setting\" OR \"nucleus sampling\" OR \"top-p sampling\" OR \"active learning\" OR \"transfer learning\" OR \"meta learning\" OR \"meta-learning\" OR \"representation learning\" OR \"continual learning\" OR \"lifelong learning\" )'\n",
    "\n",
    "# 5. Negation Block (to exclude business applications)\n",
    "Exclusion_Block = 'TS=(\"customer service\" OR marketing OR \"AI in business\" OR \"business forecasting\" OR \"predictive models\" OR \"stock market forecasting\" OR \"sentiment analysis\" OR \"AI-powered marketing\" OR \"business decisions\" OR \"AI in recruitment\" OR \"AI adoption intention\" OR \"predictive patentomics\" OR \"AI chatbot\" OR chatbot* OR \"virtual assistant\" OR \"AI assistant\" OR \"personal assistant\" OR \"conversational AI\" OR \"dialogue systems\" OR \"digital marketing\" OR \"marketing strategies\" OR \"customer experience\" OR \"AI for customer support\" OR \"AI in e-commerce\" OR \"digital transformation\" OR \"AI in finance\" OR \"banking AI\" OR \"AI in retail\")'\n",
    "\n",
    "# Year Filter Block (e.g., 2020-2024)\n",
    "Year_Block = 'PY=(2022-2025)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac605b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combinations of search blocks \n",
    "LLM_and_Survey = f'{LLM_Block} AND {Survey_Block} AND {Year_Block}'\n",
    "LLM_and_Survey_and_Methods = f'{LLM_Block} AND {Survey_Block} AND {Methods_Block} AND {Year_Block}'\n",
    "LLM_and_SimulationA = f'{LLM_Block} AND {Simulation_BlockA} AND {Year_Block}'\n",
    "LLM_and_SimulationB = f'{LLM_Block} AND {Simulation_BlockB} AND {Year_Block}'\n",
    "LLM_and_SimulationC = f'{LLM_Block} AND {Simulation_BlockC} AND {Year_Block}'\n",
    "LLM_and_Methods = f'{LLM_Block} AND {Methods_Block} AND {Year_Block}'\n",
    "LLM_and_Survey_and_SimulationA = f'{LLM_Block} AND {Survey_Block} AND {Simulation_BlockA} AND {Year_Block}'\n",
    "LLM_and_Survey_and_SimulationB = f'{LLM_Block} AND {Survey_Block} AND {Simulation_BlockB} AND {Year_Block}'\n",
    "LLM_and_Survey_and_SimulationC = f'{LLM_Block} AND {Survey_Block} AND {Simulation_BlockC} AND {Year_Block}'\n",
    "LLM_and_SimulationA_and_Methods = f'{LLM_Block} AND {Simulation_BlockA} AND {Methods_Block} AND {Year_Block}'\n",
    "LLM_and_SimulationB_and_Methods = f'{LLM_Block} AND {Simulation_BlockB} AND {Methods_Block} AND {Year_Block}'\n",
    "LLM_and_SimulationC_and_Methods = f'{LLM_Block} AND {Simulation_BlockC} AND {Methods_Block} AND {Year_Block}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eefa65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE Set of Queries\n",
    "wos_queries = {\n",
    "    \"LLM_and_SimulationA\": LLM_and_SimulationA,\n",
    "    \"LLM_and_SimulationB\": LLM_and_SimulationB,\n",
    "    \"LLM_and_SimulationC\": LLM_and_SimulationC,\n",
    "    # \"LLM_and_Survey_and_SimulationA\": LLM_and_Survey_and_SimulationA,\n",
    "    # \"LLM_and_Survey_and_SimulationB\": LLM_and_Survey_and_SimulationB,\n",
    "    # \"LLM_and_Survey_and_SimulationC\": LLM_and_Survey_and_SimulationC,\n",
    "    \"LLM_and_SimulationA_and_Methods\": LLM_and_SimulationA_and_Methods,\n",
    "    \"LLM_and_SimulationB_and_Methods\": LLM_and_SimulationB_and_Methods,\n",
    "    \"LLM_and_SimulationC_and_Methods\": LLM_and_SimulationC_and_Methods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f0bb452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: LLM_and_SimulationA\n",
      "Processing query: LLM_and_SimulationB\n",
      "Processing query: LLM_and_SimulationC\n",
      "Processing query: LLM_and_Survey_and_SimulationA\n",
      "Processing query: LLM_and_Survey_and_SimulationB\n",
      "Processing query: LLM_and_Survey_and_SimulationC\n",
      "Processing query: LLM_and_SimulationA_and_Methods\n",
      "Processing query: LLM_and_SimulationB_and_Methods\n",
      "Processing query: LLM_and_SimulationC_and_Methods\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QueryName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TotalRecords",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9e5482b4-1519-4771-8e3c-8a3eef5c679d",
       "rows": [
        [
         "0",
         "LLM_and_SimulationC",
         "3827"
        ],
        [
         "1",
         "LLM_and_Survey_and_SimulationC",
         "1503"
        ],
        [
         "2",
         "LLM_and_SimulationC_and_Methods",
         "1406"
        ],
        [
         "3",
         "LLM_and_SimulationB",
         "1223"
        ],
        [
         "4",
         "LLM_and_Survey_and_SimulationB",
         "922"
        ],
        [
         "5",
         "LLM_and_SimulationA",
         "561"
        ],
        [
         "6",
         "LLM_and_SimulationB_and_Methods",
         "329"
        ],
        [
         "7",
         "LLM_and_Survey_and_SimulationA",
         "320"
        ],
        [
         "8",
         "LLM_and_SimulationA_and_Methods",
         "169"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryName</th>\n",
       "      <th>TotalRecords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM_and_SimulationC</td>\n",
       "      <td>3827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM_and_Survey_and_SimulationC</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM_and_SimulationC_and_Methods</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM_and_SimulationB</td>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM_and_Survey_and_SimulationB</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM_and_SimulationA</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLM_and_SimulationB_and_Methods</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLM_and_Survey_and_SimulationA</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLM_and_SimulationA_and_Methods</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         QueryName  TotalRecords\n",
       "0              LLM_and_SimulationC          3827\n",
       "1   LLM_and_Survey_and_SimulationC          1503\n",
       "2  LLM_and_SimulationC_and_Methods          1406\n",
       "3              LLM_and_SimulationB          1223\n",
       "4   LLM_and_Survey_and_SimulationB           922\n",
       "5              LLM_and_SimulationA           561\n",
       "6  LLM_and_SimulationB_and_Methods           329\n",
       "7   LLM_and_Survey_and_SimulationA           320\n",
       "8  LLM_and_SimulationA_and_Methods           169"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN: Get Max Total records\n",
    "df_WoS_totals = wos_query_totals(wos_queries)\n",
    "df_WoS_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91faf724",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching WoS results for: LLM_and_SimulationA\n",
      "LLM_and_SimulationA: 561 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_SimulationB\n",
      "LLM_and_SimulationB: 1223 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_SimulationC\n",
      "LLM_and_SimulationC: 3827 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_Survey_and_SimulationA\n",
      "LLM_and_Survey_and_SimulationA: 320 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_Survey_and_SimulationB\n",
      "LLM_and_Survey_and_SimulationB: 922 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_Survey_and_SimulationC\n",
      "LLM_and_Survey_and_SimulationC: 1503 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_SimulationA_and_Methods\n",
      "LLM_and_SimulationA_and_Methods: 169 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_SimulationB_and_Methods\n",
      "LLM_and_SimulationB_and_Methods: 329 rows\n",
      "\n",
      "Fetching WoS results for: LLM_and_SimulationC_and_Methods\n",
      "LLM_and_SimulationC_and_Methods: 1406 rows\n"
     ]
    }
   ],
   "source": [
    "# RUN Fetch for ALL queries\n",
    "dfs_WoS = {}\n",
    "for name, query in wos_queries.items():\n",
    "    print(f\"\\nFetching WoS results for: {name}\")\n",
    "    df = wos_fetch_all_pages(query, limit=50)\n",
    "    print(f\"{name}: {len(df)} rows\")\n",
    "    dfs_WoS[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c28bf0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions to clean and normalize WoS DataFrames\n",
    "def authors_from_names(names_obj):\n",
    "    if isinstance(names_obj, dict):\n",
    "        people = names_obj.get(\"authors\") or []\n",
    "        out = []\n",
    "        for a in people:\n",
    "            if isinstance(a, dict):\n",
    "                dn = a.get(\"displayName\") or a.get(\"wosStandard\") or a.get(\"full_name\") or \"\"\n",
    "                if dn:\n",
    "                    out.append(dn)\n",
    "        return \"; \".join(out)\n",
    "    return \"\"\n",
    "\n",
    "def keywords_from_obj(keywords_obj):\n",
    "    if isinstance(keywords_obj, dict):\n",
    "        ak = keywords_obj.get(\"authorKeywords\")\n",
    "        if isinstance(ak, list):\n",
    "            return \"; \".join([k for k in ak if isinstance(k, str)])\n",
    "        if isinstance(ak, str):\n",
    "            return ak\n",
    "    return \"\"\n",
    "\n",
    "def doi_from_identifiers(ident_obj):\n",
    "    if isinstance(ident_obj, dict):\n",
    "        doi = ident_obj.get(\"doi\")\n",
    "        if doi:\n",
    "            return doi\n",
    "        dois = ident_obj.get(\"dois\")\n",
    "        if isinstance(dois, list) and len(dois) > 0:\n",
    "            return dois[0]\n",
    "    return None\n",
    "\n",
    "def issn_from_identifiers(ident_obj):\n",
    "    if isinstance(ident_obj, dict):\n",
    "        val = ident_obj.get(\"issn\")\n",
    "        issn = val[0] if isinstance(val, list) and val else val\n",
    "        return issn\n",
    "    return None\n",
    "\n",
    "def isbn_from_identifiers(ident_obj):\n",
    "    if isinstance(ident_obj, dict):\n",
    "        val = ident_obj.get(\"isbn\")\n",
    "        isbn = val[0] if isinstance(val, list) and val else val\n",
    "        return isbn\n",
    "    return None\n",
    "\n",
    "def year_from_source(src_obj):\n",
    "    if isinstance(src_obj, dict):\n",
    "        return src_obj.get(\"publishYear\") or src_obj.get(\"publishedYear\")\n",
    "    return None\n",
    "\n",
    "def first_source_type(st_list):\n",
    "    if isinstance(st_list, list) and st_list:\n",
    "        return st_list[0]\n",
    "    return None\n",
    "\n",
    "def clean_wos_df(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        return pd.DataFrame(columns=[\"title\", \"authors\", \"doi\", \"year\", \"keywords\", \"sourceType\"])\n",
    "\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Compute desired fields\n",
    "    df[\"authors\"] = df[\"names\"].apply(authors_from_names) if \"names\" in df.columns else \"\"\n",
    "    df[\"doi\"] = df[\"identifiers\"].apply(doi_from_identifiers) if \"identifiers\" in df.columns else None\n",
    "    df[\"issn\"] = df[\"identifiers\"].apply(issn_from_identifiers) if \"identifiers\" in df.columns else None\n",
    "    df[\"isbn\"] = df[\"identifiers\"].apply(isbn_from_identifiers) if \"identifiers\" in df.columns else None\n",
    "    df[\"year\"] = df[\"source\"].apply(year_from_source) if \"source\" in df.columns else None\n",
    "    df[\"keywords\"] = df[\"keywords\"].apply(keywords_from_obj) if \"keywords\" in df.columns else \"\"\n",
    "    df[\"sourceType\"] = df[\"sourceTypes\"].apply(first_source_type) if \"sourceTypes\" in df.columns else None\n",
    "\n",
    "    # Drop intermediate/noisy columns\n",
    "    to_drop = [\"uid\", \"types\", \"sourceTypes\", \"source\", \"names\", \"links\", \"citations\", \"identifiers\"]\n",
    "    df = df.drop(columns=[c for c in to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    # Reorder columns (keep others after the key fields)\n",
    "    key_cols = [c for c in [\"title\", \"authors\", \"doi\", \"issn\", \"isbn\",\n",
    "                            \"year\", \"keywords\", \"sourceType\"] if c in df.columns]\n",
    "    other_cols = [c for c in df.columns if c not in key_cols]\n",
    "    df = df[key_cols + other_cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df457fdf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# RUN Cleaning for Allsets\n",
    "dfs_WoS_clean = {name: clean_wos_df(df) for name, df in dfs_WoS.items()}\n",
    "\n",
    "# df_WoS_LLM_and_Survey_clean = dfs_WoS_clean[\"LLM_and_Survey\"]\n",
    "# df_WoS_LLM_and_Survey_and_Methods_clean = dfs_WoS_clean[\"LLM_and_Survey_and_Methods\"]\n",
    "df_WoS_LLM_and_SimulationA_clean = dfs_WoS_clean[\"LLM_and_SimulationA\"]\n",
    "df_WoS_LLM_and_SimulationB_clean = dfs_WoS_clean[\"LLM_and_SimulationB\"]\n",
    "df_WoS_LLM_and_SimulationC_clean = dfs_WoS_clean[\"LLM_and_SimulationC\"]\n",
    "# df_WoS_LLM_and_Methods_clean = dfs_WoS_clean[\"LLM_and_Methods\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationA_clean = dfs_WoS_clean[\"LLM_and_Survey_and_SimulationA\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationB_clean = dfs_WoS_clean[\"LLM_and_Survey_and_SimulationB\"]\n",
    "df_WoS_LLM_and_Survey_and_SimulationC_clean = dfs_WoS_clean[\"LLM_and_Survey_and_SimulationC\"]\n",
    "df_WoS_LLM_and_SimulationA_and_Methods_clean = dfs_WoS_clean[\"LLM_and_SimulationA_and_Methods\"]\n",
    "df_WoS_LLM_and_SimulationB_and_Methods_clean = dfs_WoS_clean[\"LLM_and_SimulationB_and_Methods\"]\n",
    "df_WoS_LLM_and_SimulationC_and_Methods_clean = dfs_WoS_clean[\"LLM_and_SimulationC_and_Methods\"]\n",
    "# df_WoS_LLMSurvey_or_LLMSimulationA_clean = dfs_WoS_clean[\"LLMSurvey or LLMSimulationA\"]\n",
    "# df_WoS_LLMSurvey_or_LLMSimulationB_clean = dfs_WoS_clean[\"LLMSurvey or LLMSimulationB\"]\n",
    "# df_WoS_LLMSurvey_or_LLMSimulationC_clean = dfs_WoS_clean[\"LLMSurvey or LLMSimulationC\"]\n",
    "# df_WoS_Survey_and_SimulationA_clean = dfs_WoS_clean[\"Survey_and_SimulationA\"]\n",
    "# df_WoS_Survey_and_SimulationB_clean = dfs_WoS_clean[\"Survey_and_SimulationB\"]\n",
    "# df_WoS_Survey_and_SimulationC_clean = dfs_WoS_clean[\"Survey_and_SimulationC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataframes to CSV files\n",
    "df_WoS_LLM_and_SimulationA_clean.to_csv(\"results_wos/df_WoS_LLM_and_SimulationA.csv\", index=False)\n",
    "df_WoS_LLM_and_SimulationB_clean.to_csv(\"results_wos/df_WoS_LLM_and_SimulationB.csv\", index=False)\n",
    "df_WoS_LLM_and_SimulationC_clean.to_csv(\"results_wos/df_WoS_LLM_and_SimulationC.csv\", index=False)\n",
    "df_WoS_LLM_and_Survey_and_SimulationA_clean.to_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationA.csv\", index=False)\n",
    "df_WoS_LLM_and_Survey_and_SimulationB_clean.to_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationB.csv\", index=False)\n",
    "df_WoS_LLM_and_Survey_and_SimulationC_clean.to_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationC.csv\", index=False)\n",
    "df_WoS_LLM_and_SimulationA_and_Methods_clean.to_csv(\"results_wos/df_WoS_LLM_and_SimulationA_and_Methods.csv\", index=False)\n",
    "df_WoS_LLM_and_SimulationB_and_Methods_clean.to_csv(\"results_wos/df_WoS_LLM_and_SimulationB_and_Methods.csv\", index=False)\n",
    "df_WoS_LLM_and_SimulationC_and_Methods_clean.to_csv(\"results_wos/df_WoS_LLM_and_SimulationC_and_Methods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ea6498fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all saved dataframes in \"AIRESIL\\results_wos\"\n",
    "df_WoS_LLM_and_SimulationA_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_SimulationA.csv\")\n",
    "df_WoS_LLM_and_SimulationB_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_SimulationB.csv\")\n",
    "df_WoS_LLM_and_SimulationC_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_SimulationC.csv\")\n",
    "df_WoS_LLM_and_Survey_and_SimulationA_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationA.csv\")\n",
    "df_WoS_LLM_and_Survey_and_SimulationB_clean= pd.read_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationB.csv\")\n",
    "df_WoS_LLM_and_Survey_and_SimulationC_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationC.csv\")\n",
    "df_WoS_LLM_and_SimulationA_and_Methods_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_SimulationA_and_Methods.csv\")\n",
    "df_WoS_LLM_and_SimulationB_and_Methods_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_SimulationB_and_Methods.csv\")\n",
    "df_WoS_LLM_and_SimulationC_and_Methods_clean = pd.read_csv(\"results_wos/df_WoS_LLM_and_SimulationC_and_Methods.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840ea0e",
   "metadata": {},
   "source": [
    "## Semantic Scholar API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "48f41e66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cd87858",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Search Parameters\n",
    "FIELDS_SS = [\"paperId\", \"title\", \"year\", \"authors\", \"abstract\", \"url\", \n",
    "             \"citationCount\", \"externalIds\", \"publicationTypes\"]\n",
    "YEAR_FILTER = \"2022-\"\n",
    "BULK_SORT = \"citationCount:desc\"\n",
    "MAX_PAPERS_PER_GROUP = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8c2ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Function\n",
    "\n",
    "# -------- Helper Functions --------\n",
    "def author_names(paper_authors):\n",
    "    if not paper_authors:\n",
    "        return \"\"\n",
    "    names = []\n",
    "    for a in paper_authors:\n",
    "        # supports Author objects and dicts\n",
    "        names.append(getattr(a, \"name\", a.get(\"name\") if isinstance(a, dict) else None))\n",
    "    return \", \".join([n for n in names if n])\n",
    "\n",
    "def _safe_get(container, key):\n",
    "    \"\"\"Access dict or object attribute safely.\"\"\"\n",
    "    if container is None:\n",
    "        return None\n",
    "    if isinstance(container, dict):\n",
    "        return container.get(key)\n",
    "    return getattr(container, key, None)\n",
    "\n",
    "def _fos_to_str(x):\n",
    "    if not x:\n",
    "        return None\n",
    "    vals = []\n",
    "    if isinstance(x, list):\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                vals.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                vals.append(it.get(\"name\") or it.get(\"category\"))\n",
    "            else:\n",
    "                vals.append(getattr(it, \"name\", None) or getattr(it, \"category\", None))\n",
    "    if not vals:\n",
    "        return None\n",
    "    vals = [v for v in vals if v]\n",
    "    return \"; \".join(sorted(set(vals))) if vals else None\n",
    "\n",
    "def _enrich_fos_with_batch(df: pd.DataFrame, sch: SemanticScholar) -> pd.DataFrame:\n",
    "    ids = [i for i in df.get(\"paperId\", pd.Series()).dropna().astype(str).unique().tolist() if i]\n",
    "    if not ids:\n",
    "        df[[\"fieldsOfStudy\", \"s2FieldsOfStudy\"]] = None\n",
    "        return df\n",
    "    chunksize = 100  # /graph/v1/paper/batch limit\n",
    "    rows = []\n",
    "    for i in range(0, len(ids), chunksize):\n",
    "        batch = ids[i:i+chunksize]\n",
    "        papers = sch.get_papers(batch, fields=[\"paperId\", \"fieldsOfStudy\", \"s2FieldsOfStudy\"])\n",
    "        for p in papers or []:\n",
    "            pid = getattr(p, \"paperId\", None)\n",
    "            fos = getattr(p, \"fieldsOfStudy\", None)\n",
    "            s2  = getattr(p, \"s2FieldsOfStudy\", None)\n",
    "            rows.append({\n",
    "                \"paperId\": pid,\n",
    "                \"fieldsOfStudy\": _fos_to_str(fos),\n",
    "                \"s2FieldsOfStudy\": _fos_to_str(s2),\n",
    "            })\n",
    "    fos_df = pd.DataFrame(rows).drop_duplicates(subset=[\"paperId\"]) if rows else pd.DataFrame(columns=[\"paperId\",\"fieldsOfStudy\",\"s2FieldsOfStudy\"])\n",
    "    return df.merge(fos_df, on=\"paperId\", how=\"left\")\n",
    "\n",
    "def ss_paper_row(p):\n",
    "    ext = getattr(p, \"externalIds\", None)\n",
    "\n",
    "    return {\n",
    "        \"paperId\": getattr(p, \"paperId\", None),\n",
    "        \"title\": getattr(p, \"title\", None),\n",
    "        \"year\": getattr(p, \"year\", None),\n",
    "        \"authors\": author_names(getattr(p, \"authors\", None)),\n",
    "        \"abstract\": getattr(p, \"abstract\", None),\n",
    "        \"url\": getattr(p, \"url\", None),\n",
    "        \"citationCount\": getattr(p, \"citationCount\", None),\n",
    "        \"doi\": _safe_get(ext, \"DOI\") or _safe_get(ext, \"doi\"),\n",
    "        \"publicationTypes\": getattr(p, \"publicationTypes\", None),\n",
    "    }\n",
    "\n",
    "# -------- Main Fetch Function --------\n",
    "\n",
    "# Funciton: Fetch all pages\n",
    "def ss_fetch_bulk(tag: str,\n",
    "                  year_filter: str = YEAR_FILTER,\n",
    "                  max_papers: int = MAX_PAPERS_PER_GROUP,\n",
    "                  sort: str | None = BULK_SORT,\n",
    "                  enrich_fos: bool = True) -> pd.DataFrame:\n",
    "    if tag not in QUERY_GROUPS:\n",
    "        raise ValueError(f\"Unknown group '{tag}'. Valid keys: {', '.join(QUERY_GROUPS.keys())}\")\n",
    "\n",
    "    sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "    results = sch.search_paper(\n",
    "        query=QUERY_GROUPS[tag],\n",
    "        year=year_filter,\n",
    "        fields=FIELDS_SS,\n",
    "        bulk=True,\n",
    "        sort=sort,\n",
    "    )\n",
    "\n",
    "    print(f\"Estimated total: {getattr(results, 'total', 'n/a')}\")\n",
    "    rows = []\n",
    "    for i, p in enumerate(results, 1):\n",
    "        rows.append(ss_paper_row(p))\n",
    "        if i >= max_papers:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if enrich_fos:\n",
    "        df = _enrich_fos_with_batch(df, sch)\n",
    "\n",
    "    desired = [\n",
    "        \"paperId\", \"title\", \"year\", \"authors\", \"abstract\", \"url\", \"citationCount\", \"doi\", \"publicationTypes\", \"fieldsOfStudy\", \"s2FieldsOfStudy\",\n",
    "    ]\n",
    "    for c in desired:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    others = [c for c in df.columns if c not in desired]\n",
    "    return df[desired + others]\n",
    "\n",
    "# Function: Get total record counts for each query\n",
    "def ss_query_totals(query_groups: dict,\n",
    "                    year_filter: str = YEAR_FILTER,) -> pd.DataFrame:\n",
    "    sch = SemanticScholar\n",
    "    sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "    results = []\n",
    "    for name, query in query_groups.items():\n",
    "        res = sch.search_paper(\n",
    "            query=query,\n",
    "            year=year_filter,\n",
    "            fields=FIELDS_SS,\n",
    "            bulk=True,\n",
    "            sort=BULK_SORT\n",
    "        )\n",
    "        total = getattr(res, \"total\", None)\n",
    "        results.append({\"QueryName\": name, \"TotalRecords\": total})    \n",
    "    return pd.DataFrame(results).sort_values(\"TotalRecords\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc6c4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Query Blocks\n",
    "\n",
    "# 1. LLM Block\n",
    "LLM_Block = ('(\"large language model*\" | \"foundation model*\" | LLM | LLMs | GPT | LLaMA* | Mistral | Mixtral | Claude* | Gemini | PaLM | Qwen | DeepSeek | \"Falcon 180B\" | \"Phi-3\")')\n",
    "\n",
    "# 2. Survey Block\n",
    "Survey_Block = ('(survey* | \"survey data\" | \"survey response*\" | questionnaire* | question* | \"opinion poll*\" | \"public opinion*\" | attitude* | value* | norm* | moral* | \"feeling thermometer*\" | \"open-ended\" | \"open ended\" | nonresponse | \"non-response\" | respondent* | participant* | interview* | \"self-report*\" | \"data collection\")')\n",
    "\n",
    "# 3. Simulation Block\n",
    "Simulation_BlockA = ('((estimate* | simulat* | emulat* | predict* | imput* |  \"item nonresponse\" | \"synthetic respondent*\" | \"synthetic participant*\" | \"artificial respondent*\" |  \"artificial participant*\" | \"virtual respondent*\" | \"virtual participant*\" | persona* | \"role play*\") + (survey* | questionnaire* | respondent* | response* | interview* | \"self-report*\" |  \"data collection\" | opinion* | poll*))')\n",
    "\n",
    "Simulation_BlockB = ('( simulat* | emulat* | predict* | imput* | \"synthetic data\"|  \"item nonresponse\" | \"synthetic respondent\" | \"synthetic respondents\" | \"synthetic participant\" |  \"synthetic participants\" | \"artificial respondent\" | \"artificial participant\" | \"virtual respondent\" | \"virtual participant\" | persona | personas |  \"role play\" | \"role-playing\" | (role + play*) |  \"as a respondent\" | \"LLM as respondent\" | \"model as respondent\" | proxy | surrogate | \"stand-in\" | \"stand in\" | replac* | substitut* | represent* | fidelit* | faithful* | doppelg* | (\"Synthetic Voice persona\"~5 | \"persona Synthetic Voice\"~5 |   \"representing people survey\"~5 | \"survey representing people\"~5 |  \"representing people respondent\"~5 | \"respondent representing people\"~5) )')\n",
    "\n",
    "Simulation_BlockC = ('( \"survey simulation\" | \"simulated participant\" | \"simulated respondent\" |  \"synthetic data\" | \"synthetic survey data\" | \"synthetic respondent\" | \"synthetic participant\" |  \"artificial respondent\" | \"artificial participant\" | \"virtual respondent\" | \"virtual participant\" |  \"LLM as respondent\" | \"model as respondent\" | \"as a respondent\" | \"role play\" | \"role-playing\" | persona | personas)')\n",
    "\n",
    "# 4. Model Training Block (optional)\n",
    "Methods_Block = ('(prompt* | \"few-shot\" | \"few-shot learning\" | \"zero-shot\" | \"zero-shot learning\" | \"in-context learning\" | ICL | \"chain of thought\" | \"self-consistency\" | \"system message\" | persona | personas | \"role prompt*\" | \"instruction-tun*\" | \"instruction prompt*\" | \"fine-tun*\" | (\"reinforcement learning with human feedback\" | RLHF) | (\"reinforcement learning with AI feedback\" | RLAIF) | \"temperature parameter\" | \"temperature setting\" | \"nucleus sampling\" | \"top-p sampling\" | \"active learning\" | \"transfer learning\" | \"meta learning\" | \"meta-learning\" | \"representation learning\" | \"continual learning\" | \"lifelong learning\")')\n",
    "\n",
    "#####\n",
    "Negation_Block1 = ('(customer service | marketing | \"AI in business\" | \"business forecasting\" | \"predictive models\" | \"stock market forecasting\" | \"sentiment analysis\" | \"AI-powered marketing\" | \"business decisions\" | \"AI in recruitment\" | \"AI adoption intention\" | \"predictive patentomics\" | \"AI chatbot\" | \"chatbots\" | \"virtual assistant\" | \"AI assistant\" | \"personal assistant\" | \"conversational AI\" | \"dialogue systems\" | \"digital marketing\" | \"marketing strategies\" | \"customer experience\" | \"AI for customer support\" | \"AI in e-commerce\" | \"digital transformation\" | \"AI in finance\" | \"banking AI\" | \"AI in retail\")')\n",
    "\n",
    "Negation_Block2 = ('(machine learning | artificial intelligence | deep learning | neural network | reinforcement learning | generative models | predictive models | data mining | \"classification algorithms\" | \"computer vision\" | \"robotics\" | \"chatbot\" | \"chat bots\" | \"conversational AI\" | \"dialogue systems\" | \"natural language processing\" | \"NLP applications\" | \"chatbot technology\" | \"chatbot development\" | \"chatbot applications\" | \"speech recognition\" | \"autonomous systems\" | \"virtual assistant\" | \"AI assistant\" | \"personal assistant\" | \"tasks\" | \"performance\" | \"learning\" | \"research\" | \"knowledge\" | \"language models\" | \"language model\" | \"based\" | \"llm\" | \"models\")')\n",
    "\n",
    "Negation_Block = ('(customer | service | satisfaction | marketing | loyalty | customers | chatbots | businesses | commerce | financial | business | services | banking | experience | digital | chatbot | engagement | study | crm | sales | \"Sales\" | business* | \"AI in business\" | \"business forecasting\" | \"sentiment analysis\" | \"AI-powered marketing\" | \"business decisions\" | \"AI in recruitment\" | \"AI adoption intention\" | \"predictive patentomics\" | \"AI chatbot\" | \"AI assistant\" | \"personal assistant\" | \"conversational AI\" | \"dialogue systems\" | \"digital marketing\" | \"marketing strategies\" | \"customer experience\" | \"AI for customer support\" | \"AI in e-commerce\" | \"digital transformation\" | \"AI in finance\" | service* | banking | \"banking AI\" | \"AI in retail\" | \"stock market forecasting\" | machine learning | artificial intelligence | deep learning | neural network | reinforcement learning | generative models | data mining | \"classification algorithms\" | \"computer vision\" | \"robotics\" | \"speech recognition\" | \"autonomous systems\" | \"tasks\" | \"performance\" | \"learning\" | video | visual | multimodal | image | vision | text | images | temporal | engineering)')\n",
    "\n",
    "# Combinations using + for AND and | for OR\n",
    "QUERY_GROUPS = {\n",
    "    # pairs\n",
    "    #\"ss_llm_survey\":           f'{LLM_Block} + {Survey_Block}',\n",
    "    # \"ss_llm_simA\":             f'{LLM_Block} + {Simulation_BlockA}',\n",
    "    # \"ss_llm_simB\":             f'{LLM_Block} + {Simulation_BlockB}',\n",
    "    # \"ss_llm_simC\":             f'{LLM_Block} + {Simulation_BlockC}',\n",
    "    #\"ss_llm_methods\":          f'{LLM_Block} + {Methods_Block}',\n",
    "    # \"ss_survey_simA\":          f'{Survey_Block} + {Simulation_BlockA}',\n",
    "    # \"ss_survey_simB\":          f'{Survey_Block} + {Simulation_BlockB}',\n",
    "    # \"ss_survey_simC\":          f'{Survey_Block} + {Simulation_BlockC}',\n",
    "\n",
    "    # triples\n",
    "    #\"ss_llm_survey_methods\":       f'{LLM_Block} + {Survey_Block} + {Methods_Block}',\n",
    "    \"ss_llm_survey_simA\":          f'{LLM_Block} + {Survey_Block} + {Simulation_BlockA}',\n",
    "    \"ss_llm_survey_simB\":          f'{LLM_Block} + {Survey_Block} + {Simulation_BlockB}',\n",
    "    \"ss_llm_survey_simC\":          f'{LLM_Block} + {Survey_Block} + {Simulation_BlockC}',\n",
    "    # \"ss_llm_simA_methods\":         f'{LLM_Block} + {Simulation_BlockA} + {Methods_Block}',\n",
    "    # \"ss_llm_simB_methods\":         f'{LLM_Block} + {Simulation_BlockB} + {Methods_Block}',\n",
    "    # \"ss_llm_simC_methods\":         f'{LLM_Block} + {Simulation_BlockC} + {Methods_Block}',\n",
    "\n",
    "    # with Negation Block\n",
    "    # \"ss_llm_survey_simA_negated\": f'{LLM_Block} + {Survey_Block} + {Simulation_BlockA} - {Negation_Block}',\n",
    "    # \"ss_llm_survey_simB_negated\": f'{LLM_Block} + {Survey_Block} + {Simulation_BlockB} - {Negation_Block}',\n",
    "    # \"ss_llm_survey_simC_negated\": f'{LLM_Block} + {Survey_Block} + {Simulation_BlockC} - {Negation_Block}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73a84e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "QueryName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TotalRecords",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "eaa6ef69-0227-481a-9f2b-ab0f6f79c970",
       "rows": [
        [
         "0",
         "ss_llm_survey_simB",
         "14945"
        ],
        [
         "1",
         "ss_llm_survey_simA",
         "3824"
        ],
        [
         "2",
         "ss_llm_survey_simC",
         "1944"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryName</th>\n",
       "      <th>TotalRecords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ss_llm_survey_simB</td>\n",
       "      <td>14945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ss_llm_survey_simA</td>\n",
       "      <td>3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ss_llm_survey_simC</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            QueryName  TotalRecords\n",
       "0  ss_llm_survey_simB         14945\n",
       "1  ss_llm_survey_simA          3824\n",
       "2  ss_llm_survey_simC          1944"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run: Get Max Total records\n",
    "df_SS_totals = ss_query_totals(QUERY_GROUPS)\n",
    "df_SS_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c280f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total: 3824\n"
     ]
    }
   ],
   "source": [
    "df_SS_llm_survey_simA = ss_fetch_bulk(\"ss_llm_survey_simA\", max_papers = 3824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78cf5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total: 6012\n",
      "Estimated total: 50466\n",
      "Estimated total: 4574\n",
      "Estimated total: 1839\n",
      "Estimated total: 14556\n",
      "Estimated total: 2505\n"
     ]
    }
   ],
   "source": [
    "# Run Fetch for all defined queries\n",
    "# df_SS_llm_survey = ss_fetch_bulk(\"ss_llm_survey\")\n",
    "df_SS_llm_simA = ss_fetch_bulk(\"ss_llm_simA\", max_papers = 6012)\n",
    "df_SS_llm_simB = ss_fetch_bulk(\"ss_llm_simB\", max_papers = 10000)\n",
    "df_SS_llm_simC = ss_fetch_bulk(\"ss_llm_simC\", max_papers = 4574)\n",
    "# df_SS_survey_simA = ss_fetch_bulk(\"ss_survey_simA\")\n",
    "# df_SS_survey_simB = ss_fetch_bulk(\"ss_survey_simB\")\n",
    "# df_SS_survey_simC = ss_fetch_bulk(\"ss_survey_simC\")\n",
    "# df_SS_llm_survey_simA = ss_fetch_bulk(\"ss_llm_survey_simA\", max_papers = 3550)\n",
    "# df_SS_llm_survey_simB = ss_fetch_bulk(\"ss_llm_survey_simB\", max_papers = 14893)\n",
    "# df_SS_llm_survey_simC = ss_fetch_bulk(\"ss_llm_survey_simC\", max_papers = 1939)\n",
    "df_SS_llm_simA_methods = ss_fetch_bulk(\"ss_llm_simA_methods\", max_papers = 1839)\n",
    "df_SS_llm_simB_methods = ss_fetch_bulk(\"ss_llm_simB_methods\", max_papers = 10000)\n",
    "df_SS_llm_simC_methods = ss_fetch_bulk(\"ss_llm_simC_methods\", max_papers = 2505)\n",
    "\n",
    "# df_SS_llm_survey_simA_negated = ss_fetch_bulk(\"ss_llm_survey_simA_negated\", max_papers = 2500)\n",
    "# df_SS_llm_survey_simB_negated = ss_fetch_bulk(\"ss_llm_survey_simB_negated\", max_papers = 13947)\n",
    "# df_SS_llm_survey_simC_negated = ss_fetch_bulk(\"ss_llm_survey_simC_negated\", max_papers = 201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "46cf66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fetched dataframes to CSV files\n",
    "df_SS_llm_simA.to_csv(\"results_ss/df_SS_llm_simA.csv\", index=False)\n",
    "df_SS_llm_simB.to_csv(\"results_ss/df_SS_llm_simB.csv\", index=False)\n",
    "df_SS_llm_simC.to_csv(\"results_ss/df_SS_llm_simC.csv\", index=False)\n",
    "df_SS_llm_survey_simA.to_csv(\"results_ss/df_SS_llm_survey_simA.csv\", index=False)\n",
    "# df_SS_llm_survey_simB.to_csv(\"results_ss/df_SS_llm_survey_simB.csv\", index=False)\n",
    "# df_SS_llm_survey_simC.to_csv(\"results_ss/df_SS_llm_survey_simC.csv\", index=False)\n",
    "df_SS_llm_simA_methods.to_csv(\"results_ss/df_SS_llm_simA_methods.csv\", index=False)\n",
    "df_SS_llm_simB_methods.to_csv(\"results_ss/df_SS_llm_simB_methods.csv\", index=False)\n",
    "df_SS_llm_simC_methods.to_csv(\"results_ss/df_SS_llm_simC_methods.csv\", index=False)\n",
    "\n",
    "# df_SS_llm_survey_simA_negated.to_csv(\"results_ss/df_SS_llm_survey_simA_negated.csv\", index=False)\n",
    "# df_SS_llm_survey_simB_negated.to_csv(\"results_ss/df_SS_llm_survey_simB_negated.csv\", index=False)\n",
    "# df_SS_llm_survey_simC_negated.to_csv(\"results_ss/df_SS_llm_survey_simC_negated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved dataframes\n",
    "df_SS_llm_simA = pd.read_csv(\"results_ss/df_SS_llm_simA.csv\")\n",
    "df_SS_llm_simB = pd.read_csv(\"results_ss/df_SS_llm_simB.csv\")\n",
    "df_SS_llm_simC = pd.read_csv(\"results_ss/df_SS_llm_simC.csv\")\n",
    "df_SS_llm_survey_simA = pd.read_csv(\"results_ss/df_SS_llm_survey_simA.csv\")\n",
    "df_SS_llm_survey_simB = pd.read_csv(\"results_ss/df_SS_llm_survey_simB.csv\")\n",
    "df_SS_llm_survey_simC = pd.read_csv(\"results_ss/df_SS_llm_survey_simC.csv\")\n",
    "df_SS_llm_simA_methods = pd.read_csv(\"results_ss/df_SS_llm_simA_methods.csv\")\n",
    "df_SS_llm_simB_methods = pd.read_csv(\"results_ss/df_SS_llm_simB_methods.csv\")\n",
    "df_SS_llm_simC_methods = pd.read_csv(\"results_ss/df_SS_llm_simC_methods.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eafeae",
   "metadata": {},
   "source": [
    "### Filter Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25428b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Rows of df_SS_llm_survey_simA: 3493\n",
      "No. of Rows of df_SS_llm_survey_simB: 14893\n",
      "No. of Rows of df_SS_llm_survey_simC: 1939\n",
      "No. of Rows of df_SS_llm_survey_simA_filtered: 1487\n",
      "No. of Rows of df_SS_llm_survey_simB_filtered: 6939\n",
      "No. of Rows of df_SS_llm_survey_simC_filtered: 688\n"
     ]
    }
   ],
   "source": [
    "EXCLUDE_FIELDS = (\n",
    "    'Art|Agricultural and Food Sciences|Biology|Chemistry|Philosophy|Physics|'\n",
    "    'Materials Science|Education|Engineering|Geography|Geology|History|Law|'\n",
    "    'Medicine|Mechanics|Orthopedics|Pharmacology'\n",
    ")\n",
    "EXCLUDE_PUBTYPES = 'CaseReport|Dataset|Editorial|LettersAndComments|MetaAnalysis|News|Study'\n",
    "\n",
    "def filter_ss_df(df):\n",
    "    return df[\n",
    "        df['s2FieldsOfStudy'].str.contains('Computer Science', na=False) &\n",
    "        ~df['s2FieldsOfStudy'].str.contains(EXCLUDE_FIELDS, na=False) &\n",
    "        ~df['publicationTypes'].str.contains(EXCLUDE_PUBTYPES, na=False)\n",
    "    ]\n",
    "\n",
    "df_SS_llm_survey_simA_filtered = filter_ss_df(df_SS_llm_survey_simA)\n",
    "df_SS_llm_survey_simB_filtered = filter_ss_df(df_SS_llm_survey_simB)\n",
    "df_SS_llm_survey_simC_filtered = filter_ss_df(df_SS_llm_survey_simC)\n",
    "\n",
    "\n",
    "# nrows of non filtered dataframes\n",
    "print(f'No. of Rows of df_SS_llm_survey_simA: {len(df_SS_llm_survey_simA)}')\n",
    "print(f'No. of Rows of df_SS_llm_survey_simB: {len(df_SS_llm_survey_simB)}')\n",
    "print(f'No. of Rows of df_SS_llm_survey_simC: {len(df_SS_llm_survey_simC)}')\n",
    "\n",
    "# nrows of filtered dataframes\n",
    "print(f'No. of Rows of df_SS_llm_survey_simA_filtered: {len(df_SS_llm_survey_simA_filtered)}')\n",
    "print(f'No. of Rows of df_SS_llm_survey_simB_filtered: {len(df_SS_llm_survey_simB_filtered)}')\n",
    "print(f'No. of Rows of df_SS_llm_survey_simC_filtered: {len(df_SS_llm_survey_simC_filtered)}')\n",
    "\n",
    "# save filtered dataframes to CSV files\n",
    "df_SS_llm_survey_simA_filtered.to_csv(\"results_ss/df_SS_llm_survey_simA_filtered.csv\", index=False)\n",
    "df_SS_llm_survey_simB_filtered.to_csv(\"results_ss/df_SS_llm_survey_simB_filtered.csv\", index=False)\n",
    "df_SS_llm_survey_simC_filtered.to_csv(\"results_ss/df_SS_llm_survey_simC_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183a155",
   "metadata": {},
   "source": [
    "## AI Search: Elicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3ef97435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Elicit A.I. Search Results (Elicit prompt 1.csv to Elicit prompt 4.csv) each into their respective dataframe\n",
    "df_Elicit1 = pd.read_csv(\"data/Elicit prompt 1.csv\")\n",
    "df_Elicit2 = pd.read_csv(\"data/Elicit prompt 2.csv\")\n",
    "df_Elicit3 = pd.read_csv(\"data/Elicit prompt 3.csv\")\n",
    "df_Elicit4 = pd.read_csv(\"data/Elicit prompt 4.csv\")\n",
    "\n",
    "# rename all Title to title\n",
    "df_Elicit1 = df_Elicit1.rename(columns={\"Title\": \"title\"})\n",
    "df_Elicit2 = df_Elicit2.rename(columns={\"Title\": \"title\"})\n",
    "df_Elicit3 = df_Elicit3.rename(columns={\"Title\": \"title\"})\n",
    "df_Elicit4 = df_Elicit4.rename(columns={\"Title\": \"title\"})\n",
    "\n",
    "df_Elicit = pd.concat([df_Elicit1, df_Elicit2, df_Elicit3, df_Elicit4], ignore_index=True).drop_duplicates(subset=[\"DOI\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa00d9",
   "metadata": {},
   "source": [
    "## AI Search: Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9cfbab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perplexity import Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Perplexity(api_key=api_key_Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"How are large language models applied to simulate human survey responses across question types and populations?\"\n",
    "\n",
    "search = client.search.create(\n",
    "    query=prompt1,\n",
    "    max_results=10,\n",
    "    max_tokens_per_page=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d66a166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SearchCreateResponse(id='3a6de138-2529-4073-bf7c-a5356af60dd9', results=[Result(snippet='## Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations\\n\\nYong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Rttger, Daniel Hershcovich... ##### AbstractLarge-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.\\n\\n- Anthology ID:\\n\\n- 2025.naacl-long.162\\n\\n- Volume:\\n\\n- Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\\n\\n- Month:\\n\\n- April\\n\\n- Year:\\n\\n- 2025\\n\\n- Address:\\n\\n- Albuquerque, New Mexico\\n\\n- Editors:\\n\\n- Luis Chiruzzo, Alan Ritter, Lu Wang\\n\\n- Venue:\\n\\n- NAACL\\n\\n- SIG:\\n\\n- Publisher:\\n\\n- Association for Computational Linguistics\\n\\n- Note:\\n\\n- Pages:\\n\\n- 31413154\\n\\n- Language:\\n\\n- URL:\\n\\n- https://aclanthology.org/2025.naacl-long.162/\\n\\n- DOI:\\n\\n- 10.18653/v1/2025.naacl-long.162\\n\\n- Cite (ACL):\\n\\n- Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Rttger, and Daniel Hershcovich. 2025. Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations. In... *Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*, pages 31413154, Albuquerque, New Mexico. Association for Computational Linguistics.\\n\\n- Cite (Informal):\\n\\n- Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations (Cao et al., NAACL 2025)\\n\\n- PDF:\\n\\n- https://aclanthology.org/2025.naacl-long.162.pdf', title='Specializing Large Language Models to Simulate Survey ...', url='https://aclanthology.org/2025.naacl-long.162/', date='2025-04-20', last_updated='2025-10-20'), Result(snippet='# Computer Science > Computation and Language\\n\\n**arXiv:2502.07068** (cs)\\n\\n[Submitted on 10 Feb 2025 (v1), last revised 19 Feb 2025 (this version, v2)]\\n\\n# Title: Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations\\nAuthors:Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Rttger, Daniel Hershcovich\\nAbstract:Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.\\n\\n|Comments:|15 pages, 9 figures, accepted to NAACL 2025 main|\\n|--|--|\\n|Subjects:|Computation and Language (cs.CL)|\\n|Cite as:|arXiv:2502.07068 [cs.CL]|\\n| |(or arXiv:2502.07068v2 [cs.CL] for this version)|\\n| |https://doi.org/10.48550/arXiv.2502.07068 arXiv-issued DOI via DataCite|\\n\\n## Submission history\\nFrom: Yong Cao [view email] **[v1]** Mon, 10 Feb 2025 21:59:27 UTC (17,529 KB) **[v2]** Wed, 19 Feb 2025 15:05:39 UTC (17,529 KB)\\n\\nFull-text links:\\n\\n## Access Paper:\\n- View PDF\\n- HTML (experimental)\\n- TeX Source\\n- Other Formats\\nview license', title='Specializing Large Language Models to Simulate Survey Response ...', url='https://arxiv.org/abs/2502.07068', date='2025-02-10', last_updated='2025-09-20'), Result(snippet='# Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations\\n\\nYong Cao^1^, Haijiang Liu^2^, Arnav Arora^3^, Isabelle Augenstein^3^, Paul Rttger^4^, Daniel Hershcovich^3^ ^1^University of Tbingen, Tbingen AI Center ^2^Wuhan University of Science and Technology ^3^University of Copenhagen, ^4^Bocconi University yong.cao@uni-tuebing.de, dh@di.ku.dk\\n\\n###### Abstract\\n\\nLarge-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If one could accurately simulate group-level survey results, this could be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed for this task, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.\\n\\nSpecializing Large Language Models to Simulate Survey Response Distributions for Global Populations\\n\\n Yong Cao^1^, Haijiang Liu^2^, Arnav Arora^3^, Isabelle Augenstein^3^, Paul Rttger^4^, Daniel Hershcovich^3^ ^1^University of Tbingen, Tbingen AI Center ^2^Wuhan University of Science and Technology ^3^University of Copenhagen, ^4^Bocconi University yong.cao@uni-tuebing.de, dh@di.ku.dk... ## 1 Introduction\\n\\nHumans are diverse, and they hold diverse opinions.\\nThis is why surveys are essential tools for informing decision-making in policy and industry as well as social science research.\\nRunning large-scale surveys, however, is often costly and time-intensive.\\n\\nLarge language models (LLMs) have demonstrated promising potential for simulating human behaviors across groups and individuals (Argyle et\\xa0al., 2023; Aher et al., 2023; Manning et al., 2024, inter alia). LLM simulations of survey responses, if accurate towards the corresponding populations, could accelerate social science research and aid in more informed policy decisions. Out of the box, however, LLMs are known to generate erroneous, stereotypical, or overconfident answers, especially in culturally diverse contexts Yang et\\xa0al. (2024), which limits their usefulness for survey simulations. Prior work has at most tried to improve simulation accuracy through prompting strategies (Kwok et\\xa0al., 2024; Manning et al., 2024; Sun et al., 2024).\\n\\nIn this study, our goal is instead to specialize LLMs for survey simulation and gain a better understanding of how good LLMs can be at simulating survey responses when trained to do so, rather than how good they are when prompted out of the box. As our main testbed, we use country-level response distributions from the widely-used World Values Survey (WVS; Haerpfer et\\xa0al., 2022). When prompted with a survey question (e.g., In your opinion, should the use of nuclear power in Japan be reduced, maintained at its current level, or increased?), corresponding answering options (e.g. Reduced, Maintained at current level, Increased) and a target country (e.g. Japan), we want our model to predict the distribution over the answering options for that target country. To specialize LLMs for this task, we devise a fine-tuning method based on first-token probabilities, where the goal is to minimize divergence between predicted and actual country-level response distributions for a given survey question.... ## 6 Results\\n\\nWe investigate two primary research questions:\\n\\n- **RQ1**\\n  How effectively does the proposed alignment method improve the distribution simulation of the model on unseen countries and questions?\\n- **RQ2**\\n  What is necessary to perform well on the taskhow much is dependent on modeling the prior distribution, and how much on context sensitivity?\\n\\nWe present comprehensive experimental results to address the two research questions.', title='Specializing Large Language Models to Simulate Survey Response ...', url='https://arxiv.org/html/2502.07068v1', date='2013-05-01', last_updated='2025-10-13'), Result(snippet='The article, Employing Large Language Models in Survey Research, discusses the promising potential of employing large language models (LLMs) for survey research, including generating responses to survey items. LLMs can address some of the challenges associated with survey research regarding question wording and response bias.\\n\\nThey can address issues relating to a lack of clarity and understanding but cannot yet correct for sampling or nonresponse bias challenges.\\n\\nWhile LLMs can assist with some of the challenges with survey research, at present, LLMs need to be used in conjunction with other methods and approaches.\\n\\nWith thoughtful and nuanced approaches to development, LLMs can be used responsibly and beneficially while minimizing the associated risks.\\n\\nJansen, B. J., Jung, S.G., and Salminen, J. (2023) Employing Large Language Models in Survey Research. Natural Language Processing Journal. 4, 100020.\\n\\n', title='Employing Large Language Models in Survey Research  Survey2Personas', url='https://s2p.qcri.org/blog/employing-large-language-models-in-survey-research/', date=None, last_updated='2025-07-22'), Result(snippet='## Large Language Models and Survey Research: A Recap\\n\\n11/25/2024\\n\\nLarge Language Models and Survey Research: A Recap\\n\\nSoubhik Barari,\\n\\n*NORC, *and Joshua Lerner *, NORC*\\n\\nGenerative AI is changing the worldand with it, the field of survey research. As AI tools such as large language models (LLMs) weave their way into survey practice, the need grows to equip researchers with the theoretical understanding and practical skills to leverage these tools effectively. Thats exactly what we set out to do in our recent AAPOR short course. Here are a few high-level takeaways from that course.\\n\\n**Effectively applying large language models requires prompt engineering (yes, even if youre not an engineer).**\\n\\nAt its core, the interface with LLMs relies on text-based prompts, which can be categorized as either system-level or user-level. System-level prompts define the models overarching behavior or constraints, while user-level prompts guide specific interactions or tasks. Beyond these foundational interactions, various applications built on LLMssuch as chat agents, sandboxes, and programmatic APIsoffer researchers user-friendly tools and options for advanced customization. In our course, we demonstrated how these tools can be harnessed effectively, showing workflows for both asynchronous use cases (e.g., analyzing data post-collection) and synchronous use cases (e.g., embedding real-time LLM outputs into survey instruments). Our research at NORC has shown that system-level prompts often carry more weight than user-level prompts for tasks like classification and synthetic response generation. In short, decisions about how to prompt, rather than just what to prompt, are critical.... Survey researchers: ignore prompt engineering at your own peril.\\n\\n**Large language models have applications at every stage of survey research.**\\n\\nLLMs have applications at every stage of the survey research process, from upstream decisions like question development, survey design, data collection, and interviewing to downstream tasks such as response processing, data augmentation, analysis, and weighting.\\n\\nPut differently; generative AI is an extremely versatile technology, capable of reducing every aspect of total survey error: sampling error via better processing of address-based sampling frames, measurement error via improving respondent comprehension and recall, and nonresponse error via engaging respondents with interactive experiences, along with others.\\n\\nThat said, generative AI is equally capable of increasing those same errors if researchers are not careful  which brings us to our next point \\n\\n**Validate, validate, validate.**\\n\\nIts tempting to assume that the work of validating generative AI technology has already been done  after all, millions (perhaps billions!) of dollars have been collectively spent on building, training, fine-tuning, and validating LLMs to ensure their performance on many different kinds of benchmark tasks. This assumption would be mistaken: a model may be able to ace elementary school tests (the equivalent of benchmarks), but your application may require a high-school level of vocabulary  something you may need to teach your model yourself.... We emphasize that small-scale, task-specific tests of an LLMs performance  sometimes called downstream model evaluation  are far more relevant than generalized benchmarks for abstract inference or computational tasks, which vendors may promote. Methods for demonstrating validity are well established in natural language processing (NLP) and machine learning (ML), such as cross-validation and active learning.\\n\\nWhile validation involves demonstrating acceptable performance at a single point, reliability requires demonstrating that performance holds steady across time and applications. This distinction is crucial because of two inherent characteristics of LLMs in practice. First, they operate using fuzzy logic, meaning their outputs can varyeven when strict system-level prompts are applied. Researchers may need to standardize outputs through additional processing steps or the usage of templates. Second, proprietary models are often subject to ongoing fine-tuning and pre-training, which can change their behavior unpredictably. A solution to this is to adopt open-source models and develop on-premise sandboxes where performance can be monitored and controlled.\\n\\n**The future of Generative AI in survey research: cautiously optimistic.**\\n\\nMany innovations are on the horizon, including multimode and multimedia survey designs, with some survey software vendors already supporting these innovations. At the same time, many developers of large language models are placing a greater emphasis on built-in safety guardrails and increasing transparency around their system-level prompts, making these tools more appealing for research.', title='Large Language Models and Survey Research: A Recap - AAPOR', url='https://aapor.org/newsletters/large-language-models-and-survey-research-a-recap/', date='2024-12-03', last_updated='2025-07-08'), Result(snippet='Chengpiao Huang 1 Yuhang Wu 2 Kaizheng Wang 1 3\\nAbstract\\nWe investigate the use of large language mod-\\nels (LLMs) to simulate human responses to sur-\\nvey questions, and perform uncertainty quantifi-\\ncation to assess the fidelity of the simulations.\\nOur approach converts imperfect black-box LLM-\\nsimulated responses into confidence sets for pop-\\nulation parameters of human responses. A key\\ninnovation lies in determining the optimal num-\\nber of simulated responses: too many produce\\noverly narrow confidence sets with poor cover-\\nage, while too few yield excessively loose esti-\\nmates. Our method adaptively selects the simula-\\ntion sample size that ensures valid average-case\\ncoverage guarantees. The selected sample size\\nitself further provides a quantitative measure of\\nLLM-human misalignment. Experiments on real\\nsurvey datasets reveal heterogeneous fidelity gaps\\nacross different LLMs and domains.\\n1. Introduction\\nLarge language models (LLMs) have demonstrated remark-\\nable capabilities in mimicking human behaviors. Recent\\nstudies have leveraged LLMs to simulate human responses\\nin various domains, including economic and social science\\nexperiments (Aher et al., 2023; Horton, 2023; Chen et al.,... to determine the appropriate number of simulated responses.\\nMain contributions.\\nIn this paper, we develop a general\\nframework to address these challenges. Our key contribu-\\ntions are as follows:\\n (Formulation) We provide a rigorous mathematical\\nframework for uncertainty quantification in LLM-\\nbased survey simulations.\\n (Methodology) We propose a flexible methodology that\\ntransforms simulated responses into valid confidence\\nsets for population parameters of human responses.\\nOur approach adaptively selects the simulation sample\\nsize based on the observed misalignment between the\\nLLM and human populations. It is applicable to any\\nLLM, regardless of its fidelity, and can be combined\\nwith any method for confidence set construction.\\n1... population.\\n3. The sample size k reflects the size of the target popu-\\nlation that the LLM can represent. We make an anal-\\nogy using the classical theory of parametric bootstrap.\\nSuppose a model is trained via maximum likelihood\\nestimation over k i.i.d. human samples. When perform-\\ning parametric bootstrap for uncertainty quantification,\\nthe bootstrap sample size is usually set to be the train-\\ning sample size k. Thus, our simulation sample size\\nk reveals the LLM as being made up of k people\\nfrom the population. The larger k is, the more diversity\\nthat the LLM appears to capture. In contrast, a small k\\ncould imply the peculiarity of the LLM compared to\\nthe major population.\\n2... gpt-4o-mini\\ngpt-4o\\nclaude-3-5-haiku\\nLlama-3.3-70B\\nMistral-7B\\nDeepSeek-V3\\nrandom\\nFigure 1. Average k for various LLMs and  on the OpinionQA\\ndataset (top) and the EEDI dataset (bottom)\\nDeepSeek-V3 and GPT-4o seem to outperform the random\\nbenchmark, on the OpinionQA dataset all LLMs clearly\\noutperform the random benchmark. Moreover, LLMs ex-\\nhibit uniformly higher k on the OpinionQA dataset than on\\nthe EEDI dataset, suggesting higher fidelity in simulating\\nsubjective opinions to social problems than in simulating\\nstudent answers to mathematics questions.\\nThe experiment results demonstrate the importance of a\\ndisciplined approach to using synthetic samples. The ease of\\nLLM-based simulation makes it tempting to generate a large\\nnumber of responses. Our results show great heterogeneity\\nin the simulation power of different LLMs over different\\ndatasets: the largest k is below 100, while the smallest k\\nis less than 10. This means that there is real peril in using\\nexcessive synthetic samples.... 6. Discussions\\nWe developed a general approach for converting imperfect\\nLLM-based survey simulations into statistically valid confi-\\ndence sets for population statistics of human responses. It\\nidentifies a simulation sample size which is useful for future\\nsimulation tasks and which reveals the degree of misalign-\\nment between the LLM and the target human population.\\n5... and Yang, D.\\nCan large language models transform\\ncomputational social science? Computational Linguis-\\ntics, 50(1):237291, 03 2024. ISSN 0891-2017. doi:\\n10.1162/coli a 00502. URL https://doi.org/10.\\n1162/coli_a_00502.\\n7', title='Uncertainty Quantification for LLM-Based Survey Simulations', url='https://openreview.net/pdf?id=RKQFXiIoyz', date=None, last_updated='2025-09-14'), Result(snippet='Using Large Language Models to Replicate Human Subject Studies\\nname). Importantly, the TE should be zero-shot, meaning\\nthat neither the procedure nor any training data used by the\\nAI system should include prior data specific to that experi-\\nment; otherwise the model may simply repeat the prior data.\\n(This ideal can be difficult to enforce with models pretrained\\nby others on massive corpora.) Overall findings or specific\\noutcome data can be compared to results from human sub-\\nject research to determine how faithful the simulation is. A\\nreplication TE is for replicating a finding in prior human\\nsubject research.\\nIn addition to introducing the concept of TEs, we demon-\\nstrate their feasibility by presenting a methodology for run-\\nning TEs using an LM, like GPT models, that takes a text\\nprompt and generates a randomized completion, which is\\ntext that would be likely to follow that prompt, based on its\\ntraining data. For each TE, we write a program that creates\\none or more (zero-shot) prompts that are fed into the LMs.\\nThen the text generated by the LM is used to reconstruct the\\nrecord, a text-based transcript of the simulated experiment.\\nFigure 1 illustrates the difference between a typical prompt\\nused for classification and our prompt used to run a TE,... models provided more faithful simulations than smaller\\nones, with the largest model replicating the finding and\\nproducing outcomes consistent with those of prior human\\nstudies. In the last TE, Wisdom of Crowds, the larger models\\ndid not outperform the smaller onesif anything the trend\\nwas reversed, revealing a peculiar flaw within some models.\\n2... Using Large Language Models to Replicate Human Subject Studies\\nFigure 8. Comparing Wisdom of Crowds TE simulation estimates\\nto human results for the five questions from Moussad et al. (2013).\\nAs LMs become larger and more aligned, they are more likely\\nto complete the TE prompt with inhumanly accurate answers.\\nEstimates are normalized by dividing by the correct answer. Bars\\nindicate the median normalized estimate, and black lines indicate\\nthe quartiles. All simulations for LM-6 (as well as ChatGPT and\\nGPT-4) have a median of 1.0 with 0.0 IQR. Results from all LMs\\nare in Appendix E.\\n7. Wisdom of Crowds TE\\nPhenomenon.\\nIn many cases, aggregating group estimates\\nof a quantity have significantly less error than the error of\\nmost individuals. In early work, Galton (1907) recorded\\n787 estimates of the weight of a given ox, and found that\\nthat the median of 787 estimates had a 9 lb. error (less than\\n1%), despite the variation among the estimates: a 74 lb.\\ninterquartile range (IQRthe difference between 75th and\\n25th percentiles). Similar findings have been reported across... Our new TE methodology evaluates how faithfully LMs\\nsimulate human behavior across diverse populations. TEs\\nmay contribute to the view of AIs as capable of simulat-\\ning the collective intelligence of many humans, rather than\\nanthropomorphizing or viewing AI as a single monolithic\\nintelligence. We show that TEs can reproduce economic,\\npsycholinguistic, and social psychology experiments. The\\nWisdom of Crowds TE uncovered a hyper-accuracy distor-\\ntion where larger and more aligned LMs simulate human\\nsubjects that give unhumanly accurate answers. This work\\nis merely an initial exploration of the concept of TEs. In\\nfuture work, it would be interesting to perform larger and\\nmore systematic simulations across additional LMs, to bet-\\nter understand the limitations of LMs in terms of different\\nhuman behaviors.\\nAs LMs increase in accuracy, it would be interesting to\\ntest whether or not LM-based simulations can be used to\\nform and evaluate new hypotheses, especially in situations\\nwhere it is costly to carry out experiments on humans due\\nto considerations regarding scale, selection bias, monetary\\ncost, legal, moral, or privacy considerations. For instance,\\nexperiments on what to say to a person who is suicidal would... cost lives (Bolton et al., 2015). Future LMs, if sufficiently\\nfaithful, might be useful in designing experimental protocols\\nthat may be more effective at saving lives.\\nAcknowledgements.\\nWe thank Michael Kearns, Sashank\\nVarma, Mary Gray, Elizabeth Fetterolf, Shafi Goldwasser,\\nand the anonymous reviewers for invaluable feedback.\\n9', title='Using Large Language Models to Simulate Multiple Humans ...', url='https://proceedings.mlr.press/v202/aher23a/aher23a.pdf', date=None, last_updated='2025-10-11'), Result(snippet=\"## Questioning the Survey Responses of Large Language Models\\n\\n### Ricardo Dominguez-Olmedo, Moritz Hardt, Celestine Mendler-DnnerPublished: 25 Sept 2024, Last Modified: 06 Nov 2024NeurIPS 2024 oralEveryoneRevisionsBibTeXCC BY-NC-ND 4.0\\n\\n**Keywords:**large language models, surveys\\n\\n**Abstract:**Surveys have recently gained popularity as a tool to study large language models. By comparing models survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter A. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.\\n\\n**Supplementary Material:**zip\\n\\n**Primary Area:**Natural language processing\\n\\nLoading\\n\\n**Submission Number:**16093\", title='Questioning the Survey Responses of Large Language ...', url='https://openreview.net/forum?id=Oo7dlLgqQX', date='2024-11-06', last_updated='2025-10-14'), Result(snippet='Generating Closed-Ended Survey Responses In-Silico\\nwith Large Language Models\\nGeorg Ahnert1, Anna-Carolina Haensch2,3,4, Barbara Plank2,3, Markus Strohmaier1,5,6\\n1University of Mannheim; 2LMU Munich; 3Munich Center for Machine Learning;\\n4University of Maryland, College Park; 5GESIS Cologne; 6CSH Vienna\\nCorrespondence: georg.ahnert [at] uni-mannnheim [dot] de\\nAbstract\\nMany in-silico simulations of human survey\\nresponses with large language models (LLMs)\\nfocus on generating closed-ended survey re-\\nsponses, whereas LLMs are typically trained\\nto generate open-ended text instead. Previous\\nresearch has used a diverse range of methods\\nfor generating closed-ended survey responses\\nwith LLMs and a standard practice remains\\nto be identified. In this paper, we systemati-\\ncally investigate the impact that various Sur-\\nvey Response Generation Methods have on\\npredicted survey responses. We present the\\nresults of 32 mio. simulated survey responses\\nacross 8 Survey Response Generation Meth-\\nods, 4 political attitude surveys, and 10 open-... weight language models. We find significant\\ndifferences between the Survey Response Gen-\\neration Methods in both individual-level and\\nsubpopulation-level alignment.\\nOur results\\nshow that Restricted Generation Methods per-\\nform best overall, and that reasoning output\\ndoes not consistently improve alignment. Our\\nwork underlines the significant impact that Sur-\\nvey Response Generation Methods have on sim-\\nulated survey responses, and we develop prac-\\ntical recommendations on the application of\\nSurvey Response Generation Methods.\\n1\\nIntroduction\\nA growing body of research simulates human sur-\\nvey responses by prompting large language mod-\\nels (LLMs) to answer survey questions (Argyle\\net al., 2023, inter alia). While generative LLMs\\nare designed to generate open-ended text, previ-\\nous studies have implemented various approaches\\nto constraining LLMs to closed-ended survey re-\\nsponses (Ma et al., 2024). We define Survey Re-\\nsponse Generation Methods as techniques used to\\nelicit closed-ended responses from large language\\nmodels to survey questions on attitudes, opinions,\\nand values.\\nPrevious research has shown that\\nUser Prompt, e.g.:\\nIdeologically, I am a liberal.\\nI am a woman.\\nI am from Kansas.\\nPersona & Question... ated distributions and the distribution found in the\\nhuman survey data for the respective subpopula-\\ntion using total variation distance for categorical\\nresponse options (ANES & GLES datasets) and\\n1-Wasserstein distance for ordinal response options\\n(ATP 2021).\\n4... to significant improvements in individual-level\\nalignment, followed by the Restricted Reason-\\ning Method and the Open-Ended Classification\\nMethod. The Verbalized Distribution Method and\\nthe Open-Ended Distribution Method yield signifi-\\ncant improvements for some datasets, even if they\\nare designed to generate probability distributions\\nacross all response options rather than a single re-\\nsponse. We observe similar patterns when using\\naccuracy as a metric (see Appendix Table 9). Open\\nGeneration Methods generally improve individual-\\nlevel alignment, but show smaller coefficients for\\nmost datasets. This indicates that long, open-ended\\nreasoning does not improve the results in on our\\ntask, while also being orders of magnitude less\\ncomputationally efficient (see Appendix Figure 3).\\n5... of LLMs to respond to survey questions differently\\n5Regression coefficients by dataset for distance correlation\\nare shown in Appendix Table 11.\\n7... sponses and identify the Survey Response Gener-\\nation Method that works best for a given survey\\nand LLM. Meister et al. (2024) compared differ-\\nent methods for generating response distributions\\nand found that the Verbalized Distribution Method\\nworks best, which is why we also implement it. Our\\nwork goes further, as we simulate the responses of\\nindividual survey participants instead of subpopu-\\nlations directly. We evaluate Survey Response Gen-\\neration Methods on non-English datasets (GLES\\n2017, GLES 2025), and compare a wider range of\\nmethods, including Open Generation Methods.\\nResponse Generation in Other Settings\\nAn-\\nother line of research has investigated closed-ended\\nand open-ended model responses in other contexts.\\nRttger et al. (2024) found that adding instructions\\non the response format, and forcing models to, e.g.,\\ntake a clear stance, alters the response option that\\na model chooses. Tam et al. (2024) and Long et al.\\n(2024) also observed a negative impact of format\\ninstructions for mathematical reasoning tasks. We\\nextend this line of research from evaluations of\\nLLM alignment and mathematical reasoning to the\\nsimulation of survey responses, including persona\\nprompts and human survey data for comparison.', title='Generating Closed-Ended Survey Responses In-Silico with Large ...', url='https://georgahnert.de/response_generation_10_2025.pdf', date=None, last_updated='2025-10-20'), Result(snippet='|Part of a series on|\\n|--|\\n|Machine learningand data mining|\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n| |\\n\\nA **large language model** (**LLM**) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\n\\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\\n\\nLLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale such as few-shot learning and compositional reasoning.\\n\\nReinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM\\'s output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\\n\\nBenchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.... ### Benchmarks\\n\\nBenchmarks are used to evaluate LLM performance on specific tasks. Tests evaluate capabilities such as general knowledge, bias, commonsense reasoning, question answering, and mathematical problem-solving. Composite benchmarks examine multiple capabilities. Results are often sensitive to the prompting method.\\n\\nA question-answering benchmark is termed \"open book\" if the model\\'s prompt includes text from which the expected answer can be derived (for example, the previous question could be combined with text that includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw solely on its training. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity\\'s Last Exam).\\n\\nLLM bias may be assessed through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs), Stereo Set, and Parity Benchmark.\\n\\nFact-checking and misinformation detection benchmarks are available. A 2023 study compared the fact-checking accuracy of LLMs including ChatGPT 3.5 and 4.0, Bard, and Bing AI against independent fact-checkers such as PolitiFact and Snopes. The results demonstrated moderate proficiency, with GPT-4 achieving the highest accuracy at 71%, lagging behind human fact-checkers.\\n\\nAn earlier standard tested using a portion of the evaluation dataset. It became more common to evaluate a pre-trained model directly through prompting techniques. Researchers vary in how they formulate prompts for particular tasks, particularly with respect to the number of correct examples attached to the prompt (i.e. the value of *n* in *n*-shot prompting).... ## Limitations and challenges\\n\\nDespite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.\\n\\n### Hallucinations\\n\\nHallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect. These hallucinations arise partly through memorization of training data combined with extrapolation beyond factual boundaries, with evaluations demonstrating that models can output verbatim passages from training data, when subjected to specific prompting sequences.', title='Large language model', url='https://en.wikipedia.org/wiki/Large_language_model', date='2023-03-09', last_updated='2025-10-20')], server_time=None)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6e9ed",
   "metadata": {},
   "source": [
    "## ArXiv API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d45b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41288d7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Search\n",
    "def fetch_results(query, max_results=200, page_size=100):\n",
    "    client = arxiv.Client(\n",
    "        page_size=page_size,      # results per page from API\n",
    "        delay_seconds=3,          # be nice to arXiv\n",
    "        num_retries=3\n",
    "    )\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "    seen = set()\n",
    "    rows = []\n",
    "    for r in client.results(search):\n",
    "        if r.entry_id in seen:\n",
    "            continue\n",
    "        seen.add(r.entry_id)\n",
    "        rows.append({\n",
    "            \"arxiv_id\": r.get_short_id() if hasattr(r, \"get_short_id\") else r.entry_id.split('/')[-1],\n",
    "            \"title\": r.title.strip(),\n",
    "            \"published\": r.published.strftime(\"%Y-%m-%d\") if r.published else \"\",\n",
    "            \"updated\": r.updated.strftime(\"%Y-%m-%d\") if r.updated else \"\",\n",
    "            \"primary_category\": getattr(r, \"primary_category\", \"\"),\n",
    "            \"categories\": \", \".join(getattr(r, \"categories\", []) or []),\n",
    "            \"authors\": \", \".join(a.name for a in r.authors),\n",
    "            \"summary\": r.summary.strip(),\n",
    "            \"pdf_url\": r.pdf_url,\n",
    "            \"abs_url\": r.entry_id,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c725f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "def author_names(paper_authors):\n",
    "    if not paper_authors:\n",
    "        return \"\"\n",
    "    names = []\n",
    "    for a in paper_authors:\n",
    "        # supports Author objects and dicts\n",
    "        names.append(getattr(a, \"name\", a.get(\"name\") if isinstance(a, dict) else None))\n",
    "    return \", \".join([n for n in names if n])\n",
    "\n",
    "def paper_row(p):\n",
    "    return {\n",
    "        \"paperId\": getattr(p, \"paperId\", None),\n",
    "        \"title\": getattr(p, \"title\", None),\n",
    "        \"year\": getattr(p, \"year\", None),\n",
    "        \"authors\": author_names(getattr(p, \"authors\", None)),\n",
    "        \"abstract\": getattr(p, \"abstract\", None),\n",
    "        \"url\": getattr(p, \"url\", None),\n",
    "        \"citationCount\": getattr(p, \"citationCount\", None),\n",
    "    }\n",
    "\n",
    "def fetch_bulk_group(sch: SemanticScholar, query: str,\n",
    "                     year_filter: str, fields: list,\n",
    "                     max_papers: int, sort: str | None = None):\n",
    "    \"\"\"\n",
    "    Runs a bulk search and yields up to max_papers Paper objects.\n",
    "    Prints the API estimated total and progress as it goes.\n",
    "    \"\"\"\n",
    "    results = sch.search_paper(\n",
    "        query=query,\n",
    "        year=year_filter,     # e.g., \"2023-\"\n",
    "        fields=fields,\n",
    "        bulk=True,            # /graph/v1/paper/search/bulk\n",
    "        sort=sort,            # only works with bulk=True\n",
    "    )\n",
    "    est_total = getattr(results, \"total\", None)\n",
    "    print(f\"Estimated total: {est_total if est_total is not None else 'n/a'}\")\n",
    "\n",
    "    count = 0\n",
    "    for p in results:        # iterates across pages automatically\n",
    "        yield p\n",
    "        count += 1\n",
    "        if count >= max_papers:\n",
    "            break\n",
    "\n",
    "def fetch_group_df(sch: SemanticScholar, \n",
    "                   tag: str, \n",
    "                   max_papers_override=None) -> pd.DataFrame:\n",
    "    \"\"\"Fetch a single query group and return a DataFrame.\"\"\"\n",
    "    if tag not in QUERY_GROUPS:\n",
    "        valid = \", \".join(QUERY_GROUPS.keys())\n",
    "        raise ValueError(f\"Unknown group '{tag}'. Valid keys: {valid}\")\n",
    "\n",
    "    query = QUERY_GROUPS[tag]\n",
    "    rows = []\n",
    "    for paper in fetch_bulk_group(\n",
    "        sch,\n",
    "        query=query,\n",
    "        year_filter=YEAR_FILTER,\n",
    "        fields=FIELDS,\n",
    "        max_papers=max_papers_override if max_papers_override is not None else MAX_PAPERS_PER_GROUP,\n",
    "        sort=BULK_SORT,\n",
    "    ):\n",
    "        rows.append(paper_row(paper))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=FIELDS)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main(tag: str | None = None, \n",
    "         max_papers_override=None):\n",
    "\n",
    "    sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "\n",
    "    if tag is not None:\n",
    "        return fetch_group_df(sch, tag, max_papers_override=max_papers_override)\n",
    "\n",
    "    out = {}\n",
    "    for k in QUERY_GROUPS:\n",
    "        out[k] = fetch_group_df(sch, k, max_papers_override=max_papers_override)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12b760",
   "metadata": {},
   "source": [
    "# Measure Precision & Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca95283",
   "metadata": {},
   "source": [
    "## Load Refence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f1fdbfa2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define normalization function (normalize_title)\n",
    "def normalize_title(s: str) -> str:\n",
    "    # Unicode normalize\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "    # Remove punctuation-like characters\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)   # keep letters, numbers, underscore, whitespace\n",
    "    # Collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7a901bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold list size: 25\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV of Zotero list of papers\n",
    "gold_df = pd.read_csv(\"data/LLM - Survey Proxies.csv\")\n",
    "\n",
    "gold_df = gold_df[[\"Title\", \"Item Type\", \"Abstract Note\", \n",
    "                   \"Publication Year\", \"Author\", \n",
    "                   \"DOI\", \"ISBN\", \"ISSN\"]].drop_duplicates().reset_index(drop=True)\n",
    "gold_df[\"preprint_flag\"] = gold_df[\"Item Type\"].apply(lambda x: \"preprint\" if x == \"preprint\" else \"non-preprint\")\n",
    "gold_df = gold_df.rename(columns={\"DOI\": \"doi\", \"ISBN\": \"isbn\", \"ISSN\": \"issn\"})\n",
    "gold_df[\"norm_title\"] = gold_df[\"Title\"].apply(normalize_title)\n",
    "\n",
    "print(f\"Gold list size: {len(gold_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d286990c-0982-4551-ab17-495e42325387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Gold list size: 22 after removing bad entries\n"
     ]
    }
   ],
   "source": [
    "# Remove known papers that doesn't exist in WoS Dataset\n",
    "bad_titles = [\n",
    "    \"Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study\",\n",
    "    \"Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models\",\n",
    "    \"The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models\"\n",
    "]\n",
    "\n",
    "bad_titles_normalized = [normalize_title(t) for t in bad_titles]\n",
    "\n",
    "gold_df = gold_df[~gold_df[\"norm_title\"].isin(bad_titles_normalized)].reset_index(drop=True) # filter out bad titles\n",
    "\n",
    "print(f\"Cleaned Gold list size: {len(gold_df)} after removing bad entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "eba22586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles in gold set: 25\n",
      "Unique titles in preprint set: 11\n",
      "Unique titles in journals & proceedings set: 14\n"
     ]
    }
   ],
   "source": [
    "# Separate sets for preprint and non-preprint for 'calc_recall_with_missing' function\n",
    "gold_preprint_set = set(gold_df[gold_df[\"preprint_flag\"] == \"preprint\"][\"norm_title\"])\n",
    "gold_non_preprint_set = set(gold_df[gold_df[\"preprint_flag\"] == \"non-preprint\"][\"norm_title\"])\n",
    "gold_norm_set = set(gold_df[\"norm_title\"])\n",
    "\n",
    "# print the number of unique titles in each set\n",
    "print(f\"Unique titles in gold set: {len(gold_norm_set)}\")\n",
    "print(f\"Unique titles in preprint set: {len(gold_preprint_set)}\")\n",
    "print(f\"Unique titles in journals & proceedings set: {len(gold_non_preprint_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24126d",
   "metadata": {},
   "source": [
    "## Define Recall Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8cc855c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Recall Function\n",
    "# ---------- Normalizers ----------\n",
    "def _first_token(s: str) -> str:\n",
    "    \"\"\"Return the first nonempty token split on common delimiters.\"\"\"\n",
    "    if not isinstance(s, str): \n",
    "        return \"\"\n",
    "    for tok in re.split(r\"[;,|\\s]+\", s.strip()):\n",
    "        if tok:\n",
    "            return tok\n",
    "    return \"\"\n",
    "\n",
    "def normalize_doi(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        return \"\"\n",
    "    x = _first_token(x).lower()\n",
    "    x = re.sub(r'^(https?://(dx\\.)?doi\\.org/)', '', x)\n",
    "    x = x.replace('\\u200b', '')  # zero-width\n",
    "    return x\n",
    "\n",
    "def normalize_issn(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        return \"\"\n",
    "    s = _first_token(x)\n",
    "    s = re.sub(r'[^0-9xX]', '', s).upper()\n",
    "    if len(s) == 8:\n",
    "        return s[:4] + \"-\" + s[4:]\n",
    "    return s\n",
    "\n",
    "def normalize_isbn(x: str) -> str:\n",
    "    if not isinstance(x, str): \n",
    "        return \"\"\n",
    "    s = _first_token(x)\n",
    "    s = re.sub(r'[^0-9xX]', '', s).upper()\n",
    "    return s\n",
    "\n",
    "def canonical_title(s: str) -> str:\n",
    "    if not isinstance(s, str): \n",
    "        return \"\"\n",
    "    # strip diacritics\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    # normalize quotes and dashes/hyphens\n",
    "    s = s.replace(\"\", \"'\").replace(\"\", \"'\").replace(\"\", '\"').replace(\"\", '\"')\n",
    "    s = s.replace(\"\", \"-\").replace(\"\", \"-\")\n",
    "    s = s.lower()\n",
    "    # fix known glued tokens (extend as needed)\n",
    "    s = s.replace(\"financialwellbeing\", \"financial wellbeing\")\n",
    "    # remove punctuation except spaces and alphanumerics\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _ensure_series(df, col):\n",
    "    vals = df[col] if (isinstance(col, str) and col in df.columns) else pd.Series([\"\"] * len(df), index=df.index)\n",
    "    return vals.fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "def calc_recall_with_missing(res_df,\n",
    "                             gold_df = gold_df,\n",
    "                             gold_cols=dict(doi=\"doi\", issn=\"issn\", isbn=\"isbn\", title=\"Title\"),\n",
    "                             res_cols=dict(doi=\"doi\", issn=\"issn\", isbn=\"isbn\", title=\"title\")):\n",
    "    g = gold_df.copy()\n",
    "    r = res_df.copy()\n",
    "\n",
    "    # Normalized keys (robust to missing columns)\n",
    "    g[\"_doi\"]  = _ensure_series(g, gold_cols.get(\"doi\")).map(normalize_doi)\n",
    "    r[\"_doi\"]  = _ensure_series(r, res_cols.get(\"doi\")).map(normalize_doi)\n",
    "\n",
    "    g[\"_issn\"] = _ensure_series(g, gold_cols.get(\"issn\")).map(normalize_issn)\n",
    "    r[\"_issn\"] = _ensure_series(r, res_cols.get(\"issn\")).map(normalize_issn)\n",
    "\n",
    "    g[\"_isbn\"] = _ensure_series(g, gold_cols.get(\"isbn\")).map(normalize_isbn)\n",
    "    r[\"_isbn\"] = _ensure_series(r, res_cols.get(\"isbn\")).map(normalize_isbn)\n",
    "\n",
    "    g[\"_tkey\"] = _ensure_series(g, gold_cols.get(\"title\")).map(canonical_title)\n",
    "    r[\"_tkey\"] = _ensure_series(r, res_cols.get(\"title\")).map(canonical_title)\n",
    "\n",
    "    out = g[[gold_cols[\"title\"]]].rename(columns={gold_cols[\"title\"]: \"gold_title\"}).copy()\n",
    "    out[[\"_doi\", \"_issn\", \"_isbn\", \"_tkey\"]] = g[[\"_doi\", \"_issn\", \"_isbn\", \"_tkey\"]]\n",
    "    out[\"preprint_flag\"] = g[\"preprint_flag\"] if \"preprint_flag\" in g.columns else pd.NA\n",
    "    out[\"matched_by\"] = pd.NA\n",
    "    out[\"matched_title\"] = pd.NA\n",
    "\n",
    "    def do_join(key: str, label: str):\n",
    "        nonlocal out, r\n",
    "        pending = out[out[\"matched_by\"].isna()]\n",
    "        pending = pending[pending[key].astype(bool)]\n",
    "        if pending.empty or key not in r.columns:\n",
    "            return\n",
    "        pending = pending.assign(_row_id=pending.index)\n",
    "        if res_cols.get(\"title\") not in r.columns:\n",
    "            return\n",
    "        right = r[[res_cols[\"title\"], key]].drop_duplicates().set_index(key)\n",
    "        merged = pending.join(right, on=key, how=\"left\")\n",
    "        hits = merged[merged[res_cols[\"title\"]].notna()]\n",
    "        if hits.empty:\n",
    "            return\n",
    "        out.loc[hits[\"_row_id\"], \"matched_by\"] = label\n",
    "        out.loc[hits[\"_row_id\"], \"matched_title\"] = hits[res_cols[\"title\"]].values\n",
    "\n",
    "    # Priority order\n",
    "    do_join(\"_doi\",  \"doi\")\n",
    "    do_join(\"_issn\", \"issn\")\n",
    "    do_join(\"_isbn\", \"isbn\")\n",
    "    do_join(\"_tkey\", \"title_exact\")\n",
    "\n",
    "    matched_mask = out[\"matched_title\"].notna()\n",
    "    tp = matched_mask.sum()\n",
    "    fn = (~matched_mask).sum()\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "    pre_mask = out[\"preprint_flag\"].astype(str).str.lower().eq(\"preprint\")\n",
    "    non_pre_mask = out[\"preprint_flag\"].astype(str).str.lower().eq(\"non-preprint\")\n",
    "\n",
    "    recall_preprint = (matched_mask & pre_mask).sum() / pre_mask.sum() if pre_mask.sum() else 0.0\n",
    "    recall_non_preprint = (matched_mask & non_pre_mask).sum() / non_pre_mask.sum() if non_pre_mask.sum() else 0.0\n",
    "\n",
    "    missing_non_pre_titles = out.loc[(~matched_mask) & non_pre_mask, \"gold_title\"].tolist()\n",
    "    missing_pre_titles = out.loc[(~matched_mask) & pre_mask, \"gold_title\"].tolist()\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"Number of Papers Retrieved\": len(res_df),\n",
    "        \"Recall (All)\": f\"{recall:.2%}\",\n",
    "        \"Recall (Journal & Conf. Papers)\": f\"{recall_non_preprint:.2%}\",\n",
    "        \"Recall (Preprints)\": f\"{recall_preprint:.2%}\",\n",
    "        \"Missing Journal & Conf. Papers\": \"; \".join(sorted(missing_non_pre_titles)),\n",
    "        \"Missing Preprints\": \"; \".join(sorted(missing_pre_titles)),\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344af5d5",
   "metadata": {},
   "source": [
    "## Recall Rate - WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0722654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in Preprint Papers gold set: 11\n",
      "Number of records in Journals & Conference Articles gold set: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ecd2c8a5-91f2-4def-b85a-aa8ada9e6e61",
       "rows": [
        [
         "0",
         "LLM and SimulationA",
         "561",
         "45.45%",
         "90.91%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "1",
         "LLM and SimulationB",
         "1223",
         "50.00%",
         "100.00%",
         "0.00%",
         "",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "2",
         "LLM and SimulationC",
         "3827",
         "31.82%",
         "63.64%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "3",
         "LLM and Survey and SimulationA",
         "320",
         "45.45%",
         "90.91%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "4",
         "LLM and Survey and SimulationB",
         "922",
         "50.00%",
         "100.00%",
         "0.00%",
         "",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "5",
         "LLM and Survey and SimulationC",
         "1503",
         "31.82%",
         "63.64%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "6",
         "LLM and SimulationA and Methods",
         "169",
         "27.27%",
         "54.55%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "7",
         "LLM and SimulationB and Methods",
         "329",
         "40.91%",
         "81.82%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Performance and biases of Large Language Models in public opinion simulation",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "8",
         "LLM and SimulationC and Methods",
         "1406",
         "27.27%",
         "54.55%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM and SimulationA</td>\n",
       "      <td>561</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM and SimulationB</td>\n",
       "      <td>1223</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td></td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM and SimulationC</td>\n",
       "      <td>3827</td>\n",
       "      <td>31.82%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM and Survey and SimulationA</td>\n",
       "      <td>320</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM and Survey and SimulationB</td>\n",
       "      <td>922</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td></td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM and Survey and SimulationC</td>\n",
       "      <td>1503</td>\n",
       "      <td>31.82%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLM and SimulationA and Methods</td>\n",
       "      <td>169</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLM and SimulationB and Methods</td>\n",
       "      <td>329</td>\n",
       "      <td>40.91%</td>\n",
       "      <td>81.82%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLM and SimulationC and Methods</td>\n",
       "      <td>1406</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Query  Number of Papers Retrieved Recall (All)  \\\n",
       "0              LLM and SimulationA                         561       45.45%   \n",
       "1              LLM and SimulationB                        1223       50.00%   \n",
       "2              LLM and SimulationC                        3827       31.82%   \n",
       "3   LLM and Survey and SimulationA                         320       45.45%   \n",
       "4   LLM and Survey and SimulationB                         922       50.00%   \n",
       "5   LLM and Survey and SimulationC                        1503       31.82%   \n",
       "6  LLM and SimulationA and Methods                         169       27.27%   \n",
       "7  LLM and SimulationB and Methods                         329       40.91%   \n",
       "8  LLM and SimulationC and Methods                        1406       27.27%   \n",
       "\n",
       "  Recall (Journal & Conf. Papers) Recall (Preprints)  \\\n",
       "0                          90.91%              0.00%   \n",
       "1                         100.00%              0.00%   \n",
       "2                          63.64%              0.00%   \n",
       "3                          90.91%              0.00%   \n",
       "4                         100.00%              0.00%   \n",
       "5                          63.64%              0.00%   \n",
       "6                          54.55%              0.00%   \n",
       "7                          81.82%              0.00%   \n",
       "8                          54.55%              0.00%   \n",
       "\n",
       "                      Missing Journal & Conf. Papers  \\\n",
       "0  Knowledge of cultural moral norms in large lan...   \n",
       "1                                                      \n",
       "2  Knowledge of cultural moral norms in large lan...   \n",
       "3  Knowledge of cultural moral norms in large lan...   \n",
       "4                                                      \n",
       "5  Knowledge of cultural moral norms in large lan...   \n",
       "6  Can large language models estimate public opin...   \n",
       "7  Can large language models estimate public opin...   \n",
       "8  Can large language models estimate public opin...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "1  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "2  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "3  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "4  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "5  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "6  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "7  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "8  AI-Augmented Surveys: Leveraging Large Languag...  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run recall calculations for Subset WoS set\n",
    "wos_recall_results = {\n",
    "    \"LLM and SimulationA\": calc_recall_with_missing(df_WoS_LLM_and_SimulationA_clean),\n",
    "    \"LLM and SimulationB\": calc_recall_with_missing(df_WoS_LLM_and_SimulationB_clean),\n",
    "    \"LLM and SimulationC\": calc_recall_with_missing(df_WoS_LLM_and_SimulationC_clean),\n",
    "    \"LLM and Survey and SimulationA\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationA_clean),\n",
    "    \"LLM and Survey and SimulationB\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationB_clean),\n",
    "    \"LLM and Survey and SimulationC\": calc_recall_with_missing(df_WoS_LLM_and_Survey_and_SimulationC_clean),\n",
    "    \"LLM and SimulationA and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationA_and_Methods_clean),\n",
    "    \"LLM and SimulationB and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationB_and_Methods_clean),\n",
    "    \"LLM and SimulationC and Methods\": calc_recall_with_missing(df_WoS_LLM_and_SimulationC_and_Methods_clean),\n",
    "}\n",
    "\n",
    "recall_table_WoS = pd.concat(wos_recall_results.values(), \n",
    "                                   keys=wos_recall_results.keys()).reset_index(level=1, drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "\n",
    "print(f\"Number of records in Preprint Papers gold set: {len(gold_preprint_set)}\")\n",
    "print(f\"Number of records in Journals & Conference Articles gold set: {len(gold_non_preprint_set)}\")\n",
    "recall_table_WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddbd9d-9f20-40a3-9b6f-f7687c94e02a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define function: compare_missing_journal_articles\n",
    "def compare_missing_journal_articles(df_recall, query1, query2):\n",
    "    row1 = df_recall[df_recall[\"Query\"] == query1]\n",
    "    row2 = df_recall[df_recall[\"Query\"] == query2]\n",
    "\n",
    "    # print error for missing queries\n",
    "    if row1.empty:\n",
    "        print(f\"Error: Query '{query1}' not found in the recall table.\")\n",
    "        return None\n",
    "    if row2.empty:\n",
    "        print(f\"Error: Query '{query2}' not found in the recall table.\")\n",
    "        return None\n",
    "    \n",
    "    missing1 = set(row1.iloc[0][\"Missing Journal & Conf. Papers\"].split(\"; \")) if pd.notna(row1.iloc[0][\"Missing Journal & Conf. Papers\"]) else set()\n",
    "    missing2 = set(row2.iloc[0][\"Missing Journal & Conf. Papers\"].split(\"; \")) if pd.notna(row2.iloc[0][\"Missing Journal & Conf. Papers\"]) else set()\n",
    "    \n",
    "    only_in_1 = missing1 - missing2\n",
    "    only_in_2 = missing2 - missing1\n",
    "    in_both = missing1.intersection(missing2)\n",
    "    \n",
    "    # output them in a bullet points like\n",
    "    print(f\"Comparison of Missing Journal Articles between '{query1}' and '{query2}':\\n\")\n",
    "    print(f\"ONly Missing in '{query1}' ({len(only_in_1)} articles):\")\n",
    "    for title in sorted(only_in_1):\n",
    "        print(f\" - {title}\")    \n",
    "    print(f\"\\nOnly Missing in '{query2}' ({len(only_in_2)} articles):\")\n",
    "    for title in sorted(only_in_2):\n",
    "        print(f\" - {title}\")\n",
    "    print(f\"\\nMissing In both ({len(in_both)} articles):\")\n",
    "    for title in sorted(in_both):\n",
    "        print(f\" - {title}\")\n",
    "\n",
    "compare_missing_journal_articles(recall_table_WoS_subset,\n",
    "                                 \"LLM and Survey and SimulationA\", \"LLM and SimulationA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8ae5c",
   "metadata": {},
   "source": [
    "## Recall Rate - S.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b44c00e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ce72f0d4-e42f-4fd0-a5ed-7524808be90c",
       "rows": [
        [
         "0",
         "LLM and SimulationA",
         "6012",
         "76.00%",
         "85.71%",
         "63.64%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models"
        ],
        [
         "1",
         "LLM and SimulationB",
         "10000",
         "36.00%",
         "42.86%",
         "27.27%",
         "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing Peoples Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "2",
         "LLM and SimulationC",
         "4574",
         "44.00%",
         "57.14%",
         "27.27%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information"
        ],
        [
         "3",
         "LLM and Survey and SimulationA",
         "3824",
         "80.00%",
         "92.86%",
         "63.64%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models"
        ],
        [
         "4",
         "LLM and Survey and SimulationB",
         "14893",
         "84.00%",
         "92.86%",
         "72.73%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research"
        ],
        [
         "5",
         "LLM and Survey and SimulationC",
         "1939",
         "44.00%",
         "57.14%",
         "27.27%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information"
        ],
        [
         "6",
         "LLM and SimulationA and Methods",
         "1839",
         "44.00%",
         "50.00%",
         "36.36%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "7",
         "LLM and SimulationB and Methods",
         "10000",
         "48.00%",
         "57.14%",
         "36.36%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "8",
         "LLM and SimulationC and Methods",
         "2505",
         "32.00%",
         "42.86%",
         "18.18%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM and SimulationA</td>\n",
       "      <td>6012</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>85.71%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM and SimulationB</td>\n",
       "      <td>10000</td>\n",
       "      <td>36.00%</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Algorithmic Fidelity of Large Language Models ...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM and SimulationC</td>\n",
       "      <td>4574</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM and Survey and SimulationA</td>\n",
       "      <td>3824</td>\n",
       "      <td>80.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM and Survey and SimulationB</td>\n",
       "      <td>14893</td>\n",
       "      <td>84.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>72.73%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM and Survey and SimulationC</td>\n",
       "      <td>1939</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLM and SimulationA and Methods</td>\n",
       "      <td>1839</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>36.36%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLM and SimulationB and Methods</td>\n",
       "      <td>10000</td>\n",
       "      <td>48.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>36.36%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLM and SimulationC and Methods</td>\n",
       "      <td>2505</td>\n",
       "      <td>32.00%</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>18.18%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Query  Number of Papers Retrieved Recall (All)  \\\n",
       "0              LLM and SimulationA                        6012       76.00%   \n",
       "1              LLM and SimulationB                       10000       36.00%   \n",
       "2              LLM and SimulationC                        4574       44.00%   \n",
       "3   LLM and Survey and SimulationA                        3824       80.00%   \n",
       "4   LLM and Survey and SimulationB                       14893       84.00%   \n",
       "5   LLM and Survey and SimulationC                        1939       44.00%   \n",
       "6  LLM and SimulationA and Methods                        1839       44.00%   \n",
       "7  LLM and SimulationB and Methods                       10000       48.00%   \n",
       "8  LLM and SimulationC and Methods                        2505       32.00%   \n",
       "\n",
       "  Recall (Journal & Conf. Papers) Recall (Preprints)  \\\n",
       "0                          85.71%             63.64%   \n",
       "1                          42.86%             27.27%   \n",
       "2                          57.14%             27.27%   \n",
       "3                          92.86%             63.64%   \n",
       "4                          92.86%             72.73%   \n",
       "5                          57.14%             27.27%   \n",
       "6                          50.00%             36.36%   \n",
       "7                          57.14%             36.36%   \n",
       "8                          42.86%             18.18%   \n",
       "\n",
       "                      Missing Journal & Conf. Papers  \\\n",
       "0  Extracting Affect Aggregates from Longitudinal...   \n",
       "1  Algorithmic Fidelity of Large Language Models ...   \n",
       "2  Extracting Affect Aggregates from Longitudinal...   \n",
       "3  The Potential and Challenges of Evaluating Att...   \n",
       "4  The Potential and Challenges of Evaluating Att...   \n",
       "5  Extracting Affect Aggregates from Longitudinal...   \n",
       "6  Can large language models estimate public opin...   \n",
       "7  Can large language models estimate public opin...   \n",
       "8  Can large language models estimate public opin...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  Addressing Systematic Non-response Bias with S...  \n",
       "1  Addressing Systematic Non-response Bias with S...  \n",
       "2  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "3  Addressing Systematic Non-response Bias with S...  \n",
       "4  Addressing Systematic Non-response Bias with S...  \n",
       "5  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "6  Addressing Systematic Non-response Bias with S...  \n",
       "7  Addressing Systematic Non-response Bias with S...  \n",
       "8  AI-Augmented Surveys: Leveraging Large Languag...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run recall calculations\n",
    "ss_recall_results = {\n",
    "    #\"LLM and Survey\": calc_recall_with_missing(df_SS_llm_survey),\n",
    "    \"LLM and SimulationA\": calc_recall_with_missing(df_SS_llm_simA),\n",
    "    \"LLM and SimulationB\": calc_recall_with_missing(df_SS_llm_simB),\n",
    "    \"LLM and SimulationC\": calc_recall_with_missing(df_SS_llm_simC),\n",
    "    #\"Survey and SimulationA\": calc_recall_with_missing(df_SS_survey_simA),\n",
    "    #\"Survey and SimulationB\": calc_recall_with_missing(df_SS_survey_simB),\n",
    "    #\"Survey and SimulationC\": calc_recall_with_missing(df_SS_survey_simC),\n",
    "    \"LLM and Survey and SimulationA\": calc_recall_with_missing(df_SS_llm_survey_simA),\n",
    "    \"LLM and Survey and SimulationB\": calc_recall_with_missing(df_SS_llm_survey_simB),\n",
    "    \"LLM and Survey and SimulationC\": calc_recall_with_missing(df_SS_llm_survey_simC),\n",
    "    \"LLM and SimulationA and Methods\": calc_recall_with_missing(df_SS_llm_simA_methods),\n",
    "    \"LLM and SimulationB and Methods\": calc_recall_with_missing(df_SS_llm_simB_methods),\n",
    "    \"LLM and SimulationC and Methods\": calc_recall_with_missing(df_SS_llm_simC_methods),\n",
    "    # \"LLM and Survey and SimulationA Negated\": calc_recall_with_missing(df_SS_llm_survey_simA_negated),\n",
    "    # \"LLM and Survey and SimulationB Negated\": calc_recall_with_missing(df_SS_llm_survey_simB_negated),\n",
    "    # \"LLM and Survey and SimulationC Negated\": calc_recall_with_missing(df_SS_llm_survey_simC_negated),\n",
    "    # \"LLM and Survey and SimulationA Filtered\": calc_recall_with_missing(df_SS_llm_survey_simA_filtered),\n",
    "    # \"LLM and Survey and SimulationB Filtered\": calc_recall_with_missing(df_SS_llm_survey_simB_filtered),\n",
    "    # \"LLM and Survey and SimulationC Filtered\": calc_recall_with_missing(df_SS_llm_survey_simC_filtered),\n",
    "}\n",
    "\n",
    "recall_table_SS = pd.concat(ss_recall_results.values(), \n",
    "                             keys=ss_recall_results.keys()).reset_index(level=1, drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "recall_table_SS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "979726d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "19b1a35a-b01a-4db6-9563-4533d85f7cf2",
       "rows": [
        [
         "0",
         "LLM and SimulationA",
         "6012",
         "76.00%",
         "85.71%",
         "63.64%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models",
         "Semantic Scholar"
        ],
        [
         "1",
         "LLM and SimulationB",
         "10000",
         "36.00%",
         "42.86%",
         "27.27%",
         "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing Peoples Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Semantic Scholar"
        ],
        [
         "2",
         "LLM and SimulationC",
         "4574",
         "44.00%",
         "57.14%",
         "27.27%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
         "Semantic Scholar"
        ],
        [
         "3",
         "LLM and Survey and SimulationA",
         "3824",
         "80.00%",
         "92.86%",
         "63.64%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models",
         "Semantic Scholar"
        ],
        [
         "4",
         "LLM and Survey and SimulationB",
         "14893",
         "84.00%",
         "92.86%",
         "72.73%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research",
         "Semantic Scholar"
        ],
        [
         "5",
         "LLM and Survey and SimulationC",
         "1939",
         "44.00%",
         "57.14%",
         "27.27%",
         "Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
         "Semantic Scholar"
        ],
        [
         "6",
         "LLM and SimulationA and Methods",
         "1839",
         "44.00%",
         "50.00%",
         "36.36%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Semantic Scholar"
        ],
        [
         "7",
         "LLM and SimulationB and Methods",
         "10000",
         "48.00%",
         "57.14%",
         "36.36%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Semantic Scholar"
        ],
        [
         "8",
         "LLM and SimulationC and Methods",
         "2505",
         "32.00%",
         "42.86%",
         "18.18%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Semantic Scholar"
        ],
        [
         "9",
         "LLM and SimulationA",
         "561",
         "45.45%",
         "90.91%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "10",
         "LLM and SimulationB",
         "1223",
         "50.00%",
         "100.00%",
         "0.00%",
         "",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "11",
         "LLM and SimulationC",
         "3827",
         "31.82%",
         "63.64%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "12",
         "LLM and Survey and SimulationA",
         "320",
         "45.45%",
         "90.91%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "13",
         "LLM and Survey and SimulationB",
         "922",
         "50.00%",
         "100.00%",
         "0.00%",
         "",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "14",
         "LLM and Survey and SimulationC",
         "1503",
         "31.82%",
         "63.64%",
         "0.00%",
         "Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "15",
         "LLM and SimulationA and Methods",
         "169",
         "27.27%",
         "54.55%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; Out of One, Many: Using Language Models to Simulate Human Samples; Performance and biases of Large Language Models in public opinion simulation; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "16",
         "LLM and SimulationB and Methods",
         "329",
         "40.91%",
         "81.82%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Performance and biases of Large Language Models in public opinion simulation",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ],
        [
         "17",
         "LLM and SimulationC and Methods",
         "1406",
         "27.27%",
         "54.55%",
         "0.00%",
         "Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention",
         "Web of Science"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 18
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM and SimulationA</td>\n",
       "      <td>6012</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>85.71%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM and SimulationB</td>\n",
       "      <td>10000</td>\n",
       "      <td>36.00%</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Algorithmic Fidelity of Large Language Models ...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM and SimulationC</td>\n",
       "      <td>4574</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM and Survey and SimulationA</td>\n",
       "      <td>3824</td>\n",
       "      <td>80.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM and Survey and SimulationB</td>\n",
       "      <td>14893</td>\n",
       "      <td>84.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>72.73%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM and Survey and SimulationC</td>\n",
       "      <td>1939</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>Extracting Affect Aggregates from Longitudinal...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLM and SimulationA and Methods</td>\n",
       "      <td>1839</td>\n",
       "      <td>44.00%</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>36.36%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLM and SimulationB and Methods</td>\n",
       "      <td>10000</td>\n",
       "      <td>48.00%</td>\n",
       "      <td>57.14%</td>\n",
       "      <td>36.36%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLM and SimulationC and Methods</td>\n",
       "      <td>2505</td>\n",
       "      <td>32.00%</td>\n",
       "      <td>42.86%</td>\n",
       "      <td>18.18%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Semantic Scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LLM and SimulationA</td>\n",
       "      <td>561</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LLM and SimulationB</td>\n",
       "      <td>1223</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td></td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LLM and SimulationC</td>\n",
       "      <td>3827</td>\n",
       "      <td>31.82%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LLM and Survey and SimulationA</td>\n",
       "      <td>320</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLM and Survey and SimulationB</td>\n",
       "      <td>922</td>\n",
       "      <td>50.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td></td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LLM and Survey and SimulationC</td>\n",
       "      <td>1503</td>\n",
       "      <td>31.82%</td>\n",
       "      <td>63.64%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LLM and SimulationA and Methods</td>\n",
       "      <td>169</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LLM and SimulationB and Methods</td>\n",
       "      <td>329</td>\n",
       "      <td>40.91%</td>\n",
       "      <td>81.82%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LLM and SimulationC and Methods</td>\n",
       "      <td>1406</td>\n",
       "      <td>27.27%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>Can large language models estimate public opin...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "      <td>Web of Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Query  Number of Papers Retrieved Recall (All)  \\\n",
       "0               LLM and SimulationA                        6012       76.00%   \n",
       "1               LLM and SimulationB                       10000       36.00%   \n",
       "2               LLM and SimulationC                        4574       44.00%   \n",
       "3    LLM and Survey and SimulationA                        3824       80.00%   \n",
       "4    LLM and Survey and SimulationB                       14893       84.00%   \n",
       "5    LLM and Survey and SimulationC                        1939       44.00%   \n",
       "6   LLM and SimulationA and Methods                        1839       44.00%   \n",
       "7   LLM and SimulationB and Methods                       10000       48.00%   \n",
       "8   LLM and SimulationC and Methods                        2505       32.00%   \n",
       "9               LLM and SimulationA                         561       45.45%   \n",
       "10              LLM and SimulationB                        1223       50.00%   \n",
       "11              LLM and SimulationC                        3827       31.82%   \n",
       "12   LLM and Survey and SimulationA                         320       45.45%   \n",
       "13   LLM and Survey and SimulationB                         922       50.00%   \n",
       "14   LLM and Survey and SimulationC                        1503       31.82%   \n",
       "15  LLM and SimulationA and Methods                         169       27.27%   \n",
       "16  LLM and SimulationB and Methods                         329       40.91%   \n",
       "17  LLM and SimulationC and Methods                        1406       27.27%   \n",
       "\n",
       "   Recall (Journal & Conf. Papers) Recall (Preprints)  \\\n",
       "0                           85.71%             63.64%   \n",
       "1                           42.86%             27.27%   \n",
       "2                           57.14%             27.27%   \n",
       "3                           92.86%             63.64%   \n",
       "4                           92.86%             72.73%   \n",
       "5                           57.14%             27.27%   \n",
       "6                           50.00%             36.36%   \n",
       "7                           57.14%             36.36%   \n",
       "8                           42.86%             18.18%   \n",
       "9                           90.91%              0.00%   \n",
       "10                         100.00%              0.00%   \n",
       "11                          63.64%              0.00%   \n",
       "12                          90.91%              0.00%   \n",
       "13                         100.00%              0.00%   \n",
       "14                          63.64%              0.00%   \n",
       "15                          54.55%              0.00%   \n",
       "16                          81.82%              0.00%   \n",
       "17                          54.55%              0.00%   \n",
       "\n",
       "                       Missing Journal & Conf. Papers  \\\n",
       "0   Extracting Affect Aggregates from Longitudinal...   \n",
       "1   Algorithmic Fidelity of Large Language Models ...   \n",
       "2   Extracting Affect Aggregates from Longitudinal...   \n",
       "3   The Potential and Challenges of Evaluating Att...   \n",
       "4   The Potential and Challenges of Evaluating Att...   \n",
       "5   Extracting Affect Aggregates from Longitudinal...   \n",
       "6   Can large language models estimate public opin...   \n",
       "7   Can large language models estimate public opin...   \n",
       "8   Can large language models estimate public opin...   \n",
       "9   Knowledge of cultural moral norms in large lan...   \n",
       "10                                                      \n",
       "11  Knowledge of cultural moral norms in large lan...   \n",
       "12  Knowledge of cultural moral norms in large lan...   \n",
       "13                                                      \n",
       "14  Knowledge of cultural moral norms in large lan...   \n",
       "15  Can large language models estimate public opin...   \n",
       "16  Can large language models estimate public opin...   \n",
       "17  Can large language models estimate public opin...   \n",
       "\n",
       "                                    Missing Preprints            Source  \n",
       "0   Addressing Systematic Non-response Bias with S...  Semantic Scholar  \n",
       "1   Addressing Systematic Non-response Bias with S...  Semantic Scholar  \n",
       "2   AI-Augmented Surveys: Leveraging Large Languag...  Semantic Scholar  \n",
       "3   Addressing Systematic Non-response Bias with S...  Semantic Scholar  \n",
       "4   Addressing Systematic Non-response Bias with S...  Semantic Scholar  \n",
       "5   AI-Augmented Surveys: Leveraging Large Languag...  Semantic Scholar  \n",
       "6   Addressing Systematic Non-response Bias with S...  Semantic Scholar  \n",
       "7   Addressing Systematic Non-response Bias with S...  Semantic Scholar  \n",
       "8   AI-Augmented Surveys: Leveraging Large Languag...  Semantic Scholar  \n",
       "9   AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "10  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "11  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "12  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "13  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "14  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "15  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "16  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  \n",
       "17  AI-Augmented Surveys: Leveraging Large Languag...    Web of Science  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine recall_table_SS and recall_table_WoS into a dataframe with an additional column 'Source'\n",
    "recall_table = pd.concat([\n",
    "    recall_table_SS.assign(Source=\"Semantic Scholar\"),\n",
    "    recall_table_WoS.assign(Source=\"Web of Science\")\n",
    "], ignore_index=True)\n",
    "\n",
    "recall_table.to_csv(\"recall_table_final.csv\", index=False)\n",
    "recall_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785080a",
   "metadata": {},
   "source": [
    "## Recall Rate - Elicit A.I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0c8db808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a826b5d2-cc86-4828-a4e7-b5eb3b49d96b",
       "rows": [
        [
         "0",
         "Elicit Prompt 1",
         "104",
         "36.00%",
         "21.43%",
         "54.55%",
         "AIHuman Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing Peoples Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "1",
         "Elicit Prompt 2",
         "104",
         "36.00%",
         "21.43%",
         "54.55%",
         "AIHuman Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing Peoples Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "2",
         "Elicit Prompt 3",
         "104",
         "36.00%",
         "28.57%",
         "45.45%",
         "AIHuman Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "3",
         "Elicit Prompt 4",
         "104",
         "28.00%",
         "14.29%",
         "45.45%",
         "AIHuman Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; LLM-Based Doppelgnger Models: Leveraging Synthetic Data for Human-Like Responses in Survey Simulations; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing Peoples Financial Wellbeing; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ],
        [
         "4",
         "Elicit All Prompts",
         "220",
         "32.00%",
         "28.57%",
         "36.36%",
         "AIHuman Hybrids for Marketing Research: Leveraging Large Language Models (LLMs) as Collaborators; Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study; Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias; Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models; Knowledge of cultural moral norms in large language models; Performance and biases of Large Language Models in public opinion simulation; Predicting Missing Values in Survey Data Using Prompt Engineering for Addressing Item Non-Response; SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies; The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models; Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elicit Prompt 1</td>\n",
       "      <td>104</td>\n",
       "      <td>36.00%</td>\n",
       "      <td>21.43%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>AIHuman Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elicit Prompt 2</td>\n",
       "      <td>104</td>\n",
       "      <td>36.00%</td>\n",
       "      <td>21.43%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>AIHuman Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elicit Prompt 3</td>\n",
       "      <td>104</td>\n",
       "      <td>36.00%</td>\n",
       "      <td>28.57%</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>AIHuman Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elicit Prompt 4</td>\n",
       "      <td>104</td>\n",
       "      <td>28.00%</td>\n",
       "      <td>14.29%</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>AIHuman Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elicit All Prompts</td>\n",
       "      <td>220</td>\n",
       "      <td>32.00%</td>\n",
       "      <td>28.57%</td>\n",
       "      <td>36.36%</td>\n",
       "      <td>AIHuman Hybrids for Marketing Research: Lever...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Query  Number of Papers Retrieved Recall (All)  \\\n",
       "0     Elicit Prompt 1                         104       36.00%   \n",
       "1     Elicit Prompt 2                         104       36.00%   \n",
       "2     Elicit Prompt 3                         104       36.00%   \n",
       "3     Elicit Prompt 4                         104       28.00%   \n",
       "4  Elicit All Prompts                         220       32.00%   \n",
       "\n",
       "  Recall (Journal & Conf. Papers) Recall (Preprints)  \\\n",
       "0                          21.43%             54.55%   \n",
       "1                          21.43%             54.55%   \n",
       "2                          28.57%             45.45%   \n",
       "3                          14.29%             45.45%   \n",
       "4                          28.57%             36.36%   \n",
       "\n",
       "                      Missing Journal & Conf. Papers  \\\n",
       "0  AIHuman Hybrids for Marketing Research: Lever...   \n",
       "1  AIHuman Hybrids for Marketing Research: Lever...   \n",
       "2  AIHuman Hybrids for Marketing Research: Lever...   \n",
       "3  AIHuman Hybrids for Marketing Research: Lever...   \n",
       "4  AIHuman Hybrids for Marketing Research: Lever...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  Addressing Systematic Non-response Bias with S...  \n",
       "1  Addressing Systematic Non-response Bias with S...  \n",
       "2  Addressing Systematic Non-response Bias with S...  \n",
       "3  AI-Augmented Surveys: Leveraging Large Languag...  \n",
       "4  AI-Augmented Surveys: Leveraging Large Languag...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elicit_recall_results = {\n",
    "    \"Elicit Prompt 1\": calc_recall_with_missing(df_Elicit1),\n",
    "    \"Elicit Prompt 2\": calc_recall_with_missing(df_Elicit2),\n",
    "    \"Elicit Prompt 3\": calc_recall_with_missing(df_Elicit3),\n",
    "    \"Elicit Prompt 4\": calc_recall_with_missing(df_Elicit4),\n",
    "    \"Elicit All Prompts\": calc_recall_with_missing(df_Elicit),\n",
    "}\n",
    "\n",
    "recall_table_Elicit = pd.concat(elicit_recall_results.values(), \n",
    "                                keys=elicit_recall_results.keys()).reset_index(level=1, drop=True).reset_index().rename(columns={\"index\": \"Query\"})\n",
    "recall_table_Elicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ec867",
   "metadata": {},
   "source": [
    "# Final Full Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b3d79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total: 7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:semanticscholar:IDs not found: ['2a0955f2c5ec8d42774e6cb744bdae579c74a966']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in FULL final SS dataframe: 7194\n"
     ]
    }
   ],
   "source": [
    "# Fetch the FULL final SS dataframe\n",
    "df_SS_final_FULL_raw = ss_fetch_bulk(\"ss_llm_survey_simA\", year_filter=\"2000-\", max_papers = 7194)\n",
    "\n",
    "df_SS_final_FULL_raw.to_csv(\"results_ss/df_SS_final_FULL.csv\", index=False)\n",
    "\n",
    "df_SS_final_FULL_raw = pd.read_csv(\"results_ss/df_SS_final_FULL.csv\")\n",
    "print(f\"Number of records in FULL final SS dataframe: {len(df_SS_final_FULL_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1949171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in df_WoS_final_FULL: 517\n"
     ]
    }
   ],
   "source": [
    "# Fetch the FULL final WoS dataframe\n",
    "df_WoS_final_FULL_raw = pd.read_excel(\"results_wos/df_WoS_LLM_and_Survey_and_SimulationA_FULL.xls\")\n",
    "print(f\"Number of records in df_WoS_final_FULL: {len(df_WoS_final_FULL_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f42c3",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8045d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_WoS_final_FULL_raw:\n",
      " - 180 Day Usage Count\n",
      " - Abstract\n",
      " - Addresses\n",
      " - Affiliations\n",
      " - Article Number\n",
      " - Article Title\n",
      " - Author Full Names\n",
      " - Author Keywords\n",
      " - Authors\n",
      " - Book Author Full Names\n",
      " - Book Authors\n",
      " - Book DOI\n",
      " - Book Editors\n",
      " - Book Group Authors\n",
      " - Book Series Subtitle\n",
      " - Book Series Title\n",
      " - Cited Reference Count\n",
      " - Cited References\n",
      " - Conference Date\n",
      " - Conference Host\n",
      " - Conference Location\n",
      " - Conference Sponsor\n",
      " - Conference Title\n",
      " - DOI\n",
      " - DOI Link\n",
      " - Date of Export\n",
      " - Document Type\n",
      " - Early Access Date\n",
      " - Email Addresses\n",
      " - End Page\n",
      " - Funding Name Preferred\n",
      " - Funding Orgs\n",
      " - Funding Text\n",
      " - Group Authors\n",
      " - Highly Cited Status\n",
      " - Hot Paper Status\n",
      " - IDS Number\n",
      " - ISBN\n",
      " - ISSN\n",
      " - Issue\n",
      " - Journal Abbreviation\n",
      " - Journal ISO Abbreviation\n",
      " - Keywords Plus\n",
      " - Language\n",
      " - Meeting Abstract\n",
      " - Number of Pages\n",
      " - ORCIDs\n",
      " - Open Access Designations\n",
      " - Part Number\n",
      " - Publication Date\n",
      " - Publication Type\n",
      " - Publication Year\n",
      " - Publisher\n",
      " - Publisher Address\n",
      " - Publisher City\n",
      " - Pubmed Id\n",
      " - Reprint Addresses\n",
      " - Research Areas\n",
      " - Researcher Ids\n",
      " - Since 2013 Usage Count\n",
      " - Source Title\n",
      " - Special Issue\n",
      " - Start Page\n",
      " - Supplement\n",
      " - Times Cited, All Databases\n",
      " - Times Cited, WoS Core\n",
      " - UT (Unique WOS ID)\n",
      " - Volume\n",
      " - Web of Science Index\n",
      " - Web of Science Record\n",
      " - WoS Categories\n",
      " - eISSN\n"
     ]
    }
   ],
   "source": [
    "#print out the columns names of df_WoS_final_FULL sorted\n",
    "print(\"Columns in df_WoS_final_FULL_raw:\")\n",
    "for col in sorted(df_WoS_final_FULL_raw.columns):\n",
    "    print(f\" - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "45b6fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in WoS: ['title', 'year', 'authors', 'abstract', 'doi', 'publicationTypes', 'researchAreas', 'norm_title']\n",
      "Number of records in df_WoS_final_FULL: 517\n",
      "Columns in SS: ['title', 'year', 'authors', 'abstract', 'doi', 'publicationTypes', 'researchAreas', 'norm_title']\n",
      "Number of records in df_SS_final_FULL: 7194\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare Final dataframe\n",
    "\n",
    "# WOS\n",
    "df_WoS_final_FULL = df_WoS_final_FULL_raw.rename(columns={\"Article Title\": \"title\", \n",
    "                                                          'Publication Year': 'year',\n",
    "                                                          'Abstract': 'abstract',\n",
    "                                                          'Document Type': 'publicationTypes',\n",
    "                                                          'DOI': 'doi',\n",
    "                                                          \"Research Areas\": \"researchAreas\",\n",
    "                                                          \"Author Full Names\": \"authors\",})\n",
    "df_WoS_final_FULL['norm_title'] = df_WoS_final_FULL['title'].map(normalize_title)\n",
    "df_WoS_final_FULL = df_WoS_final_FULL[['title', 'year', 'authors', 'abstract', 'doi', 'publicationTypes', 'researchAreas', 'norm_title']]\n",
    "\n",
    "# SS\n",
    "df_SS_final_FULL = df_SS_final_FULL_raw.rename(columns={\"s2FieldsOfStudy\": \"researchAreas\"})\n",
    "df_SS_final_FULL['publicationTypes'] = df_SS_final_FULL['publicationTypes'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n",
    "df_SS_final_FULL['norm_title'] = df_SS_final_FULL['title'].map(normalize_title)\n",
    "df_SS_final_FULL = df_SS_final_FULL[['title', 'year', 'authors', 'abstract', 'doi', 'publicationTypes', 'researchAreas', 'norm_title']]\n",
    "\n",
    "\n",
    "print(f'Columns in WoS: {df_WoS_final_FULL.columns.tolist()}')\n",
    "print(f'Number of records in df_WoS_final_FULL: {len(df_WoS_final_FULL)}')\n",
    "print(f'Columns in SS: {df_SS_final_FULL.columns.tolist()}')\n",
    "print(f'Number of records in df_SS_final_FULL: {len(df_SS_final_FULL)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8e5b291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "44632541-936f-4d12-8527-d433f18a6c61",
       "rows": [
        [
         "0",
         "517",
         "45.45%",
         "81.82%",
         "9.09%",
         "Knowledge of cultural moral norms in large language models; Synthetic Replacements for Human Survey Data? The Perils of Large Language Models",
         "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction; Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; Examining the Feasibility of Large Language Models as Survey Respondents; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models; Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information; The threat of analytic flexibility in using large language models to simulate human data: A call to attention"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>81.82%</td>\n",
       "      <td>9.09%</td>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>AI-Augmented Surveys: Leveraging Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Papers Retrieved Recall (All) Recall (Journal & Conf. Papers)  \\\n",
       "0                         517       45.45%                          81.82%   \n",
       "\n",
       "  Recall (Preprints)                     Missing Journal & Conf. Papers  \\\n",
       "0              9.09%  Knowledge of cultural moral norms in large lan...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  AI-Augmented Surveys: Leveraging Large Languag...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_recall_with_missing(df_WoS_final_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a20c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the doi of df_WoS_final and df_WoS_final_FULL, do they match?\n",
    "wos_doi_set = set(df_WoS_LLM_and_Survey_and_SimulationA_clean['doi'].dropna().unique())\n",
    "wos_full_doi_set = set(df_WoS_final_FULL['doi'].dropna().unique())\n",
    "print(f'Number of unique DOIs in WoS: {len(wos_doi_set)}')\n",
    "print(f'Number of unique DOIs in WoS FULL: {len(wos_full_doi_set)}')\n",
    "print(f'Number of common DOIs in both WoS and WoS FULL: {len(wos_doi_set.intersection(wos_full_doi_set))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "359ebdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Distribution of Publication Years\n",
    "fig, axes = plt.subplots(2, 1, figsize=(7, 5), sharex=True)\n",
    "\n",
    "sns.histplot(df_WoS_final['year'], color='blue', ax=axes[0], \n",
    "             kde=False, stat='count', bins=range(2022, 2027), alpha=0.7)\n",
    "axes[0].set_title('Distribution of Publication Year (2020 to 2025)')\n",
    "axes[0].set_ylabel('Number of Papers')\n",
    "axes[0].legend(['Web of Science'])\n",
    "\n",
    "sns.histplot(df_SS_final['year'], color='orange', ax=axes[1], \n",
    "             kde=False, stat='count', bins=range(2022, 2027), alpha=0.7)\n",
    "axes[1].set_title('')\n",
    "axes[1].set_xlabel('Publication Year')\n",
    "axes[1].set_ylabel('Number of Papers')\n",
    "axes[1].legend(['Semantic Scholar'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eeb33f",
   "metadata": {},
   "source": [
    "## Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2a5e4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_WoS_final_FULL: 517\n",
      "Number of rows in df_SS_final_FULL: 7194\n",
      "\n",
      "Number of rows in combined dataframe before dropping duplicates: 7711\n",
      "Number of rows in combined dataframe after dropping DOI duplicates: 5521\n",
      "Number of rows in combined dataframe after dropping duplicates: 5478\n",
      "Number of titles dropped due to duplicate titles: -2233\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2ba7ace1-b02c-4a43-9918-9c947c514831",
       "rows": [
        [
         "0",
         "5478",
         "76.00%",
         "92.86%",
         "54.55%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5478</td>\n",
       "      <td>76.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>54.55%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Papers Retrieved Recall (All) Recall (Journal & Conf. Papers)  \\\n",
       "0                        5478       76.00%                          92.86%   \n",
       "\n",
       "  Recall (Preprints)                     Missing Journal & Conf. Papers  \\\n",
       "0             54.55%  The Potential and Challenges of Evaluating Att...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  Addressing Systematic Non-response Bias with S...  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Number of rows in df_WoS_final_FULL: {len(df_WoS_final_FULL)}')\n",
    "print(f'Number of rows in df_SS_final_FULL: {len(df_SS_final_FULL)}\\n')\n",
    "\n",
    "df_combined = pd.concat([df_WoS_final_FULL, df_SS_final_FULL], ignore_index=True)\n",
    "print(f'Number of rows in combined dataframe before dropping duplicates: {len(df_combined)}')\n",
    "\n",
    "df_combined = df_combined.drop_duplicates(subset = ['doi']).reset_index(drop = True)\n",
    "print(f'Number of rows in combined dataframe after dropping DOI duplicates: {len(df_combined)}')\n",
    "\n",
    "df_combined = df_combined.drop_duplicates(subset=['norm_title']).reset_index(drop=True)\n",
    "df_combined = df_combined.drop(columns=['norm_title'])\n",
    "print(f'Number of rows in combined dataframe after dropping duplicates: {len(df_combined)}')\n",
    "\n",
    "print(f'Number of titles dropped due to duplicate titles: {len(df_combined) - len(df_WoS_final_FULL) - len(df_SS_final_FULL)}')\n",
    "\n",
    "calc_recall_with_missing(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3f6f7d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acoustics; Computer Science; Engineering; Imaging Science & Photographic Technology',\n",
       " 'Acoustics; Engineering',\n",
       " 'Acoustics; Engineering; Mechanics',\n",
       " 'Agricultural and Food Sciences',\n",
       " 'Agricultural and Food Sciences; Biology',\n",
       " 'Agricultural and Food Sciences; Biology; Economics',\n",
       " 'Agricultural and Food Sciences; Biology; Economics; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Biology; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Biology; Environmental Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Biology; Medicine',\n",
       " 'Agricultural and Food Sciences; Biology; Physics',\n",
       " 'Agricultural and Food Sciences; Business',\n",
       " 'Agricultural and Food Sciences; Business; Economics',\n",
       " 'Agricultural and Food Sciences; Business; Economics; Education; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Business; Economics; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Business; Economics; Environmental Science; Sociology',\n",
       " 'Agricultural and Food Sciences; Business; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Business; Environmental Science; Geography',\n",
       " 'Agricultural and Food Sciences; Business; Geography',\n",
       " 'Agricultural and Food Sciences; Business; Sociology',\n",
       " 'Agricultural and Food Sciences; Chemistry',\n",
       " 'Agricultural and Food Sciences; Chemistry; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Chemistry; Environmental Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Chemistry; Materials Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Chemistry; Medicine',\n",
       " 'Agricultural and Food Sciences; Computer Science',\n",
       " 'Agricultural and Food Sciences; Computer Science; Engineering; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Computer Science; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Computer Science; Environmental Science; Geography',\n",
       " 'Agricultural and Food Sciences; Computer Science; Environmental Science; Geography; Medicine',\n",
       " 'Agricultural and Food Sciences; Computer Science; Environmental Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Computer Science; Mathematics',\n",
       " 'Agricultural and Food Sciences; Computer Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Economics',\n",
       " 'Agricultural and Food Sciences; Economics; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Economics; Environmental Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Economics; Environmental Science; Physics',\n",
       " 'Agricultural and Food Sciences; Economics; Environmental Science; Sociology',\n",
       " 'Agricultural and Food Sciences; Economics; Medicine',\n",
       " 'Agricultural and Food Sciences; Economics; Physics',\n",
       " 'Agricultural and Food Sciences; Engineering',\n",
       " 'Agricultural and Food Sciences; Engineering; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Engineering; Environmental Science; Physics',\n",
       " 'Agricultural and Food Sciences; Engineering; Materials Science',\n",
       " 'Agricultural and Food Sciences; Engineering; Materials Science; Mathematics',\n",
       " 'Agricultural and Food Sciences; Environmental Science',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Geography',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Geography; Medicine',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Geology',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Materials Science',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Mathematics',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Medicine',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Medicine; Psychology',\n",
       " 'Agricultural and Food Sciences; Environmental Science; Physics',\n",
       " 'Agricultural and Food Sciences; Geography',\n",
       " 'Agricultural and Food Sciences; Geography; Sociology',\n",
       " 'Agricultural and Food Sciences; Materials Science',\n",
       " 'Agricultural and Food Sciences; Mathematics',\n",
       " 'Agricultural and Food Sciences; Mathematics; Physics',\n",
       " 'Agricultural and Food Sciences; Medicine',\n",
       " 'Agricultural and Food Sciences; Physics',\n",
       " 'Agricultural and Food Sciences; Sociology',\n",
       " 'Agriculture',\n",
       " 'Agriculture; Biotechnology & Applied Microbiology; Energy & Fuels',\n",
       " 'Agriculture; Computer Science',\n",
       " 'Agriculture; Food Science & Technology',\n",
       " 'Agriculture; Plant Sciences',\n",
       " 'Allergy; Dermatology',\n",
       " 'Allergy; Immunology',\n",
       " 'Anatomy & Morphology',\n",
       " 'Anesthesiology; General & Internal Medicine',\n",
       " 'Art',\n",
       " 'Art; Biology',\n",
       " 'Art; Business; Psychology',\n",
       " 'Art; Computer Science',\n",
       " 'Art; Computer Science; Engineering',\n",
       " 'Art; Engineering',\n",
       " 'Art; History',\n",
       " 'Art; History; Sociology',\n",
       " 'Art; Medicine',\n",
       " 'Art; Philosophy',\n",
       " 'Art; Philosophy; Sociology',\n",
       " 'Art; Psychology',\n",
       " 'Art; Psychology; Sociology',\n",
       " 'Art; Sociology',\n",
       " 'Arts & Humanities - Other Topics; Psychology',\n",
       " 'Arts & Humanities - Other Topics; Social Sciences - Other Topics',\n",
       " 'Astronomy & Astrophysics',\n",
       " 'Astronomy & Astrophysics; Computer Science; Optics',\n",
       " 'Astronomy & Astrophysics; Instruments & Instrumentation; Optics',\n",
       " 'Audiology & Speech-Language Pathology; Otorhinolaryngology',\n",
       " 'Automation & Control Systems',\n",
       " 'Automation & Control Systems; Computer Science',\n",
       " 'Automation & Control Systems; Computer Science; Transportation',\n",
       " 'Automation & Control Systems; Engineering; Instruments & Instrumentation',\n",
       " 'Automation & Control Systems; Robotics',\n",
       " 'Behavioral Sciences; Neurosciences & Neurology; Psychology',\n",
       " 'Biochemistry & Molecular Biology; Biophysics; Optics',\n",
       " 'Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology',\n",
       " 'Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics',\n",
       " 'Biochemistry & Molecular Biology; Cell Biology; Research & Experimental Medicine',\n",
       " 'Biochemistry & Molecular Biology; Chemistry',\n",
       " 'Biochemistry & Molecular Biology; Mathematical & Computational Biology',\n",
       " 'Biochemistry & Molecular Biology; Optics; Radiology',\n",
       " 'Biology',\n",
       " 'Biology; Business; Medicine',\n",
       " 'Biology; Chemistry; Computer Science',\n",
       " 'Biology; Chemistry; Computer Science; Physics',\n",
       " 'Biology; Chemistry; Engineering; Materials Science; Medicine',\n",
       " 'Biology; Chemistry; Environmental Science; Medicine',\n",
       " 'Biology; Chemistry; Medicine',\n",
       " 'Biology; Computer Science',\n",
       " 'Biology; Computer Science; Education',\n",
       " 'Biology; Computer Science; Education; Engineering; Environmental Science',\n",
       " 'Biology; Computer Science; Engineering; Medicine',\n",
       " 'Biology; Computer Science; Environmental Science',\n",
       " 'Biology; Computer Science; Environmental Science; Medicine',\n",
       " 'Biology; Computer Science; Linguistics',\n",
       " 'Biology; Computer Science; Linguistics; Psychology',\n",
       " 'Biology; Computer Science; Materials Science; Medicine',\n",
       " 'Biology; Computer Science; Medicine',\n",
       " 'Biology; Computer Science; Medicine; Psychology',\n",
       " 'Biology; Computer Science; Psychology',\n",
       " 'Biology; Education; Medicine',\n",
       " 'Biology; Engineering; Environmental Science; Geography',\n",
       " 'Biology; Environmental Science',\n",
       " 'Biology; Environmental Science; Geography',\n",
       " 'Biology; Environmental Science; Geography; Medicine',\n",
       " 'Biology; Environmental Science; Geography; Philosophy',\n",
       " 'Biology; Environmental Science; History',\n",
       " 'Biology; Environmental Science; Medicine',\n",
       " 'Biology; Environmental Science; Philosophy',\n",
       " 'Biology; Linguistics',\n",
       " 'Biology; Linguistics; Psychology',\n",
       " 'Biology; Medicine',\n",
       " 'Biology; Medicine; Philosophy',\n",
       " 'Biology; Medicine; Psychology',\n",
       " 'Biology; Physics',\n",
       " 'Biology; Psychology',\n",
       " 'Biotechnology & Applied Microbiology',\n",
       " 'Biotechnology & Applied Microbiology; Engineering',\n",
       " 'Biotechnology & Applied Microbiology; Marine & Freshwater Biology',\n",
       " 'Business',\n",
       " 'Business & Economics',\n",
       " 'Business; Computer Science',\n",
       " 'Business; Computer Science; Economics',\n",
       " 'Business; Computer Science; Economics; Mathematics',\n",
       " 'Business; Computer Science; Economics; Psychology',\n",
       " 'Business; Computer Science; Education',\n",
       " 'Business; Computer Science; Education; Engineering',\n",
       " 'Business; Computer Science; Education; Linguistics',\n",
       " 'Business; Computer Science; Engineering',\n",
       " 'Business; Computer Science; Engineering; Materials Science',\n",
       " 'Business; Computer Science; Engineering; Medicine',\n",
       " 'Business; Computer Science; Environmental Science',\n",
       " 'Business; Computer Science; Law',\n",
       " 'Business; Computer Science; Linguistics',\n",
       " 'Business; Computer Science; Mathematics',\n",
       " 'Business; Computer Science; Political Science',\n",
       " 'Business; Computer Science; Psychology',\n",
       " 'Business; Computer Science; Sociology',\n",
       " 'Business; Economics',\n",
       " 'Business; Economics; Engineering',\n",
       " 'Business; Economics; Environmental Science',\n",
       " 'Business; Economics; Environmental Science; Physics',\n",
       " 'Business; Economics; Environmental Science; Sociology',\n",
       " 'Business; Economics; Political Science',\n",
       " 'Business; Economics; Sociology',\n",
       " 'Business; Education',\n",
       " 'Business; Education; Medicine',\n",
       " 'Business; Engineering; Environmental Science',\n",
       " 'Business; Environmental Science',\n",
       " 'Business; Environmental Science; Medicine',\n",
       " 'Business; Environmental Science; Physics',\n",
       " 'Business; Environmental Science; Political Science',\n",
       " 'Business; Law',\n",
       " 'Business; Medicine',\n",
       " 'Business; Psychology',\n",
       " 'Business; Sociology',\n",
       " 'Chemistry',\n",
       " 'Chemistry; Computer Science',\n",
       " 'Chemistry; Computer Science; Education',\n",
       " 'Chemistry; Computer Science; Engineering; Environmental Science',\n",
       " 'Chemistry; Computer Science; Medicine',\n",
       " 'Chemistry; Computer Science; Physics',\n",
       " 'Chemistry; Economics; Environmental Science',\n",
       " 'Chemistry; Education; Engineering',\n",
       " 'Chemistry; Education; Environmental Science',\n",
       " 'Chemistry; Electrochemistry; Energy & Fuels',\n",
       " 'Chemistry; Energy & Fuels; Engineering',\n",
       " 'Chemistry; Engineering',\n",
       " 'Chemistry; Engineering; Environmental Science',\n",
       " 'Chemistry; Engineering; Environmental Science; Materials Science',\n",
       " 'Chemistry; Engineering; Environmental Science; Materials Science; Medicine',\n",
       " 'Chemistry; Engineering; Environmental Science; Mathematics',\n",
       " 'Chemistry; Engineering; Environmental Science; Medicine',\n",
       " 'Chemistry; Engineering; Environmental Science; Physics',\n",
       " 'Chemistry; Engineering; Instruments & Instrumentation',\n",
       " 'Chemistry; Engineering; Materials Science; Physics',\n",
       " 'Chemistry; Engineering; Medicine',\n",
       " 'Chemistry; Environmental Science',\n",
       " 'Chemistry; Environmental Science; Materials Science',\n",
       " 'Chemistry; Environmental Science; Materials Science; Medicine',\n",
       " 'Chemistry; Environmental Science; Medicine',\n",
       " 'Chemistry; Environmental Science; Physics',\n",
       " 'Chemistry; Materials Science',\n",
       " 'Chemistry; Materials Science; Physics',\n",
       " 'Chemistry; Medicine',\n",
       " 'Chemistry; Physics',\n",
       " 'Communication; Environmental Sciences & Ecology',\n",
       " 'Communication; History & Philosophy of Science',\n",
       " 'Computer Science',\n",
       " 'Computer Science; Communication',\n",
       " 'Computer Science; Economics',\n",
       " 'Computer Science; Economics; Environmental Science; Political Science',\n",
       " 'Computer Science; Economics; Geography',\n",
       " 'Computer Science; Economics; Law',\n",
       " 'Computer Science; Economics; Linguistics',\n",
       " 'Computer Science; Economics; Linguistics; Medicine; Psychology',\n",
       " 'Computer Science; Economics; Mathematics',\n",
       " 'Computer Science; Economics; Political Science',\n",
       " 'Computer Science; Economics; Political Science; Sociology',\n",
       " 'Computer Science; Economics; Psychology',\n",
       " 'Computer Science; Economics; Sociology',\n",
       " 'Computer Science; Education',\n",
       " 'Computer Science; Education & Educational Research',\n",
       " 'Computer Science; Education; Engineering',\n",
       " 'Computer Science; Education; Engineering; Environmental Science',\n",
       " 'Computer Science; Education; Engineering; Linguistics',\n",
       " 'Computer Science; Education; Engineering; Medicine',\n",
       " 'Computer Science; Education; Engineering; Psychology',\n",
       " 'Computer Science; Education; Environmental Science',\n",
       " 'Computer Science; Education; History',\n",
       " 'Computer Science; Education; Law',\n",
       " 'Computer Science; Education; Law; Medicine',\n",
       " 'Computer Science; Education; Linguistics',\n",
       " 'Computer Science; Education; Linguistics; Mathematics',\n",
       " 'Computer Science; Education; Linguistics; Medicine',\n",
       " 'Computer Science; Education; Linguistics; Psychology',\n",
       " 'Computer Science; Education; Linguistics; Psychology; Sociology',\n",
       " 'Computer Science; Education; Mathematics',\n",
       " 'Computer Science; Education; Medicine',\n",
       " 'Computer Science; Education; Medicine; Psychology',\n",
       " 'Computer Science; Education; Philosophy',\n",
       " 'Computer Science; Education; Physics',\n",
       " 'Computer Science; Education; Political Science',\n",
       " 'Computer Science; Education; Psychology',\n",
       " 'Computer Science; Education; Sociology',\n",
       " 'Computer Science; Engineering',\n",
       " 'Computer Science; Engineering; Environmental Science',\n",
       " 'Computer Science; Engineering; Environmental Science; Geography',\n",
       " 'Computer Science; Engineering; Environmental Science; Geology; Physics',\n",
       " 'Computer Science; Engineering; Environmental Science; Materials Science',\n",
       " 'Computer Science; Engineering; Environmental Science; Mathematics',\n",
       " 'Computer Science; Engineering; Environmental Science; Medicine',\n",
       " 'Computer Science; Engineering; Environmental Science; Physics',\n",
       " 'Computer Science; Engineering; Geography',\n",
       " 'Computer Science; Engineering; Geology; Physics',\n",
       " 'Computer Science; Engineering; Linguistics',\n",
       " 'Computer Science; Engineering; Materials Science',\n",
       " 'Computer Science; Engineering; Materials Science; Physics',\n",
       " 'Computer Science; Engineering; Mathematics',\n",
       " 'Computer Science; Engineering; Mathematics; Physics',\n",
       " 'Computer Science; Engineering; Medicine',\n",
       " 'Computer Science; Engineering; Physics',\n",
       " 'Computer Science; Engineering; Psychology',\n",
       " 'Computer Science; Engineering; Telecommunications',\n",
       " 'Computer Science; Environmental Science',\n",
       " 'Computer Science; Environmental Science; Geology',\n",
       " 'Computer Science; Environmental Science; Geology; Physics',\n",
       " 'Computer Science; Environmental Science; Linguistics',\n",
       " 'Computer Science; Environmental Science; Linguistics; Medicine',\n",
       " 'Computer Science; Environmental Science; Mathematics',\n",
       " 'Computer Science; Environmental Science; Medicine',\n",
       " 'Computer Science; Environmental Science; Medicine; Political Science',\n",
       " 'Computer Science; Environmental Science; Physics',\n",
       " 'Computer Science; Environmental Science; Political Science',\n",
       " 'Computer Science; Environmental Science; Sociology',\n",
       " 'Computer Science; Geography',\n",
       " 'Computer Science; Geology',\n",
       " 'Computer Science; Government & Law; Public Administration',\n",
       " 'Computer Science; Health Care Sciences & Services',\n",
       " 'Computer Science; Health Care Sciences & Services; Information Science & Library Science; Medical Informatics',\n",
       " 'Computer Science; Health Care Sciences & Services; Medical Informatics',\n",
       " 'Computer Science; History; Philosophy; Physics',\n",
       " 'Computer Science; History; Sociology',\n",
       " 'Computer Science; Information Science & Library Science',\n",
       " 'Computer Science; Information Science & Library Science; Social Sciences - Other Topics',\n",
       " 'Computer Science; Law',\n",
       " 'Computer Science; Law; Medicine',\n",
       " 'Computer Science; Law; Philosophy',\n",
       " 'Computer Science; Law; Political Science; Psychology',\n",
       " 'Computer Science; Linguistics',\n",
       " 'Computer Science; Linguistics; Medicine',\n",
       " 'Computer Science; Linguistics; Medicine; Philosophy',\n",
       " 'Computer Science; Linguistics; Medicine; Political Science',\n",
       " 'Computer Science; Linguistics; Medicine; Psychology',\n",
       " 'Computer Science; Linguistics; Medicine; Sociology',\n",
       " 'Computer Science; Linguistics; Philosophy',\n",
       " 'Computer Science; Linguistics; Political Science',\n",
       " 'Computer Science; Linguistics; Political Science; Sociology',\n",
       " 'Computer Science; Linguistics; Psychology',\n",
       " 'Computer Science; Linguistics; Psychology; Sociology',\n",
       " 'Computer Science; Linguistics; Sociology',\n",
       " 'Computer Science; Materials Science',\n",
       " 'Computer Science; Materials Science; Physics',\n",
       " 'Computer Science; Mathematics',\n",
       " 'Computer Science; Mathematics; Medicine',\n",
       " 'Computer Science; Mathematics; Physics',\n",
       " 'Computer Science; Mathematics; Political Science',\n",
       " 'Computer Science; Mathematics; Political Science; Sociology',\n",
       " 'Computer Science; Mathematics; Psychology',\n",
       " 'Computer Science; Mathematics; Sociology',\n",
       " 'Computer Science; Medical Informatics',\n",
       " 'Computer Science; Medicine',\n",
       " 'Computer Science; Medicine; Philosophy',\n",
       " 'Computer Science; Medicine; Physics; Psychology',\n",
       " 'Computer Science; Medicine; Political Science',\n",
       " 'Computer Science; Medicine; Political Science; Psychology',\n",
       " 'Computer Science; Medicine; Psychology',\n",
       " 'Computer Science; Medicine; Psychology; Sociology',\n",
       " 'Computer Science; Medicine; Sociology',\n",
       " 'Computer Science; Neurosciences & Neurology; Psychology',\n",
       " 'Computer Science; Operations Research & Management Science',\n",
       " 'Computer Science; Philosophy',\n",
       " 'Computer Science; Philosophy; Physics',\n",
       " 'Computer Science; Philosophy; Political Science',\n",
       " 'Computer Science; Philosophy; Psychology',\n",
       " 'Computer Science; Philosophy; Psychology; Sociology',\n",
       " 'Computer Science; Physics',\n",
       " 'Computer Science; Physics; Political Science; Sociology',\n",
       " 'Computer Science; Physics; Sociology',\n",
       " 'Computer Science; Political Science',\n",
       " 'Computer Science; Political Science; Psychology',\n",
       " 'Computer Science; Political Science; Sociology',\n",
       " 'Computer Science; Psychology',\n",
       " 'Computer Science; Psychology; Sociology',\n",
       " 'Computer Science; Robotics',\n",
       " 'Computer Science; Sociology',\n",
       " 'Computer Science; Telecommunications',\n",
       " 'Construction & Building Technology; Energy & Fuels; Engineering',\n",
       " 'Construction & Building Technology; Engineering',\n",
       " 'Construction & Building Technology; Engineering; Materials Science',\n",
       " 'Dentistry',\n",
       " 'Dermatology',\n",
       " 'Economics',\n",
       " 'Economics; Education',\n",
       " 'Economics; Engineering',\n",
       " 'Economics; Engineering; Environmental Science',\n",
       " 'Economics; Environmental Science',\n",
       " 'Economics; Environmental Science; Medicine',\n",
       " 'Economics; Environmental Science; Physics',\n",
       " 'Economics; Environmental Science; Sociology',\n",
       " 'Economics; Geography',\n",
       " 'Economics; History',\n",
       " 'Economics; Medicine',\n",
       " 'Economics; Medicine; Psychology',\n",
       " 'Economics; Political Science',\n",
       " 'Economics; Political Science; Sociology',\n",
       " 'Education',\n",
       " 'Education & Educational Research',\n",
       " 'Education & Educational Research; Engineering',\n",
       " 'Education & Educational Research; Surgery',\n",
       " 'Education & Educational Research; Veterinary Sciences',\n",
       " 'Education; Engineering',\n",
       " 'Education; Engineering; Environmental Science',\n",
       " 'Education; Engineering; Psychology',\n",
       " 'Education; Environmental Science; Sociology',\n",
       " 'Education; Geography; Political Science; Sociology',\n",
       " 'Education; History',\n",
       " 'Education; Linguistics',\n",
       " 'Education; Linguistics; Medicine',\n",
       " 'Education; Linguistics; Philosophy',\n",
       " 'Education; Linguistics; Psychology',\n",
       " 'Education; Medicine',\n",
       " 'Education; Medicine; Psychology',\n",
       " 'Education; Medicine; Psychology; Sociology',\n",
       " 'Education; Philosophy; Political Science; Psychology',\n",
       " 'Education; Political Science; Sociology',\n",
       " 'Education; Psychology',\n",
       " 'Education; Sociology',\n",
       " 'Emergency Medicine; Rehabilitation; Surgery',\n",
       " 'Energy & Fuels; Engineering',\n",
       " 'Energy & Fuels; Engineering; Environmental Sciences & Ecology',\n",
       " 'Engineering',\n",
       " 'Engineering; Astronomy & Astrophysics; Optics; Physics',\n",
       " 'Engineering; Environmental Science',\n",
       " 'Engineering; Environmental Science; Geography',\n",
       " 'Engineering; Environmental Science; Geology',\n",
       " 'Engineering; Environmental Science; Materials Science',\n",
       " 'Engineering; Environmental Science; Materials Science; Medicine',\n",
       " 'Engineering; Environmental Science; Materials Science; Physics',\n",
       " 'Engineering; Environmental Science; Mathematics',\n",
       " 'Engineering; Environmental Science; Medicine',\n",
       " 'Engineering; Environmental Science; Physics',\n",
       " 'Engineering; Environmental Science; Psychology',\n",
       " 'Engineering; Environmental Science; Sociology',\n",
       " 'Engineering; Environmental Sciences & Ecology',\n",
       " 'Engineering; Geology',\n",
       " 'Engineering; History',\n",
       " 'Engineering; Materials Science',\n",
       " 'Engineering; Materials Science; Medicine',\n",
       " 'Engineering; Materials Science; Medicine; Physics',\n",
       " 'Engineering; Materials Science; Physics',\n",
       " 'Engineering; Mathematics; Medicine',\n",
       " 'Engineering; Mathematics; Physics',\n",
       " 'Engineering; Medicine',\n",
       " 'Engineering; Medicine; Psychology',\n",
       " 'Engineering; Physics',\n",
       " 'Engineering; Political Science; Sociology',\n",
       " 'Engineering; Science & Technology - Other Topics',\n",
       " 'Engineering; Telecommunications',\n",
       " 'Engineering; Transportation',\n",
       " 'Environmental & Occupational Health',\n",
       " 'Environmental & Occupational Health; Medical Informatics',\n",
       " 'Environmental Science',\n",
       " 'Environmental Science; Geography',\n",
       " 'Environmental Science; Geography; History',\n",
       " 'Environmental Science; Geography; Medicine',\n",
       " 'Environmental Science; Geography; Political Science',\n",
       " 'Environmental Science; Geography; Sociology',\n",
       " 'Environmental Science; Geology',\n",
       " 'Environmental Science; Geology; Physics',\n",
       " 'Environmental Science; History; Sociology',\n",
       " 'Environmental Science; Materials Science',\n",
       " 'Environmental Science; Materials Science; Mathematics; Medicine',\n",
       " 'Environmental Science; Materials Science; Medicine',\n",
       " 'Environmental Science; Materials Science; Physics',\n",
       " 'Environmental Science; Medicine',\n",
       " 'Environmental Science; Medicine; Psychology',\n",
       " 'Environmental Science; Medicine; Sociology',\n",
       " 'Environmental Science; Philosophy; Political Science',\n",
       " 'Environmental Science; Physics',\n",
       " 'Environmental Science; Political Science',\n",
       " 'Environmental Science; Political Science; Sociology',\n",
       " 'Environmental Science; Sociology',\n",
       " 'Environmental Sciences & Ecology',\n",
       " 'Environmental Sciences & Ecology; Geography; Physical Geography; Public Administration; Urban Studies',\n",
       " 'Environmental Sciences & Ecology; Marine & Freshwater Biology; Oceanography',\n",
       " 'Environmental Sciences & Ecology; Meteorology & Atmospheric Sciences',\n",
       " 'Environmental Sciences & Ecology; Physical Geography; Geology',\n",
       " 'Environmental Sciences & Ecology; Public',\n",
       " 'Family Studies; Social Work',\n",
       " 'Fisheries',\n",
       " 'Food Science & Technology',\n",
       " 'Forestry',\n",
       " 'Forestry; Materials Science',\n",
       " 'Gastroenterology & Hepatology',\n",
       " 'General & Internal Medicine',\n",
       " 'Genetics & Heredity',\n",
       " 'Genetics & Heredity; Pharmacology & Pharmacy',\n",
       " 'Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science & Photographic Technology',\n",
       " 'Geography',\n",
       " 'Geography; History',\n",
       " 'Geography; Mathematics',\n",
       " 'Geography; Political Science',\n",
       " 'Geology',\n",
       " 'Geology; Physics',\n",
       " 'Health Care Sciences & Services',\n",
       " 'Health Care Sciences & Services; General & Internal Medicine; Neurosciences & Neurology',\n",
       " 'Health Care Sciences & Services; Medical Informatics',\n",
       " 'Health Care Sciences & Services; Public',\n",
       " 'History',\n",
       " 'History; Law',\n",
       " 'History; Law; Philosophy',\n",
       " 'History; Law; Political Science',\n",
       " 'History; Materials Science',\n",
       " 'History; Medicine',\n",
       " 'History; Medicine; Philosophy',\n",
       " 'History; Philosophy',\n",
       " 'History; Philosophy; Political Science',\n",
       " 'History; Political Science',\n",
       " 'History; Political Science; Sociology',\n",
       " 'History; Psychology',\n",
       " 'History; Sociology',\n",
       " 'Infectious Diseases; Parasitology; Tropical Medicine',\n",
       " 'Infectious Diseases; Pharmacology & Pharmacy',\n",
       " 'Information Science & Library Science',\n",
       " 'Instruments & Instrumentation; Optics',\n",
       " 'Law',\n",
       " 'Law; Medicine; Philosophy',\n",
       " 'Law; Political Science',\n",
       " 'Law; Psychology',\n",
       " 'Law; Psychology; Sociology',\n",
       " 'Law; Sociology',\n",
       " 'Life Sciences & Biomedicine - Other Topics',\n",
       " 'Linguistics',\n",
       " 'Linguistics; Medicine',\n",
       " 'Linguistics; Medicine; Psychology',\n",
       " 'Literature',\n",
       " 'Materials Science',\n",
       " 'Materials Science; Medicine',\n",
       " 'Materials Science; Metallurgy & Metallurgical Engineering',\n",
       " 'Materials Science; Physics',\n",
       " 'Mathematical & Computational Biology',\n",
       " 'Mathematical Methods In Social Sciences; Government & Law',\n",
       " 'Mathematical Methods In Social Sciences; Sociology',\n",
       " 'Mathematics',\n",
       " 'Mathematics; Medicine',\n",
       " 'Medical Informatics',\n",
       " 'Medicine',\n",
       " 'Medicine; Philosophy; Psychology',\n",
       " 'Medicine; Physics',\n",
       " 'Medicine; Psychology',\n",
       " 'Medicine; Psychology; Sociology',\n",
       " 'Medicine; Sociology',\n",
       " 'Meteorology & Atmospheric Sciences',\n",
       " 'Music',\n",
       " 'Neurosciences & Neurology',\n",
       " 'Neurosciences & Neurology; Pharmacology & Pharmacy; Psychiatry',\n",
       " 'Neurosciences & Neurology; Psychology',\n",
       " 'Nuclear Medicine & Medical Imaging',\n",
       " 'Nursing',\n",
       " 'Nursing; Government & Law',\n",
       " 'Nutrition & Dietetics',\n",
       " 'Oncology',\n",
       " 'Oral Surgery & Medicine',\n",
       " 'Orthopedics',\n",
       " 'Orthopedics; Rheumatology',\n",
       " 'Orthopedics; Sport Sciences; Surgery',\n",
       " 'Parasitology; Tropical Medicine',\n",
       " 'Pediatrics',\n",
       " 'Pharmacology & Pharmacy',\n",
       " 'Pharmacology & Pharmacy; Chemistry; Computer Science',\n",
       " 'Philosophy',\n",
       " 'Philosophy; Psychology',\n",
       " 'Philosophy; Sociology',\n",
       " 'Physics',\n",
       " 'Plant Sciences',\n",
       " 'Plant Sciences; Reproductive Biology',\n",
       " 'Political Science',\n",
       " 'Political Science; Sociology',\n",
       " 'Polymer Science',\n",
       " 'Psychiatry',\n",
       " 'Psychiatry; Psychology',\n",
       " 'Psychology',\n",
       " 'Psychology; Business & Economics',\n",
       " 'Psychology; Psychiatry',\n",
       " 'Psychology; Sociology',\n",
       " 'Public',\n",
       " 'Radiology',\n",
       " 'Remote Sensing; Imaging Science & Photographic Technology',\n",
       " 'Research & Experimental Medicine',\n",
       " 'Research & Experimental Medicine; Pharmacology & Pharmacy',\n",
       " 'Rheumatology',\n",
       " 'Science & Technology - Other Topics',\n",
       " 'Science & Technology - Other Topics; Energy & Fuels',\n",
       " 'Science & Technology - Other Topics; Engineering; Environmental Sciences & Ecology',\n",
       " 'Science & Technology - Other Topics; Environmental Sciences & Ecology',\n",
       " 'Science & Technology - Other Topics; Pharmacology & Pharmacy',\n",
       " 'Science & Technology - Other Topics; Social Sciences - Other Topics',\n",
       " 'Social Sciences - Other Topics',\n",
       " 'Social Work',\n",
       " 'Sociology',\n",
       " 'Surgery',\n",
       " 'Thermodynamics; Energy & Fuels',\n",
       " 'Thermodynamics; Energy & Fuels; Engineering; Mechanics',\n",
       " 'Thermodynamics; Engineering',\n",
       " 'Transportation',\n",
       " 'Urology & Nephrology'}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out df_combined['researchAreas'].dropna().unique() and sort\n",
    "unique_research_areas = set()\n",
    "for areas in df_combined['researchAreas'].dropna().unique():\n",
    "    for area in areas.split(','):\n",
    "        unique_research_areas.add(area.strip())\n",
    "unique_research_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "69c37175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Article',\n",
       " 'Book',\n",
       " 'Book Chapter',\n",
       " 'Book, Conference',\n",
       " 'Book, JournalArticle',\n",
       " 'Book, JournalArticle, Conference',\n",
       " 'Book, JournalArticle, Conference, Review',\n",
       " 'Book, JournalArticle, Review',\n",
       " 'Book, Review',\n",
       " 'CaseReport, JournalArticle, Review',\n",
       " 'Conference',\n",
       " 'Conference, Review',\n",
       " 'Dataset, JournalArticle',\n",
       " 'Early Access',\n",
       " 'Editorial',\n",
       " 'Editorial Material',\n",
       " 'Editorial, JournalArticle',\n",
       " 'Editorial, JournalArticle, Review',\n",
       " 'Editorial, LettersAndComments',\n",
       " 'Editorial, LettersAndComments, Review',\n",
       " 'Editorial, Review',\n",
       " 'JournalArticle',\n",
       " 'JournalArticle, Book',\n",
       " 'JournalArticle, Book, Conference',\n",
       " 'JournalArticle, Book, Conference, Review',\n",
       " 'JournalArticle, Book, Review',\n",
       " 'JournalArticle, CaseReport',\n",
       " 'JournalArticle, CaseReport, Review',\n",
       " 'JournalArticle, ClinicalTrial',\n",
       " 'JournalArticle, ClinicalTrial, Review',\n",
       " 'JournalArticle, Conference',\n",
       " 'JournalArticle, Conference, Review',\n",
       " 'JournalArticle, Editorial',\n",
       " 'JournalArticle, Editorial, Review',\n",
       " 'JournalArticle, LettersAndComments',\n",
       " 'JournalArticle, LettersAndComments, Review',\n",
       " 'JournalArticle, Review',\n",
       " 'JournalArticle, Study',\n",
       " 'JournalArticle, Study, Review',\n",
       " 'LettersAndComments',\n",
       " 'LettersAndComments, CaseReport',\n",
       " 'LettersAndComments, Editorial',\n",
       " 'LettersAndComments, Editorial, Review',\n",
       " 'LettersAndComments, JournalArticle',\n",
       " 'LettersAndComments, JournalArticle, Review',\n",
       " 'LettersAndComments, Review',\n",
       " 'LettersAndComments, Study',\n",
       " 'News',\n",
       " 'None',\n",
       " 'Proceedings Paper',\n",
       " 'Review',\n",
       " 'Review, JournalArticle',\n",
       " 'Review, MetaAnalysis, JournalArticle',\n",
       " 'Review, MetaAnalysis, JournalArticle, Study',\n",
       " 'Study, JournalArticle',\n",
       " 'Study, JournalArticle, Review'}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique values in publicationTypes column of df_combined\n",
    "unique_publication_types = set()\n",
    "for types in df_combined['publicationTypes'].dropna().unique():\n",
    "    for ptype in types.split(';'):\n",
    "        unique_publication_types.add(ptype.strip())\n",
    "unique_publication_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2dc10854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after year filter: 3662\n",
      "Rows after INCLUDE_FIELDS filter: 2876\n",
      "Rows after EXCLUDE_FIELDS filter: 1687\n",
      "Rows after EXCLUDE_PUBTYPES filter: 1683\n",
      "Number of rows in combined dataframe before final screening: 5478\n",
      "Number of rows in combined dataframe after final screening: 1683\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Number of Papers Retrieved",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recall (All)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Journal & Conf. Papers)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall (Preprints)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Journal & Conf. Papers",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Preprints",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b09ac40d-2b30-4801-8dd2-b5c0bd48bb13",
       "rows": [
        [
         "0",
         "1683",
         "72.00%",
         "92.86%",
         "45.45%",
         "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models",
         "Addressing Systematic Non-response Bias with Supervised Fine-Tuning of Large Language Models: A Case Study on German Voting Behaviour; Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices; Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses; Donald Trumps in the Virtual Polls: Simulating and Predicting Public Opinions in Surveys Using Large Language Models; More Parameters Than Populations: A Systematic Literature Review of Large Language Models within Survey Research; Questioning the Survey Responses of Large Language Models"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Papers Retrieved</th>\n",
       "      <th>Recall (All)</th>\n",
       "      <th>Recall (Journal &amp; Conf. Papers)</th>\n",
       "      <th>Recall (Preprints)</th>\n",
       "      <th>Missing Journal &amp; Conf. Papers</th>\n",
       "      <th>Missing Preprints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1683</td>\n",
       "      <td>72.00%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>The Potential and Challenges of Evaluating Att...</td>\n",
       "      <td>Addressing Systematic Non-response Bias with S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Papers Retrieved Recall (All) Recall (Journal & Conf. Papers)  \\\n",
       "0                        1683       72.00%                          92.86%   \n",
       "\n",
       "  Recall (Preprints)                     Missing Journal & Conf. Papers  \\\n",
       "0             45.45%  The Potential and Challenges of Evaluating Att...   \n",
       "\n",
       "                                   Missing Preprints  \n",
       "0  Addressing Systematic Non-response Bias with S...  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INCLUDE_FIELDS = (\n",
    "    'Arts & Humanities - Other Topics|'\n",
    "    'Social Sciences - Other Topics|'\n",
    "    'Business & Economics|'\n",
    "    'Computer Science|'\n",
    "    'Information Science & Library Science|'\n",
    "    'Linguistics|'\n",
    "    'Political Science|'\n",
    "    'Environmental Sciences & Ecology|'\n",
    "    'Mathematical Methods In Social Sciences|Government & Law'\n",
    ")\n",
    "\n",
    "EXCLUDE_FIELDS = ('Acoustics|Agricultur|Allergy|Anatomy & Morphology|Astronomy & Astrophysics|Atmospheric Sciences|Audiology & Speech-Language Pathology|Automation|Biodiversity & Conservation|Biology|Biotechnology & Applied Microbiology|Chemistry|Construction & Building Technology|Dermatology|Education|Energy & Fuels|Family Studies|Food Science & Technology|Gastroenterology & Hepatology|Geochemistry & Geophysics|Geography|Geology|Health|History|Imaging Science & Photographic Technology|Life Sciences & Biomedicine - Other Topics|Literature|Management Science|Materials Science|Mechanics|Medical|Medicine|Music|Neurosciences|Nursing|Oncology|Orthopedics|Otorhinolaryngology|Plant Sciences|Pediatrics|Pharmacology|Philosophy|Physics|Polymer Science|Psychiatry|Public Administration|Remote Sensing|Rheumatology|Robotics|Services|Social Work|Surgery|Thermodynamics|Transportation|Urology & Nephrology')\n",
    "\n",
    "EXCLUDE_PUBTYPES = 'CaseReport|Dataset|Editorial|LettersAndComments|MetaAnalysis|News|Study'\n",
    "\n",
    "\n",
    "def final_screen(df):\n",
    "    # Step 1: Year filter\n",
    "    step1 = df[df['year'].astype(str).str.isdigit() & (df['year'].astype(int) >= 2022)]\n",
    "    print(f\"Rows after year filter: {len(step1)}\")\n",
    "    \n",
    "    # Step 2: Include fields\n",
    "    step2 = step1[step1['researchAreas'].str.contains(INCLUDE_FIELDS, na=False)]\n",
    "    print(f\"Rows after INCLUDE_FIELDS filter: {len(step2)}\")\n",
    "    \n",
    "    # Step 3: Exclude fields\n",
    "    step3 = step2[~step2['researchAreas'].str.contains(EXCLUDE_FIELDS, na=False)]\n",
    "    print(f\"Rows after EXCLUDE_FIELDS filter: {len(step3)}\")\n",
    "    \n",
    "    # Step 4: Exclude publication types\n",
    "    step4 = step3[~step3['publicationTypes'].str.contains(EXCLUDE_PUBTYPES, na=False)]\n",
    "    print(f\"Rows after EXCLUDE_PUBTYPES filter: {len(step4)}\")\n",
    "    \n",
    "    return step4\n",
    "\n",
    "df_combined_filtered = final_screen(df_combined)\n",
    "\n",
    "# print number of rows before final screening\n",
    "print(f'Number of rows in combined dataframe before final screening: {len(df_combined)}')\n",
    "print(f'Number of rows in combined dataframe after final screening: {len(df_combined_filtered)}')\n",
    "\n",
    "calc_recall_with_missing(df_combined_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0adb8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_combined_filtered to CSV\n",
    "df_combined_filtered.to_csv(\"df_final_combined_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d8ef0",
   "metadata": {},
   "source": [
    "## Export RIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "79e9e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- RIS export helpers (Semantic Scholar + DOI fallback) --------\n",
    "\n",
    "def _ss_enrich_biblio_with_batch(df: pd.DataFrame, sch: SemanticScholar) -> pd.DataFrame:\n",
    "    \"\"\"Add 'venue', 'publicationVenueName', 'venueType', 'authorsList', 'oaPdfUrl' columns via /paper/batch.\"\"\"\n",
    "    ids = [i for i in df.get(\"paperId\", pd.Series()).dropna().astype(str).unique().tolist() if i]\n",
    "    if not ids:\n",
    "        for col in [\"venue\", \"publicationVenueName\", \"venueType\", \"authorsList\", \"oaPdfUrl\"]:\n",
    "            df[col] = None\n",
    "        return df\n",
    "\n",
    "    rows = []\n",
    "    chunksize = 100\n",
    "    for i in range(0, len(ids), chunksize):\n",
    "        batch = ids[i:i+chunksize]\n",
    "        papers = sch.get_papers(batch, fields=[\n",
    "            \"paperId\", \"venue\", \"publicationVenue\", \"publicationTypes\",\n",
    "            \"authors\", \"externalIds\", \"openAccessPdf\", \"url\"\n",
    "        ])\n",
    "        for p in papers or []:\n",
    "            pid = getattr(p, \"paperId\", None)\n",
    "            # venue / container\n",
    "            venue_str = getattr(p, \"venue\", None)\n",
    "            pv = getattr(p, \"publicationVenue\", None)\n",
    "            if isinstance(pv, dict):\n",
    "                pv_name = pv.get(\"name\")\n",
    "                pv_type = pv.get(\"type\")\n",
    "            else:\n",
    "                pv_name = getattr(pv, \"name\", None)\n",
    "                pv_type = getattr(pv, \"type\", None)\n",
    "            # authors list of strings\n",
    "            a = getattr(p, \"authors\", None) or []\n",
    "            a_names = []\n",
    "            for it in a:\n",
    "                if isinstance(it, dict):\n",
    "                    nm = it.get(\"name\")\n",
    "                else:\n",
    "                    nm = getattr(it, \"name\", None)\n",
    "                if nm:\n",
    "                    a_names.append(nm)\n",
    "            # OA PDF\n",
    "            o = getattr(p, \"openAccessPdf\", None)\n",
    "            if isinstance(o, dict):\n",
    "                oa_pdf = o.get(\"url\")\n",
    "            else:\n",
    "                oa_pdf = getattr(o, \"url\", None)\n",
    "            rows.append({\n",
    "                \"paperId\": pid,\n",
    "                \"venue\": venue_str,\n",
    "                \"publicationVenueName\": pv_name,\n",
    "                \"venueType\": pv_type,\n",
    "                \"authorsList\": a_names if a_names else None,\n",
    "                \"oaPdfUrl\": oa_pdf,\n",
    "            })\n",
    "    add_df = pd.DataFrame(rows).drop_duplicates(subset=[\"paperId\"]) if rows else \\\n",
    "             pd.DataFrame(columns=[\"paperId\",\"venue\",\"publicationVenueName\",\"venueType\",\"authorsList\",\"oaPdfUrl\"])\n",
    "    return df.merge(add_df, on=\"paperId\", how=\"left\")\n",
    "\n",
    "\n",
    "def _ris_type_from_pubtypes(pubtypes):\n",
    "    \"\"\"Map Semantic Scholar publicationTypes to RIS TY codes.\"\"\"\n",
    "    s = set([t.lower() for t in (pubtypes or [])]) if isinstance(pubtypes, list) else {str(pubtypes).lower()} if pubtypes else set()\n",
    "    if any(x in s for x in [\"journalarticle\", \"journal-article\", \"journal\"]): return \"JOUR\"\n",
    "    if any(x in s for x in [\"conference\", \"proceedings-article\", \"conferencepaper\", \"conference paper\"]): return \"CONF\"\n",
    "    if any(x in s for x in [\"book\"]): return \"BOOK\"\n",
    "    if any(x in s for x in [\"bookchapter\", \"book-chapter\", \"chapter\"]): return \"CHAP\"\n",
    "    if any(x in s for x in [\"thesis\", \"phdthesis\", \"mastersthesis\"]): return \"THES\"\n",
    "    if any(x in s for x in [\"preprint\", \"workingpaper\", \"report\"]): return \"RPRT\"\n",
    "    return \"GEN\"\n",
    "\n",
    "\n",
    "def _ris_name(last_first: str) -> str:\n",
    "    \"\"\"Ensure 'Last, First' format; best-effort for simple 'First Last' names.\"\"\"\n",
    "    if not last_first:\n",
    "        return \"\"\n",
    "    name = last_first.strip()\n",
    "    if \",\" in name:\n",
    "        return name  # already 'Last, First'\n",
    "    parts = name.split()\n",
    "    if len(parts) == 1:\n",
    "        return parts[0]\n",
    "    return f\"{parts[-1]}, {' '.join(parts[:-1])}\"\n",
    "\n",
    "\n",
    "def _row_to_ris(rec: dict) -> str:\n",
    "    \"\"\"Build a single RIS record from a dict-like row.\"\"\"\n",
    "    ty = _ris_type_from_pubtypes(rec.get(\"publicationTypes\"))\n",
    "    title = rec.get(\"title\") or \"\"\n",
    "    year = rec.get(\"year\")\n",
    "    doi = rec.get(\"doi\") or \"\"\n",
    "    url = rec.get(\"url\") or rec.get(\"oaPdfUrl\") or \"\"\n",
    "    # container (journal/conference name)\n",
    "    container = rec.get(\"publicationVenueName\") or rec.get(\"venue\") or \"\"\n",
    "    # authors: prefer list from enrichment; else split the string column\n",
    "    authors_list = rec.get(\"authorsList\")\n",
    "    if not authors_list:\n",
    "        s = rec.get(\"authors\") or \"\"\n",
    "        authors_list = [a.strip() for a in s.split(\",\") if a.strip()] if s else []\n",
    "    fields = []\n",
    "    fields.append(f\"TY  - {ty}\")\n",
    "    if title:\n",
    "        fields.append(f\"TI  - {title}\")\n",
    "    if container:\n",
    "        # JO for journals; T2 is also accepted broadly. Use JO as generic container.\n",
    "        fields.append(f\"JO  - {container}\")\n",
    "    for a in authors_list:\n",
    "        nm = _ris_name(a)\n",
    "        if nm:\n",
    "            fields.append(f\"AU  - {nm}\")\n",
    "    if year:\n",
    "        fields.append(f\"PY  - {int(year)}\")\n",
    "    if doi:\n",
    "        fields.append(f\"DO  - {doi}\")\n",
    "    if url:\n",
    "        fields.append(f\"UR  - {url}\")\n",
    "    # keywords from fieldsOfStudy\n",
    "    fos = rec.get(\"fieldsOfStudy\")\n",
    "    if isinstance(fos, str):\n",
    "        kws = [k.strip() for k in fos.split(\";\") if k.strip()]\n",
    "        for k in kws:\n",
    "            fields.append(f\"KW  - {k}\")\n",
    "    # abstract (optional; can be long)\n",
    "    abstract = rec.get(\"abstract\")\n",
    "    if abstract:\n",
    "        fields.append(f\"AB  - {abstract}\")\n",
    "    # identifier\n",
    "    pid = rec.get(\"paperId\")\n",
    "    if pid:\n",
    "        fields.append(f\"ID  - S2:{pid}\")\n",
    "    fields.append(\"ER  - \")\n",
    "    return \"\\n\".join(fields)\n",
    "\n",
    "\n",
    "def _doi_provider_ris(doi: str, timeout=15) -> str | None:\n",
    "    \"\"\"Try DOI content negotiation to fetch RIS from the publisher/Crossref.\"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            f\"https://doi.org/{doi}\",\n",
    "            headers={\"Accept\": \"application/x-research-info-systems\"},\n",
    "            timeout=timeout,\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        if resp.status_code == 200 and resp.text.strip():\n",
    "            return resp.text\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def export_df_to_ris(df: pd.DataFrame, out_path: str,\n",
    "                     use_doi_provider: bool = True,\n",
    "                     provider_backoff_sec: float = 0.2,\n",
    "                     enrich_biblio: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Export a DataFrame of papers to a .ris file.\n",
    "    If use_doi_provider=True and DOI exists, try to fetch publisher RIS first; fallback to local RIS.\n",
    "    If enrich_biblio=True, add venue/container and clean authors via S2 /paper/batch before export.\n",
    "    \"\"\"\n",
    "    dfx = df.copy()\n",
    "\n",
    "    sch = None\n",
    "    if enrich_biblio:\n",
    "        sch = SemanticScholar(api_key=api_key_SS, timeout=45, retry=True)\n",
    "        dfx = _ss_enrich_biblio_with_batch(dfx, sch)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    records = []\n",
    "    for _, row in dfx.iterrows():\n",
    "        ris_txt = None\n",
    "        if use_doi_provider:\n",
    "            ris_txt = _doi_provider_ris(str(row.get(\"doi\", \"\")).strip())\n",
    "            if provider_backoff_sec and ris_txt is not None:\n",
    "                time.sleep(provider_backoff_sec)\n",
    "        if not ris_txt:\n",
    "            ris_txt = _row_to_ris(row)\n",
    "        records.append(ris_txt.rstrip())  # ensure clean boundary\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(records) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(records)} RIS records to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ensure df_SS_llm_survey_simB exists, then export\n",
    "out_file = os.path.join(\"asreview\", \"finalscreened.ris\")\n",
    "\n",
    "export_df_to_ris(df_combined_filtered, out_file,\n",
    "                 use_doi_provider=False,   \n",
    "                 enrich_biblio=True)        # add venue/authorsList columns before export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ab71b",
   "metadata": {},
   "source": [
    "# Compare AI vs. Database Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5a9efe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out df_Elicit, where doi contains arXiv or biorxiv\n",
    "df_Elicit_filtered = df_Elicit[~df_Elicit['DOI'].str.contains('arxiv|biorxiv', case=False, na=False)].reset_index(drop=True)\n",
    "df_Elicit_filtered_archive = df_Elicit[df_Elicit['DOI'].str.contains('arxiv|biorxiv', case=False, na=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "63487b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DOI",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DOI link",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Venue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Citation count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Year",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Abstract summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Methodology",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Supporting quotes for \"Methodology\"",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Supporting tables for \"Methodology\"",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Reasoning for \"Methodology\"",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "b543cfd7-8f96-4880-a544-6a8d6c746815",
       "rows": [
        [
         "0",
         "Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations",
         "Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Rottger, Daniel Hershcovich",
         "10.48550/arXiv.2502.07068",
         "https://doi.org/10.48550/arXiv.2502.07068",
         "North American Chapter of the Association for Computational Linguistics",
         "6",
         "2025",
         "Large language models can be specialized to simulate survey response distributions across countries and question types, though accuracy is still limited.",
         "- Specializing LLMs for survey simulation using fine-tuning based on first-token probabilities.\r\n- Minimizing divergence between predicted and actual response distributions using Kullback-Leibler Divergence loss.\r\n- Using datasets from World Values Survey (WVS) and Pew Global Attitudes Survey.\r\n- Dividing dataset into parts based on topics and countries for robust cross-cultural representation.\r\n- Evaluating alignment using Jensen-Shannon divergence (JSD) and Earth Mover Distance (EMD).\r\n- Training and testing on different subsets to assess generalization.",
         "  - \"In this study, our goal is instead to specialize LLMs for survey simulation and gain a better understanding of how good LLMs can be at simulating survey responses when trained to do so, rather than how good they are when prompted out of the box.\"\r\n  - \"To spe-cialize LLMs for this task, we devise a fine-tuning method based on first-token probabilities, where the goal is to minimize divergence between predicted and actual country-level response distributions for a given survey question.\"\r\n  - \"Using the dataset introduced in 3, we present firsttoken alignment fine-tuning, to align model outputs with the observed response distributions of specific population groups (e.g., countries).\"\r\n  - \"The processed question Q is used as input into LLMs. The model outputs logits {z 1 , z 2 , . . . , z n } for the first token of each question's corresponding options O.\"\r\n  - \"For the training optimization objective, we employ Kullback-Leibler Divergence loss (Loss KL ) to align the LLM's first-token probability distribution with the human response distribution:\"\r\n  - \"We fine-tune seven models across three model families using our processed dataset: Vicuna1.5 (Chiang et al., 2023) in its 7B and 13B parameter versions, Llama3 (AI@Meta, 2024) in its 8B Base and Instruct versions, and Deepseek-Distilled-Qwen (Guo et al., 2025) in 7B, 14B, and 32B.\"\r\n  - \"To evaluate generalization to a completely unseen survey, we use an additional subset of GlobalOp-inionQA (Durmus et al., 2023), the Pew Global Attitudes Survey (Pew), which maintains a similar format to the WVS but includes different cultural questions.\"\r\n  - \"We conduct experiments using the English and Chinese versions of the datasets obtained from the official source, enabling the analysis of cross-linguistic differences in this task3\"\r\n  - \"We use the first 259 questions of the WVS to construct our dataset, excluding demographics and notes for interviewers.\"\r\n  - \"We divide them into three parts based on topics: Q 1 (questions 1-163), Q 2 (questions 164-198), and Q 3 (questions 199-259).\"\r\n  - \"We split training, validation, and test sets for the aforementioned questions and country subsets.\"\r\n  - \"We investigate two primary research questions:\n\nRQ1 How effectively does the proposed alignment method improve the distribution simulation of the model on unseen countries and questions?\n\nRQ2 What is necessary to perform well on the task-how much is dependent on modeling the prior distribution, and how much on context sensitivity?\"",
         "  - \"(Page 3, Table 1) | **Instruction** | How would someone from Andorra answer the following question:                                                                 |\r\\n|-----------------|-------------------------------------------------------------------------------------------------------------------------------|\r\\n| **Input**       | How interested would you say you are in politics? Here are the options:                                                       |\"\r\n  - \"(Page 3, Table 2) Table 1: Countries and Questions Description\r\\n--------------------------------------------------------------\r\\n| Countries | Description                          | N  |\"\r\n  - \"(Page 4, Table 1) | Split     | Train | Valid |         Test          |         |         |\r\\n|-----------|-------|-------|-----------------------|---------|---------|\r\\n|           |       |       |                       |         |         |\"\r\n  - \"<table_quotation page_num=5 table_on_page=1></table_quotation>\"\r\n  - \"(Page 7, Table 1) | Base Model          | Methods | (1JSD)                           |                |                |                |                |      | EMD                               |                |                |                |                |      |\r\\n|---------------------|---------|------------------------------------|----------------|----------------|----------------|----------------|------|------------------------------------|----------------|----------------|----------------|----------------|------|\r\\n|                     |         | CQ                             | CQ         | CQ         | CQ         | CQ         | Avg. | CQ                             | CQ         | CQ         | CQ         | CQ         | Avg. |\"\r\n  - \"(Page 8, Table 1) | Methods       | \\[C'_1\\]         |         \\[C_3\\]         |\r\\n|---------------|------------------|-------------------------|\r\\n|               | (1JSD)  | ACC  | (1JSD)  | ACC       |\"\r\n  - \"(Page 8, Table 2) | Methods   |  \\(C_1-Q_3\\) |  \\(C_2-Q_1\\) |  \\(C_2-Q_3\\) |  \\(C_3-Q_1\\) |  \\(C_3-Q_3\\) |\r\\n|-----------|--------------|--------------|--------------|--------------|--------------|\r\\n| KL (Orig) | **0.777**    | **0.881**    | **0.783**    | **0.890**    | **0.784**    |\"\r\n  - \"(Page 11, Table 1) | Country        | Continent     | GDP-level     | N    |\r\\n|----------------|---------------|---------------|------|\r\\n| Germany        | Europe        | High          | 1130 |\"\r\n  - \"(Page 11, Table 2) | C2                        |          | C3                        |       |       |\r\\n|---------------------------|----------|---------------------------|-------|-------|\r\\n| Country                   | CSS-N    | Country                   | CSS-N | PWE-N |\"\r\n  - \"(Page 14, Table 1) | Methods      | C1-Q3 | C2-Q1 | C2-Q3 | C3-Q1 | C3-Q3 | Avg. |\r\\n|--------------|-------|-------|-------|-------|-------|------|\r\\n| KNN          | 0.381 | 0.518 | 0.371 | 0.541 | 0.384 | 0.439|\"",
         "- The study aims to specialize large language models (LLMs) for simulating survey response distributions, which involves fine-tuning these models to better align with actual human response distributions.\r\n- The fine-tuning method is based on first-token probabilities, which involves minimizing the divergence between predicted and actual response distributions using Kullback-Leibler Divergence loss.\r\n- The study uses a dataset from the World Values Survey (WVS) and another from the Pew Global Attitudes Survey to evaluate generalization.\r\n- The dataset is divided into parts based on topics and countries to ensure robust cross-cultural representation.\r\n- The study uses two metrics, Jensen-Shannon divergence (JSD) and Earth Mover Distance (EMD), to evaluate the alignment of predicted distributions with actual distributions.\r\n- The methodology includes training and testing on different subsets of questions and countries to assess generalization capabilities.\r\n- The study investigates two research questions related to the effectiveness of the alignment method and the contribution of prior distribution modeling versus context sensitivity."
        ],
        [
         "1",
         "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
         "Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, Jang Hyun Kim",
         "10.48550/arXiv.2402.18144",
         "https://doi.org/10.48550/arXiv.2402.18144",
         "arXiv.org",
         "24",
         "2024",
         "Large language models can simulate human survey responses across question types and populations using only group-level demographic information.",
         "- Created a \"random silicon sample\" of synthetic respondents with demographic information similar to a human subpopulation.\r\n- Prompted a language model with demographic information and survey questions to generate responses.\r\n- Used the ANES dataset for American public opinion and the 2020 pre-election sample.\r\n- Assessed replicability by comparing response distributions with actual ANES responses.\r\n- Conducted experiments on stratified sampling, down-sampling, and multiple questions to evaluate generalizability.",
         "  - \"Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions.\"\r\n  - \"In our experiments, we created a \"random silicon sample,\" which is a group of randomly generated synthetic respondents that follow the demographic distribution of a human subpopulation.\"\r\n  - \"We obtained survey responses by prompting the model with the demographic information of synthetic individual respondents, along with survey questions.\"\r\n  - \"We proposed the possibility of using a language model to generate opinions that accurately mirror specific groups based on their demographic distribution.\"\r\n  - \"The proposed random silicon sampling method simulates the response distribution of a specific human subpopulation by conditioning LLMs on the demographic distribution of the group.\"\r\n  - \"We utilize the ANES dataset, which is the primary source for comprehending the American public opinion\"\r\n  - \"We leverage the ANES 2020 pre-election sample consisting of interviews with respondents between August 18, 2020, and November 3, 2020.\"\r\n  - \"We compute the similarity between the two response distributions, ANES and A RSS to assess the replicability of the language model to simulate the response distribution of actual ANES respondents.\"\r\n  - \"We conducted down sampling experiments to determine the appropriate sample size for random silicon sampling.\"\r\n  - \"We tested whether random silicon sampling could be applied to issues beyond presidential candidate selection.\"\r\n  - \"We selected questions from the ANES survey on ten politically sensitive issues\"\r\n  - \"We conducted stratified experiments on multiple questions to ascertain whether this trend was more pronounced in certain demographic subgroups.\"",
         null,
         "- The study uses a method called \"random silicon sampling\" to generate survey responses that mirror human opinions based on demographic distribution.\r\n- The method involves creating a \"random silicon sample\" of synthetic respondents with demographic information similar to a human subpopulation.\r\n- The language model is prompted with demographic information and survey questions to generate responses.\r\n- The study uses the ANES dataset for American public opinion and leverages the 2020 pre-election sample.\r\n- The researchers assess the replicability of the language model by comparing response distributions with actual ANES responses.\r\n- Experiments include stratified sampling, down-sampling, and testing across multiple questions to evaluate generalizability.\r\n- The study also explores the applicability of the method to various demographic subgroups and topics."
        ],
        [
         "2",
         "Uncertainty Quantification for LLM-Based Survey Simulations",
         "Chengpiao Huang, Yuhang Wu, Kaizheng Wang",
         "10.48550/arXiv.2502.17773",
         "https://doi.org/10.48550/arXiv.2502.17773",
         "arXiv.org",
         "1",
         "2025",
         "The paper presents a method to use large language models to simulate human survey responses and quantify the uncertainty in the simulated responses.",
         "- Use LLMs to simulate human responses to survey questions.\r\n- Apply uncertainty quantification to ensure reliable insights.\r\n- Adaptively select simulation sample size based on LLM-human population misalignment.\r\n- Use real data to select a good confidence set from candidate sets.\r\n- Achieve empirical coverage probability over multiple survey questions.\r\n- Construct confidence sets for population statistics using real data.\r\n- Select k based on empirical miscoverage threshold determined by  and .",
         "  - \"Ideally, we would like to pick k such that (1 -) coverage is achieved empirically over the m survey questions:\"\r\n  - \"We propose to address this challenge through the lens of uncertainty quantification. Specifically, we seek to construct confidence sets for population statistics of human responses based on LLMgenerated data.\"\r\n  - \"We propose a flexible methodology that transforms simulated responses into valid confidence sets for population parameters of human responses. Our approach adaptively selects the simulation sample size based on the observed misalignment between the LLM and human populations.\"\r\n  - \"We will pick k  {0, 1, ..., K} such that S syn j ( k) is a good confidence interval for ( j ) for each j  [m]. This choice of k will also be good for S syn (k), thanks to the i.i.d. assumption on the survey questions.\"\r\n  - \"It is applicable to any LLM, regardless of its fidelity, and can be combined with any method for confidence set construction.\"",
         null,
         "- The methodology involves using large language models (LLMs) to simulate human responses to survey questions and then applying uncertainty quantification to ensure reliable insights.\r\n- The approach adaptively selects the simulation sample size based on the misalignment between the LLM and human populations, which is crucial for achieving valid confidence intervals.\r\n- The method is flexible and can be applied to any LLM, regardless of its fidelity, and can be combined with any confidence set construction method.\r\n- Real data is used to select a good confidence set from a family of candidate sets generated by the LLM.\r\n- The selection of the simulation sample size k is based on achieving a certain coverage probability empirically over multiple survey questions.\r\n- The process involves constructing confidence sets for population statistics using real data and then using these to guide the selection of k.\r\n- The criterion for selecting k involves ensuring that the empirical miscoverage is within a certain threshold, which is determined by a confidence level  and the desired miscoverage probability ."
        ],
        [
         "3",
         "Large Language Models as Subpopulation Representative Models: A Review",
         "Gabriel Simmons, Christopher Hare",
         "10.48550/arXiv.2310.17888",
         "https://doi.org/10.48550/arXiv.2310.17888",
         "arXiv.org",
         "20",
         "2023",
         "Large language models can be used to simulate human survey responses across subpopulations as an alternate or complementary way to measure public opinion.",
         "The methodology involves using Large Language Models (LLMs) to approximate subpopulation characteristics and aggregate sentiment through techniques such as pre-training, fine-tuning, and prompting. Decoding strategies, prompting, and iterative refinement are used to elicit desired outputs. Existing studies are split between fine-tuning and prompting. The paper reviews existing applications and evaluation methods for SRMs.",
         "  - \"This review draws together a body of literature that uses LLMs to approximate subpopulation characteristics, or aggregate subpopulation sentiment, under the umbrella of Subpopulation Representative Models (SRMs).\"\r\n  - \"In Section 2, we provide background on LLMs and the techniques used to steer their behavior.\"\r\n  - \"The following sections provide an overview of methods for behavior elicitation from LLMs.\"\r\n  - \"LLMs begin with their parameters randomly initialized. The first step in the training process, pre-training, uses stochastic gradient descent to optimize the model parameters against a self-supervised learning objective on a large corpus of text data.\"\r\n  - \"After pre-training, LLMs commonly undergo finetuning processes to improve their performance for specific applications, which results in further updates to the model parameters.\"\r\n  - \"Existing SRM studies are split roughly evenly between fine-tuning and prompting.\"\r\n  - \"This section provides a review of present applications of large language models to estimate subpopulation representative models.\"\r\n  - \"Several studies have proposed methods to evaluate SRM behavior.\"",
         "  - \"(Page 10, Table 1) | Method                                           | Models               | Elicitation techniques | Inference techniques | Tasks                                                | Domains                  | Data sources                        | Subpopulations                              | Risks identified                  |\r\\n|--------------------------------------------------|----------------------|------------------------|----------------------|------------------------------------------------------|--------------------------|-------------------------------------|---------------------------------------------|-----------------------------------|\r\\n| CommunityLM (H. Jiang et al. 2022)               | GPT-2                | FT                     | OTG, SA              | Opinion mining                                        | Politics                 | Twitter                             | American partisan groups                    | Erasure                           |\"",
         "- The paper is a review of existing literature on using Large Language Models (LLMs) as Subpopulation Representative Models (SRMs), so it does not conduct new experiments but rather synthesizes existing methodologies.\r\n- The methodology involves using LLMs to approximate subpopulation characteristics and aggregate sentiment, which is achieved through various techniques such as pre-training, fine-tuning, and prompting.\r\n- The paper discusses the use of pre-training and fine-tuning as part of the LLM training process, which are common methodologies in machine learning.\r\n- The paper also mentions decoding strategies, prompting, and iterative refinement as techniques used to elicit desired outputs from LLMs.\r\n- The table at the end of the paper likely summarizes various studies and their methodologies, but without specific content, it's inferred that these studies use a mix of fine-tuning and prompting techniques.\r\n- The paper reviews existing applications and evaluation methods for SRMs, indicating a focus on synthesizing existing methodologies rather than introducing new ones."
        ],
        [
         "4",
         "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
         "Joseph Suh, Erfan Jahanparast, Suhong Moon, Minwoo Kang, Serina Chang",
         "10.48550/arXiv.2502.16761",
         "https://doi.org/10.48550/arXiv.2502.16761",
         "Annual Meeting of the Association for Computational Linguistics",
         "6",
         "2025",
         "Fine-tuning large language models on scaled survey data can improve their ability to predict distributions of public opinions across diverse subpopulations.",
         "- Fine-tuning large language models (LLMs) on SubPOP dataset.\r\n- Using pairs of subpopulation and survey question as input prompts.\r\n- Employing LoRA fine-tuning with forward Kullback-Leibler (KL) divergence as loss.\r\n- Evaluating with Wasserstein distance.\r\n- Fine-tuning multiple LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, Llama-3-70B).\r\n- Using pretrained models over instruction-following models.\r\n- Hyperparameter tuning with learning rates and batch sizes.\r\n- Using AdamW optimizer with weight decay.",
         "  - \"Here, we propose directly finetuning LLMs on large-scale, high-quality survey data, consisting of questions about diverse topics and responses from each subpopulation, defined by demographic, socioeconomic, and ideological traits.\"\r\n  - \"By casting pairs of (subpopulation, survey question) as input prompts, we train the LLM to align its response distribution against that of human subjects in a supervised manner.\"\r\n  - \"We fine-tune this model so that its predicted distribution for each (q, g) mirrors the human response distribution p H (A q | q,g)\"\r\n  - \"we apply LoRA fine-tuning (Hu et al., 2021) and use the forward Kullback-Leibler (KL) divergence as our loss.\"\r\n  - \"We use Wasserstein distance (WD) to quantify how closely the model's predicted opinion distribution matches human survey data\"\r\n  - \"We fine-tune four LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, and Llama-3-70B) on SubPOP-Train.\"\r\n  - \"We opt for pretrained LLMs rather than instruction-following models, as previous work has shown that pretrained models perform better on this task\"\r\n  - \"We use offline batched inference of vLLM (version 0.7.2) (Kwon et al., 2023) for inference and measuring response probability distribution of all methods.\"\r\n  - \"Hyperparameter tuning is performed over learning rates {5e-5, 1e-4, 2e-4} and batch sizes {64, 128, 256}.\"\r\n  - \"The AdamW (Loshchilov, 2017) optimizer is used with a weight decay of 0.\"",
         null,
         "- The study involves fine-tuning large language models (LLMs) on a dataset called SubPOP, which contains survey data from various subpopulations.\r\n- The fine-tuning process uses pairs of subpopulation and survey question as input prompts to align the model's response distribution with human responses.\r\n- The study employs LoRA fine-tuning and uses the forward Kullback-Leibler (KL) divergence as the loss function.\r\n- Wasserstein distance is used to evaluate how well the model's predictions match human survey data.\r\n- Multiple LLMs are fine-tuned, including Llama-2-7B, Llama-2-13B, Mistral-7B, and Llama-3-70B.\r\n- Pretrained models are preferred over instruction-following models due to their better performance.\r\n- Hyperparameter tuning involves adjusting learning rates and batch sizes.\r\n- The AdamW optimizer is used with a specific weight decay."
        ],
        [
         "5",
         "Questioning the Survey Responses of Large Language Models",
         "Ricardo Dominguez-Olmedo, Moritz Hardt, Celestine Mendler-Dunner",
         "10.48550/arXiv.2306.07951",
         "https://doi.org/10.48550/arXiv.2306.07951",
         "Neural Information Processing Systems",
         "38",
         "2023",
         "Large language models' survey responses do not faithfully represent any human population due to biases and lack of statistical signals found in human populations.",
         "- Employed the methodology introduced by Santurkar et al. [2023] to survey language models.\r\n- Used questions from the American Community Survey (ACS) to prompt language models.\r\n- Surveyed 43 language models of varying sizes individually and in sequence.\r\n- Used ACS Public Use Microdata Sample (PUMS) files as reference data.\r\n- Utilized the Folktables Python package to download data from the U.S. Census.\r\n- Open-sourced the code for reproducibility.",
         "  - \"We employ the de-facto standard methodology to survey language models introduced by Santurkar et al. [2023].\"\r\n  - \"For every survey question, we generate a prompt containing the multiple-choice question and we collect language models' probability distribution over answer choices.\"\r\n  - \"We prompt language models with questions from the American Community Survey (ACS).\"\r\n  - \"We use a representative subset of 25 multiple-choice questions from the 2019 ACS questionnaire.\"\r\n  - \"We use the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files made available by the U.S. Census Bureau.\"\r\n  - \"We download the data directly from the U.S. Census using the Folktables Python package [Ding et al., 2021].\"\r\n  - \"We open source the code to replicate all experiments.7\"",
         null,
         "- The study uses a methodology introduced by Santurkar et al. [2023] to survey language models, which involves generating prompts with multiple-choice questions and collecting probability distributions over answer choices.\r\n- The study uses questions from the American Community Survey (ACS) to prompt language models, specifically a subset of 25 multiple-choice questions from the 2019 ACS questionnaire.\r\n- The study surveys 43 language models of varying sizes, both individually and in sequence, to record their probability distributions over answers.\r\n- The study uses the ACS Public Use Microdata Sample (PUMS) files from the U.S. Census Bureau as reference data.\r\n- The study makes use of the Folktables Python package to download data from the U.S. Census.\r\n- The code for the experiments is open-sourced to ensure reproducibility."
        ],
        [
         "6",
         "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys",
         "Junsol Kim, Byungkyu Lee",
         "10.48550/arXiv.2305.09620",
         "https://doi.org/10.48550/arXiv.2305.09620",
         "arXiv.org",
         "27",
         "2023",
         "Large language models can be used to impute missing survey responses and predict public opinion trends, but have limitations in zero-shot prediction and demographic representation.",
         "- Personalization of large language models (LLMs) using neural embeddings of survey questions, individual beliefs, and temporal contexts.\r\n- Fine-tuning of LLMs with a small dataset to improve accuracy.\r\n- Prediction of individual opinions and aggregation at the population level using survey weights.\r\n- Use of three neural embeddings: survey question semantic, individual belief, and temporal context.\r\n- Data from General Social Survey (GSS) with 68,846 individuals and 3,110 questions.\r\n- Evaluation using 10-fold cross-validation.\r\n- Utilization of Huggingface's API and TensorFlow Recommenders.\r\n- Fine-tuning with Adam optimizer and binary cross-entropy loss function.",
         "  - \"We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction.\"\r\n  - \"Fine-tuning is the process of partially updating the parameters of LLMs using a relatively small set of data, enabling these models to perform specific tasks more accurately.\"\r\n  - \"Our approach first predicts individuals' opinions and then aggregates them at the population level using survey weights to account for sample selection bias\"\r\n  - \"we incorporate the three most important neural embeddings3 for predicting opinions -survey question semantic embedding, individual belief embedding, and temporal context embedding -that capture latent characteristics of survey questions, individual heterogeneities, and survey periods, respectively\"\r\n  - \"We use 68,846 individuals' responses to 3,110 questions collected for 33 repeated cross-sectional data between 1972 and 2021 for fine-tuning the LLMs.\"\r\n  - \"We evaluate the model's performance in predicting opinions at the individual and population levels in the GSS data by conducting 10-fold cross-validation to measure their accuracy in restoring missing data\"\r\n  - \"We fine-tune our models using the Adam optimizer with a learning rate of 2e-5, and a binary cross-entropy is used as the loss function.\"",
         null,
         "- The study uses a new methodological framework that involves personalizing large language models (LLMs) for opinion prediction by incorporating neural embeddings of survey questions, individual beliefs, and temporal contexts.\r\n- Fine-tuning is a key process where LLMs are updated with a small dataset to improve accuracy in specific tasks.\r\n- The approach involves predicting individual opinions and then aggregating them at the population level using survey weights to address sample selection bias.\r\n- The study uses three types of neural embeddings: survey question semantic embedding, individual belief embedding, and temporal context embedding.\r\n- The data used for fine-tuning comes from the General Social Survey (GSS), involving responses from 68,846 individuals to 3,110 questions over 33 years.\r\n- The model's performance is evaluated using 10-fold cross-validation to assess accuracy in predicting missing data.\r\n- The study utilizes specific tools and frameworks like Huggingface's API and TensorFlow Recommenders for model deployment.\r\n- The fine-tuning process involves using the Adam optimizer and binary cross-entropy as the loss function."
        ],
        [
         "7",
         "Using LLMs to Model the Beliefs and Preferences of Targeted Populations",
         "Keiichi Namikoshi, Alexandre L. S. Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Archiga",
         "10.48550/arXiv.2403.20252",
         "https://doi.org/10.48550/arXiv.2403.20252",
         "arXiv.org",
         "6",
         "2024",
         "The paper explores using large language models to simulate human survey responses across different question types and populations.",
         "- Fine-tuning pre-trained LLMs to match human survey responses.\r\n- Using Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) techniques.\r\n- Evaluating performance with KL-divergence and RMSE metrics.\r\n- Introducing a novel penalty term for numerical response questions.\r\n- Investigating effects of model size, quantization, and sampling temperature.\r\n- Splitting dataset into training, validation, and test sets with specific hyperparameters.",
         "  - \"We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs).\"\r\n  - \"we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.\"\r\n  - \"Our proposed approach provides virtual survey participants with a prompted, fine-tuned LLM to generate survey responses that statistically match those of a target population.\"\r\n  - \"Next, the pre-trained LLM is fine-tuned to emulate the preferences of the human survey participants.\"\r\n  - \"we use Llama 2 (Touvron et al., 2023). The model sizes are 7B, 13B, and 70B, using chat models published on HuggingFace1\"\r\n  - \"We set LoRA r = 8,  = 32, and LoRA dropout 0.1 for fine-tuning.\"\r\n  - \"We use 3 epochs across all experiments.\"\r\n  - \"We split our dataset randomly into a training set, a validation set, and a test set using an 8:1:1 split by subject\"\r\n  - \"Evaluation on the test data was performed using the following procedure.\"\r\n  - \"we compare the model against three baselines that are not language models.\"\r\n  - \"The combined loss function is L(D; ) = (1)L cross (D; ) + L penalty (D; ).\"\r\n  - \"We propose and evaluate a novel penalty term on the loss function to improve model performance on survey questions that require a numerical response.\"",
         null,
         "- The study uses large language models (LLMs) to model human preferences, specifically focusing on battery electric vehicles (BEVs).\r\n- The methodology involves fine-tuning pre-trained LLMs to match human survey responses, using techniques like Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA).\r\n- The study evaluates the performance of these models using metrics like KL-divergence and RMSE, comparing them to baseline models.\r\n- A novel penalty term is introduced to improve performance on numerical response questions.\r\n- The study investigates the effects of model size, quantization, and sampling temperature on performance.\r\n- The dataset is split into training, validation, and test sets, with specific hyperparameters set for fine-tuning."
        ],
        [
         "8",
         "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
         "Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li",
         "10.48550/arXiv.2312.11970",
         "https://doi.org/10.48550/arXiv.2312.11970",
         "Humanities and Social Sciences Communications",
         "151",
         "2023",
         "This paper does not directly address how large language models are applied to simulate human survey responses across question types and populations.",
         "- Traditional methods: predefined rules, symbolic equations, stochastic modeling, machine learning models\r\n- Systematic review methodology following PRISMA guidelines\r\n- Search strategy: keyword searches and controlled vocabulary terms\r\n- Filtering process: ensuring relevance to agent-based modeling and simulation using large language models",
         "  - \"The development of modeling technologies utilized in agent-based simulation has also gone through the early stage of knowledgedriven approaches and the recent stage of data-driven approaches.\"\r\n  - \"Predefined rules: This approach involves defining explicit rules that govern agent behaviors.\"\r\n  - \"Symbolic equations: Compared with predefined rules, symbolic equations are used to represent relationships or behaviors in a more formal, mathematical manner.\"\r\n  - \"Stochastic modeling: This approach introduces randomness and probability into agent decision-making,\"\r\n  - \"Machine learning models: Machine learning models allow agents to learn from data or through interaction with their environment.\"\r\n  - \"Following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), we provide more details of how we collect and organize the related works.\"\r\n  - \"The search strategy we used incorporates a combination of keyword searches and controlled vocabulary terms related to LLMs, ABMS, and their intersection,\"\r\n  - \"After deploying the search strategy on the various information sources, we select the proper papers presented in this review.\"\r\n  - \"The filtering process mainly focuses on two specific problems: (1) double-checking whether the paper belongs to agent-based modeling simulation and uses LLM agents and ( 2) what kind of sub-category this paper belongs to.\"",
         null,
         "- The paper discusses various methodologies used in agent-based modeling and simulation, including predefined rules, symbolic equations, stochastic modeling, and machine learning models. These are traditional methods used in the field.\r\n- The paper also follows the PRISMA guidelines for systematic reviews, which involves a structured approach to collecting and organizing literature related to large language models and agent-based modeling and simulation.\r\n- The search strategy includes keyword searches and controlled vocabulary terms, indicating a systematic review methodology.\r\n- The filtering process involves ensuring that selected papers are relevant to agent-based modeling and simulation using large language models, which is part of the methodology for selecting literature."
        ],
        [
         "9",
         "Aligning Large Language Models with Human: A Survey",
         "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu",
         "10.48550/arXiv.2307.12966",
         "https://doi.org/10.48550/arXiv.2307.12966",
         "arXiv.org",
         "357",
         "2023",
         "This paper does not directly address how large language models are applied to simulate human survey responses across question types and populations.",
         "- Data collection: Use of NLP benchmarks, human annotations, and leveraging strong LLMs.\r\n- Training methods: Supervised Fine-tuning, Online and Offline human preference training, parameter-efficient training mechanisms.\r\n- Evaluation methods: Various benchmarks, automatic metrics like BLUE and ROUGE.",
         "  - \"This survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) Data collection : the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs.\"\r\n  - \"Our exploration encompasses Supervised Fine-tuning, both Online and Ofine human preference training, along with parameter-efcient training mechanisms.\"\r\n  - \"In this survey, we aim to provide a comprehensive overview of alignment technologies for large language models.\"\r\n  - \"In Section 2, we summarize various methods in effective high-quality data collection.\"\r\n  - \"Section 3 focuses on popular training methods to incorporate human preference data into LLMs.\"\r\n  - \"The evaluation benchmarks and automatic protocols for instruction-following LLMs are discussed in Section 4.\"\r\n  - \"After collecting instructions and training LLMs on these instructions, we finally consider the evaluation for alignment quality.\"\r\n  - \"There are various benchmarks to evaluate the aligned LLMs.\"\r\n  - \"Automatic metrics, such as BLUE (Papineni et al., 2002) and ROUGE (Lin, 2004) et al., 2016)\"\r\n  - \"This survey provides an up-to-date review to recent advances of LLMs alignment technologies.\"",
         null,
         "- The paper is a survey, which means it provides an overview of existing methodologies rather than conducting new experiments.\r\n- The methodology involves reviewing and summarizing existing methods for data collection, training, and evaluation of LLMs.\r\n- Data collection methods include using NLP benchmarks, human annotations, and leveraging strong LLMs.\r\n- Training methods include Supervised Fine-tuning, Online and Offline human preference training, and parameter-efficient training mechanisms.\r\n- Evaluation methods involve using various benchmarks and automatic metrics like BLUE and ROUGE.\r\n- The paper does not conduct new experiments but rather reviews and discusses existing methodologies."
        ],
        [
         "10",
         "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
         "Xi Fang, Weijie Xu, F. Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan H. Sengamedu, Christos Faloutsos",
         "10.48550/arXiv.2402.17944",
         "https://doi.org/10.48550/arXiv.2402.17944",
         "Trans. Mach. Learn. Res.",
         "100",
         "2024",
         "This paper surveys the application of large language models to tabular data tasks, but does not specifically address simulating human survey responses.",
         "The methodology involves a comprehensive review and taxonomy of existing literature, consolidating recent progress, identifying strengths and limitations, and providing insights for future research directions.",
         "  - \"This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized.\"\r\n  - \"It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field.\"\r\n  - \"Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.\"",
         null,
         "- The abstract describes the study as a \"survey,\" which indicates that the methodology involves reviewing and summarizing existing literature.\r\n- The phrase \"consolidating recent progress\" suggests that the authors are gathering and synthesizing information from recent studies.\r\n- The mention of \"offering a thorough survey and taxonomy\" implies that the authors are categorizing and organizing the information they've gathered.\r\n- The identification of \"strengths, limitations, unexplored territories, and gaps\" suggests an analytical approach to evaluating the current state of research in the field.\r\n- The provision of \"insights for future research directions\" indicates that the authors are also synthesizing their findings to suggest areas for further study."
        ],
        [
         "11",
         "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",
         "Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, Erik Cambria",
         "10.48550/arXiv.2310.05694",
         "https://doi.org/10.48550/arXiv.2310.05694",
         "Information Fusion",
         "186",
         "2023",
         "This paper does not directly address how large language models are applied to simulate human survey responses across question types and populations.",
         "- Descriptive analysis of LLMs' capabilities and development process\r\n- Comparative analysis between PLMs and LLMs, and among different LLMs\r\n- Review of training data, methods, optimization strategies, and usage\r\n- Qualitative analysis of ethical concerns\r\n- Compilation of open-source resources",
         "  - \"This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs.\"\r\n  - \"Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations.\"\r\n  - \"Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other.\"\r\n  - \"Then we summarize related Healthcare training data, training methods, optimization strategies, and usage.\"\r\n  - \"Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics.\"\r\n  - \"Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty.\"\r\n  - \"Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github.\"",
         null,
         "- The abstract describes the study as a \"survey,\" which implies a comprehensive review of existing literature and data rather than an experimental study.\r\n- The methodology involves outlining the capabilities and development process of LLMs in healthcare, which suggests a descriptive analysis.\r\n- The study includes a comparison between previous PLMs and current LLMs, as well as among different LLMs, indicating a comparative analysis.\r\n- The abstract mentions summarizing training data, methods, optimization strategies, and usage, which suggests a review of existing methodologies.\r\n- The investigation of concerns related to fairness, accountability, transparency, and ethics indicates a qualitative analysis of ethical implications.\r\n- The compilation of open-source resources for the computer science community suggests a resource collection and documentation methodology."
        ],
        [
         "12",
         "Large Language Models for Information Retrieval: A Survey",
         "Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-rong Wen",
         "10.48550/arXiv.2308.07107",
         "https://doi.org/10.48550/arXiv.2308.07107",
         "arXiv.org",
         "336",
         "2023",
         "This paper is a survey on how large language models are applied to improve information retrieval systems, but does not mention simulating human survey responses.",
         "- Query rewriting: Prompting, supervised fine-tuning, reinforcement learning\r\n- Retrieval: Enhancing model architecture, generating search data (pseudo queries, relevance labels)\r\n- Reader module: Generating answers based on retrieved documents, categorized by retrieval frequency\r\n- Search agents: Autonomous search and synthesis using LLMs",
         "  - \"The utilization of LLMs in query rewriting can be categorized into three primary methodologies: prompting, supervised fine-tuning, and reinforcement learning.\"\r\n  - \"Prompting in LLMs refers to the technique of providing a specific instruction or context to guide the model's generation of text.\"\r\n  - \"supervised fine-tuning techniques adapt pre-trained LLMs to the specific task of query rewriting.\"\r\n  - \"reinforcement learning methods utilize feedback from downstream applications, thereby improving the performance of query rewriters.\"\r\n  - \"In the context of e-commerce search, a wealth of supervised training data for query rewriting is naturally available.\"\r\n  - \"in an ad-hoc retrieval scenario, the acquisition of query rewrite training data is often a challenge.\"\r\n  - \"Query rewriters typically serve as intermediaries for retrieval systems, implying that they do not operate independently, and there is no specific loss attributed solely to the query rewriting process.\"\r\n  - \"Reinforcement Learning (RL) presents an alternative training mechanism for the query rewriter.\"\r\n  - \"The strategies for utilizing LLMs within IR systems' reader modules can be categorized into the following three groups according to the frequency of retrieving documents for LLMs.\"\r\n  - \"To generate answers for users, a straightforward strategy is to supply the retrieved documents according to the queries or previously generated texts from IR systems as inputs to LLMs for creating passages\"\r\n  - \"To obtain useful references for LLMs to generate responses for user queries, an intuitive way is to retrieve the top documents based on the queries themselves in the beginning.\"\r\n  - \"RETRO [23] introduces a novel approach incorporating cross-attention between the generating texts and the references within the Transformer attention calculation,\"\r\n  - \"Self-Ask [218], DSP [219], and PlanRAG [220] try to employ fewshot prompts for LLMs, triggering them to search queries when they believe it is required.\"\r\n  - \"LPKG [221] constructs high-quality active retrieval-augmented reasoning paths from existing knowledge graphs.\"\r\n  - \"MRC [222] further improves these methods by prompting LLMs to explore multiple reasoning chains and subsequently combining all generated answers using LLMs.\"\r\n  - \"To mimic human search patterns, a straightforward approach is to design a static system to browse the web and synthesize information step by step\"\r\n  - \"We-bGPT [24] takes an alternate approach by training LLMs to use search engines automatically.\"\r\n  - \"ASH (Actor-Summarizer-Hierarchical) prompting [266] significantly enhances the ability of LLMs on WebShop benchmark.\"\r\n  - \"TRAD [267] enhances LLM agents for sequential decision-making tasks.\"\r\n  - \"AutoWebGLM [268] aims address the limitations of current web browsing agents, which often struggle with the versatility of webpage actions, the length of HTML text, and the open-domain nature of web decision-making.\"\r\n  - \"KwaiAgents [269] is a generalized information-seeking agent system built around LLMs.\"\r\n  - \"WebVoyager [270] is a large multimodal modelpowered agent, capable of navigating and interacting with websites by processing both visual inputs (like screenshots) and textual content, mimicking human web browsing behavior.\"\r\n  - \"IoA [271] is a framework that enables collaboration among a diverse range of autonomous agents, particularly those based on LLMs.\"\r\n  - \"Agent-E [272] aims to address the problems of dynamic and noisy nature of web content, the complexity of user interfaces, and the need for sophisticated planning to execute multi-step tasks.\"\r\n  - \"MindSearch [273] is a multi-agent framework that mimics human cognitive processes in web information seeking and integration.\"\r\n  - \"Several IR systems applying the RAG strategy, such as New Bing and Langchain, have already entered commercial use.\"",
         "  - \"(Page 5, Table 1) Below is a rendering of the page up to the first error.\"\r\n  - \"(Page 6, Table 1) | Methods       | Prompts                                                                                                                                                                                                 |\r\\n|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\\n|               | **Zero-shot**                                                                                                                                                                                           |\"\r\n  - \"(Page 10, Table 1) Below is a rendering of the page up to the first error.\"\r\n  - \"(Page 11, Table 1) Below is a rendering of the page up to the first error.\"\r\n  - \"(Page 12, Table 1) | Paradigm                    | Type     | Method                                                                                                                                                      |\r\\n|-----------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\\n| Supervised Rerankers        | Enc-only | monoBERT [12]                                                                                                                                               |\"\r\n  - \"(Page 15, Table 1) |                             | Methods                     | LLM          | Size   | Properties         |            |            | Performance |            |\r\\n|-----------------------------|-----------------------------|--------------|--------|--------------------|------------|------------|-------------|------------|\r\\n|                             |                             |              |        | Complexity         | Logits     | Batching   | TREC-DL19   | TREC-DL20  |\"\r\n  - \"(Page 16, Table 1) Below is a rendering of the page up to the first error.\"",
         "- The paper discusses various methodologies for utilizing Large Language Models (LLMs) in Information Retrieval (IR) systems, focusing on query rewriting, retrieval, reranking, and reader modules.\r\n- For query rewriting, the methodologies include prompting, supervised fine-tuning, and reinforcement learning. These methods are used to refine user queries for more effective retrieval.\r\n- In retrieval, LLMs are used to enhance model architecture and generate search data, such as pseudo queries and relevance labels.\r\n- The reader module involves using LLMs to generate answers based on retrieved documents, with strategies categorized by the frequency of document retrieval.\r\n- The paper also discusses the use of LLMs as search agents, where they can autonomously search and synthesize information from the web.\r\n- Various frameworks and systems are mentioned, such as WebGPT, ASH, TRAD, AutoWebGLM, KwaiAgents, WebVoyager, IoA, Agent-E, and MindSearch, which utilize LLMs for different aspects of IR.\r\n- The tables at the end of the paper provide detailed comparisons of different methods and models used in IR systems."
        ],
        [
         "13",
         "Using Large Language Models to Simulate Multiple Humans",
         "Gati Aher, RosaI. Arriaga, A. Kalai",
         "10.48550/arXiv.2208.10264",
         "https://doi.org/10.48550/arXiv.2208.10264",
         "arXiv.org",
         "51",
         "2022",
         "Large language models can be used to simulate responses of different humans across various experimental contexts.",
         "- Use large language models (LMs) like GPT-3 to simulate human responses.\r\n- Create prompt templates for each experiment.\r\n- Vary subject details such as names to simulate different individuals.\r\n- Use a zero-shot approach with pre-trained LMs from the OpenAI API.\r\n- Validate prompts without influencing outcomes.\r\n- Use diverse names from the U.S. Census Data for gender and racial diversity.\r\n- Output records or probability distributions over records.",
         "  - \"We propose a method for using a large language model, such as GPT-3, to simulate responses of different humans in a given context.\"\r\n  - \"The method requires prompt templates for each experiment.\"\r\n  - \"Simulations are run by varying the (hypothetical) subject details, such as name, and analyzing the text generated by the language model.\"\r\n  - \"In addition to introducing the concept of TEs, we demonstrate their feasibility by presenting a methodology for running TEs using an LM, like GPT models, that takes a text prompt and generates a randomized completion, which is text that would be likely to follow that prompt, based on its training data.\"\r\n  - \"For each TE, we write a program that creates one or more (zero-shot) prompts that are fed into the LMs.\"\r\n  - \"Our methodology includes an important validation step that involves the tweaking of prompts without examining the experimental outcomes (so as to avoid \"p-hacking.\")\"\r\n  - \"We conduct our simulations using pre-trained LMs based on the transformer architecture.\"\r\n  - \"We use the widely-used OpenAI API to query the following GPT text models: text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002, text-davinci-003, gpt-35-turbo (commonly referred to as ChatGPT), gpt-4, which we refer to as LM-1 through LM-8, respectively.\"\r\n  - \"Names. For our TEs, the inputs include subject names consisting of a title, either Mr. or Ms., indicating binary gender, followed by a surname.\"\r\n  - \"Lists of surnames were sourced from the U.S. 2010 Census Data.\"\r\n  - \"Considering all combinations of the two titles, five racial groups, and one hundred surnames in each group, we have a pool of 1,000 names.\"\r\n  - \"The simulator is assumed to output a record or, more generally, a probability distribution over a set of records with non-negative weights that sum to 1.\"\r\n  - \"Validating prompts. After one has formulated an hypothesis, one must design the sequence of prompts that will be used in simulation process.\"\r\n  - \"Our strategy for designing prompts that maximize the validity rate includes clearly specifying the desired completions in the first few lines of the prompt.\"\r\n  - \"Models. We conduct our simulations using pre-trained LMs based on the transformer architecture.\"",
         null,
         "- The study uses large language models (LMs) like GPT-3 to simulate human responses in various experiments.\r\n- The methodology involves creating prompt templates for each experiment and varying subject details such as names to simulate different individuals.\r\n- The study uses a zero-shot approach, meaning the models are not trained on specific experiment data but generate responses based on their general training.\r\n- The researchers use pre-trained LMs from the OpenAI API, specifically models like text-ada-001 to gpt-4.\r\n- The validation process involves tweaking prompts to ensure they are effective without influencing the outcomes.\r\n- The study uses a diverse set of names sourced from the U.S. Census Data to simulate gender and racial diversity.\r\n- The simulator outputs records or probability distributions over records, indicating how the simulations are structured."
        ],
        [
         "14",
         "Virtual Personas for Language Models via an Anthology of Backstories",
         "Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, David M. Chan",
         "10.48550/arXiv.2407.06576",
         "https://doi.org/10.48550/arXiv.2407.06576",
         "Conference on Empirical Methods in Natural Language Processing",
         "20",
         "2024",
         "Anthology method conditions large language models to virtual personas via backstories to better simulate human survey responses across populations.",
         "- Introduction of Anthology: A method for conditioning LLMs using open-ended life narratives (backstories).\r\n- Generation of backstories using language models.\r\n- Demographic surveys on backstories to estimate persona demographics.\r\n- Bipartite matching to select virtual personas matching target demographics.\r\n- Use of maximum weight matching and greedy matching methods.\r\n- Assignment of demographic traits to matched backstories for use in surveys.",
         "  - \"In this work, we introduce Anthology, a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as backstories.\"\r\n  - \"we propose using language models to generate realistic backstories as a more cost-efficient alternative.\"\r\n  - \"We use the term backstories to refer to firstperson narratives that encompass various aspects of an individual's life,\"\r\n  - \"Our proposed approach is to condition language models with backstories by placing them as prefixes to the LLM\"\r\n  - \"we perform demographic surveys on each of these backstory-conditioned personas to estimate the persona demographics.\"\r\n  - \"we methodologically select a representative set of virtual personas that match a desired distribution of demographics,\"\r\n  - \"We use OpenAI's davinci-002 for generating backstories with the prompt specified in the top of Figure 6\"\r\n  - \"Target demographics-primed backstories are generated by prompting a language model with demographic information of a human from a target population.\"\r\n  - \"We use two styles of prompt, which we refer to a Question-Answer and a Biography as presented in Figure 7\"\r\n  - \"We perform bipartite matching to select the virtual personas whose demographic probability distributions are most similar to the real, human user population.\"\r\n  - \"We explore two matching methods: (1) maximum weight matching, and (2) greedy matching.\"\r\n  - \"we assign the demographic traits of the target population to the matched backstories.\"\r\n  - \"we append these demographic information to backstories and use the matched subset of backstories,\"",
         null,
         "- The study introduces a methodology called Anthology, which involves using open-ended life narratives (backstories) to condition large language models (LLMs) to represent specific virtual personas.\r\n- The backstories are generated using language models, which is a cost-efficient method compared to sourcing human-written backstories.\r\n- The process involves generating backstories using prompts, either naturally or with demographic priming.\r\n- Demographic surveys are conducted on these backstories to estimate the demographic traits of the virtual personas.\r\n- The study uses bipartite matching to select virtual personas that closely match the demographic distribution of the target human population.\r\n- Two matching methods are employed: maximum weight matching and greedy matching.\r\n- The demographic traits are then assigned to the matched backstories, which are used in downstream surveys."
        ],
        [
         "15",
         "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge",
         "Hongjian Zhou, Boyang Gu, Xinyu Zou, Jinfa Huang, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Y. Hua, Chengfeng Mao, Xian Wu, Zheng Li, Fenglin Liu",
         "10.48550/arXiv.2311.05112",
         "https://doi.org/10.48550/arXiv.2311.05112",
         "arXiv.org",
         "136",
         "2023",
         "This paper reviews the development, application, and challenges of using large language models in medicine, but does not address their use for simulating human survey responses.",
         "- Analyzing principles of existing medical LLMs (model structures, parameters, data sources)\r\n- Comparing performance across medical tasks and with lightweight models\r\n- Discussing challenges and opportunities in clinical deployment\r\n- Evaluating effectiveness in medical education\r\n- Addressing regulatory challenges",
         "  - \"this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face.\"\r\n  - \"In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development.\"\r\n  - \"In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine.\"\r\n  - \"By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource.\"\r\n  - \"We also maintain a regularly updated list of practical guides on medical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide.\"\r\n  - \"this review seeks to answer the following questions. Section 2: What are LLMs? How can medical LLMs be effectively built? Section 3: How are the current medical LLMs evaluated? What capabilities do medical LLMs offer beyond traditional models? Section 4: How should medical LLMs be applied in clinical settings? Section 5: What challenges should be addressed when implementing medical LLMs in clinical practice? Section 6: How can we optimize the construction of medical LLMs to enhance their applicability in clinical settings, ultimately contributing to medicine and creating a positive societal impact?\"\r\n  - \"By establishing robust training data, benchmarks, metrics, and deployment strategies through co-development efforts, we aim to accelerate the responsible and efficacious integration of medical LLMs into clinical practice.\"\r\n  - \"This study therefore seeks to stimulate continued research and development in this interdisciplinary field, with the objective of realizing the profound potential of medical LLMs in enhancing clinical practice and advancing medical science for the betterment of society.\"\r\n  - \"The impressive performance of LLMs can be attributed to Transformer-based language models, large-scale pre-training, and scaling laws.\"\r\n  - \"In this section, we will introduce two popular types of medical machine learning tasks: generative and discriminative tasks, including ten representative tasks that further build up clinical applications.\"\r\n  - \"Figure 3 shows that some existing general LLMs (e.g., GPT-3.5-turbo and GPT-4 7 ) have achieved strong performance on existing medical machine learning tasks.\"\r\n  - \"Currently, most of existing medical LLMs are still in the research and development stage, with limited application and validation in real-world clinical scenarios.\"\r\n  - \"Researchers are also exploring the integration of large language models into clinical decision support systems to provide evidence-based recommendations and references\"\r\n  - \"To evaluate the effectiveness of integrating LLMs into medical education, a combination of quantitative and qualitative methods should be employed.\"\r\n  - \"Current evaluation of these systems often involves the calculation of metrics such as accuracy, precision, recall, and F1-score\"\r\n  - \"The regulatory landscape of LLMs presents distinct challenges due to their large scale, broad applicability and varying reliability across applications.\"\r\n  - \"To address the complex regulatory challenges without hindering innovation, regulators should devise adaptive, flexible, and robust frameworks.\"",
         "  - \"(Page 4, Table 1) | Domains                                      | Model Structures | Models          | # Params         | Pre-train Data Scale         |\r\\n|----------------------------------------------|------------------|-----------------|------------------|------------------------------|\r\\n| General-domain                               | Encoder-only     | BERT            | 110M/340M        | 3.3B tokens                  |\"\r\n  - \"<table_quotation page_num=5 table_on_page=1></table_quotation>\"",
         "- The paper is a review that aims to provide a detailed overview of the development and deployment of Large Language Models (LLMs) in medicine.\r\n- The methodology involves analyzing the principles of existing medical LLMs, including their model structures, parameters, and data sources.\r\n- The review compares the performance of different LLMs across various medical tasks and with lightweight models.\r\n- It discusses the challenges and opportunities in deploying LLMs in clinical settings and strategies for optimizing their construction.\r\n- The paper also addresses the need for robust training data, benchmarks, and deployment strategies.\r\n- The methodology includes evaluating the effectiveness of LLMs in medical education and discussing regulatory challenges.\r\n- The paper uses tables to summarize existing general and medical-domain LLMs, indicating a structured approach to reviewing the field."
        ],
        [
         "16",
         "Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models",
         "Philip G. Feldman, Aaron Dant, James R. Foulds, Shimei Pan",
         "10.48550/arXiv.2204.07483",
         "https://doi.org/10.48550/arXiv.2204.07483",
         "arXiv.org",
         "3",
         "2022",
         "The paper demonstrates how large language models can be used to generate synthetic text that accurately reflects the opinions and linguistic behaviors of specific subgroups within larger corpora.",
         "- Fine-tuning GPT-2 models on Yelp data to capture user opinions and biases.\r\n- Using prompt-based queries to generate insights into user opinions.\r\n- Introducing language model memorization through metawrapping to identify patterns in generated text.\r\n- Systematic approach: downloading and storing Yelp data, analyzing review categories, creating corpora with metainformation, fine-tuning models, and evaluating model responses to prompts.\r\n- Training three sets of models: on stars and votes, on reviews with stars, and on masked corpora without \"vegetarian options.\"\r\n- Evaluating statistical properties against ground truth data and comparing model performance.",
         "  - \"In this paper, we finetune (Sun et al., 2019) a set of GPT-2 models on a Yelp corpora that reflect populations of users with distinctive views.\"\r\n  - \"We then use prompt-based queries to probe these models to reveal insights into the biases and opinions of the users.\"\r\n  - \"We demonstrate how this approach can be used to produce results more accurately than traditional keyword or keyphrase searches, particularly when data is sparse or missing.\"\r\n  - \"In addition to the concepts of interpolation and extrapolation, we introduce the concept of language model memorization, where models can be trained to incorporate repeating patterns.\"\r\n  - \"We incorporate this concept by introducing the technique of metawrapping, which adds information to the training corpora that aids in the automated identifying of particular parts of the generated text.\"\r\n  - \"We extensively study our methodology on Yelp data, where we have ground truth in the form of user-submitted stars, and discuss applications in other domains.\"\r\n  - \"Our methods focus on understanding the memorization, interpolation, and extrapolation behaviors of these language models.\"\r\n  - \"We trained and evaluated three sets of models. The first sets were trained exclusively on stars and votes (See training corpora example in Figure 1 ). This was used to evaluate the statistical properties of the GPT against well-characterized numeric data.\"\r\n  - \"The second sets were trained using corpora of reviews followed by stars (Figure 2 ). These models were used to evaluate how effectively the models learned the relationship of the generated text to the star review.\"\r\n  - \"The third set was trained using a masked corpora that did not include the phrase \"vegetarian options\" to compare against the other model and ground truth.\"\r\n  - \"We then compared the behavior of the interpolating model that had been trained on corpora \"vegetarian options\" reviews, and a baseline of statis- tical samples taken from the known ground truth of 97 samples of all three-star reviews in our set of \"no/some/several/many\" samples.\"",
         null,
         "- The study involves fine-tuning GPT-2 models on Yelp data to capture user opinions and biases.\r\n- The methodology includes using prompt-based queries to generate insights into user opinions.\r\n- The concept of language model memorization is introduced through metawrapping to identify patterns in generated text.\r\n- The study uses a systematic approach to create and evaluate models, including downloading and storing Yelp data, analyzing review categories, creating corpora with metainformation, fine-tuning models, and evaluating model responses to prompts.\r\n- Three sets of models were trained: one on stars and votes, one on reviews with stars, and one on masked corpora without \"vegetarian options.\"\r\n- The study evaluates the statistical properties of the models against ground truth data and compares the performance of different models."
        ],
        [
         "17",
         "Large Language Models for Time Series: A Survey",
         "Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, Jingbo Shang",
         "10.48550/arXiv.2402.01801",
         "https://doi.org/10.48550/arXiv.2402.01801",
         "International Joint Conference on Artificial Intelligence",
         "74",
         "2024",
         "This paper is not relevant to the query about using large language models to simulate human survey responses across question types and populations.",
         "- Direct Prompting: Treating numerical time series as text and directly inputting it into LLMs.\r\n- Time Series Quantization: Converting numerical data into discrete representations for input into LLMs.\r\n- Aligning Techniques: Training a separate encoder for time series and aligning it with the semantic space of language models.\r\n- Vision as a Bridge: Interpreting time series data through visual representations to align with textual data.\r\n- Combining LLMs with Tools: Using LLMs to generate indirect tools like code and API calls for time series tasks.",
         "  - \"This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis.\"\r\n  - \"We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools.\"\r\n  - \"In this section, we detail our taxonomy of applying LLMs for time series analysis, categorized by five groups.\"\r\n  - \"Number-Agnostic Tokenization: The method treats numerical time series as raw textual data and directly prompts existing LLMs.\"\r\n  - \"Number-Specific Tokenization: More recently, LLM-Time [Gruver et al., 2023] pointed out that Byte Pair Encoding (BPE) tokenization has the limitation of breaking a single number into tokens that don't align with the digits,\"\r\n  - \"Quantization based method [Rabanser et al., 2020] converts numerical data into discrete representations as input to LLMs.\"\r\n  - \"The third type of works trains a separate encoder for time series, and aligns the encoded time series to the semantic space of language models.\"\r\n  - \"Time series data can be effectively interpreted or associated with visual representations, which align closer with textual data and have demonstrated successful integrations with large language models.\"\r\n  - \"This type of method does not directly use large language models to process time series. Instead, it applies large language models to generate indirect tools z(), such as code and API calls, to benefit time series related tasks.\"",
         "  - \"(Page 5, Table 1) | Method            | Subcategory         | Representative Works                | Equations                                      | Advantages                                                                 | Limitations                          |\r\\n|-------------------|---------------------|-------------------------------------|------------------------------------------------|----------------------------------------------------------------------------|--------------------------------------|\r\\n| Prompting         | Number-Agnostic     | PromptCast [Xue and Salim, 2022]    | \\( \\mathbf{y} = f_{\\theta}(\\mathbf{x}_s, \\mathbf{x}_t) \\) | easy to implement; zero-shot capability                                    | lose semantics; not efficient        |\"",
         "- The paper provides a taxonomy of methodologies for applying Large Language Models (LLMs) to time series analysis, which includes five main categories: direct prompting, time series quantization, aligning techniques, using vision as a bridge, and combining LLMs with tools.\r\n- Direct prompting involves treating numerical time series as text and directly inputting it into LLMs.\r\n- Time series quantization converts numerical data into discrete representations for input into LLMs.\r\n- Aligning techniques involve training a separate encoder for time series and aligning it with the semantic space of language models.\r\n- The use of vision as a bridge involves interpreting time series data through visual representations to align with textual data.\r\n- Combining LLMs with tools involves using LLMs to generate indirect tools like code and API calls for time series tasks."
        ],
        [
         "18",
         "Empowering Time Series Analysis with Large Language Models: A Survey",
         "Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song",
         "10.48550/arXiv.2402.03182",
         "https://doi.org/10.48550/arXiv.2402.03182",
         "International Joint Conference on Artificial Intelligence",
         "61",
         "2024",
         "This paper is not relevant to the query about using large language models to simulate human survey responses across question types and populations.",
         "- Direct query: Using pre-trained LLMs to directly query time series data.\r\n- Tokenization: Converting numerical time series data into string-based tokens for LLM processing.\r\n- Prompt design: Designing prompts to enhance time series representation and facilitate LLM understanding.\r\n- Fine-tuning: Updating LLM parameters to improve performance in specific tasks.\r\n- Model integration: Incorporating LLMs into time series models for feature enhancement.",
         "  - \"In this section, we review the existing applications of LLMs to general and spatial-temporal time series data, which covers universal and domain-specific areas including finance, transportation, healthcare, and computer vision.\"\r\n  - \"Fine-tuning pre-trained LLMs is pivotal to leveraging LLMs' strong pattern recognition and reasoning capabilities to facilitate downstream tasks.\"\r\n  - \"In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis.\"\r\n  - \"The rapid development of LLMs in natural language processing has unveiled unprecedented capabilities in sequential modeling and pattern recognition.\"",
         "  - \"<table_quotation page_num=5 table_on_page=1></table_quotation>\"",
         "- The paper provides a systematic overview of existing methods that leverage LLMs for time series analysis, indicating a focus on methodology.\r\n- The methodology involves five techniques: direct query, tokenization, prompt design, fine-tuning, and model integration, which are used to apply LLMs to time series analysis.\r\n- The paper discusses the general pipeline for LLM-based time series analysis, which includes direct querying, fine-tuning, and feature enhancement.\r\n- The conversion of numerical time series data into string-based tokens is a key methodological step to enable LLMs to process time series data.\r\n- Fine-tuning pre-trained LLMs is a crucial method to enhance their performance in downstream tasks.\r\n- The paper reviews applications of LLMs in various domains, indicating a broad methodological approach to time series analysis."
        ],
        [
         "19",
         "Aligning Language Models to User Opinions",
         "EunJeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon",
         "10.48550/arXiv.2305.14929",
         "https://doi.org/10.48550/arXiv.2305.14929",
         "Conference on Empirical Methods in Natural Language Processing",
         "80",
         "2023",
         "The paper describes methods to align large language models with specific user opinions, beyond just demographics and ideology, to improve prediction of user survey responses across topics.",
         "- Used public opinion surveys from Pew Research to analyze user opinions, demographics, and ideologies.\r\n- Modeled LLMs using both user opinions and demographics/ideology to predict public opinions.\r\n- Analyzed survey responses in the OpinionQA dataset with respect to demographics, ideology, and implicit opinions.\r\n- Used GPT3 with various input combinations to model users and predict opinions.\r\n- Employed a question-answering setup to measure performance, sampling 100 users per topic.\r\n- Evaluated using overall accuracy and collapsed accuracy.",
         "  - \"Mining public opinion surveys (by Pew Research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors.\"\r\n  - \"We use this insight to align LLMs by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics.\"\r\n  - \"In this paper, we give a thorough analysis of public survey responses in the OpinionQA dataset (Santurkar et al., 2023) with respect to their demographics, ideology, and implicit opinions and present comprehensive experimental results using the GPT3 model with various combinations of inputs (i.e., demographic, ideology, user's past opinions).\"\r\n  - \"Task We use LLMs to model a user; however, to concretely measure the performance, we use a simple question-answering (QA) setup.\"\r\n  - \"Modeling Approaches We sample 100 users per topic. 20% of implicit questions belonging to the specific user are used as the user's implicit persona, and the rest are used to test the model's personalization ability.\"\r\n  - \"Evaluation Metric For evaluation, we utilize two types of accuracy measures, overall accuracy and collapsed accuracy.\"",
         "  - \"(Page 6, Table 1) | Model                                      | Exact match | Collapsed match |\r\\n|--------------------------------------------|-------------|-----------------|\r\\n| no persona                                 | 0.430.01   | 0.620.01       |\"\r\n  - \"(Page 8, Table 1) |                         | no-persona | Accuracy with exact match |                      |\r\\n|-------------------------|------------|---------------------------|----------------------|\r\\n|                         |            | demo. + ideo.             | demo. + ideo.+ top8 op. |\"",
         "- The study uses public opinion surveys from Pew Research to analyze the relationship between user opinions, demographics, and ideologies.\r\n- The authors align LLMs by modeling both user opinions and demographics/ideology, achieving significant accuracy gains in predicting public opinions.\r\n- The study uses the OpinionQA dataset to analyze survey responses with respect to demographics, ideology, and implicit opinions.\r\n- The methodology involves using LLMs (specifically GPT3) with various input combinations to model users and predict opinions.\r\n- The study uses a question-answering setup to measure performance, sampling 100 users per topic and using a portion of their implicit questions as their persona.\r\n- Evaluation metrics include overall accuracy and collapsed accuracy, as shown in the tables."
        ],
        [
         "20",
         "From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents",
         "Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xuanjing Huang, Zhongyu Wei",
         "10.48550/arXiv.2412.03563",
         "https://doi.org/10.48550/arXiv.2412.03563",
         "arXiv.org",
         "27",
         "2024",
         "Large language models can be used to simulate human behavior and social interactions at individual, scenario, and societal levels.",
         "- Conducted a comprehensive survey of simulations driven by LLM-empowered agents.\r\n- Categorized simulations into three types: individual, scenario, and society simulations.\r\n- Discussed architectures, construction methods, objectives, and evaluation methods for each type.\r\n- Summarized commonly used datasets and benchmarks.\r\n- Analyzed trends across the three simulation types.",
         "  - \"In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents.\"\r\n  - \"We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics.\"\r\n  - \"We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method.\"\r\n  - \"Afterward, we summarize commonly used datasets and benchmarks.\"\r\n  - \"Finally, we discuss the trends across these three types of simulation.\"\r\n  - \"This motivates us to present this survey, aiming to contribute to the research and development of simulations driven by LLM-based agents, as well as a wider range of interdisciplinary studies.\"\r\n  - \"To comprehensively describe our landscape, we organize our survey as follows. After a brief introduction to the background in  2, we begin in  3 by detailing how to conduct individual simulation through discussions of (1) the architecture of a single agent, (2) construction method of individual simulation, (3) the classification of objectives, and (4) the evaluation of individual simulation.\"\r\n  - \"Next, in  4, we summarize scenario simulation, including (1) the elements that constitute a scenario simulation system, (2) the classification of scenarios, and (3) the evaluation of scenario simulation, exploring how multiple agents collaborate to achieve objectives within a single scenario.\"\r\n  - \"Following this, in  5, we introduce society simulation, examining how multi-agent systems can construct complex social dynamics through (1) the social construction elements of society simulation, (2) the classification of society simulation scenarios, and (3) the evaluation of society simulation.\"\r\n  - \"In  6, we summarize existing datasets and benchmarks.\"\r\n  - \"Based on the earlier sections, we analyze trends in these three aspects in  7 and present the conclusion in  8.\"",
         "  - \"<table_quotation page_num=4 table_on_page=1></table_quotation>\"\r\n  - \"<table_quotation page_num=10 table_on_page=1></table_quotation>\"\r\n  - \"<table_quotation page_num=15 table_on_page=1></table_quotation>\"\r\n  - \"(Page 19, Table 1) | Domain       | Dataset                     | Type                      | Source                                      | # individual num | # dialogue num | Paper | Link |\r\\n|--------------|-----------------------------|---------------------------|---------------------------------------------|------------------|----------------|-------|------|\r\\n| Characters   | Final Dialogue Dataset      | Dialogue                  | Wikipedia                                   | /                | 22,311         | [269] | Link |\"\r\n  - \"<table_quotation page_num=20 table_on_page=1></table_quotation>\"\r\n  - \"<table_quotation page_num=21 table_on_page=1></table_quotation>\"",
         "- The paper is a comprehensive survey of simulations driven by large language model (LLM)-empowered agents, focusing on three types: individual, scenario, and society simulations.\r\n- The methodology involves categorizing these simulations and discussing their architectures, construction methods, objectives, and evaluation methods.\r\n- The paper provides a detailed discussion of each simulation type, indicating a structured approach to understanding the field.\r\n- The authors summarize commonly used datasets and benchmarks, which are crucial for evaluating and comparing different simulations.\r\n- The paper is organized into sections that systematically cover the background, individual simulation, scenario simulation, society simulation, datasets, and trends, suggesting a thorough and structured methodology.\r\n- The use of tables to summarize datasets and benchmarks indicates a systematic approach to cataloging and analyzing existing resources in the field."
        ],
        [
         "21",
         "Demonstrations of the Potential of AI-based Political Issue Polling",
         "Nathan Sanders, Alex Ulinich, B. Schneier",
         "10.48550/arXiv.2307.04781",
         "https://doi.org/10.48550/arXiv.2307.04781",
         "Issue 5.4, Fall 2023",
         "23",
         "2023",
         "Large language models like ChatGPT can simulate human survey responses on policy issues, but have limitations in capturing demographic-level differences.",
         "- Developed a prompt engineering methodology to elicit human-like survey responses from ChatGPT.\r\n- Executed large-scale experiments to generate thousands of simulated responses.\r\n- Compared simulated data with human issue polling data from the Cooperative Election Study (CES).\r\n- Used the OpenAI Chat Completion API and the gpt-3.5-turbo-0301 LLM model.\r\n- Generated a balanced sample across demographic and ideological categories.\r\n- Systematically compared AI-generated and human data.\r\n- Used metrics like Normalized Earth Mover's Distance (NEMD) to evaluate distributional similarity.",
         "  - \"We have developed a prompt engineering methodology for eliciting human-like survey responses from ChatGPT, which simulate the response to a policy question of a person described by a set of demographic factors, and produce both an ordinal numeric response score and a textual justification.\"\r\n  - \"We execute large scale experiments, querying for thousands of simulated responses at a cost far lower than human surveys.\"\r\n  - \"We compare simulated data to human issue polling data from the Cooperative Election Study (CES).\"\r\n  - \"We have developed experimental methods ( 2) to prompt the AI chatbot ChatGPT to generate public polling-like responses such that it can simulate a survey panel.\"\r\n  - \"We use the OpenAI Chat Completion API endpoint, through OpenAI's openai python library, 2 to query the gpt-3.5-turbo-0301 LLM for polling responses.\"\r\n  - \"We generate a balanced sample of n = 100 responses per prompt per demographic cross-tab per issue across ideology (in five bins) and three demographic fields with simple categorizations (age in four bins, \"man\" or \"woman\" gender, and \"white\" or \"non-white\" race), for a total of 8, 000 responses across each of seven issue prompts (see Table 1 ) for 56, 000 total responses.\"\r\n  - \"We systematically compare the AI-generated and human respondent issue polling data across the seven queried issues, ideology, and three demographics to understand the quality of the AI-driven approach through its correspondence to a human population.\"\r\n  - \"We make some manipulations to the survey data to accommodate generation of equivalent LLM completions.\"\r\n  - \"We use the default OpenAI system prompt of, \"You are a helpful assistant\".\"\r\n  - \"We queried the OpenAI API endpoint for this model multiple times between April and August of 2023.\"\r\n  - \"We use the Normalized Earth Mover's Distance (NEMD) metric (the Wasserstein distance normalized by the cardinality of each question response scale) to evaluate distributional similarity, reported in each figure facet.\"",
         null,
         "- The study uses a \"prompt engineering methodology\" to elicit human-like survey responses from ChatGPT, indicating a focus on designing effective prompts to simulate human responses.\r\n- The methodology involves executing large-scale experiments to generate thousands of simulated responses, which is a key part of the study.\r\n- The study compares these simulated responses with human data from the Cooperative Election Study (CES), which is a critical component of the methodology.\r\n- The use of the OpenAI Chat Completion API and the specific LLM model (gpt-3.5-turbo-0301) is detailed, showing how the researchers accessed and utilized AI technology.\r\n- The generation of a balanced sample across various demographic and ideological categories is a methodological choice to ensure comprehensive data collection.\r\n- The study involves systematic comparison of AI-generated and human data, which is a methodological step to evaluate the effectiveness of the AI approach.\r\n- The use of specific metrics like the Normalized Earth Mover's Distance (NEMD) indicates a methodological focus on evaluating distributional similarity."
        ],
        [
         "22",
         "Large Language Models for Cyber Security: A Systematic Literature Review",
         "Hanxiang Xu, Shenao Wang, Ningke Li, Kailong Wang, Yanjie Zhao, Kai Chen, Ting Yu, Yang Liu, Haoyu Wang",
         "10.48550/arXiv.2405.04760",
         "https://doi.org/10.48550/arXiv.2405.04760",
         "arXiv.org",
         "51",
         "2024",
         "This paper is not relevant to the query about using large language models to simulate human survey responses across question types and populations.",
         "- Employed the \"Quasi-Gold Standard\" (QGS) strategy for literature search.\r\n- Identified related venues and databases.\r\n- Established QGS and defined search keywords.\r\n- Conducted automated searches.\r\n- Applied inclusion and exclusion criteria for paper selection.\r\n- Used quality assessment criteria for paper quality.\r\n- Performed forward and backward snowballing to expand literature coverage.",
         "  - \"Step1: Identify related venues and databases.\"\r\n  - \"Step2: Establish QGS.\"\r\n  - \"Step3: Define search keywords.\"\r\n  - \"Step4: Conduct an automated search.\"\r\n  - \"After obtaining the initial pool of 38,112 papers (38,071 from the automated search and 41 from the QGS), we conducted a multi-stage study selection process to identify the most relevant and high-quality papers for our systematic review.\"\r\n  - \"To select relevant papers for our research questions, we defined four inclusion criteria and eight exclusion criteria (as listed in Table 2\"\r\n  - \"To further expand the coverage of relevant literature, we performed forward and backward snowballing on the 93 selected papers.\"\r\n  - \"After conducting searches and snowballing, a total of 127 relevant research papers were ultimately obtained.\"",
         null,
         "- The study used a systematic literature review methodology, employing the \"Quasi-Gold Standard\" (QGS) strategy to identify relevant papers.\r\n- The process involved identifying related venues and databases, establishing a QGS, defining search keywords, and conducting automated searches.\r\n- A multi-stage study selection process was used to filter papers based on inclusion and exclusion criteria.\r\n- Quality assessment criteria were applied to ensure the selected papers were of high quality.\r\n- Forward and backward snowballing were used to expand the literature coverage.\r\n- The methodology resulted in the selection of 127 relevant papers for analysis."
        ],
        [
         "23",
         "LLMs for Explainable AI: A Comprehensive Survey",
         "Ahsan Bilal, David Ebert, Beiyu Lin",
         "10.48550/arXiv.2504.00125",
         "https://doi.org/10.48550/arXiv.2504.00125",
         "arXiv.org",
         "12",
         "2025",
         "This paper is a comprehensive survey on using large language models to enhance explainable AI, but does not specifically address simulating human survey responses.",
         "- Three main approaches to LLM-based explainability: post-hoc explanations, intrinsic interpretability, and human-centered narratives.\r\n- Post-hoc explanations: Analyzing why a specific input led to a particular output.\r\n- Intrinsic interpretability: Designing model architectures to make them inherently explainable.\r\n- Human-centered narratives: Enhancing explanations through natural language to foster user trust.\r\n- Evaluation techniques: Qualitative (comprehensibility, controllability) and quantitative (faithfulness, plausibility).\r\n- Use of benchmark datasets for training and evaluation.",
         "  - \"Our survey discusses various approaches to LLM-based Explainability, as shown in Figure 2 and in Table 1\"\r\n  - \"We will specifically discuss three approaches: 1). The first approach, post-hoc explanations, corresponds to causal interpretability and focuses on analyzing why a specific input led to a particular output, providing explanations for output generated by the machine learning (ML) model; 2). The second approach, intrinsic explainability, corresponds with engineers' interpretability and involves designing machine learning model architectures using LLMs to make machine learning models more explainable.; 3). The third approach, human-centered narratives, corresponds to trust-inducing interpretability by enhancing explanations for the outputs generated by ML models through natural language, making the outputs more understandable and fostering user trust.\"\r\n  - \"Techniques for explainability in AI systems using Large Language Models (LLMs) are categorized into 1) post-hoc explanations, 2) intrinsic interpretability, and 3) human-centered explanations [150].\"\r\n  - \"Evaluation can be divided into two categories: 1) qualitative and 2) quantitative evaluation as shown in Table 2\"\r\n  - \"Qualitative evaluation focuses on how easy the explanations are to understand and read, emphasizing aspects, such as 1) comprehensibility and human understanding, and 2) controllability.\"\r\n  - \"quantitative evaluation, looks at things, such as relevant, and efficient explanations, such as 1) faithfulness and 2) plausibility.\"\r\n  - \"Benchmark datasets include examples, such as human-written explanations, steps of reasoning of prediction, and structured knowledge that aligns with factual information [35].\"\r\n  - \"The overview of each dataset is given below.\"",
         "  - \"(Page 6, Table 1) --------------------------------------------------------------------------------------------------------\r\\n| Topic                                    | Key Points                                                                 | Sources                                      |\r\\n|------------------------------------------|----------------------------------------------------------------------------|----------------------------------------------|\"\r\n  - \"(Page 8, Table 1) | Category     | Metric                                   | Description                                                                 | Example                                                                                                                                  |\r\\n|--------------|------------------------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\r\\n| Qualitative  | Comprehensibility and human understanding [86, 126] | Ease of understanding and clarity in delivering the models reasoning to humans | Explaining sentiment analysis by highlighting key phrases, such as \"masterful direction\" and \"innovative storytelling\"                    |\"\r\n  - \"(Page 9, Table 1) | Dataset       | Focus/Use                                                        | Example                                                                                                                                                                                                                                                                                                                                 |\r\\n|---------------|------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\\n| e-SNLI [28]   | Human-written explanations for tasks to find logical relation.   | Sentence and label pairs, such as \"entailment,\" \"contradiction,\" or \"neutral,\" e.g., \"A dog is a type of animal, and running through a park outside.\"                                                                                                                                                                                   |\"",
         "- The paper discusses three main approaches to LLM-based explainability: post-hoc explanations, intrinsic interpretability, and human-centered narratives. These are the primary methods used in the study.\r\n- Post-hoc explanations involve analyzing why a specific input led to a particular output, focusing on causal interpretability.\r\n- Intrinsic interpretability involves designing model architectures to make them inherently explainable.\r\n- Human-centered narratives focus on enhancing explanations through natural language to foster user trust.\r\n- The study also categorizes evaluation techniques into qualitative and quantitative methods, focusing on comprehensibility, controllability, faithfulness, and plausibility.\r\n- The use of benchmark datasets is mentioned as a method to train and evaluate LLMs for explainability."
        ],
        [
         "24",
         "Large Language Models for Market Research: A Data-augmentation Approach",
         "Mengxin Wang, Dennis J. Zhang, Heng Zhang",
         "10.48550/arXiv.2412.19363",
         "https://doi.org/10.48550/arXiv.2412.19363",
         "arXiv.org",
         "4",
         "2024",
         "A data augmentation approach integrating LLM-generated data with real human data can improve the use of LLMs for simulating survey responses in market research.",
         "- Data augmentation approach combining LLM-generated data with real data.\r\n- Two-step process: obtaining an estimator using primary data and constructing an estimator using auxiliary data.\r\n- Transfer learning principles to debias LLM-generated data.\r\n- Use of a small feed-forward neural network to model relationships.\r\n- Training the neural network using the Adam algorithm.",
         "  - \"Our method leverages transfer learning principles to debias the LLM-generated data using a small amount of human data.\"\r\n  - \"our approach departs from nave methods illustrated in Figure 1 . Rather than assuming LLM-generated labels are perfect, we combine them with real data to transfer knowledge from the LLM to the logistic regression model.\"\r\n  - \"we propose the following data augmentation approach that allows us to use the AI-generated data to fit the model.\"\r\n  - \"Estimation with AI-Augmented Data\n\nStep 1. Obtain an estimator  to  * , where P(y = j | x, z) = gj(x, z;  * ), using the primary data.\n\nStep 2. With the auxiliary data, we construct the estimator AAE as AAE  arg max\"\r\n  - \"We use P to denote the estimator obtained using only the primary data. When the primary data size is sufficiently large, P can be close to \"\r\n  - \"We then compute the primary-data-only estimator, auxiliary-data-only estimator (i.e., using only the AI-generated data), nave augmentation estimator and AAE based on (D P , D A ), yielding P , A , Naive and AAE , respectively.\"\r\n  - \"A small feed-forward neural network was trained in Step 1 of AAE to model g j (x, z;  * ) : j  K\"\r\n  - \"We used two hidden layers with ten and five neurons, respectively, and the sigmoid function as the active function for all neurons.\"\r\n  - \"With a learning rate of 10 -4 , we adopt the standard Adam algorithm (Kingma and Ba 2014) to minimize the cross-entropy loss to training the neural network.\"",
         null,
         "- The study uses a data augmentation approach that combines LLM-generated data with real data to improve the accuracy of conjoint analysis.\r\n- The method involves two steps: first, obtaining an estimator using primary data, and second, constructing an estimator using auxiliary data.\r\n- The study uses transfer learning principles to debias LLM-generated data, indicating a focus on leveraging knowledge from one domain to another.\r\n- A small feed-forward neural network is used to model the relationship between human and LLM-generated data, which is a key component of the methodology.\r\n- The Adam algorithm is used to train the neural network, which is a common optimization technique in machine learning."
        ],
        [
         "25",
         "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
         "Qingyang Ren, Zilin Jiang, Jinghan Cao, Sijia Li, Chiqu Li, Yiyang Liu, Shuning Huo, Tiange He",
         "10.48550/arXiv.2405.13025",
         "https://doi.org/10.48550/arXiv.2405.13025",
         "arXiv.org",
         "11",
         "2024",
         "This paper does not address how large language models are applied to simulate human survey responses across question types and populations.",
         "- The study is a survey that reviews existing literature on LLMs in e-commerce.\r\n- The methodology involves examining the principles of LLMs, specifically pre-training, fine-tuning, and prompting.\r\n- Pre-training: Training LLMs from scratch on e-commerce texts.\r\n- Fine-tuning: Further training existing models on specific datasets for e-commerce tasks.\r\n- Prompt-tuning: Freezing models and tuning them with task-specific prompts.",
         "  - \"This survey explores the fairness of large language models (LLMs) in e-commerce, examining their progress, applications, and the challenges they face.\"\r\n  - \"The paper begins by introducing the key principles underlying the use of LLMs in e-commerce, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific needs.\"\r\n  - \"The paper critically addresses the fairness challenges in e-commerce, highlighting how biases in training data and algorithms can lead to unfair outcomes,\"\r\n  - \"Finally, the work outlines future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce.\"\r\n  - \"LLM training comprises three major different approaches: pre-training, fine-tuning and prompting.\"\r\n  - \"Pre-training involves training a large language model (LLM) from scratch on a substantial corpus of e-commerce texts.\"\r\n  - \"Fine-tuning is based on an existing model and then further trained with specific datasets of samples and parameter-efficient tuning approaches\"",
         "  - \"(Page 3, Table 1) | Domain      | Model Type   | Model                  | Base                                      | Param          | Data Source                                      |\r\\n|-------------|--------------|------------------------|-------------------------------------------|----------------|--------------------------------------------------|\r\\n| General     | Pre-training | ALBERT [10]            | BERT                                      | 12M/18M/60M/235M | BooksCorpus, English Wikipedia                   |\"",
         "- The paper is a survey, which means it reviews existing literature and methodologies rather than conducting new experiments.\r\n- The methodology involves examining the principles of LLMs in e-commerce, specifically focusing on pre-training, fine-tuning, and prompting.\r\n- Pre-training involves training LLMs from scratch on e-commerce texts, which is a foundational approach.\r\n- Fine-tuning involves further training existing models on specific datasets to adapt them to e-commerce tasks.\r\n- Prompt-tuning is used to adapt models to specific tasks by freezing the model and tuning it with task-specific prompts.\r\n- The table at the end of the paper likely provides a summary of existing LLMs in e-commerce, which is part of the survey methodology."
        ],
        [
         "26",
         "S3: Social-network Simulation System with Large Language Model-Empowered Agents",
         "Chen Gao, Xiaochong Lan, Zhi-jie Lu, Jinzhu Mao, J. Piao, Huandong Wang, Depeng Jin, Yong Li",
         "10.48550/arXiv.2307.14984",
         "https://doi.org/10.48550/arXiv.2307.14984",
         "Social Science Research Network",
         "176",
         "2023",
         "The paper presents a social network simulation system that uses large language models to emulate human behavior, including emotion, attitude, and interaction.",
         "- Establishing an environment using real-world social network data.\r\n- Inferring user demographics through prompt engineering and tuning.\r\n- Simulating individual-level behaviors (attitudes, emotions, content generation) using LLMs.\r\n- Simulating collective behavior through accumulation of individual actions.\r\n- Evaluating efficacy using accuracy metrics at individual and population levels.\r\n- Designing a message propagation framework.\r\n- Characterizing users based on attributes and social relationships.\r\n- Using Markov chain approaches for emotion simulation.\r\n- Adapting LLMs for agent-based simulation.",
         "  - \"In this study, we present the Social-network Simulation System (S 3 ), which employs LLM-empowered agents to simulate users within a social network effectively.\"\r\n  - \"Initially, we establish an environment using real-world social network data.\"\r\n  - \"To ensure the authenticity of this environment, we propose a user-demographic inference module that combines prompt engineering with prompt tuning, to infer user demographics such as age, gender, and occupation.\"\r\n  - \"Within the constructed environment, users have the ability to observe content from individuals they follow, thereby influencing their own attitudes, emotions, and subsequent behaviors.\"\r\n  - \"at the individual level, we employ prompt engineering and prompt tuning methodologies to simulate attitudes, emotions, and behaviors.\"\r\n  - \"At the population level, the accumulation of individual behaviors, including content generation and forwarding, alongside the evolving internal states of attitudes and emotions, leads to the emergence of collective behavior.\"\r\n  - \"To assess the efficacy of the proposed S 3 system, we have chosen two exemplary scenarios, namely, gender discrimination and nuclear energy.\"\r\n  - \"To evaluate the precision of our simulations, we employ metrics that measure accuracy at both the individual and population levels.\"\r\n  - \"In order to simulate the process of information propagation on the online social network, we have designed a message propagation simulation framework illustrated in Figure 1 and is explained in detail below.\"\r\n  - \"Environment Construction: The construction of the environment involves the formation of a social network on a public platform, comprising users and connections among them.\"\r\n  - \"User Characterization In addition to the social relationships present within the network, each user possesses their own attribute descriptions.\"\r\n  - \"Update and Evolution Mechanism: During a social gathering, various official accounts and individual users contribute posts concerning the event, encompassing news reports and personal viewpoints.\"\r\n  - \"In our emotion simulation model, we adopt a Markov chain approach to capture the dynamic process of emotional changes triggered by a user receiving a message.\"\r\n  - \"In our social network simulation model, we incorporate an advanced approach utilizing Large Language Models (LLMs) to reproduce the dynamic process of content creation, shaped by users' emotions and attitudes towards specific events.\"\r\n  - \"The system employs various techniques for utilizing or adapting large language models to the agentbased simulation.\"",
         "  - \"(Page 5, Table 1) Below is a rendering of the page up to the first error.\"\r\n  - \"(Page 5, Table 2) | Scenario                | Prediction Task    | Accuracy |  AUC  | F1-Score |\r\\n|-------------------------|--------------------|----------|-------|----------|\r\\n| Gender Discrimination   | Emotion Level      | 71.8%    |      |         |\"\r\n  - \"(Page 6, Table 1) | Scenario                | Perplexity | Cosine Similarity |\r\\n|-------------------------|------------|-------------------|\r\\n| Gender Discrimination   | 19.289     | 0.723             |\"\r\n  - \"(Page 11, Table 1) | Demographic | Performance         |            |              |\r\\n|-------------|---------------------|------------|--------------|\r\\n|             | Acc                 | F1         | AUC          |\"",
         "- The study uses a Social-network Simulation System (S 3 ) that employs large language models (LLMs) to simulate user behavior in social networks.\r\n- The methodology involves establishing an environment using real-world social network data and inferring user demographics through prompt engineering and tuning.\r\n- The system simulates individual-level behaviors such as attitudes, emotions, and content generation using LLMs.\r\n- At the population level, it simulates collective behavior through the accumulation of individual actions.\r\n- The study evaluates the system's efficacy using metrics for accuracy at both individual and population levels.\r\n- The methodology includes designing a message propagation framework and characterizing users based on their attributes and social relationships.\r\n- Markov chain approaches are used for emotion simulation, and LLMs are used for content creation simulation.\r\n- The study uses various techniques to adapt LLMs for agent-based simulation."
        ],
        [
         "27",
         "Evaluating the Moral Beliefs Encoded in LLMs",
         "Nino Scherrer, Claudia Shi, Amir Feder, D. Blei",
         "10.48550/arXiv.2307.14324",
         "https://doi.org/10.48550/arXiv.2307.14324",
         "Neural Information Processing Systems",
         "150",
         "2023",
         "The paper presents a method to evaluate the moral beliefs encoded in large language models across ambiguous and unambiguous moral scenarios.",
         "- Designed a large-scale survey with high-ambiguity and low-ambiguity moral scenarios.\r\n- Administered the survey to 28 open- and closed-source LLMs.\r\n- Used statistical measures like action likelihood and marginal action likelihood to quantify choices and uncertainty.\r\n- Employed sampling to approximate action likelihood.\r\n- Utilized multiple question styles (A/B, Repeat, Compare) to assess sensitivity.\r\n- Conducted an ablation study on different decoding techniques.",
         "  - \"This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs).\"\r\n  - \"We introduce statistical measures and evaluation metrics that quantify the probability of an LLM\"making a choice\", the associated uncertainty, and the consistency of that choice.\"\r\n  - \"We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g.,\" Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g.,\" Should I stop for a pedestrian on the road?\").\"\r\n  - \"We administer the survey to 28 open- and closed-source LLMs.\"\r\n  - \"To address the first challenge, we define action likelihood, which measures the \"choices\" made by the model.\"\r\n  - \"For the second challenge, we define the marginal action likelihood, which measures the choices made by the model when a question is presented with randomly sampled question forms.\"\r\n  - \"We approximate the action likelihood through sampling.\"\r\n  - \"we employ three hand-curated question styles: A/B, Repeat, and Compare\"\r\n  - \"we record the API query timestamps and report them along model download timestamps in Appendix C.2.\"\r\n  - \"We estimate these quantities through Monte Carlo approximation as described in Eq. 7.\"\r\n  - \"We complement our main evaluation setup with an ablation study using different decoding techniques in Appendix D.5.\"",
         null,
         "- The study involves designing and administering a survey to large language models (LLMs) to evaluate their moral beliefs.\r\n- The survey includes both high-ambiguity and low-ambiguity moral scenarios to assess the models' decision-making.\r\n- Statistical measures such as action likelihood and marginal action likelihood are used to quantify the models' choices and uncertainty.\r\n- The study uses sampling to approximate action likelihood due to computational challenges.\r\n- Multiple question styles (A/B, Repeat, Compare) are employed to assess sensitivity to question forms.\r\n- The study includes an ablation study to evaluate different decoding techniques."
        ],
        [
         "28",
         "A Survey of Large Language Model Agents for Question Answering",
         "Murong Yue",
         "10.48550/arXiv.2503.19213",
         "https://doi.org/10.48550/arXiv.2503.19213",
         "arXiv.org",
         "7",
         "2025",
         "This paper surveys the development of large language model-based agents for question answering, but does not mention simulating human survey responses.",
         "The methodology involves a systematic review of the design of LLM agents for QA tasks, focusing on key stages such as planning, question understanding, information retrieval, and answer generation, based on research from top-tier NLP conferences and journals.",
         "  - \"We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation.\"\r\n  - \"This paper surveys the development of large language model (LLM)-based agents for question answering (QA).\"\r\n  - \"We break down the answering process into multiple sub-tasks, demonstrating how cutting-edge methods are leveraged in improving the LLM agent QA system.\"\r\n  - \"The research covered in this survey is primarily drawn from top-tier conferences and journals in the NLP field.\"",
         null,
         "- The paper is a survey, which means it reviews existing literature rather than conducting new experiments or data collection.\r\n- The methodology involves systematically reviewing the design of LLM agents across key stages of QA tasks.\r\n- The review is based on research from top-tier conferences and journals in the NLP field, indicating a comprehensive literature review.\r\n- The paper breaks down the QA process into sub-tasks, which suggests an analytical approach to understanding how LLM agents are designed and improved."
        ],
        [
         "29",
         "Generative Agent Simulations of 1,000 People",
         "Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, C. Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein",
         "10.48550/arXiv.2411.10109",
         "https://doi.org/10.48550/arXiv.2411.10109",
         "arXiv.org",
         "125",
         "2024",
         "Large language models are applied to simulate the attitudes and behaviors of 1,052 real individuals, replicating their survey responses with 85% accuracy.",
         "- Recruited a stratified sample of 1,052 individuals from the U.S. based on demographic factors.\r\n- Conducted two-hour audio interviews with an AI interviewer.\r\n- Created generative agents for each participant using interview data.\r\n- Evaluated agent accuracy by comparing responses to participants' original responses, normalized for self-consistency.\r\n- Conducted additional tests on interview content volume and style.",
         "  - \"We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent.\"\r\n  - \"To create simulations that better reflect the myriad, often idiosyncratic, factors that influence individuals' attitudes, beliefs, and behaviors, we turn to in-depth interviews-a method that previous work on predicting human life outcomes has employed to capture insights beyond what can be obtained through traditional surveys and demographic instruments\"\r\n  - \"The process of collecting participant data and creating generative agents begins by recruiting a stratified sample of 1,052 individuals from the U.S., selected based on age, census division, education, ethnicity, gender, income, neighborhood, political ideology, and sexual identity.\"\r\n  - \"Once recruited, participants complete a two-hour audio interview with our AI interviewer, followed by surveys and experiments.\"\r\n  - \"We create generative agents for each participant using their interview data.\"\r\n  - \"To evaluate these agents, both the generative agents and participants complete the same surveys and experiments.\"\r\n  - \"We conducted additional tests by ablating portions of the generative agents' interviews to examine the impact of interview content volume and style.\"\r\n  - \"We created over 1,000 generative agents, each modeling a real individual in the U.S., collectively forming a representative sample of the U.S. population.\"",
         null,
         "- The study uses a novel agent architecture that simulates human behavior based on qualitative interviews.\r\n- Participants were recruited using stratified sampling to ensure a representative sample of the U.S. population.\r\n- Data collection involved two-hour audio interviews with an AI interviewer, followed by surveys and experiments.\r\n- Generative agents were created for each participant using their interview data.\r\n- The accuracy of these agents was evaluated by comparing their responses to participants' original responses, with normalization for self-consistency.\r\n- Additional tests were conducted to assess the impact of interview content volume and style on agent performance."
        ],
        [
         "30",
         "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",
         "Enzo Sinacola, Arnault Pachot, Thierry Petit",
         "10.48550/arXiv.2503.16498",
         "https://doi.org/10.48550/arXiv.2503.16498",
         "arXiv.org",
         "1",
         "2025",
         "Large language models can be used to simulate human survey responses, but exhibit biases for certain demographic groups.",
         "- The study is structured into four phases: \r\n  1. Prompt and temperature optimization for LLMs.\r\n  2. Comparative testing of multiple LLMs and Random Forests on binary questions.\r\n  3. Performance evaluation across ethnic and religious groups.\r\n  4. Comparison of censored and uncensored LLM versions.\r\n- Uses World Values Survey dataset for diverse demographic information and large sample size.\r\n- Compares performance of various LLMs (GPT-4o, GPT-3.5, Claude 3.5-Sonnet, Llama3-8B, Llama3.1-8B, Mistral-7B) and Random Forests.",
         "  - \"The methodology of this study is structured into four key phases, each designed to address specific research objectives outlined in the introduction.\"\r\n  - \"Phase 1: Prompt and Temperature Optimization for LLM This phase serves as a foundational step to identify the optimal prompt and temperature settings.\"\r\n  - \"Phase 2: Comparative Testing of Multiple Models In this phase, we broaden our evaluation by comparing the performance of several LLMs and a traditional machine learning algorithm, Random Forests, on binary questions.\"\r\n  - \"Phase 3: Performance Across Ethnic and Religious Groups The third phase investigates how LLMs perform across different population segments on binary questions, specifically ethnic and religious groups.\"\r\n  - \"Phase 4: Censored vs. Uncensored LLM Comparison Building on the findings from the third phase, this phase compares the performance of censored and uncensored versions of the LLMs across the same demographic groups.\"\r\n  - \"The models tested are Llama3-8B and Llama3.1-8B, both with their Dolphin version.\"\r\n  - \"The models tested include GPT-4o and GPT-3.5 [13], Claude 3.5-Sonnet [1], as well as open-source models Llama3-8B, Llama3.1-8B [11], and Mistral-7B [12], along with their Dolphinenhanced versions2 [7] [8] [6], and Random Forests models.\"\r\n  - \"The Random Forests models were trained using varying portions of the dataset, excluding the 384 individuals, and tested on these 384 to ensure a fair comparison with the LLMs.\"\r\n  - \"We distinct between demographic characteristics and opinions in the data extracted from the World Values Survey (WVS) dataset: demographic characteristics can be collected directly or through questions, allowing for simulation in virtual populations, whereas opinions must be gathered through surveys, reflecting individuals' subjective responses.\"\r\n  - \"The dataset was selected for its rich diversity of demographic information, including age, religion, education, and opinions on societal issues such as politics, economics, and culture.\"\r\n  - \"The key factors for choosing this dataset are as follows: Diverse population representation, covering various regions, cultures, and social backgrounds; Comprehensive opinion data, facilitating comparisons across numerous demographic characteristics; Large sample size, allowing for robust statistical analysis across multiple subgroups; and High scientific recognition and reliability, as this dataset is widely cited in social science research, enhancing the credibility of our analysis.\"",
         null,
         "- The study is structured into four phases, each addressing specific research objectives.\r\n- Phase 1 involves optimizing prompt and temperature settings for LLMs.\r\n- Phase 2 compares the performance of multiple LLMs and Random Forests on binary questions.\r\n- Phase 3 evaluates LLM performance across ethnic and religious groups.\r\n- Phase 4 compares censored and uncensored LLM versions.\r\n- The study uses a variety of LLMs and Random Forests models for comparison.\r\n- The World Values Survey dataset is used for its diverse demographic information and large sample size.\r\n- The study distinguishes between demographic characteristics and opinions in the dataset."
        ],
        [
         "31",
         "Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation",
         "Se-eun Yoon, Zhankui He, J. Echterhoff, Julian McAuley",
         "10.48550/arXiv.2403.09738",
         "https://doi.org/10.48550/arXiv.2403.09738",
         "North American Chapter of the Association for Computational Linguistics",
         "24",
         "2024",
         "Large language models show promise in simulating human-like behavior for conversational recommendation, but a new protocol is needed to evaluate their ability to accurately emulate diverse human responses.",
         "- Introduced a new evaluation protocol for LLM-based user simulators in conversational recommendation.\r\n- Protocol consists of five tasks: ItemsTalk, BinPref, OpenPref, RecRequest, and Feedback.\r\n- Used real-world datasets (ReDial, Reddit, MovieLens, IMDB) for comparison.\r\n- Baseline simulators created using OpenAI models (gpt-3.5-turbo, gpt-4, text-davinci-003) with prompt-based inputs.\r\n- Evaluation methods include item distribution comparison, Pearson correlation coefficient calculation, aspect-based sentiment analysis, and request diversity assessment using type-token ratios and embeddings.",
         "  - \"We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation.\"\r\n  - \"This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback.\"\r\n  - \"We propose a new evaluation protocol for measuring the extent to which LLM-based simulators can represent users in conversational recommendation.\"\r\n  - \"We tackle these challenges by decomposing evaluation into five independent tasks, each measuring a key property that a user simulator should exhibit.\"\r\n  - \"We use real-world datasets to compare simulator outputs to human output.\"\r\n  - \"For each task, we create a population of simulators, each given a taskspecific prompt.\"\r\n  - \"we compare the distribution of mentioned items between the simulator and the dataset (items in prompt are removed).\"\r\n  - \"We compute the Pearson correlation coefficient between the average rating and positive rate.\"\r\n  - \"we conduct aspectbased sentiment analysis with PyABSA\"\r\n  - \"We formulate two sub-tasks. Accept/reject: simulators should reject negative recommendations. Comparison: simulators should prefer positive recommendations over negative ones.\"",
         null,
         "- The study introduces a new protocol for evaluating large language models as user simulators in conversational recommendation systems.\r\n- The protocol consists of five tasks: ItemsTalk, BinPref, OpenPref, RecRequest, and Feedback. Each task evaluates a different aspect of user behavior.\r\n- The study uses real-world datasets (ReDial, Reddit, MovieLens, IMDB) to compare simulator outputs with human data.\r\n- Baseline simulators are created using OpenAI models (gpt-3.5-turbo, gpt-4, text-davinci-003) with prompt-based inputs.\r\n- Evaluation methods include comparing item distributions, calculating Pearson correlation coefficients for preference alignment, conducting aspect-based sentiment analysis, and assessing request diversity using type-token ratios and embeddings."
        ],
        [
         "32",
         "Multilingual Large Language Models: A Systematic Survey",
         "Shaolin Zhu, Supryadi, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, Ant'onio Branco, Deyi Xiong",
         "10.48550/arXiv.2411.11072",
         "https://doi.org/10.48550/arXiv.2411.11072",
         "arXiv.org",
         "11",
         "2024",
         "This paper is a comprehensive survey of multilingual large language models, but does not discuss their application to simulating human survey responses.",
         "- Discussing key architectural components and pre-training objectives of MLLMs.\r\n- Exploring data construction strategies and the importance of data quality and diversity.\r\n- Evaluating MLLMs' cross-lingual knowledge, reasoning, alignment with human values, safety, interpretability, and specialized applications.\r\n- Addressing challenges in handling linguistic diversity, including non-English and low-resource languages.\r\n- Analyzing core responsible AI issues such as fairness, bias, and toxicity.\r\n- Reviewing real-world applications across diverse domains.",
         "  - \"This paper provides a comprehensive survey of the latest research on multilingual large language models (MLLMs).\"\r\n  - \"We first discuss the architecture and pre-training objectives of MLLMs, highlighting the key components and methodologies that contribute to their multilingual capabilities.\"\r\n  - \"We then discuss the construction of multilingual pre-training and alignment datasets, underscoring the importance of data quality and diversity in enhancing MLLM performance.\"\r\n  - \"An important focus of this survey is on the evaluation of MLLMs. We present a detailed taxonomy and roadmap covering the assessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human values, safety, interpretability and specialized applications.\"\r\n  - \"To enhance MLLMs from black to white boxes, we also address the interpretability of multilingual capabilities, cross-lingual transfer and language bias within these models.\"\r\n  - \"Finally, we provide a comprehensive review of real-world applications of MLLMs across diverse domains, including biology, medicine, computer science, mathematics and law.\"\r\n  - \"The present survey aims to fill this gap by providing a comprehensive survey of the research on MLLMs.\"\r\n  - \"We will analyze the specific challenges MLLMs face in handling linguistic diversity, thus including non-English and low-resource languages, and explore data construction strategies, model training and fine-tuning approaches, as well as core responsible AI issues.\"\r\n  - \"We will discuss data construction strategies, model training and fine-tuning approaches, and core Responsible AI issues such as fairness, bias and toxicity in the context of MLLMs.\"\r\n  - \"We will also analyze the specific challenges MLLMs encounter in handling linguistic diversity, along with relevant evaluation methodologies.\"\r\n  - \"our aim is to address and help to clarify the following fundamental questions:  What are the capabilities of MLLMs?\n\n What is the language boundary of MLLMs?\n\n What factors must be taken into account when constructing and tuning MLLMs?\n\n How to evaluate the multilingual transfer capabilities of MLLMs?\"\r\n  - \"the survey showcases diverse real-world applications of MLLMs.\"\r\n  - \"Our overarching objective is to address these six fundamental domains, as illustrated in Figure 1\"\r\n  - \"The next Section 3 discusses the key architectural components and pre-training objectives that contribute to the development and training of MLLMs.\"\r\n  - \"Section 4 discusses the training data for MLLMs.\"\r\n  - \"Section 6 presents strategies for multilingual tuning, with the goal of adapting the general capabilities of multilingual LLMs to specific objectives.\"\r\n  - \"Section 8 addresses the core question of how the model represents multilingual capacities.\"",
         "  - \"(Page 14, Table 1) Below is a rendering of the page up to the first error.```markdown\r\\n| name                                                                 | open-source | from/type          | size            | language | low% | non-English% | date    | used by |\r\\n|----------------------------------------------------------------------|-------------|--------------------|-----------------|----------|------|--------------|---------|---------|\"\r\n  - \"(Page 16, Table 1) Below is a rendering of the page up to the first error.```markdown\r\\n| name                                      | open-source | entries         | language | low%  | non-English% | date    | used by     |\r\\n|-------------------------------------------|-------------|-----------------|----------|-------|--------------|---------|-------------|\"",
         "- The paper is a comprehensive survey of multilingual large language models (MLLMs), focusing on their architecture, pre-training objectives, data construction, evaluation methodologies, and real-world applications.\r\n- The methodology involves discussing key architectural components and pre-training objectives, which are crucial for understanding MLLMs' multilingual capabilities.\r\n- The paper explores data construction strategies, including the importance of data quality and diversity, which are essential for enhancing MLLM performance.\r\n- Evaluation methodologies are discussed in detail, covering cross-lingual knowledge, reasoning, alignment with human values, safety, interpretability, and specialized applications.\r\n- The paper addresses challenges in handling linguistic diversity, including non-English and low-resource languages, and explores strategies for model training and fine-tuning.\r\n- The methodology includes analyzing core responsible AI issues such as fairness, bias, and toxicity in the context of MLLMs.\r\n- The paper provides a comprehensive review of real-world applications across diverse domains, highlighting the capabilities and challenges of MLLMs."
        ],
        [
         "33",
         "Language Models Trained on Media Diets Can Predict Public Opinion",
         "Eric Chu, Jacob Andreas, S. Ansolabehere, Dwaipayan Roy",
         "10.48550/arXiv.2303.16779",
         "https://doi.org/10.48550/arXiv.2303.16779",
         "arXiv.org",
         "30",
         "2023",
         "Language models trained on media diets can predict public opinion across question types and populations.",
         "- Building a media diet model involves three steps: creating or using a base language model (BERT), adapting it to specific media diets, and scoring answers to survey questions.\r\n- Regression models are used to predict survey response proportions.\r\n- A synonym-grouping method is employed to improve prediction accuracy.\r\n- Paraphrasing methods are used to test robustness to different question phrasings.\r\n- Media diet groups are created based on demographic factors and primary news sources.\r\n- Data from national polls on COVID-19 and consumer confidence are used.",
         "  - \"The main idea behind our approach is to build a computational model that takes as input a description of an subpopulation's media diet, and a survey question, and produces as output a prediction of how the subpopulation will respond to the survey question.\"\r\n  - \"Building a media diet model involves three steps. In step one, we create or use a base language model that can predict missing words in text. We use pretrained models in our work, with BERT as our main model.\"\r\n  - \"In step two, we adapt the language model by fine-tuning it on a specific media diet dataset, which contains media content from one or a mixture of news sources from a given time period.\"\r\n  - \"In step three, we query the media diet model and score answers to survey questions.\"\r\n  - \"To do public opinion prediction, we fit regression models that use (i) to predict (ii). The survey data comes from national polls conducted about COVID-19 and consumer confidence.\"\r\n  - \"we use a nearest neighbor approach to trace predictions for a given survey question back to the original media diet datasets.\"\r\n  - \"our probing method produces a score s for a language model LM, a fill-in-the-blank prompt, and target word w.\"\r\n  - \"We test two methods for automatically generating paraphrases of our manually constructed prompts.\"\r\n  - \"We use the English-Dutch and Dutch-English translation models provided by FairSeq.\"\r\n  - \"we create media diet groups by matching on demographics as following: (i) bucket Pew respondents according to four demographic factors (age, gender, region, education), (ii) compute which buckets have at least one \"primary\" news source, defined as any outlet for which at least 50% of respondents use that outlet, (iii) combine buckets with the same set of news sources, (iv) compute consumer confidence responses per bucket.\"",
         "  - \"(Page 6, Table 1) | Variable                                      | (Model 1) Media diet       | (Model 2) Media diet + attention + demographics |\r\\n|-----------------------------------------------|----------------------------|-----------------------------------------------|\r\\n| *intercept*                                   | 0.194*** (0.134, 0.254) | 0.139 (-0.137, 0.416)                        |\"\r\n  - \"(Page 8, Table 1) | Nearest neighbors in CNN training set                                                                 | Nearest neighbors in FOX training set                                                                                                                                                                                                 |\r\\n|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\\n| () The novel coronavirus outbreak is raging in China, but fewer than 1,000 people have been infected outside the country. | () They have told us, most people are not at serious risk from coronavirus, and while anyone can get it and spread it so far, only a small number end up sick enough to be hospitalized.                                            |\"",
         "- The study uses a computational model to predict public opinion by emulating survey respondents based on their media diet.\r\n- The model is built in three steps: creating or using a base language model (BERT), adapting it to specific media diets, and scoring answers to survey questions.\r\n- Regression models are used to predict survey response proportions.\r\n- A synonym-grouping method is employed to improve the accuracy of predictions by summing probabilities over synonyms.\r\n- Paraphrasing methods are used to test the robustness of the model to different question phrasings.\r\n- Media diet groups are created based on demographic factors and primary news sources.\r\n- The study uses data from national polls on COVID-19 and consumer confidence."
        ],
        [
         "34",
         "Large Language Models as Psychological Simulators: A Methodological Guide",
         "Zhicheng Lin",
         "10.48550/arXiv.2506.16702",
         "https://doi.org/10.48550/arXiv.2506.16702",
         "arXiv.org",
         "2",
         "2025",
         "The paper provides a framework for using large language models to simulate human roles, personas, and cognitive processes in psychological research.",
         "- Simulating roles and personas to explore diverse contexts\r\n- Serving as computational models to investigate cognitive processes\r\n- Developing psychologically grounded personas\r\n- Validating against human data\r\n- Probing internal representations\r\n- Causal interventions\r\n- Relating model behavior to human cognition\r\n- Addressing prompt sensitivity and temporal limitations\r\n- Ethical considerations: transparency, representation auditing, appropriate use boundaries, community engagement",
         "  - \"This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes.\"\r\n  - \"For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments.\"\r\n  - \"For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition.\"\r\n  - \"We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review.\"\r\n  - \"Throughout, we emphasize the need for transparency about model capabilities and constraints.\"\r\n  - \"The methodological framework presented in Table 1 provides a foundation for rigorous LLM-based research, but its value depends on consistent application and empirical validation.\"\r\n  - \"The article proceeds as follows. We first examine role and persona simulation, providing guidelines for prompt design, response validation, and appropriate use cases-from simulating rare populations to prototyping survey instruments.\"\r\n  - \"We then explore cognitive modeling applications, reviewing methodological approaches for probing internal representations, synthesizing advances in causal interventions, and examining strategies for relating findings to human cognition.\"\r\n  - \"Finally, we address ethical considerations that extend beyond traditional human subjects protections.\"\r\n  - \"The use of LLMs in psychological research raises ethical questions that extend beyond traditional human subjects protections.\"\r\n  - \"researchers using LLMs for psychological simulation should adopt specific ethical practices:\n\nTransparency requirements. Document key aspects of LLM use, including model versions, prompts, parameters, and validation procedures\"\r\n  - \"Representation auditing. Before simulating any population, critically examine whether the model can credibly represent that group.\"\r\n  - \"Appropriate use boundaries. Clearly establish when LLM simulation is appropriate.\"\r\n  - \"Community engagement. Involve community members in research design and validation, ensuring their perspectives shape methodological choices and help identify potential harms.\"",
         "  - \"(Page 6, Table 1) | Domain                                           | Guideline                                           | Rationale                                                                                                                                                                                                 |\r\\n|--------------------------------------------------|-----------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\\n| Model selection, customization (fine-tuning), and settings | Track performance across different models, model versions, and sizes | Different families of models (e.g., GPT, LLaMA), model versions (e.g., raw/base vs. instruct/RLHF-tuned) and sizes (e.g., GPT-3, GPT-3.5, GPT-4) can produce varying results, revealing the impact of architecture, design, and scale |\"\r\n  - \"(Page 7, Table 1) |                                      |                                                                                           |\r\\n|--------------------------------------|-------------------------------------------------------------------------------------------|\r\\n| Consider open vs. closed-source models | Open-source models offer transparency in training data and methodologies, making it easier to assess biases, track model changes, and evaluate prompt contamination |\"\r\n  - \"(Page 8, Table 1) | Ethics                                                                 |                                                                                   |\r\\n|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------|\r\\n| Make fair comparisons between humans and LLMs                         | Use similar tasks and prompts (instructions) when comparing LLM and human data    |\"",
         "- The paper discusses two primary applications of LLMs: simulating roles and personas, and serving as cognitive models. This indicates a dual methodology approach.\r\n- For role simulation, the paper provides guidelines for developing psychologically grounded personas, validating against human data, and using LLMs for inaccessible populations and prototyping research instruments.\r\n- For cognitive modeling, the paper synthesizes approaches for probing internal representations, causal interventions, and relating model behavior to human cognition.\r\n- The paper emphasizes the need for transparency about model capabilities and constraints, addressing challenges like prompt sensitivity and temporal limitations.\r\n- Ethical considerations are a significant part of the methodology, including transparency requirements, representation auditing, appropriate use boundaries, and community engagement.\r\n- The tables at the end of the paper likely provide detailed guidelines and frameworks for these methodologies, although their content is not directly quoted here."
        ],
        [
         "35",
         "A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions",
         "Rajesh Ranjan, Shailja Gupta, Surya Narayan Singh",
         "10.48550/arXiv.2409.16430",
         "https://doi.org/10.48550/arXiv.2409.16430",
         "arXiv.org",
         "16",
         "2024",
         "This paper does not address the application of large language models to simulate human survey responses across question types and populations.",
         "The methodology involves a comprehensive survey and systematic review of existing literature and practices related to bias detection and mitigation in LLMs, synthesizing current research findings and critically assessing existing bias mitigation techniques to propose future research directions.",
         "  - \"This paper presents a comprehensive survey of biases in LLMs, aiming to provide an extensive review of the types, sources, impacts, and mitigation strategies related to these biases.\"\r\n  - \"Our survey synthesizes current research findings and discusses the implications of biases in real-world applications.\"\r\n  - \"Additionally, we critically assess existing bias mitigation techniques and propose future research directions to enhance fairness and equity in LLMs.\"\r\n  - \"This survey serves as a foundational resource for researchers, practitioners, and policymakers concerned with addressing and understanding biases in LLMs.\"\r\n  - \"Our objectives include: The scope of this survey encompasses an examination of both theoretical and practical aspects of bias in LLMs to provide a holistic view of the challenges and solutions related to this critical issue.\"\r\n  - \"This survey aims to provide a comprehensive overview of biases in LLMs by systematically reviewing the existing literature and current practices related to bias detection and mitigation.\"",
         "  - \"(Page 6, Table 1) | Category of Bias          | Type of Bias       | Description                                                                 |\r\\n|---------------------------|--------------------|-----------------------------------------------------------------------------|\r\\n| Application-Specific Bias | Task Bias          | It occurs when the model performs differently across various tasks or domains. |\"\r\n  - \"(Page 7, Table 1) | Source of Bias        | Description                                                                 | Examples                                                                 |\r\\n|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------|\r\\n| Training Data         | Biases originating from the data used to train the model because data may be skewed or unrepresentative. | Data from certain social media reflects societal prejudices.             |\"",
         "- The paper is described as a \"comprehensive survey,\" which indicates that the methodology involves reviewing and synthesizing existing research findings.\r\n- The authors aim to provide an \"extensive review\" of biases in LLMs, suggesting a thorough examination of literature on the topic.\r\n- The paper critically assesses existing bias mitigation techniques, which implies a review of current methods and strategies.\r\n- The authors propose future research directions, indicating an analysis of current gaps and areas for future study.\r\n- The methodology involves a systematic review of existing literature and practices related to bias detection and mitigation."
        ],
        [
         "36",
         "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
         "Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, Joo Sedoc, L. Ungar, J. Eichstaedt",
         "10.48550/arXiv.2405.06058",
         "https://doi.org/10.48550/arXiv.2405.06058",
         "arXiv.org",
         "9",
         "2024",
         "Large language models exhibit social desirability biases in their survey responses, limiting their use as proxies for human participants.",
         "- Used a standardized 100-item Big Five personality questionnaire.\r\n- Administered the questionnaire in batches with varying numbers of questions.\r\n- Started a new context window for each batch to prevent access to previous items.\r\n- Instructed LLMs to respond on a 5-point Likert scale.\r\n- Analyzed models from OpenAI, Anthropic, Google, and Meta.\r\n- Tested reverse-coded and positively-coded survey variations.\r\n- Used paraphrased survey variants to check for memorization effects.\r\n- Employed randomization strategies for question sets.\r\n- Varied the temperature parameter to assess stochastic output effects.",
         "  - \"We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs.\"\r\n  - \"By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated.\"\r\n  - \"To evaluate response biases in LLMs, we conducted a series of experiments using a standardized 100-item Big Five personality questionnaire.\"\r\n  - \"We administered the questionnaire in batches, systematically varying the number of questions per batch (denoted as Q n\"\r\n  - \"To ensure the LLM had no access to previous items, we started a new context window (session) for each batch.\"\r\n  - \"We provided standardized instructions asking the LLMs to respond on a 5-point Likert scale.\"\r\n  - \"We analyzed models from OpenAI, Anthropic, Google, and Meta to ensure broad generalizability.\"\r\n  - \"We tested two variations of the Big Five survey: one fully reverse-coded (with negations in the questions) and another with only positively-coded items.\"\r\n  - \"LLMs may be relying on the memorized versions of the Big Five items, we also administered paraphrased variants of the survey and found similar levels of bias.\"\r\n  - \"we used three distinct randomization strategies (as described in SI. Section D.) to assemble the question sets.\"\r\n  - \"We used a suite of widely available models: OpenAI's text-davinci-002, GPT-3.5, and GPT-4 (the 0613 fixed versions), Anthropic's Claude 3 Opus (claude-3-opus-20240229), Claude 3 Haiku (claude-3-haiku-20240229), Google's PaLM 2 (chat-bison-001), and Meta's Llama 3 70B Instruct, Llama 2 70B Chat.\"\r\n  - \"We assessed the impact of the LLMs' stochastic output on response biases by varying the temperature parameter at 0.0, 0.4, 0.8, and 1.2.\"",
         null,
         "- The study used a standardized 100-item Big Five personality questionnaire to evaluate response biases in LLMs.\r\n- The questionnaire was administered in batches with varying numbers of questions to assess how LLMs infer evaluation contexts.\r\n- New context windows were used for each batch to prevent access to previous items.\r\n- LLMs were instructed to respond on a 5-point Likert scale.\r\n- The study analyzed models from multiple sources (OpenAI, Anthropic, Google, Meta) for broad generalizability.\r\n- Variations of the survey included reverse-coded and positively-coded items to assess bias.\r\n- Paraphrased variants of the survey were used to check for memorization effects.\r\n- Randomization strategies were employed to assemble question sets.\r\n- The study used a range of LLM models and varied the temperature parameter to assess stochastic output effects."
        ],
        [
         "37",
         "A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions",
         "Hongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei Yu, Lilin Wang, Wei Wang, John Torous, Ling Chen",
         "10.48550/arXiv.2502.11095",
         "https://doi.org/10.48550/arXiv.2502.11095",
         "Annual Meeting of the Association for Computational Linguistics",
         "6",
         "2025",
         "This paper does not address how large language models are applied to simulate human survey responses across question types and populations.",
         "The study is a survey that provides a comprehensive overview of the current landscape of LLM applications in psychotherapy, presenting a novel conceptual taxonomy and addressing research gaps and future directions.",
         "  - \"This survey provides a comprehensive overview of the current landscape of LLM applications in psychotherapy, highlighting the roles of LLMs in symptom detection, severity estimation, cognitive assessment, and therapeutic interventions.\"\r\n  - \"We present a novel conceptual taxonomy to organize the psychotherapy process into three core components: assessment, diagnosis, and treatment, and examine the challenges and advancements in each area.\"\r\n  - \"Finally, we discuss future directions to integrate LLMs into a holistic, end-to-end psychotherapy framework, addressing the evolving nature of mental health conditions and fostering more inclusive, personalized care.\"\r\n  - \"The survey also addresses key research gaps, including linguistic biases, limited disorder coverage, and underrepresented therapeutic models.\"",
         null,
         "- The abstract describes the study as a \"survey,\" which typically involves reviewing and summarizing existing literature rather than conducting new experiments or data collection.\r\n- The mention of providing a \"comprehensive overview\" and presenting a \"novel conceptual taxonomy\" suggests that the methodology involves categorizing and analyzing existing research on LLMs in psychotherapy.\r\n- The focus on highlighting roles, examining challenges, and discussing future directions indicates that the study is more about reviewing and synthesizing existing knowledge rather than conducting empirical research.\r\n- The abstract does not mention any specific empirical methods such as data collection, experiments, or statistical analysis, which further supports the conclusion that the study is a literature review or survey."
        ],
        [
         "38",
         "Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction",
         "Sarah Ball, Simeon Allmendinger, Frauke Kreuter, Niklas Khl",
         "10.48550/arXiv.2502.16280",
         "https://doi.org/10.48550/arXiv.2502.16280",
         "arXiv.org",
         "0",
         "2025",
         "Large language models fail to reliably simulate human survey responses across demographics and question types.",
         "- Technical analysis of demographic attributes and prompt variations on latent opinion mappings in LLMs.\r\n- Probe-based methodology to reveal political affiliations in LLMs' latent space.\r\n- Training linear probes to predict parties based on residual streams.\r\n- Extracting value vectors from intermediate LLM layers.\r\n- Calculating normalized entropy for comparison with historical human preferences.\r\n- Using Wasserstein distance to assess prompt sensitivity.\r\n- Regression analysis of entropy and prompt sensitivity relationship.",
         "  - \"Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions.\"\r\n  - \"Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups.\"\r\n  - \"we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models.\"\r\n  - \"Understanding how LLMs encode and generate synthetic survey responses necessitates to investigate persona-to-party mappings within the models' latent space.\"\r\n  - \"Building upon prior research, our methodology integrates trained probes to systematically identify model-specific representations of political ascriptions, thereby offering insights into the underlying value vectors.\"\r\n  - \"We aim to extract value vectors from the intermediate layers of LLMs, as these layers capture conceptual structures and high-level semantic representations more effectively than final layers, which are predominantly specialized for next-token prediction\"\r\n  - \"To achieve this, we train linear probes that predict the party on the basis of the residual stream xl of layer l.\"\r\n  - \"The probes are trained as binary classifiers, distinguishing between residual streams corresponding to a specific party n (y = 1) and all others (y = 0).\"\r\n  - \"We analyze how LLMs model persona-to-party mappings and compare their voting distributions to real-world election data.\"\r\n  - \"To compare the characteristics of the LLM persona-to-party mapping with historical human preferences, we calculate the normalized entropy H norm () for the distribution  aggregated over all personas p  P = {p 1 , p 2 , . . . } and prompt variants j  J = {j 1 , j 2\"\r\n  - \"To assess how entropy varies across different prompts j, we examine the relationship between entropy and prompt sensitivity using the Wasserstein distance W ( j,g , j,g\"\r\n  - \"We repeat our analyses by asking the model to select a party for a specific persona given the election was tomorrow.\"\r\n  - \"To analyze prompt sensitivity, we regress the entropy of persona-to-party mappings on the Wasserstein Distance as a proxy for prompt instability.\"",
         "  - \"(Page 3, Table 1) | Family   | Size | Model                          | Reference            |\r\\n|----------|------|--------------------------------|----------------------|\r\\n| Llama 3.2| 3B   | Llama-3.2-3B-Instruct          | MetaAI [2024a]       |\"\r\n  - \"(Page 13, Table 1) | Parameter       | Values                                                                                                                                                                                                 |\r\\n|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\\n| models          | meta-llama/Llama-3.1-8B-Instruct, meta-llama/Llama-3.1-8B, meta-llama/Meta-Llama-3-8B-Instruct, meta-llama/Meta-Llama-3-8B, meta-llama/Llama-3.2-3B-Instruct, meta-llama/Llama-3.2-3B,                 |\"",
         "- The study uses a technical analysis to evaluate how demographic attributes and prompt variations affect latent opinion mappings in LLMs.\r\n- A probe-based methodology is adapted to reveal how LLMs encode political affiliations in their latent space.\r\n- The methodology involves training linear probes to predict parties based on residual streams in LLMs.\r\n- The study extracts value vectors from intermediate layers of LLMs to understand conceptual structures and semantic representations.\r\n- Normalized entropy is calculated to compare LLM persona-to-party mappings with historical human preferences.\r\n- Wasserstein distance is used to assess prompt sensitivity and its impact on voting distributions.\r\n- The study uses regression to analyze the relationship between entropy and prompt sensitivity."
        ],
        [
         "39",
         "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
         "Nuo Chen, Yang Deng, Jia Li",
         "10.48550/arXiv.2407.11484",
         "https://doi.org/10.48550/arXiv.2407.11484",
         "arXiv.org",
         "23",
         "2024",
         "This paper surveys the use of large language models for role-playing and character simulation, but does not mention their application to simulating human survey responses.",
         "- Comprehensive review of current methodologies and challenges\r\n- Taxonomy: Data, Models & Alignment, Agent Architecture, and Evaluation\r\n- Categorization of role-playing applications: Persona-based Role-Playing (P-RP) and Character-based Role-Playing (C-RP)\r\n- Data collection methods: Employing Crowdsourced Workers and Extracting from Social Media\r\n- Role-related information: Explicit and Implicit forms\r\n- Foundation model development: Non-pretrained model, PLM, and LLM\r\n- Alignment methodologies: Parameter-tuning and Parameter-frozen approaches\r\n- Evaluation methods: Reference-based, Human-based, and LLM-based evaluation",
         "  - \"This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications.\"\r\n  - \"The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement.\"\r\n  - \"The proposed taxonomy includes: Data, Models & Alignment, Agent Architecture, and Evaluation.\"\r\n  - \"In the following sections, we present a comprehensive survey along with our taxonomy.\"\r\n  - \"In this study, based on the different objectives of the targeted datasets, we categorize roleplaying applications into two types: Personabased Role-Playing (P-RP) and Character-based Role-Playing (C-RP).\"\r\n  - \"In general, the datasets associated with personabased data tend to provide personas that are coarsegrained.\"\r\n  - \"Role-related Information. Role-related information in P-RP datasets is crucial for generating realistic and context-appropriate responses, and it can be categorized into two distinct forms: explicit and implicit.\"\r\n  - \"Foundation models are critical in setting the base capabilities of role-playing models.\"\r\n  - \"The development of foundation models and architectures for role-playing can be viewed as a progressive evolution across three distinct stages: non-pretrained model, PLM, and LLM.\"\r\n  - \"Role-playing hinges on the precise alignment of language models with distinctive character-related information.\"\r\n  - \"Current methodologies for aligning language models with different roles fall into two broad categories: parameter-tuning alignment and parameter-frozen alignment.\"\r\n  - \"Evaluating role-playing models is essential to ensure their effectiveness and realism in simulated environments.\"",
         "  - \"(Page 6, Table 1) Below is a rendering of the page up to the first error.\"",
         "- The paper is a survey that outlines current methodologies and challenges in role-playing with language models, indicating that the methodology involves a comprehensive review of existing practices.\r\n- The taxonomy proposed includes four main components: Data, Models & Alignment, Agent Architecture, and Evaluation, which are central to the methodology.\r\n- The study categorizes role-playing applications into two types: Persona-based Role-Playing (P-RP) and Character-based Role-Playing (C-RP), which is part of the methodology.\r\n- Data collection methods are classified into two streams: employing crowdsourced workers and extracting from social media, which are key methodologies for data collection.\r\n- The paper discusses the importance of role-related information, which is categorized into explicit and implicit forms, indicating a focus on data quality and relevance.\r\n- The development of foundation models is described as a progressive evolution from non-pretrained models to LLMs, highlighting the methodology of model development.\r\n- Alignment methodologies are categorized into parameter-tuning and parameter-frozen approaches, which are critical for role-playing.\r\n- Evaluation methods are categorized into reference-based, human-based, and LLM-based evaluation, which are part of the methodology for assessing role-playing models."
        ],
        [
         "40",
         "Mixture-of-Personas Language Models for Population Simulation",
         "Ngoc Bui, Hieu Trung Nguyen, Shantanu Kumar, Julian Theodore, Weikang Qiu, Viet Anh Nguyen, Rex Ying",
         "10.48550/arXiv.2504.05019",
         "https://doi.org/10.48550/arXiv.2504.05019",
         "Annual Meeting of the Association for Computational Linguistics",
         "7",
         "2025",
         "Mixture-of-Personas language models can be used to simulate diverse human survey responses across populations.",
         "- Mixture of Personas (MoP) is a probabilistic prompting method.\r\n- MoP is a contextual mixture model with LM agents characterized by personas and exemplars.\r\n- Personas and exemplars are randomly chosen based on learned mixing weights.\r\n- MoP is flexible, requires no model finetuning, and is transferable across base models.\r\n- Experiments involve synthetic data generation to test alignment and diversity metrics.",
         "  - \"we propose \\textit{Mixture of Personas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM responses with the target population.\"\r\n  - \"MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors.\"\r\n  - \"MoP is flexible, requires no model finetuning, and is transferable across base models.\"\r\n  - \"Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.\"\r\n  - \"The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation.\"",
         null,
         "- The abstract describes the methodology as involving a \"Mixture of Personas\" (MoP), which is a probabilistic prompting method. This indicates that the method involves using personas to guide the language model's responses.\r\n- MoP is characterized as a contextual mixture model, which suggests that it combines different components to represent various subpopulation behaviors.\r\n- The method involves randomly selecting personas and exemplars based on learned mixing weights, which is a key part of how MoP generates diverse responses.\r\n- The abstract also mentions that MoP does not require model finetuning and is transferable across different base models, highlighting its flexibility and adaptability.\r\n- The methodology is tested through experiments for synthetic data generation, where MoP is compared to other methods in terms of alignment and diversity metrics."
        ],
        [
         "41",
         "Susceptibility to Influence of Large Language Models",
         "L. D. Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T. Mai, Maria Vau, M. Caldwell, Augustine Marvor-Parker",
         "10.48550/arXiv.2303.06074",
         "https://doi.org/10.48550/arXiv.2303.06074",
         "arXiv.org",
         "14",
         "2023",
         "Large language models can be used to model psychological changes following exposure to influential input, but may not fully capture all effects seen in human data.",
         "- Two studies were conducted: one on the Illusory Truth Effect (ITE) and another on Populist Framing of News (PFN).\r\n- Data was collected from 1000 human participants and 1000 simulated participants using Large Language Models (LLMs).\r\n- The ITE study involved collecting 64 ratings per participant on attributes like truth, interest, sentiment, and importance.\r\n- A balanced design using random Latin Squares was used to ensure equal testing of all attribute combinations.\r\n- Human participants were recruited through the Prolific platform with specific demographic criteria.\r\n- The PFN study adapted the experiment for GPT-3 participants by simulating demographic information and using GPT-3 completion for ratings.",
         "  - \"Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input.\"\r\n  - \"Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion.\"\r\n  - \"64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance.\"\r\n  - \"We have devised an experiment suitable for human and GPT-3 participants, allowing a direct comparison of results.\"\r\n  - \"We first compare the ratings given by GPT-3 and humans to unexposed statements.\"\r\n  - \"To adapt this for GPT-3 participants we simulate steps 1-3, providing answers generated from Bos et al.'s summary statistics of their respondents' demographics, and then use GPT-3 completion for step 4 to generate ratings for the probe given the earlier responses\"\r\n  - \"We intended to collect data for 7286 GPT-3 simulated participants, matching the size of the Bos et al.\n\nstudy, but due to other usage hit our monthly cap for GPT-3 queries after 2153 participants.\"",
         null,
         "- The study involved two main experiments: one testing the Illusory Truth Effect (ITE) and another testing Populist Framing of News (PFN).\r\n- For the ITE study, data was collected from both human participants and simulated participants using Large Language Models (LLMs).\r\n- The experiment involved collecting ratings from participants on various attributes such as truth, interest, sentiment, and importance.\r\n- The study used a balanced design with random Latin Squares to ensure that all combinations of attributes were tested equally.\r\n- Human participants were recruited through the Prolific platform with specific demographic criteria.\r\n- For the PFN study, the experiment was adapted for GPT-3 participants by simulating demographic information and using GPT-3 completion to generate ratings.\r\n- The study aimed to compare the responses of human and GPT-3 participants to assess whether LLMs can model psychological change."
        ],
        [
         "42",
         "A Survey on Human-Centric LLMs",
         "Jing Yi Wang, Nicholas Sukiennik, Tong Li, Weikang Su, Qianyue Hao, Jingbo Xu, Zihan Huang, Fengli Xu, Yong Li",
         "10.48550/arXiv.2411.14491",
         "https://doi.org/10.48550/arXiv.2411.14491",
         "arXiv.org",
         "9",
         "2024",
         "The paper surveys how large language models are applied to simulate human cognition, behavior, and social interaction across various domains.",
         "The methodology involves evaluating LLMs in individual and collective tasks, exploring real-world applications in domains like behavioral science, political science, and sociology, and identifying challenges and future directions. The methods used include basic prompting, multi-agent prompting, fine-tuning, and human-in-the-loop approaches.",
         "  - \"This survey provides a comprehensive examination of such human-centric LLM capabilities, focusing on their performance in both individual tasks (where an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics).\"\r\n  - \"We first evaluate LLM competencies across key areas including reasoning, perception, and social cognition, comparing their abilities to human-like skills.\"\r\n  - \"Then, we explore real-world applications of LLMs in human-centric domains such as behavioral science, political science, and sociology, assessing their effectiveness in replicating human behaviors and interactions.\"\r\n  - \"Finally, we identify challenges and future research directions, such as improving LLM adaptability, emotional intelligence, and cultural sensitivity, while addressing inherent biases and enhancing frameworks for human-AI collaboration.\"\r\n  - \"the methodologies used in these studies can be categorized into three main approaches and are laid out in Figure 5: (1) basic prompting, which involves sequentially querying a model to generate responses that build a broader understanding; (2) multiagent prompting, where several LLMs interact autonomously based on predefined rules and theories;\n\n(3) fine-tuning, which involves retraining models with additional data to improve their performance in specific domains; and (4) human-in-the-loop, which involves a real person who interacts with the agents by inputting a stimulus into the system.\"",
         "  - \"(Page 37, Table 1) | Field              | Paper | Year | LLM(s) Used                      | Method                              | Problem Addressed                                                                 | Human Phenomenon/Theory          | Research Goal         |\r\\n|--------------------|-------|------|----------------------------------|-------------------------------------|-----------------------------------------------------------------------------------|----------------------------------|-----------------------|\r\\n| Behavioral Science | [128] | 2024 | LLaMA-2                          | Multi-Agent Prompting Human-in-the-Loop | Incorporates LLM into a collaborative work scenario                                | Collaborative ideation           | Capability Discovery  |\"",
         "- The paper is a survey, which means it reviews and synthesizes existing research rather than conducting new experiments. Therefore, the methodology involves evaluating existing studies and their methods.\r\n- The paper focuses on evaluating LLMs in individual and collective tasks, which suggests that the methodology includes assessing these different types of tasks.\r\n- The paper explores real-world applications in various domains, indicating that the methodology involves examining how LLMs are used in these areas.\r\n- The paper identifies challenges and future directions, which implies that the methodology includes analyzing current limitations and potential future research paths.\r\n- The methodologies mentioned include basic prompting, multi-agent prompting, fine-tuning, and human-in-the-loop approaches, which are used to evaluate LLMs in different contexts.\r\n- The tables at the end of the paper likely provide more detailed information about the specific studies and methods used in each domain."
        ],
        [
         "43",
         "Investigating Cultural Alignment of Large Language Models",
         "Badr AlKhamissi, Muhammad N. ElNokrashy, Mai AlKhamissi, Mona Diab",
         "10.48550/arXiv.2402.13231",
         "https://doi.org/10.48550/arXiv.2402.13231",
         "Annual Meeting of the Association for Computational Linguistics",
         "68",
         "2024",
         "Large language models demonstrate greater cultural alignment when prompted with dominant languages and refined language mixtures, but misalignment increases for underrepresented personas and culturally sensitive topics.",
         "- Simulated sociological surveys to measure cultural alignment of LLMs.\r\n- Used English and Arabic as primary languages for prompting.\r\n- Employed four pretrained LLMs with different pretraining language compositions.\r\n- Prompted models with specific personas and questions, comparing responses with actual survey participants.\r\n- Used Hard and Soft metrics to assess cultural alignment at the persona level.\r\n- Introduced Anthropological Prompting to enhance cultural alignment using anthropological reasoning.",
         "  - \"Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture.\"\r\n  - \"We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references.\"\r\n  - \"We focus on a survey conducted in two countries: Egypt (EG) and the United States of America (US).\"\r\n  - \"We consider two languages for prompting: English and Arabic as they are the primary languages used in the surveys.\"\r\n  - \"The survey simulations involve prompting each model with a specific persona, followed by an instruction and a question\"\r\n  - \"we assess a model's cultural alignment by comparing its responses for each persona separately with the original subject's response in one of the two surveys.\"\r\n  - \"we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment.\"\r\n  - \"We use this similarity as a proxy for the degree of a model's knowledge of a particular culture.\"\r\n  - \"The metrics we use compare responses on the persona-level allowing us to analyze the model's alignment with respect to several attributes such as social class and education level.\"\r\n  - \"we prompt each model with the languages native to the countries under study and thereby studying the significance of language on cultural alignment with implications to cross-lingual transfer research.\"\r\n  - \"we introduce Anthropological Prompting, a novel method that utilizes a framework adopted from the toolkit of anthropological methods to guide the model to reason about the persona before answering for improving cultural alignment.\"",
         null,
         "- The study uses a methodology that involves simulating sociological surveys to measure cultural alignment of Large Language Models (LLMs).\r\n- The researchers use two primary languages for prompting: English and Arabic, which are dominant in the surveyed countries (Egypt and the US).\r\n- Four pretrained LLMs are used in the study, each with different pretraining language compositions.\r\n- The study involves prompting each model with specific personas and questions, and then comparing the model responses with actual survey participant responses.\r\n- The researchers use two metrics (Hard and Soft) to assess cultural alignment, which involves comparing model responses with survey responses at the persona level.\r\n- A novel method called Anthropological Prompting is introduced to enhance cultural alignment by guiding the model to reason about the persona using anthropological frameworks."
        ],
        [
         "44",
         "Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms",
         "Petter Trnberg, D. Valeeva, J. Uitermark, Christopher Bail",
         "10.48550/arXiv.2310.05984",
         "https://doi.org/10.48550/arXiv.2310.05984",
         "arXiv.org",
         "45",
         "2023",
         "The paper simulates social media using large language models and agent-based modeling to evaluate how different news feed algorithms shape the quality of online conversations.",
         "- Created realistic personas using American National Election Study data.\r\n- Simulated three social media platforms with different news feed algorithms.\r\n- Agents read, shared, liked, and commented on news articles based on personas.\r\n- Used text analysis tools and behavioral measures to assess toxicity and cross-party dialogue.\r\n- Calibrated to US electorate using ANES data.\r\n- Automated text analysis tools like Perspective API were used.",
         "  - \"We create realistic personas using data from the American National Election Study to populate simulated social media platforms.\"\r\n  - \"Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms.\"\r\n  - \"We create three synthetic social media environments designed to emulate text-based platforms such as Twitter, Threads, Mastodon, or Bluesky for approximately one day during July 2020.\"\r\n  - \"To calibrate the model to the US electorate, we construct a persona for each agent using data from the 2020 American National Election Study.\"\r\n  - \"We use a combination of well-established text analysis tools and behavioral measures to assess the toxicity and amount of cross-party dialogue within each synthetic environment\"\r\n  - \"We simulate behavior on each of these three platforms as follows. In the first step, a sample of agents is selected to produce posts, based upon their self-reported frequency of social media use according to the ANES data using an Urn model\"\r\n  - \"The LLMs are shown stories from the news sources that they consume according to their ANES persona.\"\r\n  - \"The agents can then \"like\" or comment on the posts according to the interests and preferences of their persona.\"\r\n  - \"we use Perspective API, a popular automated text analysis tool to estimate the toxicity of discourse within the three simulated social media environments described above.\"",
         "  - \"(Page 11, Table 1) | Category               | Measure                        | Value     |\r\\n|------------------------|--------------------------------|-----------|\r\\n| Age                    | Average Age                    | 38.14 years |\"",
         "- The study uses data from the American National Election Study to create realistic personas for agents in simulated social media platforms.\r\n- Three different news feed algorithms are tested: one showing most liked and commented posts from followed users, one showing all high-engagement posts, and a \"bridging\" algorithm highlighting posts liked across partisan lines.\r\n- The simulation involves agents reading, sharing, liking, and commenting on news articles based on their personas.\r\n- The study uses text analysis tools and behavioral measures to assess toxicity and cross-party dialogue.\r\n- The simulation is calibrated to the US electorate using ANES data, and agents' behavior is based on their self-reported social media use frequency.\r\n- Automated text analysis tools like Perspective API are used to estimate toxicity and interpartisan interaction."
        ],
        [
         "45",
         "AutoSurvey: Large Language Models Can Automatically Write Surveys",
         "Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang",
         "10.48550/arXiv.2406.10252",
         "https://doi.org/10.48550/arXiv.2406.10252",
         "Neural Information Processing Systems",
         "35",
         "2024",
         "This paper is not relevant to the query about using large language models to simulate human survey responses across question types and populations.",
         "- Initial Retrieval and Outline Generation: Uses an embedding-based retrieval technique to identify relevant papers and generate an outline.\r\n- Subsection Drafting: Specialized LLMs draft each section of the outline in parallel.\r\n- Integration and Refinement: Refines each section for readability and coherence.\r\n- Rigorous Evaluation and Iteration: Evaluates the survey using a Multi-LLM-as-Judge strategy.",
         "  - \"AutoSurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration.\"\r\n  - \"Our approach systematically progresses through four distinct phases-Initial Retrieval and Outline Generation, Subsection Drafting, Integration and Refinement, and Rigorous Evaluation and Iteration.\"\r\n  - \"The process begins with the Initial Retrieval and Outline Generation phase. Utilizing an embedding-based retrieval technique, AutoSurvey scans a database of publications to identify papers most pertinent to the specified survey topic T\"\r\n  - \"Subsection Drafting With the structured outline in place, the Subsection Drafting phase commences. During this phase, specialized LLMs draft each section of the outline in parallel.\"\r\n  - \"Integration and Refinement Following the drafting phase, each section S i is individually refined to enhance readability, eliminate redundancies, and ensure a seamless narrative.\"\r\n  - \"The final phase involves a rigorous evaluation and iteration process, where the survey document is assessed through a Multi-LLM-as-Judge strategy.\"\r\n  - \"The methodology outlined here-from initial data retrieval to sophisticated multi-faceted evaluation-ensures that AutoSurvey effectively addresses the complexities of survey creation in evolving research fields using advanced LLM technologies.\"",
         null,
         "- The paper describes a systematic approach to automating the creation of comprehensive literature surveys using AutoSurvey.\r\n- The methodology involves four phases: Initial Retrieval and Outline Generation, Subsection Drafting, Integration and Refinement, and Rigorous Evaluation and Iteration.\r\n- The first phase uses an embedding-based retrieval technique to identify relevant papers and generate an outline.\r\n- The second phase involves parallel drafting of subsections by specialized LLMs.\r\n- The third phase refines each section for readability and coherence.\r\n- The fourth phase evaluates the survey using a Multi-LLM-as-Judge strategy.\r\n- The methodology is designed to address challenges in survey creation, such as context window limitations and parametric knowledge constraints."
        ],
        [
         "46",
         "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
         "Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, Jang Hyun Kim",
         "10.48550/arXiv.2402.18144",
         "https://doi.org/10.48550/arXiv.2402.18144",
         "arXiv.org",
         "24",
         "2024",
         "Large language models can simulate human survey responses across demographic groups by leveraging group-level demographic information.",
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations",
         "Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Rottger, Daniel Hershcovich",
         "10.48550/arXiv.2502.07068",
         "https://doi.org/10.48550/arXiv.2502.07068",
         "North American Chapter of the Association for Computational Linguistics",
         "6",
         "2025",
         "Large language models can be specialized to simulate survey response distributions for global populations, though accuracy is still limited.",
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "Uncertainty Quantification for LLM-Based Survey Simulations",
         "Chengpiao Huang, Yuhang Wu, Kaizheng Wang",
         "10.48550/arXiv.2502.17773",
         "https://doi.org/10.48550/arXiv.2502.17773",
         "arXiv.org",
         "1",
         "2025",
         "The paper presents a method to use large language models to simulate human survey responses and quantify the uncertainty in the simulations.",
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "Questioning the Survey Responses of Large Language Models",
         "Ricardo Dominguez-Olmedo, Moritz Hardt, Celestine Mendler-Dunner",
         "10.48550/arXiv.2306.07951",
         "https://doi.org/10.48550/arXiv.2306.07951",
         "Neural Information Processing Systems",
         "38",
         "2023",
         "Large language models' survey responses do not faithfully represent any human population due to biases and lack of statistical signals found in human populations.",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 191
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>DOI</th>\n",
       "      <th>DOI link</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Citation count</th>\n",
       "      <th>Year</th>\n",
       "      <th>Abstract summary</th>\n",
       "      <th>Methodology</th>\n",
       "      <th>Supporting quotes for \"Methodology\"</th>\n",
       "      <th>Supporting tables for \"Methodology\"</th>\n",
       "      <th>Reasoning for \"Methodology\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Specializing Large Language Models to Simulate...</td>\n",
       "      <td>Yong Cao, Haijiang Liu, Arnav Arora, Isabelle ...</td>\n",
       "      <td>10.48550/arXiv.2502.07068</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2502.07068</td>\n",
       "      <td>North American Chapter of the Association for ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>Large language models can be specialized to si...</td>\n",
       "      <td>- Specializing LLMs for survey simulation usin...</td>\n",
       "      <td>- \"In this study, our goal is instead to spe...</td>\n",
       "      <td>- \"(Page 3, Table 1) | **Instruction** | How...</td>\n",
       "      <td>- The study aims to specialize large language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Silicon Sampling: Simulating Human Sub-...</td>\n",
       "      <td>Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangyi...</td>\n",
       "      <td>10.48550/arXiv.2402.18144</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2402.18144</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>24</td>\n",
       "      <td>2024</td>\n",
       "      <td>Large language models can simulate human surve...</td>\n",
       "      <td>- Created a \"random silicon sample\" of synthet...</td>\n",
       "      <td>- \"Our study analyzed 1) a language model th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- The study uses a method called \"random silic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uncertainty Quantification for LLM-Based Surve...</td>\n",
       "      <td>Chengpiao Huang, Yuhang Wu, Kaizheng Wang</td>\n",
       "      <td>10.48550/arXiv.2502.17773</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2502.17773</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>The paper presents a method to use large langu...</td>\n",
       "      <td>- Use LLMs to simulate human responses to surv...</td>\n",
       "      <td>- \"Ideally, we would like to pick k such tha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- The methodology involves using large languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Language Models as Subpopulation Represe...</td>\n",
       "      <td>Gabriel Simmons, Christopher Hare</td>\n",
       "      <td>10.48550/arXiv.2310.17888</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2310.17888</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>20</td>\n",
       "      <td>2023</td>\n",
       "      <td>Large language models can be used to simulate ...</td>\n",
       "      <td>The methodology involves using Large Language ...</td>\n",
       "      <td>- \"This review draws together a body of lite...</td>\n",
       "      <td>- \"(Page 10, Table 1) | Method              ...</td>\n",
       "      <td>- The paper is a review of existing literature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Language Model Fine-Tuning on Scaled Survey Da...</td>\n",
       "      <td>Joseph Suh, Erfan Jahanparast, Suhong Moon, Mi...</td>\n",
       "      <td>10.48550/arXiv.2502.16761</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2502.16761</td>\n",
       "      <td>Annual Meeting of the Association for Computat...</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fine-tuning large language models on scaled su...</td>\n",
       "      <td>- Fine-tuning large language models (LLMs) on ...</td>\n",
       "      <td>- \"Here, we propose directly finetuning LLMs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- The study involves fine-tuning large languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Exposing Bias in Online Communities through La...</td>\n",
       "      <td>Celine Wald, Lukas Pfahler</td>\n",
       "      <td>10.48550/arXiv.2306.02294</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2306.02294</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>6</td>\n",
       "      <td>2023</td>\n",
       "      <td>Large language models trained on online commun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Style Over Substance: Evaluation Biases for La...</td>\n",
       "      <td>Minghao Wu, Alham Fikri Aji</td>\n",
       "      <td>10.48550/arXiv.2307.03025</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2307.03025</td>\n",
       "      <td>International Conference on Computational Ling...</td>\n",
       "      <td>52</td>\n",
       "      <td>2023</td>\n",
       "      <td>Crowd-sourced and expert evaluations of large ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Tackling Bias in Pre-trained Language Models: ...</td>\n",
       "      <td>Vithya Yogarajan, Gillian Dobbie, Te Taka Keeg...</td>\n",
       "      <td>10.48550/arXiv.2312.01509</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2312.01509</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>14</td>\n",
       "      <td>2023</td>\n",
       "      <td>The paper discusses the need to address biases...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Challenging the appearance of machine intellig...</td>\n",
       "      <td>Alaina N. Talboy, Elizabeth Fuller</td>\n",
       "      <td>10.48550/arXiv.2304.01358</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2304.01358</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>27</td>\n",
       "      <td>2023</td>\n",
       "      <td>Large language models exhibit cognitive biases...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Take Caution in Using LLMs as Human Surrogates...</td>\n",
       "      <td>Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Faze...</td>\n",
       "      <td>10.48550/arXiv.2410.19599</td>\n",
       "      <td>https://doi.org/10.48550/arXiv.2410.19599</td>\n",
       "      <td>Proceedings of the National Academy of Science...</td>\n",
       "      <td>16</td>\n",
       "      <td>2024</td>\n",
       "      <td>Large language models can exhibit biased or un...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Specializing Large Language Models to Simulate...   \n",
       "1    Random Silicon Sampling: Simulating Human Sub-...   \n",
       "2    Uncertainty Quantification for LLM-Based Surve...   \n",
       "3    Large Language Models as Subpopulation Represe...   \n",
       "4    Language Model Fine-Tuning on Scaled Survey Da...   \n",
       "..                                                 ...   \n",
       "186  Exposing Bias in Online Communities through La...   \n",
       "187  Style Over Substance: Evaluation Biases for La...   \n",
       "188  Tackling Bias in Pre-trained Language Models: ...   \n",
       "189  Challenging the appearance of machine intellig...   \n",
       "190  Take Caution in Using LLMs as Human Surrogates...   \n",
       "\n",
       "                                               Authors  \\\n",
       "0    Yong Cao, Haijiang Liu, Arnav Arora, Isabelle ...   \n",
       "1    Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangyi...   \n",
       "2            Chengpiao Huang, Yuhang Wu, Kaizheng Wang   \n",
       "3                    Gabriel Simmons, Christopher Hare   \n",
       "4    Joseph Suh, Erfan Jahanparast, Suhong Moon, Mi...   \n",
       "..                                                 ...   \n",
       "186                         Celine Wald, Lukas Pfahler   \n",
       "187                        Minghao Wu, Alham Fikri Aji   \n",
       "188  Vithya Yogarajan, Gillian Dobbie, Te Taka Keeg...   \n",
       "189                 Alaina N. Talboy, Elizabeth Fuller   \n",
       "190  Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Faze...   \n",
       "\n",
       "                           DOI                                   DOI link  \\\n",
       "0    10.48550/arXiv.2502.07068  https://doi.org/10.48550/arXiv.2502.07068   \n",
       "1    10.48550/arXiv.2402.18144  https://doi.org/10.48550/arXiv.2402.18144   \n",
       "2    10.48550/arXiv.2502.17773  https://doi.org/10.48550/arXiv.2502.17773   \n",
       "3    10.48550/arXiv.2310.17888  https://doi.org/10.48550/arXiv.2310.17888   \n",
       "4    10.48550/arXiv.2502.16761  https://doi.org/10.48550/arXiv.2502.16761   \n",
       "..                         ...                                        ...   \n",
       "186  10.48550/arXiv.2306.02294  https://doi.org/10.48550/arXiv.2306.02294   \n",
       "187  10.48550/arXiv.2307.03025  https://doi.org/10.48550/arXiv.2307.03025   \n",
       "188  10.48550/arXiv.2312.01509  https://doi.org/10.48550/arXiv.2312.01509   \n",
       "189  10.48550/arXiv.2304.01358  https://doi.org/10.48550/arXiv.2304.01358   \n",
       "190  10.48550/arXiv.2410.19599  https://doi.org/10.48550/arXiv.2410.19599   \n",
       "\n",
       "                                                 Venue  Citation count  Year  \\\n",
       "0    North American Chapter of the Association for ...               6  2025   \n",
       "1                                            arXiv.org              24  2024   \n",
       "2                                            arXiv.org               1  2025   \n",
       "3                                            arXiv.org              20  2023   \n",
       "4    Annual Meeting of the Association for Computat...               6  2025   \n",
       "..                                                 ...             ...   ...   \n",
       "186                                          arXiv.org               6  2023   \n",
       "187  International Conference on Computational Ling...              52  2023   \n",
       "188                                          arXiv.org              14  2023   \n",
       "189                                          arXiv.org              27  2023   \n",
       "190  Proceedings of the National Academy of Science...              16  2024   \n",
       "\n",
       "                                      Abstract summary  \\\n",
       "0    Large language models can be specialized to si...   \n",
       "1    Large language models can simulate human surve...   \n",
       "2    The paper presents a method to use large langu...   \n",
       "3    Large language models can be used to simulate ...   \n",
       "4    Fine-tuning large language models on scaled su...   \n",
       "..                                                 ...   \n",
       "186  Large language models trained on online commun...   \n",
       "187  Crowd-sourced and expert evaluations of large ...   \n",
       "188  The paper discusses the need to address biases...   \n",
       "189  Large language models exhibit cognitive biases...   \n",
       "190  Large language models can exhibit biased or un...   \n",
       "\n",
       "                                           Methodology  \\\n",
       "0    - Specializing LLMs for survey simulation usin...   \n",
       "1    - Created a \"random silicon sample\" of synthet...   \n",
       "2    - Use LLMs to simulate human responses to surv...   \n",
       "3    The methodology involves using Large Language ...   \n",
       "4    - Fine-tuning large language models (LLMs) on ...   \n",
       "..                                                 ...   \n",
       "186                                                NaN   \n",
       "187                                                NaN   \n",
       "188                                                NaN   \n",
       "189                                                NaN   \n",
       "190                                                NaN   \n",
       "\n",
       "                   Supporting quotes for \"Methodology\"  \\\n",
       "0      - \"In this study, our goal is instead to spe...   \n",
       "1      - \"Our study analyzed 1) a language model th...   \n",
       "2      - \"Ideally, we would like to pick k such tha...   \n",
       "3      - \"This review draws together a body of lite...   \n",
       "4      - \"Here, we propose directly finetuning LLMs...   \n",
       "..                                                 ...   \n",
       "186                                                NaN   \n",
       "187                                                NaN   \n",
       "188                                                NaN   \n",
       "189                                                NaN   \n",
       "190                                                NaN   \n",
       "\n",
       "                   Supporting tables for \"Methodology\"  \\\n",
       "0      - \"(Page 3, Table 1) | **Instruction** | How...   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3      - \"(Page 10, Table 1) | Method              ...   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "186                                                NaN   \n",
       "187                                                NaN   \n",
       "188                                                NaN   \n",
       "189                                                NaN   \n",
       "190                                                NaN   \n",
       "\n",
       "                           Reasoning for \"Methodology\"  \n",
       "0    - The study aims to specialize large language ...  \n",
       "1    - The study uses a method called \"random silic...  \n",
       "2    - The methodology involves using large languag...  \n",
       "3    - The paper is a review of existing literature...  \n",
       "4    - The study involves fine-tuning large languag...  \n",
       "..                                                 ...  \n",
       "186                                                NaN  \n",
       "187                                                NaN  \n",
       "188                                                NaN  \n",
       "189                                                NaN  \n",
       "190                                                NaN  \n",
       "\n",
       "[191 rows x 12 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Elicit_filtered_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "28dc0f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Elicit DOIs present in the final combined filtered dataset: 22.43%\n"
     ]
    }
   ],
   "source": [
    "# Find the percentage of df_Elicit DOI that are present in df_combined_filtered doi\n",
    "elicit_doi_set = set(df_Elicit_filtered['DOI'].dropna().apply(normalize_doi).tolist())\n",
    "combined_doi_set = set(df_combined_filtered['doi'].dropna().apply(normalize_doi).tolist())\n",
    "common_dois = elicit_doi_set.intersection(combined_doi_set)\n",
    "percentage_doi_in_combined = (len(common_dois) / len(elicit_doi_set)) * 100 if elicit_doi_set else 0.0\n",
    "print(f\"Percentage of Elicit DOIs present in the final combined filtered dataset: {percentage_doi_in_combined:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
